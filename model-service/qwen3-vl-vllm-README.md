# Qwen3-VL vLLM æœåŠ¡

Qwen3-VL-8B-Instruct è§†è§‰è¯­è¨€æ¨¡å‹çš„ vLLM éƒ¨ç½²æœåŠ¡ã€‚

## ğŸ“Š æœåŠ¡é…ç½®

- **æ¨¡å‹**: Qwen3-VL-8B-Instruct
- **GPU**: 2x NVIDIA RTX 4090 D (å¼ é‡å¹¶è¡Œ)
- **æ˜¾å­˜**: ~48GB æ€»è®¡
- **ä¸Šä¸‹æ–‡é•¿åº¦**: 8192 tokens
- **API ç«¯ç‚¹**: `http://localhost:8000/v1`
- **æ¨¡å‹åç§°**: `qwen-vl-max`

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æäº¤ Slurm ä½œä¸š

```bash
cd /remote-home1/bwli/get_open_review/model-service
sbatch qwen-vl-sbatch.sh
```

### æŸ¥çœ‹ä½œä¸šçŠ¶æ€

```bash
squeue -u $USER
```

### æŸ¥çœ‹æ—¥å¿—

```bash
tail -f qwen-vl-vllm-slurm.log
```

### æ›´å¤šæ“ä½œ

```bash
# æŸ¥çœ‹ä½œä¸šè¯¦ç»†ä¿¡æ¯
scontrol show job <JOB_ID>

# æŸ¥çœ‹ç‰¹å®šç”¨æˆ·çš„æ‰€æœ‰ä½œä¸š
squeue -u $USER

# æŸ¥çœ‹æ—¥å¿—æœ€å 100 è¡Œ
tail -n 100 qwen-vl-vllm-slurm.log
```

### æµ‹è¯• API

```bash
# æµ‹è¯•æœåŠ¡æ˜¯å¦å¯ç”¨
curl http://localhost:8000/v1/models

# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ
nvidia-smi
```

## âš™ï¸ é…ç½®å‚æ•°

ç¼–è¾‘ `qwen-vl-sbatch.sh` ä¿®æ”¹ä»¥ä¸‹å‚æ•°ï¼š

### Slurm èµ„æºé…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--partition` | `fnlp-4090d` | åˆ†åŒºåç§° |
| `--nodelist` | `fnlp-4090-59108` | æŒ‡å®šèŠ‚ç‚¹ |
| `--gres=gpu` | `2` | GPU æ•°é‡ |
| `--cpus-per-task` | `12` | CPU æ ¸å¿ƒæ•° |
| `--mem-per-cpu` | `4G` | æ¯æ ¸å¿ƒå†…å­˜ï¼ˆæ€»è®¡ 48GBï¼‰ |

### æ¨¡å‹æœåŠ¡é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `MODEL_PATH` | `/remote-home1/share/models/Qwen/Qwen3-VL-8B-Instruct` | æ¨¡å‹è·¯å¾„ |
| `PORT` | `8000` | API æœåŠ¡ç«¯å£ |
| `HOST` | `0.0.0.0` | ç›‘å¬åœ°å€ï¼ˆ0.0.0.0 å…è®¸å¤–éƒ¨è®¿é—®ï¼‰ |
| `TENSOR_PARALLEL_SIZE` | `2` | GPU æ•°é‡ï¼ˆ1=å•å¡ï¼Œ2=åŒå¡å¹¶è¡Œï¼‰ |
| `--max-model-len` | `8192` | æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ |
| `--gpu-memory-utilization` | `0.9` | GPU æ˜¾å­˜ä½¿ç”¨ç‡ |

## ğŸ”§ å¸¸è§é—®é¢˜

### ç«¯å£è¢«å ç”¨

```bash
# æŸ¥çœ‹å ç”¨ç«¯å£çš„è¿›ç¨‹
lsof -i :8000

# æˆ–æŸ¥æ‰¾ vllm è¿›ç¨‹
ps aux | grep "vllm serve"

# åœæ­¢ä½œä¸š
scancel <JOB_ID>
```

### ä½œä¸šå¤±è´¥

```bash
# æŸ¥çœ‹æ—¥å¿—æ‰¾åˆ°é”™è¯¯åŸå› 
tail -n 100 qwen-vl-vllm-slurm.log

# æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
sinfo -N -l

# é‡æ–°æäº¤ä½œä¸š
sbatch qwen-vl-sbatch.sh
```

### æ¸…ç©ºæ—¥å¿—

```bash
# å¤‡ä»½æ—§æ—¥å¿—
mv qwen-vl-vllm-slurm.log qwen-vl-vllm-slurm.log.backup
```

## ğŸ”— API ä½¿ç”¨

### Python ç¤ºä¾‹

```python
import openai
import base64

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = openai.OpenAI(
    api_key="EMPTY",
    base_url="http://localhost:8000/v1"
)

# è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸º base64
with open("image.jpg", "rb") as f:
    image_base64 = base64.b64encode(f.read()).decode()

# å‘é€è¯·æ±‚
response = client.chat.completions.create(
    model="qwen-vl-max",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹"},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                }
            ]
        }
    ]
)

print(response.choices[0].message.content)
```

### é…ç½®è¯´æ˜

æœåŠ¡é…ç½®å·²ç¡¬ç¼–ç åœ¨ `process_pdf_fully.py` ä¸­ï¼š

```python
# vLLM æœåŠ¡é…ç½®
base_url = "http://localhost:8000/v1"
model_name = "qwen-vl-max"
```

å¦‚éœ€ä¿®æ”¹ç«¯å£æˆ–æ¨¡å‹åç§°ï¼Œè¯·åŒæ­¥æ›´æ–° `qwen-vl-vllm.sh` å’Œ `process_pdf_fully.py` æ–‡ä»¶ã€‚

---

**æç¤º**: ç¡®ä¿æœåŠ¡å®Œå…¨å¯åŠ¨åå†è°ƒç”¨ APIï¼ˆçº¦éœ€ 30-60 ç§’åŠ è½½æ¨¡å‹ï¼‰ã€‚

