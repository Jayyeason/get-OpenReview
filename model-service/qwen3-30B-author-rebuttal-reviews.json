{
  "02DCEU6vSU": {
    "paper_id": "02DCEU6vSU",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces Gen-LRA, a novel Membership Inference Attack (MIA) for generative models that leverages likelihood ratios to detect local overfitting. The approach evaluates the likelihood of synthetic data under hypotheses of overfitting to training examples, demonstrating superior performance across diverse benchmarks compared to existing MIAs."
          },
          "strengths": {
            "value": "The paper presents a principled approach to MIAs by formalizing privacy leakage through hypothesis testing, addressing a critical gap in generative model auditing. The methodology is computationally efficient, and the experiments cover diverse datasets, architectures, and attack parameters, showcasing broad applicability. The contribution of identifying subgroup-specific privacy risks adds significant value to the field."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough comparison with state-of-the-art MIAs, particularly those tailored to specific generative model types (e.g., GANs, VAEs). The assumption of an independent reference dataset may not hold in practical scenarios, and the paper does not address how Gen-LRA performs under adversarial model configurations. Additionally, the theoretical analysis of why likelihood ratios capture overfitting is limited."
          },
          "questions": {
            "value": "1. How does Gen-LRA generalize to non-tabular data or models like GANs/VAEs? 2. What are the practical limitations of the reference dataset assumption? 3. How does Gen-LRA compare to MIAs that exploit model-specific vulnerabilities (e.g., gradient-based attacks)? 4. Are there scenarios where the likelihood ratio metric fails to detect overfitting?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces Gen-LRA, a novel Membership Inference Attack (MIA) for generative models that leverages likelihood ratio testing to detect privacy leakage caused by local overfitting. The method operates under a shadow-box threat model without requiring assumptions about model architecture or access, and the authors claim it outperforms existing MIAs across multiple benchmarks."
          },
          "strengths": {
            "value": "The paper's originality lies in its principled approach to MIAs via hypothesis testing, which contrasts with heuristic methods in prior work. The comprehensive experimental evaluation across diverse datasets and models demonstrates strong empirical performance. The formalization of the MIA game and threat model is clear, and the identification of overfitting as a critical privacy risk is significant. The work also highlights the importance of subgroup-specific privacy leakage, adding depth to the analysis."
          },
          "weaknesses": {
            "value": "The methodology for Gen-LRA is under-specified, particularly the exact computation of the likelihood ratio and the design of the hypothesis test, making replication difficult. The experiments lack comparisons with state-of-the-art MIAs like those using neural networks or advanced statistical tests. The paper does not address how Gen-LRA performs under differential privacy guarantees, which is critical for real-world applicability. Additionally, the theoretical justification for why overfitting leads to privacy leakage is superficial."
          },
          "questions": {
            "value": "1. Can the authors clarify the exact formulation of the likelihood ratio used in Gen-LRA and how it differentiates between overfit and non-overfit data points? 2. Why were certain state-of-the-art MIAs (e.g., neural network-based methods) excluded from comparisons? 3. How does Gen-LRA handle cases where the training data is subject to differential privacy, and what are its limitations in such scenarios? 4. What is the computational complexity of Gen-LRA, and how does it scale to large datasets?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces Gen-LRA, a principled Membership Inference Attack (MIA) for generative models that exploits local overfitting by using a likelihood ratio framework. The method operates under a shadow-box threat model with minimal assumptions, aiming to detect privacy leakage by testing whether synthetic data is overfit to potential training examples. The authors claim superior performance over existing MIAs across multiple datasets and metrics."
          },
          "strengths": {
            "value": "The paper presents a novel approach to MIAs by framing membership inference as a hypothesis test, which is both theoretically grounded and computationally efficient. The comprehensive experimental evaluation across diverse datasets and model architectures demonstrates strong empirical performance. The clarity of the formalism and the practical relevance of the threat model are notable strengths. The work also highlights the risks of overfitting in generative models, a critical privacy concern."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art MIAs for generative models, making it difficult to assess the magnitude of Gen-LRA's improvement. The threat model assumes access to a reference dataset, which may not always be feasible in practice. The analysis of overfitting mechanisms is superficial, and the paper does not address how Gen-LRA generalizes to other generative model types (e.g., GANs, VAEs). Additionally, the theoretical justification for the likelihood ratio approach is underdeveloped, and the paper does not discuss potential countermeasures against Gen-LRA."
          },
          "questions": {
            "value": "1. How does Gen-LRA handle generative models with different architectures (e.g., GANs vs. VAEs)? 2. What specific baselines were used for comparison, and why were other MIAs not included? 3. How sensitive is Gen-LRA to the quality and representativeness of the reference dataset? 4. Are there practical scenarios where the shadow-box threat model is invalid, and how might this affect Gen-LRA's effectiveness? 5. What are the limitations of the likelihood ratio framework in capturing overfitting in complex generative models?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "02kZwCo0C3": {
    "paper_id": "02kZwCo0C3",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes SAIL, a self-improving online alignment framework for large language models (LLMs) that addresses distribution shift and preference oracle limitations in reinforcement learning from human feedback (RLHF). By formalizing online alignment as a bilevel optimization problem and reducing it to a computationally efficient single-level method, SAIL enables continuous model refinement through online exploration and iterative feedback. The approach claims significant performance gains over state-of-the-art methods while maintaining low computational overhead."
          },
          "strengths": {
            "value": "The paper introduces a principled bilevel optimization framework for online RLHF, addressing critical challenges like distribution shift and oracle dependency. The theoretical foundation is sound, and the self-improving mechanism offers a novel solution to reduce reliance on preference oracles. Experimental results demonstrate measurable improvements in win rate and evaluation rewards, with clear ablation studies and comparisons to existing methods. The connection to DPO and KL-regularized policy optimization is well-articulated."
          },
          "weaknesses": {
            "value": "The experimental evaluation lacks comprehensive comparisons with recent advanced methods like ORPO and SimPO, which are critical for establishing the approach's competitiveness. The AlpacaEval 2.0 results on LLama-3 (8B) are underwhelming, and the explanation for this limitation is insufficient. The computational efficiency claims require more detailed analysis of training time and resource usage. The paper also lacks ablation studies on the self-improvement mechanism's components and their impact on performance."
          },
          "questions": {
            "value": "1. Why were ORPO and SimPO not included in the experiments, given their prominence in recent alignment research? 2. How does SAIL handle scenarios with extreme distribution shift during online training? 3. What specific computational optimizations enable the claimed low overhead, and how do they scale to larger models? 4. Can the authors provide more details on the self-improvement mechanism's feedback refinement process and its convergence properties?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes SAIL, a method for online alignment of large language models (LLMs) through bilevel optimization. By formalizing online RLHF as a bilevel problem and reducing it to a computationally efficient single-level formulation, SAIL enables continuous self-improvement through online exploration and iterative feedback refinement. The approach addresses distribution shift and reduces reliance on preference oracles, achieving significant performance gains in experiments."
          },
          "strengths": {
            "value": "The paper introduces a novel bilevel optimization framework for online RLHF, addressing key challenges like distribution shift and preference oracle dependency. The theoretical analysis is rigorous, and the experiments demonstrate measurable improvements in win rate and reward metrics. The self-improvement mechanism is a significant contribution, and the paper provides clear comparisons to existing methods like DPO. The problem formulation is well-motivated, and the methodology is structured to address real-world alignment challenges."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons to recent preference optimization methods like ORPO and SimPO, which could provide context for SAIL's relative effectiveness. The explanation for suboptimal AlpacaEval 2.0 results on Llama-3 (8B) is speculative, citing model-specific factors without thorough analysis. The computational overhead claims are not rigorously validated, and the self-improvement mechanism's robustness to distribution shift remains underexplored. Experimental results on Zephyr-7B show promise, but the paper does not fully generalize these findings across diverse settings."
          },
          "questions": {
            "value": "1. How does SAIL's performance on AlpacaEval 2.0 compare to ORPO/SimPO on the same base model (Llama-3)? 2. What specific metrics were used to quantify 'low computational overhead' compared to DPO? 3. How does the self-improvement mechanism handle catastrophic forgetting or divergence in long-term training? 4. Are there ablation studies showing the contribution of the additional gradient term in SAIL? 5. How does the preference oracle relaxation perform in scenarios with sparse feedback?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes SAIL, a unified framework for online reinforcement learning from human feedback (RLHF) in large language models (LLMs). It addresses distribution shift and preference oracle dependencies by formulating online alignment as a bilevel optimization problem, reducing it to a computationally efficient single-level method. SAIL introduces online exploration, iterative feedback refinement, and claims significant performance gains over state-of-the-art methods."
          },
          "strengths": {
            "value": "The paper presents a novel theoretical framework for online RLHF, leveraging bilevel optimization to address distribution shift. The method's computational efficiency and self-improvement mechanism are well-motivated. The empirical results demonstrate measurable improvements in win rates and evaluation rewards, and the paper clarifies the relationship between SAIL and prior work like DPO."
          },
          "weaknesses": {
            "value": "The experiments lack comparisons with recent advanced methods like ORPO and SimPO, which are critical for establishing the method's competitiveness. The AlpacaEval 2.0 results on Llama-3 (8B) show no improvement, raising questions about the method's effectiveness in certain scenarios. The self-improvement mechanism's reliance on initial offline data and its generalization to diverse tasks remain underexplored. The paper also does not fully address the computational overhead claims or provide ablation studies for key components."
          },
          "questions": {
            "value": [
              "How does SAIL compare to ORPO and SimPO in terms of alignment performance and efficiency, given their distinct objectives and lack of KL regularization?",
              "What specific factors in the UltraFeedback dataset or Llama-3 (8B) base model limit SAIL's improvements on AlpacaEval 2.0, and how can these be mitigated?",
              "Can the self-improvement mechanism be quantitatively analyzed to demonstrate its impact on reducing preference oracle dependence?",
              "Are the computational overhead claims validated through detailed runtime or resource usage metrics?",
              "What ablation studies confirm the necessity of the proposed gradient term for exploration and the bilevel optimization reduction?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "04RGjODVj3": {
    "paper_id": "04RGjODVj3",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes HyperEEGNet, a novel architecture combining HyperNetworks with EEGNet to adaptively generate weights for motor imagery classification using resting-state EEG data. The approach aims to improve cross-user and cross-session generalization while reducing memory requirements for edge computing. Experiments show comparable performance on a small dataset and improved generalization on a larger one."
          },
          "strengths": {
            "value": "The paper introduces a creative combination of HyperNetworks and EEGNet, addressing a critical gap in BCI calibration and generalization. The methodology is well-structured, with experiments on two standard datasets. The significance of leveraging resting-state data for downstream tasks is highlighted, and the potential for edge computing applications is promising. The paper also acknowledges limitations and provides clear ablation studies in the rebuttal."
          },
          "weaknesses": {
            "value": "The initial dataset size (9 participants) limits the generalizability of findings, though the rebuttal adds evaluations on a larger dataset. The mechanism by which resting-state data is utilized remains under-explained, and the paper lacks ablation studies to isolate the contribution of resting-state features. The complexity of HyperNetworks and their computational costs are not thoroughly addressed, despite the rebuttal's claims about memory reduction. Interpretability of the model remains a concern, with limited analysis of how resting-state features influence classifications."
          },
          "questions": {
            "value": "1. How does the HyperNetwork specifically generate adaptive weights for EEGNet, and what architectural details ensure stability during training? 2. Are there ablation studies demonstrating the necessity of resting-state data for performance gains, or could the results stem from other factors? 3. What metrics were used to quantify the memory footprint reduction, and how does this compare to baseline models? 4. How does the model handle variations in resting-state data quality across users/sessions, given the rebuttal's acknowledgment of this issue?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes HyperEEGNet, a novel architecture combining EEGNet with HyperNetworks to adaptively generate weights for motor imagery classification using resting-state EEG data. The approach aims to improve cross-user and cross-session generalization by leveraging user-specific resting-state features, demonstrating comparable performance to baseline EEGNet on a small dataset and improved generalization on a larger dataset. The work highlights potential benefits for edge computing and faster user calibration in BCIs."
          },
          "strengths": {
            "value": "The paper introduces a novel architecture (HyperEEGNet) that creatively integrates HyperNetworks with EEGNet, addressing a key gap in BCI generalization. The focus on underutilized resting-state EEG data is timely and relevant. The method shows promise in cross-user and cross-session scenarios, with potential practical benefits for edge computing. The paper provides clear technical details about the model architecture and connectivity analysis methods."
          },
          "weaknesses": {
            "value": "The experimental validation remains limited, with only two datasets (9 and 42 participants) and no comparison to state-of-the-art methods. The paper lacks ablation studies to isolate the contribution of resting-state data vs. other components. The theoretical justification for using HyperNetworks in this context is underdeveloped. Key implementation details (e.g., hypernetwork architecture, training procedures) are only partially described, and the rebuttal's claims about additional evaluations need verification."
          },
          "questions": {
            "value": "1. How does HyperEEGNet compare to other transfer learning approaches for BCI? 2. What specific features from resting-state data drive performance improvements? 3. Are the reported generalization gains statistically significant compared to baseline models? 4. How sensitive is the model to variations in resting-state data quality across users/sessions? 5. What is the exact computational cost and inference speed of HyperEEGNet compared to EEGNet?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces HyperEEGNet, a novel architecture combining EEGNet with HyperNetworks to generate adaptive weights for motor imagery classification using resting-state EEG data. The approach aims to improve cross-user and cross-session generalization by leveraging user-specific resting-state features, demonstrating comparable performance to baseline EEGNet on a 9-participant dataset and showing scalability with 33 participants."
          },
          "strengths": {
            "value": "The paper presents a novel integration of HyperNetworks with EEGNet, addressing a gap in utilizing resting-state EEG for motor imagery tasks. The focus on cross-user/generalization is significant for real-world BCI applications. The practical benefit of reduced memory footprint for edge computing is a compelling contribution. The methodology is well-structured, and the problem statement is clearly motivated by existing challenges in BCI calibration and variability."
          },
          "weaknesses": {
            "value": "The experimental validation remains limited despite the rebuttal. The comparison to state-of-the-art methods is insufficient, with no ablation studies on HyperNetwork components. The claims about generalization to unseen subjects lack statistical significance testing. The resting-state data preprocessing and feature extraction methods are under-described, making it hard to assess their robustness. The computational complexity and memory trade-offs are not thoroughly analyzed, and the interpretability issues persist without deeper analysis."
          },
          "questions": {
            "value": "1. How exactly is resting-state EEG data integrated into the HyperNetwork for weight generation? What specific features are extracted from the resting-state data?\n2. The rebuttal mentions additional evaluations, but the results are summarized in the appendix. Can the authors provide detailed quantitative comparisons with strong baselines (e.g., transfer learning methods or other state-of-the-art models)?\n3. What ablation studies were conducted to verify the necessity of HyperNetworks versus simpler adaptation methods?\n4. How does the model handle extreme variations in resting-state data quality across users? Are there failure cases or edge scenarios analyzed?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "04qx93Viwj": {
    "paper_id": "04qx93Viwj",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper provides a comprehensive evaluation of the environmental impact of developing and training large language models (LLMs), including hardware manufacturing, model development, and training phases. The authors report that model development accounts for ~50% of total carbon emissions, highlight fluctuations in power consumption during training, and emphasize the need for transparency in reporting environmental costs. They also estimate water usage and compare their findings to existing studies, calling for industry-wide standards."
          },
          "strengths": {
            "value": "The paper's strengths lie in its holistic approach, covering operational and embodied emissions, water consumption, and model development costs—areas often overlooked in prior work. The detailed power consumption measurements at sub-second intervals and region-specific carbon intensity analysis demonstrate methodological rigor. The work sets a new benchmark for transparency by reporting metrics beyond training, such as development and inference, and encourages the community to adopt similar standards. The discussion on the limitations of current practices and the call for broader accountability are impactful."
          },
          "weaknesses": {
            "value": "The paper's methodology lacks novelty, as the authors acknowledge, and relies on existing frameworks like the Greenhouse Gas Protocol. Generalizability is limited by the specific model sizes (20M–13B parameters) and data center configurations studied. Deployment impact estimates are speculative due to lack of real-world data, and embodied emissions remain uncertain due to opaque supply chains. The paper also does not fully address how to scale their reporting framework to other models or architectures."
          },
          "questions": {
            "value": "1. How does the authors' proposed reporting standard differ from existing practices, given that the methodology is not novel? 2. What steps can be taken to improve the generalizability of their findings across diverse model architectures and hardware setups? 3. How can the uncertainties in embodied emissions estimates be systematically reduced, and what role should industry stakeholders play in this? 4. Are the authors' deployment impact simulations based on conservative assumptions, and how might real-world data alter these estimates?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper provides a comprehensive analysis of the environmental impact of developing and training large language models (LLMs), including carbon emissions, water consumption, and embodied emissions. The authors measure these impacts across three stages of the machine learning pipeline (model development, training, and inference) and highlight the significant contribution of model development to overall environmental costs, which is often overlooked in prior work."
          },
          "strengths": {
            "value": "The paper's strengths lie in its holistic approach, covering multiple environmental metrics (carbon, water, embodied emissions) and addressing the often-neglected impact of model development. The methodology is meticulous, with sub-second power consumption measurements and region-specific carbon intensity calculations. The work sets a new standard for transparency in reporting environmental impacts, and the findings on fluctuating power usage during training offer actionable insights for grid planning. The paper also emphasizes the importance of broader industry accountability, which is critical for sustainable AI development."
          },
          "weaknesses": {
            "value": "The paper's deployment impact estimates are limited by the lack of real-world data, as the authors do not host their models. Embodied emissions calculations rely on assumptions and incomplete data, which the authors acknowledge but do not fully address. The generalizability of their methodology to different hardware or data center configurations is not thoroughly validated. Additionally, the comparison to other models (e.g., OLMo, Llama) is constrained by the absence of publicly available power consumption data, reducing the paper's comparative scope."
          },
          "questions": {
            "value": "1. How did the authors handle uncertainties in embodied emissions estimates, and what specific assumptions were made for data not provided by suppliers? 2. Can the authors clarify how their methodology scales to different hardware architectures or data center locations, given the reliance on location-specific metrics like PUE and WUE? 3. What steps were taken to validate the accuracy of power consumption measurements during training, especially given the sub-second granularity? 4. How do the authors reconcile their claim of 'first public estimates of development impact' with prior work (e.g., Luccioni et al. 2024) that also addressed inference impacts?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper presents a comprehensive analysis of the environmental impact of developing and training large language models (LLMs), including carbon emissions, water consumption, and embodied impacts across model development, training, and inference stages. The authors emphasize the importance of transparency in reporting these metrics, highlighting that model development contributes ~50% of total environmental costs, a factor often overlooked in prior work."
          },
          "strengths": {
            "value": "The paper's strengths lie in its holistic approach, covering operational and embodied impacts, detailed power consumption measurements, and advocacy for transparency. It addresses critical gaps in prior work by including water usage and development-phase costs, which are rarely reported. The methodology is thorough, with explicit acknowledgment of limitations and conservative assumptions. The work also raises important societal questions about the scalability of AI systems."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons to prior methodologies for calculating environmental impacts, making it difficult to assess its novelty or improvements over existing approaches. Key metrics like embodied emissions and water usage rely on approximations and assumptions, with limited justification for their accuracy. The deployment impact estimates are based on simulations without real-world validation, and the paper does not fully address how hardware, data center locations, or model architectures affect results. Additionally, the claims about OLMo and Llama comparisons are underdeveloped due to incomplete data."
          },
          "questions": {
            "value": "1. How does the authors' methodology for calculating embodied emissions differ from prior work, and what specific improvements do they claim? 2. What are the exact assumptions and data sources used for estimating water consumption and embodied carbon, and how sensitive are the results to these assumptions? 3. How do the authors reconcile their deployment impact estimates with the lack of real-world data from model hosting? 4. What steps were taken to validate the accuracy of power consumption measurements during training, and how do they address the variability in hardware efficiency? 5. How do the authors respond to concerns about the replicability of their results given the absence of public power consumption data from models like OLMo and Llama?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "06B23UkNid": {
    "paper_id": "06B23UkNid",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces MV-CLAM, a framework that integrates 2D and 3D molecular representations into a unified text space using a novel cross-modal projector called MQ-Former. The method addresses limitations of prior work by aligning multiple molecular views simultaneously, reducing computational overhead, and improving molecule-text retrieval and captioning tasks."
          },
          "strengths": {
            "value": "The paper makes a clear contribution by combining 2D and 3D molecular data with a novel cross-modal projector, addressing a gap in existing methods. The methodology is well-structured, with detailed experiments on molecule-text retrieval, captioning, and QA tasks. The architecture leverages shared self-attention layers to unify embeddings, which is a promising approach. The rebuttal provides additional evidence of the model's effectiveness in downstream tasks."
          },
          "weaknesses": {
            "value": "The novelty of MQ-Former is somewhat unclear, as it builds on existing cross-modal frameworks like Q-Former. The paper lacks detailed ablation studies to validate the effectiveness of key components (e.g., shared self-attention layers). Computational efficiency claims (e.g., 50% training time reduction) are not substantiated with concrete metrics. The use of LLaMA2's tokenizer for SMILES generation is a limitation that is not fully addressed."
          },
          "questions": {
            "value": "1. How does the shared self-attention layer specifically address the issue of separate embeddings? Are there ablation studies demonstrating its effectiveness? 2. What are the limitations of using LLaMA2's tokenizer for SMILES generation, and how might this affect the model's performance in tasks requiring SMILES? 3. How does the model handle scenarios where 2D and 3D data contribute unequally to textual descriptions?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces MV-CLAM, a framework that integrates 2D and 3D molecular representations into a unified text token space using a novel multi-querying transformer (MQ-Former). The approach aligns multiple molecular views simultaneously, aiming to improve molecule-text retrieval, captioning, and zero-shot editing. The method leverages shared self-attention layers to consolidate molecular embeddings while addressing limitations of prior single-view models."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in multi-modal molecular representation learning by proposing a novel architecture (MQ-Former) for simultaneous 2D/3D alignment. The integration of complementary molecular views (topological vs. spatial) is conceptually strong. The experiments demonstrate state-of-the-art performance on molecule-text tasks, and the rebuttal provides additional QA results that strengthen the claims. The framework's potential for downstream tasks like zero-shot editing is promising. The technical details of the cross-modal projector and shared self-attention mechanism are well-justified."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the necessity of key components (e.g., shared self-attention vs. separate encoders). The computational efficiency claims (e.g., 'reduces training time by more than half') are not empirically supported. The comparison with existing methods is limited, and the limitations of using LLaMA2 (e.g., SMILES generation issues) are not thoroughly discussed. The rebuttal addresses some gaps but leaves open questions about the architecture's generalizability and the impact of tokenization on results."
          },
          "questions": {
            "value": [
              "How does MQ-Former dynamically balance contributions from 2D and 3D molecular views when their relevance to text varies? Are there mechanisms to weight modalities based on context?",
              "What is the exact architecture of the MQ-Former? How does the 'multi-querying' mechanism differ from prior Q-Former variants?",
              "The rebuttal mentions ablation studies, but the paper lacks explicit results. Could the authors provide details on these experiments?",
              "How do the results compare to models like MolT5 or UniMoT, which also use multi-modal approaches? Are there direct comparisons in the literature?",
              "The use of LLaMA2's tokenizer for SMILES generation is a limitation. Could the authors discuss potential solutions or alternative tokenization strategies?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes MV-CLAM, a framework that integrates 2D and 3D molecular representations into a unified text token space using a novel multi-querying transformer (MQ-Former). The approach addresses limitations of prior work by aligning multiple molecular modalities simultaneously, improving molecule-text retrieval, captioning, and zero-shot tasks."
          },
          "strengths": {
            "value": "The paper introduces a novel cross-modal projector (MQ-Former) that addresses key limitations of prior work, such as separate embedding spaces and computational inefficiency. The method's ability to align 2D and 3D molecular data simultaneously with text is a significant technical contribution. The paper demonstrates strong performance on multiple tasks, including molecule-text retrieval and QA, with detailed ablation studies and visualization of modality-specific attention patterns. The work also highlights practical applications in chemical property prediction and zero-shot editing."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive baseline comparisons against state-of-the-art multi-view molecular modeling approaches (e.g., GIT-Mol or MolLM). While the rebuttal provides QA results, the original submission does not clearly contextualize these results against prior work. The computational efficiency claims (e.g., 'reducing training time by more than half') are not empirically validated. Additionally, the choice of LLaMA2 as the language model is not thoroughly justified, particularly given its suboptimal SMILES generation capabilities. The paper also does not address how MQ-Former handles cases where 2D/3D modalities contribute unequally to textual descriptions."
          },
          "questions": {
            "value": [
              "How does the paper ensure the shared self-attention layer effectively captures complementary information from 2D and 3D modalities, especially when their relevance varies per task?",
              "What specific baselines were compared against in the molecule-text retrieval and captioning experiments? Are these comparisons representative of the current state-of-the-art?",
              "The rebuttal mentions ablation studies, but the original paper does not provide details. How do these studies validate the necessity of the multi-querying architecture versus simpler alternatives?",
              "The paper claims improvements in zero-shot molecule editing, but no quantitative results or case studies are provided. How is this claim supported?",
              "Why was LLaMA2 chosen over other language models (e.g., T5-based architectures) that might be better suited for SMILES generation? How does this choice impact the framework's applicability?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "06ZvHHBR0i": {
    "paper_id": "06ZvHHBR0i",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a multi-agent framework for evaluating large language models (LLMs) by simulating a courtroom-like process where LLMs act as advocates, judges, and juries. The proposed architectures, MORE and SAMRE, aim to improve evaluation accuracy by leveraging structured debate, adversarial arguments, and ensemble-based aggregation. The work draws on theories from decision theory, legal systems, and social choice to design a dynamic evaluation mechanism."
          },
          "strengths": {
            "value": "The paper demonstrates strong interdisciplinary inspiration, integrating decision theory, legal adversarial processes, and social choice theory into a novel evaluation framework. The structured architecture (MORE/SAMRE) is clearly defined, with detailed algorithmic pseudocode. The motivation for addressing limitations of human and automated evaluation methods is well-articulated, and the paper's organization is logical and methodical."
          },
          "weaknesses": {
            "value": "The paper lacks concrete experimental results or quantitative validation of the proposed frameworks. Key components such as the probabilistic error reduction model and the specific implementation of the 'judge' and 'jury' roles are not elaborated. The comparison with existing LLM evaluation methods is superficial, and the practical feasibility of the multi-agent system (e.g., computational costs, scalability) is not discussed. The absence of empirical data undermines the claims of 'effectiveness' and 'comprehensiveness'."
          },
          "questions": {
            "value": "1. How are the 'advocates' trained or initialized? Are they pre-trained LLMs or fine-tuned for specific tasks? 2. What metrics are used to evaluate the performance of MORE/SAMRE compared to baselines? 3. How is the probabilistic model for error reduction implemented and validated? 4. What are the computational requirements and scalability limitations of the proposed architectures? 5. How are biases in the 'juries' or 'judges' mitigated?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces a multi-agent framework where large language models (LLMs) act as advocates, judges, and juries in a courtroom-inspired system to evaluate other LLM outputs. It proposes two architectures, MORE (Multi-Advocate One-Round Evaluation) and SAMRE (Single Advocate Multi-Round Evaluation), and draws on theories from decision theory, legal systems, and voting theory to structure the evaluation process."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by integrating legal and psychological theories into LLM evaluation, offering a novel perspective on automated assessment. The framework is theoretically grounded, with clear distinctions between advocate, judge, and jury roles. The structure is well-organized, and the motivation for the approach is compelling. The discussion of voting theory and social choice principles adds depth to the methodology."
          },
          "weaknesses": {
            "value": "The paper lacks concrete experimental validation. While it mentions experiments, there are no results, metrics, or comparisons to existing evaluation methods, making it difficult to assess the practical effectiveness of the proposed architectures. The algorithms are described in pseudocode but not implemented or tested. Additionally, the paper does not address potential limitations, such as computational costs or scalability, nor does it clarify how the framework handles edge cases or ambiguous outputs."
          },
          "questions": {
            "value": "1. What specific experiments were conducted to validate the MORE and SAMRE architectures? Were there benchmarks or datasets used for comparison? 2. How do the authors address the computational complexity of multi-round evaluations, especially with multiple advocates and juries? 3. How does the framework handle cases where advocates produce conflicting or incomplete arguments? 4. Are there plans to open-source the implementation or provide details about the jury selection process?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes a novel framework for evaluating large language models (LLMs) by modeling them as interacting agents in a courtroom-inspired multi-agent system. The approach includes two architectures (MORE and SAMRE) where LLMs act as advocates, judges, and juries to assess outputs through structured debate and aggregation. The paper draws on theories from decision theory, legal systems, and voting theory but lacks detailed experimental validation."
          },
          "strengths": {
            "value": "The paper demonstrates originality by integrating diverse theoretical foundations (decision theory, legal adversarial processes, voting theory) into a multi-agent LLM evaluation framework. The architecture designs (MORE and SAMRE) are creatively structured, and the paper provides a clear roadmap for future work. The motivation for addressing limitations in human and automated evaluation methods is well-articulated."
          },
          "weaknesses": {
            "value": "The paper lacks critical experimental details, such as specific metrics, baselines, or results to validate the proposed architectures. The probabilistic model for error reduction is not elaborated, and the theoretical claims remain untested. The figure and algorithm descriptions are incomplete, with no evidence of implementation or scalability analysis. The absence of comparisons to existing LLM evaluation methods (e.g., human judgments, automated metrics) weakens the contribution."
          },
          "questions": {
            "value": "1. What datasets or tasks were used to evaluate the MORE/SAMRE frameworks? 2. How are the LLM advocates trained or prompted to generate arguments? 3. What baselines (e.g., human evaluations, BLEU, ROUGE) were compared? 4. How is the probabilistic model for error reduction formalized and validated? 5. Are there limitations in the scalability or computational cost of the multi-agent system?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "09LEjbLcZW": {
    "paper_id": "09LEjbLcZW",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces AutoKaggle, a multi-agent framework for automated data science competitions. It employs a phase-based workflow with specialized agents (Reader, Planner, Developer, Reviewer, Summarizer) to handle tasks like data cleaning, feature engineering, and modeling. The framework emphasizes iterative debugging, unit testing, and a machine learning tools library to ensure code quality and transparency. Evaluation on 8 Kaggle competitions shows high validation submission rates and comprehensive scores."
          },
          "strengths": {
            "value": "The paper presents a structured, multi-agent approach to automate complex data science pipelines, addressing gaps in existing methods. The phase-based workflow with clear division of tasks (e.g., data cleaning, feature engineering) is well-organized. The integration of iterative debugging and unit testing enhances code reliability. The machine learning tools library provides reusable, validated functions, improving efficiency. The evaluation on real-world Kaggle competitions demonstrates practical applicability, and the emphasis on transparency through detailed reporting is commendable."
          },
          "weaknesses": {
            "value": "The paper lacks critical comparisons to baseline methods or human performance, making it difficult to assess the framework's true effectiveness. The claim of 'fully automated' is undermined by the need for manual configuration via config files, as noted in the rebuttal. Unit tests are custom-written by humans, contradicting the automation claim and raising questions about their coverage and rigor. The evaluation metrics (e.g., 'comprehensive score') are not sufficiently explained, and the paper does not address potential data leakage from using Classic Kaggle competitions. The context length handling is described but lacks detailed technical justification."
          },
          "questions": {
            "value": "1. How do the reported validation submission rate (0.85) and comprehensive score (0.82) compare to baselines or human performance? 2. What is the exact definition of the 'comprehensive score,' and how is it calculated? 3. How are the unit tests designed, and what is their coverage? Are they manually crafted, and if so, how does this affect the framework's automation claims? 4. What is the role of the human-in-the-loop, and how does the framework handle tasks outside predefined phases? 5. How does AutoKaggle ensure generalization to datasets not used in evaluation?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces AutoKaggle, a multi-agent framework for automated data science competitions. It proposes a phase-based workflow with specialized agents (Reader, Planner, Developer, Reviewer, Summarizer) and integrates iterative debugging, unit testing, and a machine learning tools library. The framework is evaluated on 8 Kaggle competitions, achieving a validation submission rate of 0.85 and a comprehensive score of 0.82."
          },
          "strengths": {
            "value": "The paper presents a novel multi-agent framework with a structured phase-based workflow, addressing complex data science tasks through collaboration and iterative debugging. The integration of a comprehensive machine learning tools library enhances code generation efficiency. The experiments on real-world Kaggle competitions demonstrate practical effectiveness. The framework's transparency through detailed reporting and adaptability to user customization are significant strengths."
          },
          "weaknesses": {
            "value": "The paper lacks detailed descriptions of unit test design, coverage, and guarantees, raising concerns about the 'fully automated' claim. The evaluation on only 8 competitions and absence of comparisons with human baselines or alternative frameworks limit generalizability. The role of human-in-the-loop (HITL) remains ambiguous despite rebuttal clarifications. The rebuttal acknowledges manual adjustments via config files, which conflicts with the 'fully automated' assertion. The paper also does not address scalability or handling of non-tabular data."
          },
          "questions": {
            "value": "1. How are the unit tests designed? What is their coverage and how are they validated? 2. What metrics were used to determine the 'comprehensive score' of 0.82? 3. How does AutoKaggle handle unexpected data anomalies or edge cases not covered by predefined tools? 4. What is the computational cost and runtime efficiency of the framework? 5. How does the multi-agent system resolve conflicts or coordinate tasks when agents have diverging strategies?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces AutoKaggle, a multi-agent framework for automated data science competitions. It proposes a phase-based workflow with collaborative agents, iterative debugging, a machine learning tools library, and comprehensive reporting. The framework is evaluated on 8 Kaggle competitions, achieving 0.85 valid submission rate and 0.82 comprehensive score."
          },
          "strengths": {
            "value": "The paper presents a structured, phase-based approach to data science automation, integrating multi-agent collaboration and iterative debugging. The machine learning tools library is comprehensive and well-documented. The evaluation on real-world Kaggle competitions demonstrates practical applicability. The framework's transparency through detailed reporting is a notable strength."
          },
          "weaknesses": {
            "value": "The claim of a 'fully automated framework' is undermined by the manual writing of unit tests and the presence of human-in-the-loop customization options. The paper lacks comparisons to baselines, human performance, or alternative automated systems, making it difficult to assess the significance of the 0.85/0.82 metrics. The unit tests' design, coverage, and guarantees are not sufficiently detailed. The rebuttal clarifies some issues but does not fully resolve concerns about automation and human involvement."
          },
          "questions": {
            "value": [
              "How are the unit tests designed, and what guarantees do they provide? Are they manually written, and if so, how does this affect the framework's automation claims?",
              "What baseline methods or human performance metrics were used to validate the 0.85/0.82 scores? Without comparison, it is unclear if these results are competitive.",
              "The paper mentions 'manual adjustments' via config.json. How does this affect the 'fully automated' claim, and what percentage of tasks require such interventions?",
              "The rebuttal states that unit tests are 'custom-written and individually verified.' How are these tests generated and maintained at scale for diverse Kaggle competitions?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0EP01yhDlg": {
    "paper_id": "0EP01yhDlg",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces a multi-token prediction method for transformers using rank-r canonical probability decomposition, aiming to enhance sampling efficiency. It generalizes the rank-1 approach by incorporating a mixture of experts framework, enabling simultaneous token prediction while maintaining low computational overhead. The method is evaluated on text and code generation tasks, showing improved inference speed with minimal accuracy loss."
          },
          "strengths": {
            "value": "The paper makes a novel connection between multi-token prediction and tensor decomposition, offering a theoretically grounded approach. The integration of a mixture-of-experts framework with learnable weights is innovative. Experiments demonstrate reduced joint loss with higher ranks and scalability to different model sizes. The rebuttal addresses notational inconsistencies and provides additional empirical results on a 1B-parameter model, strengthening the claims."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons with state-of-the-art methods like Medusa or Eagle, which limits the assessment of its competitiveness. The code generation results (e.g., PyCode) are underwhelming, and the authors trained only the heads, not the full model. The auxiliary loss tuning is not thoroughly discussed, and hyperparameters were selected using the test set, which is a methodological flaw. The experiments primarily focus on synthetic or small-scale datasets, with limited real-world validation."
          },
          "questions": {
            "value": "1. How does the proposed method compare to established approaches like Medusa or Eagle in terms of speed and accuracy? 2. What are the practical implications of the auxiliary loss tuning for different model sizes? 3. Why was the test set used for hyperparameter selection, and how might this affect generalization? 4. Can the method be extended to larger models like Llama 8B with full training? 5. What are the theoretical guarantees for the rank-r decomposition's superiority over other multi-token prediction techniques?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces a multi-token prediction method for transformers using rank-r canonical probability decomposition, generalizing prior rank-1 approaches. It frames the model as a mixture of experts with learnable weights, aiming to improve joint probability approximation for speculative decoding. The method claims minimal computational overhead and demonstrates speedups in inference tasks."
          },
          "strengths": {
            "value": "The paper presents a novel methodological extension of rank-1 multi-token prediction to rank-r decomposition, leveraging tensor factorization and mixture-of-experts principles. The connection to canonical polyadic decomposition is theoretically sound. The experiments show reduced joint loss with higher ranks and stable first-token performance. The auxiliary balancing loss addresses expert imbalance, a critical issue in MoE frameworks. The approach is well-motivated for speculative decoding scenarios."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons with state-of-the-art methods like Medusa or Eagle, which are central to the multi-token prediction domain. The code generation results (e.g., PyCode) are underwhelming, and the rebuttal acknowledges this. The experimental validation is limited to a single dataset (Tiny Stories) and a small model. The auxiliary loss's hyperparameter sensitivity is not thoroughly analyzed. The notation inconsistency (z_t vs. e_t) in the original manuscript caused confusion, though it was addressed in the rebuttal."
          },
          "questions": {
            "value": "1. How does the proposed method compare to established approaches like Medusa or Eagle in terms of token acceptance rates and inference speed? 2. What is the impact of the auxiliary loss coefficient on training stability and performance? 3. Are the results on the 1B model generalizable to larger models like Llama 8B? 4. How does the method handle long-range token dependencies beyond the first few tokens? 5. Why does rank-4 outperform rank-2 in inference time despite lower token acceptance rates?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes a multi-token prediction method for transformers using rank-r canonical probability decomposition, aiming to improve inference efficiency while maintaining accuracy. The approach generalizes prior rank-1 methods by modeling token dependencies through a mixture of experts framework, with experimental results showing speed improvements in text and code generation tasks."
          },
          "strengths": {
            "value": "The paper introduces a novel connection between multi-token prediction and tensor decomposition, offering a theoretically grounded approach to model token dependencies. The method's integration into existing transformer architectures with minimal overhead is well-explained. The experiments demonstrate practical speed improvements, and the auxiliary balancing loss addresses a critical challenge in expert mixture training. The clarity of mathematical formulations and figures is commendable."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons with state-of-the-art methods like Medusa or Eagle, which are central to the multi-token prediction domain. The code generation results are weak, and the rebuttal acknowledges this limitation. The experimental evaluation on Llama 8B is incomplete, with only inference time reported. The hyperparameter selection using the test set is a methodological flaw. The theoretical analysis of the rank-r decomposition's advantages over rank-1 remains underdeveloped."
          },
          "questions": {
            "value": "1. How does the proposed method compare to specialized multi-token prediction frameworks like Medusa in terms of speed and accuracy? 2. What are the limitations of training only the prediction heads while keeping the main model fixed? 3. Can the auxiliary loss formulation be theoretically justified for tensor decomposition tasks? 4. How does the method scale to very large models beyond the 1B parameter experiments? 5. What is the exact relationship between the rank-r decomposition and the mixture-of-experts interpretation?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0F1rIKppTf": {
    "paper_id": "0F1rIKppTf",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces mirror Schrödinger bridges (MSB), a method for conditional resampling by solving the Schrödinger bridge problem between a distribution and itself. The approach leverages time symmetry and an alternating minimization procedure (AMP) to reduce computational complexity, enabling in-distribution variations of input samples. Theoretical analysis and preliminary empirical results demonstrate algorithmic simplifications and controllable sample proximity."
          },
          "strengths": {
            "value": "The paper presents a novel application of Schrödinger bridges to self-mapping, addressing an underexplored problem. The theoretical framework is rigorous, with clear connections to existing optimal transport and diffusion literature. The alternating minimization procedure offers algorithmic efficiency gains, and the empirical results, while limited, suggest practical utility. The work also highlights the importance of time-symmetry in reducing computational overhead, a key contribution to the field."
          },
          "weaknesses": {
            "value": "The empirical evaluation is limited, with insufficient analysis of the method's performance across diverse datasets and hyperparameter regimes. The rebuttal acknowledges computational constraints, but the proposed experiments (e.g., path regularity) are not yet fully integrated into the paper. The focus on Ornstein-Uhlenbeck (OU) processes excludes other potential mean-reverting processes, and the paper lacks comparisons to existing self-mapping methods. Additionally, the theoretical guarantees for the AMP-based algorithm require deeper exploration."
          },
          "questions": {
            "value": "1. How do the results scale with increased computational resources, as suggested in the rebuttal? 2. Are there specific benchmarks or datasets where MSB outperforms existing self-mapping methods? 3. What are the limitations of restricting the prior to OU processes, and how might this affect generalization? 4. Can the theoretical analysis of AMP convergence be strengthened to address potential instability in practice?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces mirror Schrödinger bridges (MSB), a novel method for conditional resampling by solving the Schrödinger bridge problem between a distribution and itself. The approach leverages time symmetry to derive an efficient algorithm that generates in-distribution variations of input samples, offering control over the proximity of generated samples to the input. Theoretical analysis and empirical results are presented across multiple domains."
          },
          "strengths": {
            "value": "The paper demonstrates strong theoretical originality by extending Schrödinger bridges to self-mapping scenarios, a largely unexplored area. The use of time symmetry to simplify the optimization process is a notable contribution. The clarity of mathematical formulations and the structured presentation of the problem are commendable. The significance lies in the potential to improve resampling techniques in generative models, with practical benefits for controlling sample diversity."
          },
          "weaknesses": {
            "value": "The empirical evaluation is limited, with insufficient analysis of the method's robustness across varying hyperparameters (e.g., σ). The paper lacks comparative experiments against baseline methods and fails to address how the choice of prior (e.g., OU process) impacts performance. The rebuttal's added experiments on path regularity are a step forward but do not fully resolve concerns about the method's practical efficacy. Additionally, the paper does not thoroughly explore alternative mean-reverting processes or their implications for the method's generality."
          },
          "questions": {
            "value": "1. How does the method perform when using non-OU mean-reverting processes, and what are the trade-offs? 2. What are the specific computational advantages of the proposed algorithm compared to existing IPFP-based methods? 3. Are there quantitative metrics to validate the 'regularity' of path measures beyond the proposed total path length metric? 4. How does the method handle distributions with complex structures (e.g., multi-modal data) compared to standard generative models?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces mirror Schrödinger bridges (MSB), a novel approach to solving the Schrödinger bridge problem between a distribution and itself by leveraging time-symmetry. The method aims to generate in-distribution variations of input samples through efficient algorithmic simplifications, particularly via an Alternating Minimization Procedure (AMP). Empirical results demonstrate benefits in generating proximal samples, though the evaluation is limited in scope."
          },
          "strengths": {
            "value": "The paper presents a theoretically grounded approach to a niche problem (self-maps in Schrödinger bridges) with clear algorithmic innovations. The focus on time-symmetry and the proposed AMP procedure offer potential efficiency gains. The connection to existing methods like IPFP and diffusion processes is well-articulated, and the rebuttal addresses some empirical concerns by adding experiments on path regularity."
          },
          "weaknesses": {
            "value": "The empirical evaluation is insufficient, with limited quantitative analysis and reliance on qualitative claims. The paper does not thoroughly compare MSB to existing methods (e.g., diffusion models or IPFP) on standard benchmarks. The theoretical analysis of the AMP procedure's convergence and optimality is underdeveloped. The rebuttal's added experiments on path regularity are a positive step but lack depth and context for broader implications."
          },
          "questions": {
            "value": "How does MSB compare to existing methods like diffusion models or IPFP in terms of quantitative metrics (e.g., FID, IS)? What are the limitations of using the Ornstein-Uhlenbeck process as the reference measure? Can the method generalize to non-time-symmetric priors? How does the AMP procedure ensure convergence in practice? The rebuttal's path regularity experiment (e.g., total path length vs. time step size) needs clearer interpretation and validation against baseline methods."
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0GzqVqCKns": {
    "paper_id": "0GzqVqCKns",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates the latent hierarchical structure of data using diffusion models, proposing that forward-backward experiments can reveal correlated token changes at a critical noise level. The authors validate their theoretical predictions on synthetic hierarchical models and real-world text/image datasets, demonstrating phase transitions in susceptibility that align with hierarchical structures."
          },
          "strengths": {
            "value": "The paper presents a novel framework linking diffusion models to hierarchical data analysis, combining rigorous theoretical analysis (mean-field approximation) with empirical validation on diverse modalities. The synthetic Random Hierarchy Model (RHM) provides a controlled setting to study phase transitions, while the application to real data (text and images) demonstrates practical relevance. The work also highlights limitations of Gaussian models, advancing the understanding of structured data generation."
          },
          "weaknesses": {
            "value": "The generalizability of the RHM to real-world data remains underexplored, despite the authors' claims about abstract hierarchical abstractions. The phase transition in real data is inferred via susceptibility peaks, but direct classification experiments (e.g., ConvNeXt) only partially validate this. The theoretical analysis assumes uniform noise distribution, which may not capture real data's complexity. Additionally, the paper lacks a detailed discussion on how varying hierarchical depths affect susceptibility dynamics."
          },
          "questions": {
            "value": [
              "How do the authors reconcile the RHM's discrete structure with continuous data like images? What empirical evidence supports the abstraction of semantic hierarchies in continuous domains?",
              "The susceptibility peak in real data is tied to the RHM's phase transition. Could alternative models (e.g., non-tree-structured graphical models) also produce similar peaks, and how might this affect the interpretation of results?",
              "The rebuttal mentions that single-level RHM lacks the phase transition. How sensitive are the findings to the depth of the hierarchical structure in real-world data, which may not have explicit depth parameters?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates the latent hierarchical structure of data using diffusion models, proposing that forward-backward experiments can reveal hierarchical dependencies. The authors validate their theoretical predictions on synthetic hierarchical models (RHM) and real-world data (text and images), demonstrating that token changes occur in correlated blocks with a diverging length scale at a phase transition. They introduce dynamical susceptibility as a measure to quantify these effects."
          },
          "strengths": {
            "value": "The paper presents a novel approach to probing hierarchical structures in data via diffusion models, combining theoretical analysis with empirical validation. The use of a synthetic hierarchical model (RHM) allows controlled experimentation, while the application to real data (text and images) demonstrates practical relevance. The clarity of the methodology, including mean-field approximations and dynamical susceptibility analysis, is strong. The work addresses both discrete and continuous data modalities, showcasing broad applicability."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous justification for extending the discrete RHM to continuous data (e.g., images), despite the rebuttal's argument about abstract hierarchical representations. The empirical validation of phase transitions in real data relies on susceptibility peaks, which are linked to the RHM's phase transition without direct evidence of class changes. The theoretical analysis assumes mean-field approximations that may oversimplify real-world complexities. Additionally, the paper does not fully address how the observed correlations distinguish hierarchical structures from other forms of dependency."
          },
          "questions": {
            "value": "1. How does the dynamical susceptibility peak in real data conclusively indicate a phase transition, rather than other forms of structural changes? 2. What evidence supports the claim that the RHM's discrete hierarchical structure is a valid abstraction for continuous data like images? 3. Could the observed correlations in token changes be explained by alternative models (e.g., Markov processes) without hierarchical assumptions?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper explores the use of diffusion models to probe latent hierarchical structures in data, combining theoretical analysis with experiments on text and image datasets. It introduces a synthetic hierarchical model (RHM) and demonstrates that forward-backward diffusion processes reveal correlated token changes, with a critical phase transition at a specific noise level."
          },
          "strengths": {
            "value": "The paper's originality lies in connecting diffusion models to hierarchical data structures, a novel approach. The theoretical analysis using mean-field approximation is rigorous, and experiments on both text (WikiText) and image (ImageNet) datasets are well-designed. The clarity of the presentation is strong, with clear definitions and logical organization. The significance is high, as it bridges generative modeling with hierarchical structure analysis, offering new insights into data learnability."
          },
          "weaknesses": {
            "value": "The synthetic RHM model is highly simplified, assuming a strict tree-like structure that may not generalize to real-world data. The experiments on real data (text/image) lack depth in explaining how the hierarchical structure manifests beyond the susceptibility peak. The mean-field approximation assumes uniform uncertainty, which may not reflect real-world complexities. The paper also does not fully address how the RHM's assumptions translate to continuous data like images. Additionally, the connection between the synthetic model and real-world applications remains underexplored."
          },
          "questions": {
            "value": [
              "How does the RHM's hierarchical structure translate to real data, given that real-world data may not strictly follow a tree-like model?",
              "The paper mentions a phase transition in susceptibility but does not explicitly link it to the number of hierarchical levels. How can the model distinguish between single-level and multi-level hierarchies?",
              "The ImageNet experiments use a classifier to detect class changes. What specific metrics or thresholds were used to determine the phase transition, and how robust are these results?",
              "The mean-field approximation assumes uniform uncertainty. How might this affect the validity of results for complex, real-world data with non-uniform noise patterns?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "0JjsZC0w8x": {
    "paper_id": "0JjsZC0w8x",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes Context-Wise Order-Agnostic Language Modeling (COrAL), a framework that integrates iterative refinement into large language models (LLMs) by modeling token dependencies within context windows. It introduces sliding blockwise order-agnostic decoding to enable parallel multi-token prediction and backward reconstruction, achieving performance improvements and inference speedups on reasoning tasks. However, the approach shows limitations in code generation due to order-agnostic output inconsistencies."
          },
          "strengths": {
            "value": "The paper presents a novel integration of iterative refinement into LLM architecture, addressing sequential generation limitations of autoregressive (AR) models. The sliding blockwise decoding strategy demonstrates clear efficiency gains (3.9× speedup on GSM8K) while improving accuracy (4.6% on GSM8K). The target-aware RoPE modification provides a specific technical contribution to position-aware generation. The work systematically compares with AR baselines and acknowledges limitations, showing thoroughness in analysis."
          },
          "weaknesses": {
            "value": "The generalizability of COrAL remains limited, particularly in tasks requiring strict syntactic coherence (e.g., code generation) as noted in the rebuttal. The computational overhead (5.48 TFLOPS vs 2.81 for next-token baselines) raises concerns about practical deployment on resource-constrained devices. The theoretical analysis of order-agnostic modeling lacks depth, and the paper does not sufficiently compare with permutation-based AR or diffusion-based NAR approaches. The code generation experiments are cursory, leaving questions about the trade-offs between quality and speed."
          },
          "questions": {
            "value": "1. How does the sliding block decoding handle long-range dependencies beyond the context window size? 2. What specific architectural modifications enable target-aware RoPE without additional parameters? 3. How does the multi-forward mechanism avoid compounding errors during iterative refinement? 4. Can the framework be adapted to handle variable-length sequences more effectively? 5. What are the exact trade-offs between context window size and performance in code generation tasks?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces CORAL, a context-wise order-agnostic language modeling framework that integrates iterative refinement into LLM architectures. It proposes sliding blockwise order-agnostic decoding to enable parallel refinement, achieving improvements in reasoning tasks while maintaining computational efficiency. The method uses target-aware RoPE to preserve position information and addresses limitations of autoregressive (AR) and non-autoregressive (NAR) models."
          },
          "strengths": {
            "value": "The paper presents a novel approach to order-agnostic modeling by combining iterative refinement with context-aware dependencies, addressing limitations of sequential AR generation. The sliding blockwise decoding strategy effectively balances speed and accuracy, demonstrated through significant gains on reasoning tasks. The use of target-aware RoPE is a creative solution to preserve position information without architectural changes. The empirical evaluation is comprehensive, with detailed ablation studies and comparisons to baselines."
          },
          "weaknesses": {
            "value": "The generalizability of CORAL remains limited, as evidenced by its performance drop in code generation tasks. The computational overhead (e.g., 5.48 TFLOPS vs. 2.81 TFLOPS for next-token baselines) may restrict its applicability to resource-constrained settings. The paper lacks analysis of how hyperparameters (e.g., context window size) affect performance across diverse tasks. Additionally, the theoretical justification for the entropy-based correction in Equation 8 is insufficiently explained."
          },
          "questions": {
            "value": [
              "How does the computational overhead scale with larger context windows or longer sequences, and what are the practical limitations for real-world deployment?",
              "What specific aspects of code generation tasks (e.g., syntax constraints, long-range dependencies) cause the performance drop, and how might CORAL be adapted to address these challenges?",
              "The rebuttal mentions a two-stage training protocol for AR baselines. How does this affect the fairness of comparisons with other methods, and what are the trade-offs between training complexity and final performance?",
              "Can the target-aware RoPE be generalized to other architectures or tasks beyond the ones tested, and what are the theoretical guarantees for its effectiveness?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Context-Wise Order-Agnostic Language Modeling (COrAL), a method that integrates iterative refinement into large language models (LLMs) by modeling token dependencies within context windows. It proposes sliding blockwise order-agnostic decoding to enable parallel refinement, achieving speedups and accuracy gains on reasoning tasks while highlighting trade-offs in code generation."
          },
          "strengths": {
            "value": "COrAL presents a novel approach to order-agnostic modeling that combines iterative refinement with context-aware token dependencies, addressing limitations of autoregressive (AR) models. The method demonstrates significant empirical improvements on reasoning tasks (e.g., +4.6% on GSM8K) and introduces a modified RoPE for target-aware decoding. The two-stage training strategy and sliding block decoding framework are technically sound and well-justified. The paper also acknowledges limitations, such as code generation performance drops, showing self-awareness."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough comparison with existing non-autoregressive (NAR) methods, making it difficult to assess relative improvements. The computational overhead (e.g., 5.48 TFLOPS vs. 2.81 TFLOPS for next-token baselines) is not fully contextualized, and the trade-off between accuracy and code generation quality remains unresolved. The theoretical analysis is minimal, and the generalizability to tasks like code generation is limited without further exploration. The rebuttal addresses some concerns but does not fully resolve the computational cost and task-specific limitations."
          },
          "questions": {
            "value": "1. How does COrAL handle the trade-off between accuracy and code generation quality? Can the authors provide specific examples of where the order-agnostic outputs fail syntactically? 2. What is the exact computational overhead in terms of memory usage and inference latency for real-world deployment? 3. How does the two-stage training protocol compare to pretraining-based approaches for order-agnostic modeling? 4. Are there plans to extend COrAL to tasks like instruction following or dialogue generation, and what challenges might arise? 5. How does the sliding block decoding strategy scale to longer sequences beyond the 512-token limit tested in the experiments?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0OB3RVmTXE": {
    "paper_id": "0OB3RVmTXE",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper identifies a critical vulnerability in diffusion models called 'concept resurgence,' where fine-tuning a model after unlearning certain concepts can unintentionally reintroduce those concepts. The authors demonstrate this phenomenon through experiments on Stable Diffusion v1.4 using MACE (Mass Concept Erasure) and highlight the risks for model safety and alignment."
          },
          "strengths": {
            "value": "The paper addresses a timely and important problem in AI safety, focusing on the fragility of incremental model updates. The systematic experiments with MACE and Stable Diffusion provide concrete evidence of concept resurgence. The work introduces a novel vulnerability (concept resurgence) and includes quantitative metrics (e.g., GCD accuracy, CLIP scores) to evaluate its impact. The paper is well-structured and contextualizes its contributions within existing literature on machine unlearning and diffusion models."
          },
          "weaknesses": {
            "value": "The paper is truncated, limiting the depth of experimental analysis and theoretical discussion. Key details about the fine-tuning datasets, ablation studies, and mechanisms driving concept resurgence are missing. The paper lacks a thorough investigation into why resurgence occurs, such as whether it stems from model architecture, training dynamics, or data leakage. Additionally, the evaluation of object erasure tasks is underdeveloped compared to celebrity erasure, and the paper does not address potential mitigations for the identified vulnerability."
          },
          "questions": {
            "value": "1. How were the fine-tuning datasets selected, and what specific characteristics (e.g., domain, diversity) contributed to concept resurgence? 2. Were ablation studies conducted to isolate the impact of unlearning algorithms versus fine-tuning data? 3. What are the underlying mechanisms causing resurgence—e.g., model capacity, optimization dynamics, or data overlap? 4. How do the authors plan to address this vulnerability in future work, given the paper's withdrawal?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper identifies a critical vulnerability in text-to-image diffusion models called 'concept resurgence,' where previously unlearned concepts (e.g., specific celebrities or objects) reappear after fine-tuning on unrelated data. The authors demonstrate this phenomenon through experiments with Stable Diffusion v1.4 and the MACE unlearning method, highlighting risks for model safety and alignment. They systematically investigate the conditions and mechanisms behind concept resurgence, emphasizing the fragility of compositional model updates."
          },
          "strengths": {
            "value": "The paper addresses a timely and important problem in AI safety, particularly for generative models used in real-world applications. The experiments are well-designed, with clear metrics (e.g., CLIP accuracy, GCD) to quantify resurgence. The focus on compositional updates aligns with practical deployment scenarios, and the connection to GDPR compliance adds relevance. The work also builds on prior unlearning methods (e.g., MACE) while extending their limitations, showing technical rigor."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed analysis of why concept resurgence occurs, relying heavily on empirical results without theoretical insights. The experiments are limited to Stable Diffusion v1.4 and MACE, reducing generalizability. The mechanism behind resurgence (e.g., weight interference, representation collapse) is not thoroughly explored. Additionally, the rebuttal indicates unresolved concerns, suggesting potential gaps in methodology or claims."
          },
          "questions": {
            "value": "1. What specific architectural or training dynamics in diffusion models enable concept resurgence? 2. How do the results generalize to other unlearning methods beyond MACE? 3. Are there mitigation strategies proposed, or is the focus solely on characterization? 4. How were hyperparameters chosen for fine-tuning, and what sensitivity analysis was performed? 5. Could the resurgence be attributed to dataset biases or model capacity constraints?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper identifies a critical vulnerability in diffusion models called 'concept resurgence,' where fine-tuning after unlearning reintroduces previously erased concepts. The authors demonstrate this phenomenon through systematic experiments on Stable Diffusion v1.4 using MACE for unlearning, showing that even benign fine-tuning on unrelated data can cause resurgence. They analyze factors like the generality of erased concepts and regularization during unlearning."
          },
          "strengths": {
            "value": "The paper makes a significant contribution by uncovering a novel vulnerability in diffusion model updates, which has important implications for safety and alignment. The experiments are well-structured, with clear benchmarks for celebrity and object erasure. The work is timely, addressing a critical gap in understanding the composability of incremental updates. The clarity of the problem statement and figures (e.g., Figure 1) effectively communicates the core issue."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough analysis of the underlying mechanisms driving concept resurgence. While it identifies correlations (e.g., between concept generality and resurgence), it does not provide a theoretical explanation or causal analysis. The experiments are limited to Stable Diffusion and MACE, leaving open questions about generalizability to other models or unlearning methods. The rebuttal indicates unresolved issues, suggesting insufficient depth in addressing the problem's complexity."
          },
          "questions": {
            "value": "1. What specific architectural or training dynamics cause concept resurgence? 2. How do different unlearning algorithms (beyond MACE) interact with fine-tuning to influence resurgence? 3. Are there ways to mitigate resurgence without compromising model performance on unrelated tasks? 4. How do the results generalize to other modalities (e.g., text or audio) or non-English datasets?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0QZcoGdmtJ": {
    "paper_id": "0QZcoGdmtJ",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces an efficient auditing procedure for $f$-differential privacy (DP) that requires only a single run of the mechanism, addressing computational inefficiencies in existing methods. It leverages the $f$-DP curve for tighter empirical privacy estimates and proposes a recursive analysis to bound adversary success rates, validated on Gaussian mechanisms and DP-SGD."
          },
          "strengths": {
            "value": "The work addresses a critical gap in privacy auditing by enabling single-run evaluation, which is computationally efficient. The use of $f$-DP curves provides a more nuanced privacy analysis than traditional $\\epsilon, \\delta$ parameters. The recursive bound analysis is technically novel, and experiments demonstrate superior performance over prior methods. The paper also clarifies ambiguities in the literature, such as the limitations of $(\\epsilon, \\delta)$-DP for mechanisms like the Gaussian mechanism."
          },
          "weaknesses": {
            "value": "The theoretical analysis of the recursive bounds lacks detailed justification for the convexity and Jensen’s inequality application. The experimental section is limited in scope, with unclear comparisons between the proposed method and Steinke et al. (2023) (e.g., Figure 1 vs. Figure 7). The paper does not provide a complete code artifact, though code snippets are added in the appendix. The claim about tighter bounds with more canaries is not thoroughly explained, relying on the $f$-curve analysis without concrete examples."
          },
          "questions": {
            "value": "1. How is the $f$-DP curve determined for real-world mechanisms? Is it derived from theoretical properties or empirical data? 2. The recursive analysis assumes convexity of trade-off functions—what guarantees ensure this holds for practical $f$-DP curves? 3. The rebuttal mentions 'combinatorial analysis' but does not elaborate on the double-counting argument. Could this introduce errors in the bound derivation? 4. How does the method handle mechanisms with non-convex $f$-DP curves?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces an efficient auditing procedure for f-differential privacy (f-DP) that requires only a single training run of a mechanism, addressing computational inefficiencies in prior methods. The approach leverages the f-DP curve for tighter privacy estimates compared to traditional (ε, δ) parameters, with theoretical analysis and experiments on Gaussian mechanisms and DP-SGD demonstrating improved accuracy."
          },
          "strengths": {
            "value": "The paper presents a novel and computationally efficient method for auditing f-DP, which generalizes traditional DP parameters and offers finer-grained privacy analysis. The technical contributions, including a recursive bound on adversary success and combinatorial analysis, are original and address limitations in prior work. The experiments show clear improvements over Steinke et al. (2023), and the rebuttal clarifies key points, enhancing the paper's credibility."
          },
          "weaknesses": {
            "value": "The paper lacks code in the initial submission, hindering reproducibility, though the authors added code snippets in the appendix. The experimental scope is limited to Gaussian mechanisms and DP-SGD, with insufficient discussion of broader applicability. The theoretical analysis, while sound, assumes familiarity with advanced DP concepts, which may reduce accessibility. Some sections (e.g., the role of trade-off function ordering) require further clarification."
          },
          "questions": {
            "value": "How does the computational overhead of the proposed method compare to Steinke et al. (2023)? What factors influence the choice of trade-off functions in practice? Could the recursive analysis be extended to non-i.i.d. data or other DP mechanisms? The authors should elaborate on the practical implications of their tighter bounds and address potential limitations in real-world scenarios."
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces an efficient single-run auditing procedure for f-differential privacy (f-DP) that leverages the f-DP curve to provide tighter empirical privacy estimates compared to existing methods. The approach addresses limitations of prior work, such as the suboptimal auditing of mechanisms like the Gaussian mechanism, by analyzing the entire privacy curve rather than relying on fixed ε, δ parameters. The authors validate their method on Gaussian mechanisms and DP-SGD, demonstrating improved performance over Steinke et al. (2023)."
          },
          "strengths": {
            "value": "The paper's originality lies in its single-run auditing framework and the use of f-DP curves for more granular privacy analysis. The technical contributions include a novel recursive bound for adversarial success probabilities and a generalized auditing strategy for canary injection. The experiments show empirical improvements over prior work, and the rebuttal addresses key clarifications (e.g., defining 'one run' and fixing references). The paper also provides a rigorous theoretical foundation for f-DP auditing, with clear connections to existing privacy definitions."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient experimental detail to fully validate the claims. For instance, the tightness of results in Figure 7 (revised Figure 11) is not convincingly explained, and the comparison with Steinke et al. (2023) remains unclear in some cases. The theoretical analysis, while detailed, does not explicitly address how the recursive bound improves over prior work. Additionally, the code artifacts are limited to appendices, raising concerns about reproducibility. The paper also does not thoroughly discuss the practical implications of using f-DP curves in real-world scenarios."
          },
          "questions": {
            "value": "1. How does the recursive bound in the paper compare to the tail bounds used in Steinke et al. (2023) in terms of theoretical guarantees? 2. The rebuttal mentions that the authors fixed the 'gubernatorial analysis' typo, but the revised section still requires clarification on the combinatorial analysis. 3. The paper claims tighter empirical estimates but does not provide a detailed analysis of how the f-DP curve's properties directly contribute to this improvement. 4. What are the computational costs of the proposed algorithm compared to existing methods, especially for large-scale models?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0R3ha8oNPU": {
    "paper_id": "0R3ha8oNPU",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces SECCodePLT, a unified platform for evaluating the security risks of code generation AI models. It addresses limitations of existing benchmarks by combining expert-verified data with automated generation, dynamic testing, and real-world cyberattack scenarios. The platform includes two main components: a secure coding benchmark with test cases and a cyberattack helpfulness benchmark with executable attack environments. Experiments show SECCodePLT outperforms existing benchmarks in identifying security risks, including revealing vulnerabilities in the Cursor code agent."
          },
          "strengths": {
            "value": "The paper presents a novel, comprehensive approach to code generation security evaluation by addressing key gaps in existing benchmarks. The methodology combines expert verification with scalable automated generation, ensuring data quality while enabling large-scale testing. The inclusion of dynamic metrics (e.g., test case execution) and real-world cyberattack scenarios significantly improves evaluation accuracy compared to static metrics. The practical application to a state-of-the-art code agent (Cursor) demonstrates the platform's real-world relevance. The paper also provides detailed technical descriptions of the data generation pipeline and evaluation metrics."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough comparison with all relevant benchmarks, particularly in terms of quantitative performance metrics. While the rebuttal addresses some gaps, the original manuscript does not fully clarify how SECCodePLT's dynamic evaluation environment is implemented or validated. The experiments section is somewhat brief, and the results could be more detailed (e.g., specific metrics for comparing with CYBERSEVAL). The paper also does not sufficiently discuss potential biases in the security-relevance judgment process or the limitations of the current approach. The rebuttal's addition of coverage metrics is helpful but does not fully resolve concerns about the completeness of the test cases."
          },
          "questions": {
            "value": "1. How exactly is the dynamic evaluation environment for cyberattacks implemented? What specific metrics are used to assess attack execution success? 2. What criteria were used to select the 27 CWEs, and how does this selection affect the platform's generalizability? 3. How are the security policy reminders integrated into the evaluation process, and what impact do they have on model behavior? 4. What are the limitations of the current data generation pipeline, and how might they affect the benchmark's effectiveness? 5. How do the authors ensure that the dynamic test cases cover a representative range of security scenarios?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces SECCODEPLT, a unified platform for evaluating the security risks of code generation AI models. It addresses limitations of existing benchmarks by combining expert verification with automated data generation, incorporating dynamic evaluation metrics, and covering both insecure coding and cyberattack helpfulness. The platform demonstrates superior performance in identifying security risks compared to existing benchmarks like CYBERSECEVAL."
          },
          "strengths": {
            "value": "The paper's originality lies in its hybrid approach to data creation, balancing scalability and quality. It comprehensively addresses two critical security dimensions (insecure coding and cyberattack helpfulness) and introduces dynamic evaluation metrics, which are more reliable than static methods. The experiments are thorough, comparing with SOTA models and revealing novel security risks in Cursor. The clarity of methodology and structure is strong, with detailed sections on data generation and evaluation."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient detail on the implementation of dynamic evaluation metrics, such as the attack environment and specific metrics used for cyberattack helpfulness. While the rebuttal addresses some issues (e.g., line coverage, diversity filters), the original paper does not fully elaborate on these aspects. The claim about identifying risks in Cursor requires stronger validation, and the comparison with LLMSecEval, though improved in the rebuttal, remains somewhat superficial."
          },
          "questions": {
            "value": "How is the cyberattack environment implemented, and what specific dynamic metrics are used to evaluate attack execution? What are the exact criteria for determining 'security relevance' in the manual verification process? How were the security risks in Cursor confirmed beyond the platform's metrics? The rebuttal's line coverage experiment is promising, but the original paper's methodology for dynamic testing remains under-specified."
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces SECCodePLT, a unified platform for evaluating the security risks of code generation AIs (GenAIs) in two key areas: insecure coding and cyberattack helpfulness. It addresses limitations of existing benchmarks by combining expert-verified data with automated generation, dynamic testing, and real-world attack scenarios. The platform demonstrates superior performance over existing benchmarks like CYBERSECEVAL and identifies security risks in the Cursor code agent."
          },
          "strengths": {
            "value": "The paper presents a comprehensive solution to a critical problem in code GenAI security. Key strengths include: (1) A novel methodology for generating high-quality, scalable data by combining expert input with automated mutators, ensuring security relevance and dynamic evaluation; (2) A clear focus on practical, real-world security scenarios (e.g., end-to-end cyberattacks) rather than static metrics; (3) Extensive experiments showing the platform's superiority over existing benchmarks and its ability to uncover risks in state-of-the-art models; (4) The application of the platform to a real-world code agent (Cursor) to identify non-trivial security issues, demonstrating practical impact."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical specifics about the dynamic evaluation environment for cyberattack helpfulness, such as how attack execution and metrics are implemented. While the rebuttal addresses some concerns (e.g., test coverage, redundancy filtering), the original work does not sufficiently justify the choice of metrics for security relevance or explain how the platform ensures robustness against adversarial prompts. Additionally, the comparison with LLMSecEval is not fully integrated into the main text, and the paper could better clarify how SECCodePLT's structured inputs (e.g., security policies) improve evaluation over existing benchmarks."
          },
          "questions": {
            "value": "1. How is the 'real environment' for cyberattack helpfulness implemented? What specific metrics are used to evaluate attack execution? 2. The rebuttal mentions 90.92% line coverage, but how does this translate to security-relevant test coverage (e.g., testing for specific vulnerabilities)? 3. How are security policies integrated into prompts, and what evidence supports their effectiveness in guiding models to avoid insecure code? 4. The paper claims to cover 27 CWEs, but what criteria were used to select these, and how do they compare to the 50 CWEs in Table 1? 5. What are the limitations of the redundancy filtering (e.g., LCS/Levenshtein threshold of 0.9), and how were these thresholds validated?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "0RHMnPj8no": {
    "paper_id": "0RHMnPj8no",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes improved differentially private (DP) algorithms for nonsmooth nonconvex optimization, achieving better sample complexity bounds for finding Goldstein-stationary points. It introduces a single-pass algorithm with a $\tilde{\\Omega}(1/(\\alpha\beta^3) + d/(\\varepsilon\\alpha\beta^2) + d^{3/4}/(\\varepsilon^{1/2}\\alpha\beta^{5/2}))$ sample complexity and a multi-pass algorithm with $\tilde{\\Omega}(d^{3/4}/(\\varepsilon\\alpha^{1/2}\beta^{3/2}))$, improving on prior work by Zhang et al. (2024). The paper also establishes generalization guarantees from empirical to population loss."
          },
          "strengths": {
            "value": "The paper makes a significant theoretical contribution by addressing a challenging problem in DP optimization. The use of Goldstein-stationarity is well-motivated for nonsmooth nonconvex settings. The single-pass and multi-pass algorithms improve upon existing sample complexity bounds, with the multi-pass approach achieving sublinear dimension dependence. The generalization analysis from ERM to population loss fills a critical gap in the literature. The paper is technically rigorous, with clear connections to prior work and a structured presentation."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation, making it difficult to assess practical relevance. The theoretical analysis assumes Lipschitz continuity but does not address potential limitations in real-world scenarios. The multi-pass algorithm's computational efficiency and practical feasibility are not discussed. The tree mechanism's explanation is somewhat unclear, and the rebuttal highlights typographical errors in the original submission. The tightness of the bounds remains unproven due to missing lower bounds in the private regime."
          },
          "questions": {
            "value": "1. How do the generalization bounds (Proposition 5.1) handle specific data distributions or function classes? 2. What are the practical implications of the multi-pass algorithm's polynomial-time complexity? 3. Can the results be extended to non-Lipschitz settings, or are there implicit assumptions? 4. How do the proposed algorithms compare to non-private baselines in terms of utility? 5. Are the claimed improvements in sample complexity achievable under realistic privacy constraints?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes improved differentially private (DP) algorithms for nonsmooth nonconvex optimization, achieving better sample complexity bounds for finding Goldstein-stationary points. It introduces a single-pass algorithm with dimension-dependent improvements and a multi-pass algorithm that leverages empirical risk minimization (ERM) to further reduce sample complexity. The work also establishes generalization guarantees from empirical to population loss."
          },
          "strengths": {
            "value": "The paper makes significant theoretical contributions by improving sample complexity bounds for DP nonsmooth nonconvex optimization, which is a critical area given the prevalence of nonconvex losses in machine learning. The use of randomized smoothing and the tree mechanism for privacy ensures rigorous DP guarantees. The generalization analysis from empirical to population loss is novel and addresses a key gap in prior work. The authors also clarify notational conventions to avoid confusion with standard DP terminology, enhancing readability."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation, which limits the practical relevance of the theoretical results. The absence of lower bounds for the private terms makes it unclear whether the proposed improvements are tight. The analysis assumes Lipschitz continuity of the component functions, but the paper does not discuss how violations of this assumption might affect the results. Additionally, the multi-pass algorithm's dependence on polynomial-time ERM raises questions about computational feasibility in high-dimensional settings."
          },
          "questions": {
            "value": "1. Are the proposed algorithms implementable in practice, and what are the computational bottlenecks for high-dimensional data? 2. How do the generalization guarantees hold under non-i.i.d. or adversarial data distributions? 3. Can the theoretical bounds be extended to other notions of stationarity (e.g., Clarke stationarity) or weaker privacy guarantees? 4. What are the practical implications of the dimension-dependent terms in the sample complexity, especially for large-scale machine learning tasks?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper presents improved algorithms for differentially private (DP) optimization of non-smooth non-convex (NSNC) objectives, achieving tighter sample complexity bounds than prior work. It introduces a single-pass algorithm with reduced dimension-dependent terms and a multi-pass algorithm leveraging empirical risk minimization (ERM) to further improve bounds. The work also establishes generalization guarantees from empirical to population loss, addressing a gap in the literature."
          },
          "strengths": {
            "value": "The paper demonstrates strong theoretical novelty by improving sample complexity for DP NSNC optimization, particularly the single-pass algorithm's dimension-dependent terms. The multi-pass approach with ERM and generalization analysis from empirical to population loss are significant contributions. The use of randomized smoothing and tree mechanisms for DP is well-justified. The paper also provides zero-order and first-order variants, enhancing practical applicability. The clarity of the problem formulation and comparison to prior work is strong."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation to support theoretical claims, which weakens the practical impact. The generalization analysis from empirical to population loss relies on assumptions that are not thoroughly justified. The tree mechanism's description contains technical issues (e.g., typos) that were corrected in the rebuttal but may have affected the original paper's rigor. The absence of lower bounds to establish tightness of the proposed bounds is a critical gap. The analysis of gradient estimator sensitivity and its impact on privacy guarantees is not fully detailed."
          },
          "questions": {
            "value": "1. How do the authors ensure the tightness of their sample complexity bounds, given the lack of lower bounds in the private regime? 2. What are the specific challenges in reducing gradient estimator sensitivity, and how do the proposed methods address them? 3. Are there empirical results demonstrating the practical effectiveness of the algorithms, particularly in high-dimensional settings? 4. How does the tree mechanism's implementation affect the overall privacy-utility trade-off in the proposed algorithms? 5. Can the generalization analysis be extended to other loss functions or settings beyond the current assumptions?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0RUQmLFF1D": {
    "paper_id": "0RUQmLFF1D",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces Concept2Concept, a framework for auditing text-to-image (T2I) models by analyzing concept associations in generated images. The approach involves extracting high-level concepts from generated images using visual grounding models and characterizing their distributions to detect biases, harmful content, and alignment issues. The framework is demonstrated through case studies and implemented as an open-source interactive tool."
          },
          "strengths": {
            "value": "The paper presents a novel framework for systematic auditing of T2I models, emphasizing interpretability and scalability. The use of visual grounding models for concept extraction is well-justified, and the case studies (e.g., detecting CSAM in datasets) highlight practical utility. The interactive visualization tool enhances accessibility for non-technical users. The framework's flexibility in handling user-defined and empirical prompt distributions addresses a critical gap in existing auditing methods."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough comparison with existing methods like TBYB and CUPID, making it unclear how Concept2Concept improves upon them. The experiments on closed-source models are limited to a single model (ChatGPT) and do not fully leverage the framework's claimed model-agnosticity. The choice of object detectors (e.g., Florence 2, BLIP VQA) is not critically evaluated for its impact on results. Additionally, the paper does not address how the framework handles ambiguous or overlapping concepts."
          },
          "questions": {
            "value": "1. How does the framework handle concept ambiguity or overlapping concepts in images? 2. What are the limitations of using object detectors for concept extraction, and how do these affect the reliability of the analysis? 3. How generalizable is the framework across different T2I architectures, especially closed-source models? 4. Can the framework detect subtle biases that are not captured by co-occurrence metrics?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces Concept2Concept, a framework for auditing text-to-image (T2I) models by analyzing concept associations in generated images. It uses visual grounding models to extract concepts, computes metrics like frequency, stability, and co-occurrence, and provides an interactive visualization tool. The framework is applied to audit models and synthetic datasets, revealing biases and harmful content."
          },
          "strengths": {
            "value": "The framework addresses a critical gap in auditing T2I models by focusing on concept-level associations rather than just social biases. The use of visual grounding for concept extraction is innovative, and the interactive tool makes the analysis accessible to non-experts. The experiments demonstrate practical utility in uncovering hidden biases and harmful content. The paper also highlights important real-world applications, such as auditing synthetic datasets for RLHF alignment."
          },
          "weaknesses": {
            "value": "The paper lacks depth in explaining how the framework handles ambiguous or abstract concepts. The choice of object detectors (Florence 2 and BLIP VQA) is not thoroughly justified, and their limitations (e.g., detection confidence, probabilistic outputs) are not fully addressed. Experiments on closed-source models are limited to ChatGPT, which may not generalize. The scalability of the framework for large-scale or complex prompts is not explored. The rebuttal acknowledges these issues but does not fully resolve them."
          },
          "questions": {
            "value": [
              "How does the framework handle ambiguous or abstract concepts that may not be well-represented by object detectors?",
              "What are the specific limitations of the chosen object detectors (Florence 2 and BLIP VQA) in different scenarios, and how can they be mitigated?",
              "Can the framework be adapted to other types of models or tasks beyond T2I generation?",
              "How does the interactive tool perform with real-time user inputs or large datasets?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Concept2Concept, a framework for auditing text-to-image (T2I) models by analyzing concept associations in generated images. The approach involves extracting high-level concepts using object detectors and summarizing their distributions to identify biases, harmful content, and other issues. The framework is applied to real-world datasets, revealing cases of child-sexual abuse material (CSAM) and misaligned classes, and is accompanied by an open-source interactive visualization tool."
          },
          "strengths": {
            "value": "The paper presents a novel framework for auditing T2I models by focusing on concept associations, addressing a critical gap in understanding how prompts map to generated content. The experiments are comprehensive, demonstrating practical applications in detecting biases and harmful content. The interactive tool enhances usability for non-technical users, and the work contributes to the broader discourse on T2I model safety and fairness. The methodology is well-structured, with clear metrics for concept frequency, stability, and co-occurrence."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing auditing methods, such as TIBET or CUPID, beyond superficial mentions. The evaluation of object detectors (e.g., Florence 2, BLIP VQA) is insufficient, with no analysis of their impact on concept extraction accuracy. The experiments on closed-source models (e.g., ChatGPT) are limited and not representative of the broader landscape. The framework's scalability and generalizability to other domains or complex concepts are not thoroughly discussed. Additionally, the rebuttal mentions new models (e.g., Lumina-Next SFT, SD3), but these are not included in the main paper, leaving gaps in the evaluation."
          },
          "questions": {
            "value": "1. How does the choice of object detector (e.g., Florence 2 vs. BLIP VQA) affect the robustness of concept extraction? Are there ablation studies to validate this? 2. The closed-source model experiments with ChatGPT are limited—could the framework be adapted to other closed-source models with API access? 3. How does the framework handle ambiguous or context-dependent concepts (e.g., 'man' vs. 'person')? 4. Are there plans to evaluate the framework on larger-scale datasets or more diverse T2I architectures?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0Th6bCZwKt": {
    "paper_id": "0Th6bCZwKt",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes GMM-GDA, a graph data augmentation method leveraging Gaussian Mixture Models (GMMs) to enhance the generalization of Graph Neural Networks (GNNs). It introduces a theoretical framework based on Rademacher complexity to analyze generalization error and demonstrates the effectiveness of GMM-based augmentation in improving performance on graph classification tasks."
          },
          "strengths": {
            "value": "The paper presents a rigorous theoretical analysis using Rademacher complexity to bound generalization error, which is a significant contribution to understanding GNN generalization. The proposed GMM-GDA method is computationally efficient and demonstrates competitive performance against existing augmentation techniques. The clarity of the mathematical formulations and the structured presentation of the framework and experiments are strong. The work addresses a critical challenge in GNNs—generalization to out-of-distribution data—and offers a novel approach to graph augmentation."
          },
          "weaknesses": {
            "value": "The paper lacks a comprehensive comparison with alternative generative methods (e.g., GANs or VAEs) for graph augmentation, which limits the understanding of GMM-GDA's relative advantages. While the theoretical analysis is sound, the practical implications of the Rademacher complexity bound and the assumption of Lipschitz continuity are not thoroughly discussed. The experiments focus on a limited set of datasets and hyperparameters (e.g., K values for GMMs), and the ablation studies could be more extensive. Additionally, the paper does not address potential limitations of GMMs in modeling complex graph distributions, such as sparsity or hierarchical structures."
          },
          "questions": {
            "value": "1. How does GMM-GDA compare to other generative approaches (e.g., GANs, VAEs) in terms of augmentation quality and computational efficiency? 2. What are the specific scenarios where GMMs outperform or underperform compared to alternative methods? 3. How sensitive is the performance to the choice of K (number of Gaussian components) across different graph datasets? 4. Can the theoretical bound be tightened or adapted to account for non-Lipschitz losses? 5. Are there cases where the augmentation-induced distribution diverges significantly from the true data distribution, and how is this mitigated?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes GMM-GDA, a graph data augmentation method leveraging Gaussian Mixture Models (GMMs) to improve the generalization of Graph Neural Networks (GNNs). The work introduces a theoretical framework analyzing generalization error through Rademacher complexity and demonstrates the effectiveness of GMM-based augmentation in enhancing training diversity while maintaining computational efficiency."
          },
          "strengths": {
            "value": "The paper makes significant contributions by establishing a rigorous theoretical connection between data augmentation and GNN generalization via Rademacher complexity. The proposed GMM-GDA method is computationally efficient compared to prior approaches like G-Mixup, and the experimental results on benchmark datasets validate its effectiveness. The clarity of mathematical formalism and the structured presentation of the theoretical analysis are notable strengths."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with state-of-the-art augmentation methods beyond a few baselines, which limits the assessment of GMM-GDA's relative advantages. The theoretical analysis assumes Lipschitz continuity of the loss function, which may not hold for all practical scenarios. Additionally, the paper does not thoroughly address how GMM-GDA performs on graphs with highly heterogeneous structures or varying node/edge distributions."
          },
          "questions": {
            "value": "1. How does GMM-GDA compare to other GMM-based augmentation methods in terms of performance and computational cost? 2. Are there specific graph characteristics (e.g., sparsity, heterogeneity) where GMM-GDA excels or struggles? 3. How sensitive is the method to hyperparameters like the number of Gaussian components (K) across different datasets?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces GMM-GDA, a graph data augmentation method leveraging Gaussian Mixture Models (GMMs) to enhance GNN generalization. It provides a theoretical framework using Rademacher complexity to analyze generalization error and demonstrates improved performance over existing techniques."
          },
          "strengths": {
            "value": "The paper offers a rigorous theoretical analysis of graph augmentation via Rademacher complexity, which is a novel contribution to understanding GNN generalization. The GMM-GDA method is computationally efficient and demonstrates empirical effectiveness. The work also addresses the scalability of augmentation techniques for large graphs, which is significant for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with state-of-the-art generative models (e.g., GANs, VAEs) for graph augmentation, which limits the assessment of GMMs' superiority. The theoretical analysis in Theorem 3.1 has unresolved issues regarding the upper bound on Rademacher complexity, and the experimental validation is limited to a small set of datasets. The hyperparameter sensitivity analysis is superficial, with minimal discussion of how K (number of Gaussians) affects performance across different tasks."
          },
          "questions": {
            "value": "1. How does GMM-GDA ensure it captures the true distribution of graph embeddings compared to other generative models? 2. What is the computational complexity of GMM-GDA relative to existing methods, and how does it scale to very large graphs? 3. Are there specific scenarios where GMM-GDA fails, and how does this relate to the theoretical guarantees provided?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0XT3Lg6S2Q": {
    "paper_id": "0XT3Lg6S2Q",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces AdaWarp, a novel neural network module for medical image registration that leverages the piece-wise smooth (P-S) physical prior through a differentiable bilateral grid. The method aims to balance accuracy and efficiency by incorporating edge-preserving low-frequency approximations of deformation fields, demonstrated on two multimodal datasets."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in learning-based registration by systematically integrating a physical prior (P-S Assumption) into a neural architecture. The differentiable bilateral grid design is innovative, combining traditional image processing concepts with modern deep learning. The experiments show competitive performance across datasets, and the modular design of AdaWarp is well-explained. The rebuttal clarifies the flexibility of AdaWarp as a plug-and-play module, enhancing its practical relevance."
          },
          "weaknesses": {
            "value": "The literature review initially lacked recent multi-scale methods, though the authors added references to CorrMLP and RDP in the rebuttal. The comparison with existing methods could be more thorough, particularly in demonstrating the P-S prior's unique advantages. The paper does not fully address how AdaWarp's performance scales with varying deformation complexities. The experimental analysis of accuracy-efficiency trade-offs is somewhat superficial, with limited ablation studies on the bilateral grid's components."
          },
          "questions": {
            "value": "1. How does AdaWarp's performance compare to methods explicitly designed for large deformations (e.g., LapIRN or VTN)? 2. Can the authors provide ablation studies on the impact of the guidance map and bilateral grid parameters? 3. What are the computational costs of AdaWarp compared to baselines, and how does this align with the claimed efficiency gains? 4. How does the flexibility of AdaWarp as a module affect its performance when integrated with different backbones?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes AdaWarp, a neural network module for medical image registration that incorporates a piece-wise smooth (P-S) physical prior through a differentiable bilateral grid. It aims to balance accuracy and efficiency by leveraging low-resolution feature maps and edge-preserving filtering, demonstrating superior performance on two medical imaging datasets."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in learning-based registration by integrating a physical prior (P-S assumption) in an end-to-end manner, which is a novel contribution. The methodology is well-structured, with clear explanations of the bilateral grid and its differentiable operations. The experiments show competitive results on two datasets, and the rebuttal adds relevant comparisons to recent methods like CorrMLP and RDP. The flexibility of AdaWarp as a module is emphasized, with examples of integration into other frameworks."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of computational efficiency, which is a key claim. The experiments are limited to two datasets, and the ablation studies on the bilateral grid's design (e.g., kernel choices, range dimension) are insufficient. The rebuttal acknowledges that AdaWarp's performance varies across datasets (e.g., Ada-Res vs. Ada-Cost), raising concerns about generalizability. The connection between the P-S assumption and the bilateral grid's effectiveness is not thoroughly justified."
          },
          "questions": {
            "value": "1. How does AdaWarp's computational complexity compare to baselines like VoxelMorph or RDP? 2. What ablation studies were conducted to validate the choice of kernel functions or range dimension in the bilateral grid? 3. How does the guidance map generator ensure robustness across diverse anatomical structures and modalities? 4. Are there theoretical guarantees that the P-S assumption directly improves registration accuracy, or is this empirically observed?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces AdaWarp, a novel neural network module for medical image registration that incorporates a piece-wise smooth (P-S) prior through a differentiable bilateral grid. The method aims to balance accuracy and efficiency by leveraging low-resolution feature maps and edge-preserving filtering, demonstrating superior performance on two multi-modality datasets compared to existing methods."
          },
          "strengths": {
            "value": "The paper presents a well-motivated approach by integrating the P-S prior into learning-based registration, which addresses a gap in prior work. The differentiable bilateral grid design is innovative, and the experimental results on diverse datasets show improvements in accuracy-efficiency trade-offs. The clarity of the problem statement and the connection to physical priors are strong. The rebuttal also adds relevant comparisons with recent methods, enhancing the paper's credibility."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient ablation studies to validate the effectiveness of individual components (e.g., the guidance map generator or bilateral grid). The experimental comparisons are limited to a few baselines, and the rebuttal's added results (e.g., CorrMLP, RDP) are not thoroughly analyzed in the main text. The generalizability of AdaWarp across different network backbones and datasets is not fully demonstrated, despite the rebuttal's claims. Additionally, the theoretical justification for the P-S assumption in the context of deep learning remains underexplored."
          },
          "questions": {
            "value": "1. How does the guidance map generator explicitly encode local intensity differences to enforce the P-S prior? 2. What ablation studies were conducted to isolate the contributions of the bilateral grid versus other components? 3. How does AdaWarp handle extreme deformations where the P-S assumption might fail? 4. Are the added comparisons (CorrMLP, RDP) statistically significant, and how do they align with the paper's claims? 5. What are the computational costs of AdaWarp compared to baselines, and how does it scale to 3D volumetric data?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "0Yfjerm9Zp": {
    "paper_id": "0Yfjerm9Zp",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes a dual-reward probabilistic inference framework to enhance the faithfulness of LLM-generated rationales by incorporating domain-specific proposal distributions and both local/global rewards. The method aims to improve logical coherence and contextual alignment without sacrificing computational efficiency."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in LLM interpretability—faithful rationales—which is increasingly important for safety-critical applications. The method introduces novel reward mechanisms (local and global) that integrate domain-specific knowledge, demonstrating empirical improvements across multiple tasks. The connection to Feynman-Kac models provides a theoretically grounded approach, and the experiments show consistent gains in accuracy and faithfulness."
          },
          "weaknesses": {
            "value": "The paper lacks detailed descriptions of how the domain-specific expert model is trained and what data it uses, which limits reproducibility. The construction of the proposal distribution is not clearly explained, and the faithfulness evaluation metrics remain under-specified. Additionally, the ablation studies are insufficient to isolate the contributions of local vs. global rewards. The rebuttal addresses some references and Figure 2, but core methodological details (e.g., expert model training) remain vague."
          },
          "questions": {
            "value": "1. How is the domain-specific expert model trained? What datasets or knowledge sources are used? 2. Can the authors clarify the exact mechanism for constructing the proposal distribution? 3. What specific metrics are used to quantify 'faithfulness' in the experiments? 4. Are there ablation studies demonstrating the individual impact of local vs. global rewards?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes a probabilistic inference framework called Dual-Reward Probabilistic Inference to improve the faithfulness of LLM-generated rationales. The method uses domain-specific proposal distributions and integrates local and global rewards to enhance context coherence and accuracy while maintaining computational efficiency similar to beam search."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in LLMs—faithful rationales—which is essential for interpretability and safety. The dual-reward mechanism is novel, combining local (token-level) and global (context-level) constraints to guide generation. The method is computationally efficient, with experiments showing 33% accuracy improvement and 10% faithfulness gains across three tasks. The structure is clear, with detailed algorithm descriptions and theoretical grounding in Feynman-Kac models."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with state-of-the-art faithfulness methods like constrained decoding or expert model ensembles. The explanation of the Feynman-Kac framework and its integration into LLMs is superficial, making the method's novelty and mechanics hard to assess. Experimental details are sparse (e.g., no ablation studies, unclear metrics for 'faithfulness'). The rebuttal addresses some issues, but the original submission had weak related work and insufficient justification for the proposed approach."
          },
          "questions": {
            "value": "1. How do the local and global rewards specifically interact to improve faithfulness? 2. Are there ablation studies showing the contribution of each reward component? 3. What is the exact mechanism of the domain-specific proposal distribution, and how is it trained? 4. How does the method generalize across domains with varying levels of expert knowledge? 5. The paper mentions 'domain-specific expert models' but does not compare against approaches that use similar strategies (e.g., expert model ensembles)."
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a dual-reward probabilistic inference framework to enhance the faithfulness of LLM-generated rationales by incorporating domain-specific proposal distributions and integrating local/global rewards. It claims improvements in accuracy and faithfulness across three reasoning tasks without significant computational overhead."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to faithfulness by combining local and global rewards, which addresses a critical gap in LLM interpretability. The use of domain-specific experts and the Feynman-Kac framework shows creative problem formulation. The experiments span multiple tasks, and the method's computational efficiency is highlighted. The rebuttal addresses some concerns, such as adding references and clarifying Figure 2."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with strong baselines (e.g., RLHF, DPO, or explicit MCTS). The domain-specific proposal distribution and expert model are not well-defined (e.g., how they are trained or integrated). The experiments on other backbones (Mistral 7B) are relegated to an appendix, and the computational cost analysis is vague. The theoretical justification for the Feynman-Kac model's application is insufficient, and the explanation of Algorithm 1 is incomplete."
          },
          "questions": {
            "value": "1. How is the domain-specific proposal distribution constructed? Is it a pre-trained model or fine-tuned? 2. What is the exact role of the 'local expert' and 'global expert' in the reward mechanism? 3. How are the 'local mask' and 'global reward' functions implemented in practice? 4. Why is the computational cost comparison with beam search not explicitly quantified? 5. Are the results on Mistral 7B (Appendix B1) statistically significant?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0YkZe9nwiC": {
    "paper_id": "0YkZe9nwiC",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces SIGnAL, a framework that combines reinforcement learning (RL) with active learning to generate and select informative data instances. The method uses a generative model to create synthetic data, which is then evaluated for informativeness and relevance using a novel acquisition function. The framework is validated on text classification tasks, showing effectiveness when original data is limited."
          },
          "strengths": {
            "value": "The paper presents a novel approach by integrating RL with active learning for data generation, addressing limitations of traditional pool-based methods. The acquisition function that balances informativeness and relevance is well-motivated and theoretically grounded. The experiments on text classification tasks demonstrate practical utility, and the framework's generality is highlighted. The use of PPO for optimization is a strong methodological choice."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with text-specific query-synthesizing methods, which could undermine the novelty claim. The acquisition function's design, while inspired by CAL, is not sufficiently differentiated from prior work. The experiments do not address synthetic data quality (e.g., redundancy from LLMs) or ablation studies on key components. The theoretical justification for the KL divergence-based relevance metric is underdeveloped, and the paper does not discuss how the framework adapts to domain shifts."
          },
          "questions": {
            "value": "How does SIGnAL handle domain-specific challenges in text generation (e.g., semantic coherence)? What is the impact of prompt size and in-context examples on performance? Are there cases where synthetic data negatively affects the classifier? How is the KL divergence metric adapted for text data, given its discrete nature? What ablation studies could validate the contribution of the acquisition function versus RL optimization?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes SIGnAL, a reinforcement learning (RL)-based framework for generative active learning that combines data generation and selection. It introduces an acquisition function that balances informativeness and relevance, using KL divergence and nearest-neighbor distances. The method is validated on text classification tasks with limited data."
          },
          "strengths": {
            "value": "The paper addresses a relevant problem in active learning by integrating RL for dynamic informativeness optimization, which is a novel approach. The acquisition function that combines informativeness and relevance is well-motivated. The framework's alignment with large language models (LLMs) and practical experiments on text tasks demonstrate its applicability. The method's theoretical grounding in RL and PPO provides a solid foundation."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with existing query-synthesizing methods, particularly in non-text domains. The experiments are limited to text classification, which restricts the assessment of the framework's generalizability. The KL divergence-based acquisition function's implementation details and robustness are underexplored. The paper does not address how the framework handles data distribution shifts or scalability to larger datasets."
          },
          "questions": {
            "value": "How does the acquisition function perform under varying data distributions or task complexities? What ablation studies were conducted to validate the individual components of the acquisition function? How does SIGnAL compare to non-RL-based generative active learning methods in terms of efficiency and effectiveness? Are there specific LLMs or training configurations used that could impact reproducibility?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes SIGnAL, a framework that combines reinforcement learning (RL) with active learning to generate and select informative data instances for text classification tasks. The method uses an acquisition function that measures both informativeness (via KL divergence) and relevance (via latent space distance) to guide data generation, with RL optimizing the generative model through reward signals."
          },
          "strengths": {
            "value": "The paper introduces a novel RL-based approach to generative active learning, addressing limitations of pool-based methods in data-scarce scenarios. The acquisition function effectively combines informativeness and relevance, and the framework is generalizable to large language models. The methodology is well-structured, and the motivation for using RL is justified through clear problem analysis."
          },
          "weaknesses": {
            "value": "Experiments are limited to text classification tasks, with no comparison to existing query-synthesizing methods for text. The reward function's design lacks thorough validation, and computational costs of KL divergence and distance calculations are not addressed. The paper does not analyze how synthetic data quality degrades over iterations or provide ablation studies on the acquisition function's components."
          },
          "questions": {
            "value": "How does SIGnAL balance trade-offs between informativeness and relevance during training? What ablation studies were conducted on the acquisition function's components (e.g., KL divergence vs. distance metrics)? How does the method scale to larger datasets or non-text modalities? What is the computational overhead of the KL divergence and KNN steps?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0a7TRHhhcS": {
    "paper_id": "0a7TRHhhcS",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces a preference-driven spatial-temporal counting process model that integrates choice theory and social intelligence to capture human decision-making patterns in event data like crime or bike-sharing activities. It employs a mixture-of-experts framework with sparse gating functions to model latent utility functions and social influences, validated on real-world datasets."
          },
          "strengths": {
            "value": "The paper's originality lies in combining choice theory and social intelligence with spatial-temporal modeling, offering a novel framework for interpreting human-driven event data. The methodology is well-structured, leveraging established techniques like mixture-of-experts and α-entmax for sparsity. The experiments demonstrate competitive predictive performance on crime and bike-sharing datasets, and the model's interpretability through utility functions and sparse selections is a key strength. The paper also addresses the limitations of traditional models by explicitly incorporating social dynamics."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of key components like the sparse gating function or social influence modeling. The integration of social norms into the utility function is not sufficiently explained, and the theoretical justification for the α-entmax sparsity mechanism requires deeper analysis. While the rebuttal clarifies some aspects (e.g., embedding details), the role of social factors in driving preferences remains underexplored. Additionally, the comparison with state-of-the-art models is limited to quantitative metrics without discussing qualitative interpretability gains."
          },
          "questions": {
            "value": "1. How are social norms and mutual influences explicitly encoded in the utility function? Are there specific features or interactions that capture these factors? 2. What ablation studies were conducted to validate the contribution of the sparse gating mechanism versus the mixture-of-experts architecture? 3. The α-entmax function is central to the model, but the paper does not discuss hyperparameter sensitivity or how the choice of α/τ affects performance. 4. How does the model scale to larger or more heterogeneous datasets, and are there limitations in its generalizability?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes a preference-driven spatial-temporal counting process model that integrates choice theory and social intelligence to analyze human-generated event data. The approach uses a mixture-of-experts framework with sparse gating functions to model latent utility functions, capturing how social dynamics and individual preferences influence event distributions. The model is evaluated on crime and bike-sharing datasets, demonstrating competitive predictive performance."
          },
          "strengths": {
            "value": "The paper introduces a novel framework that bridges choice theory with spatial-temporal modeling, addressing a gap in traditional methods that overlook human decision-making factors. The use of mixture-of-experts and sparse gating functions for interpretability is innovative. Empirical results on real-world datasets show competitive performance, and the theoretical foundation is well-grounded in utility modeling and social intelligence. The paper also provides clear methodological details on the gating mechanism and sparsity patterns."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient technical depth in explaining spatial and positional embeddings, relying on references without detailed derivation. The comparison with baselines in the rebuttal (HintNet, STNSCM, UniST) was not included in the original submission, raising questions about the completeness of the experimental evaluation. The interpretability claims are not thoroughly validated with case studies or visualization of latent classes. Additionally, the hyperparameter selection (e.g., α, τ) and their impact on performance are under-explained."
          },
          "questions": {
            "value": "1. How are the spatial embeddings derived, and what specific features do they encode compared to the referenced work [1]? 2. Can the authors provide ablation studies to demonstrate the necessity of the sparse gating mechanism and the mixture-of-experts architecture? 3. What is the rationale behind the choice of α and τ, and how sensitive is the model to these hyperparameters? 4. How are the latent classes (H) interpreted in practice, and are there qualitative examples of their distinct preference patterns?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a preference-driven spatial-temporal counting process model that integrates choice theory and social intelligence to analyze human-generated event data (e.g., crime, bike-sharing). The approach uses a mixture-of-experts framework with sparse gating mechanisms to capture latent utility functions and social influences, aiming to improve interpretability and predictive accuracy."
          },
          "strengths": {
            "value": "The paper's originality lies in combining choice theory with social intelligence for spatial-temporal modeling, offering a novel framework for understanding human decision-making. The two-stage decision process (ranking + sparse selection) and mixture-of-experts architecture provide structured interpretability. The empirical evaluation on real-world datasets demonstrates competitive predictive performance, and the integration of social norms into utility functions is a significant contribution."
          },
          "weaknesses": {
            "value": "The paper lacks detailed theoretical analysis of the model's properties (e.g., convergence, identifiability). The explanation of spatial/temporal embeddings is vague, relying on a reference to Vision Transformers without sufficient justification. The comparison with baselines is limited, as some state-of-the-art models (e.g., UniST) outperform the proposed method in certain metrics. The role of hyperparameters (α, τ) and the scalability of the model are underexplored. The rebuttal addresses some issues but leaves gaps in understanding the model's limitations and theoretical foundations."
          },
          "questions": {
            "value": "1. How does the model handle dynamic shifts in social influences over time? 2. What is the theoretical basis for the α-entmax gating function's effectiveness? 3. How does the model scale with large datasets, and what are its computational bottlenecks? 4. Why does the model's performance lag slightly behind UniST in some metrics despite lower variance? 5. How are the matrices A and B initialized, and what is their relationship to the latent classes?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0bcRCD7YUx": {
    "paper_id": "0bcRCD7YUx",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces VALL-E 2, a neural codec language model for zero-shot text-to-speech synthesis (TTS) that achieves human parity. The key innovations are 'Repetition Aware Sampling' (to stabilize decoding) and 'Grouped Code Modeling' (to improve efficiency by reducing sequence length). Experiments on LibriSpeech and VCTK datasets demonstrate superior performance in robustness, naturalness, and speaker similarity compared to prior work."
          },
          "strengths": {
            "value": "The paper presents a clear and structured approach to addressing stability and efficiency challenges in zero-shot TTS. The 'Grouped Code Modeling' technique effectively reduces inference complexity while maintaining quality, and 'Repetition Aware Sampling' offers a practical solution to infinite loop issues. The experimental validation on standard benchmarks and the claim of human parity are significant contributions. The methodology is well-explained, and the paper highlights practical applications for individuals with speech impairments."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of the two proposed techniques. The definition of 'human parity' is not rigorously justified—relative scores compared to ground truth do not conclusively prove equivalence to human performance. The experiments do not compare against state-of-the-art models like ELLA-V or UniAudio, which could weaken the novelty claim. Additionally, the paper does not address potential limitations, such as performance on non-English languages or rare speaker profiles."
          },
          "questions": {
            "value": "1. How is 'human parity' quantitatively defined? Are there human evaluation metrics beyond the reported scores? 2. What ablation studies confirm the effectiveness of 'Repetition Aware Sampling' and 'Grouped Code Modeling' individually? 3. How does VALL-E 2 handle out-of-domain speakers or non-English text? 4. Are there comparisons with other high-performing models (e.g., ELLA-V, UniAudio) on the same benchmarks? 5. What are the computational costs of the proposed methods compared to prior work?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces VALL-E 2, a neural codec language model for zero-shot text-to-speech synthesis (TTS) that claims to achieve human parity. It proposes two key improvements: Repetition Aware Sampling to enhance decoding stability and Grouped Code Modeling to improve inference efficiency. The method is evaluated on LibriSpeech and VCTK datasets, with claims of superior performance in robustness, naturalness, and speaker similarity compared to prior work."
          },
          "strengths": {
            "value": "The paper presents a novel approach to zero-shot TTS with clear technical innovations (repetition-aware sampling and grouped code modeling). The experiments demonstrate strong performance on standard benchmarks, and the claim of human parity is significant. The work addresses critical limitations of prior systems (stability and efficiency) and provides a simplified training pipeline. The writing is structured and accessible, with clear motivation for the proposed methods."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the contribution of each proposed method. The human parity claim relies on relative metrics (e.g., ΔScore) without direct comparison to human judgments. The experiments do not include diverse speaker demographics or challenging edge cases (e.g., non-English text, noisy inputs). The technical description of Grouped Code Modeling is superficial, and the paper does not address potential limitations in long-term sequence modeling. The comparison with prior work is cursory, and the significance of the improvements is not quantified beyond qualitative assertions."
          },
          "questions": {
            "value": "1. How were the human parity metrics (ΔWERR, ΔCMOS, ΔSMOS) validated against human judgments? 2. What specific ablation experiments were conducted to isolate the impact of Repetition Aware Sampling vs. Grouped Code Modeling? 3. How does VALL-E 2 handle out-of-domain speakers or non-English text? 4. What is the computational cost of Grouped Code Modeling compared to the original VALL-E? 5. Are there any cases where VALL-E 2 fails to generate high-quality speech despite the claims of robustness?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces VALL-E 2, a zero-shot text-to-speech synthesis system that achieves human parity through two key innovations: Repetition Aware Sampling (RAS) and Grouped Code Modeling (GCM). RAS addresses instability in decoding by adapting sampling strategies based on token repetition, while GCM reduces sequence length by grouping codec codes, improving efficiency and long-context modeling. The paper claims superior performance on LibriSpeech and VCTK datasets compared to prior work."
          },
          "strengths": {
            "value": "The paper presents two novel technical contributions (RAS and GCM) that address critical limitations of prior work. The experiments demonstrate improved robustness, naturalness, and speaker similarity, with claims of human parity. The approach simplifies data requirements by avoiding complex alignment or additional speaker data. The writing is clear, and the problem formulation is well-structured."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of RAS and GCM. The human parity claim relies on relative metrics (e.g., ΔScore) without direct comparison to human judgments. The experiments do not thoroughly evaluate edge cases (e.g., highly repetitive or complex sentences). The theoretical justification for GCM's benefits is minimal, and the paper does not address potential limitations in real-world deployment scenarios."
          },
          "questions": {
            "value": "1. How were the human parity metrics validated? Were they based on subjective listening tests or objective metrics? 2. What is the exact definition of 'human parity' in terms of quantitative thresholds? 3. How do RAS and GCM compare to alternative methods (e.g., non-autoregressive approaches) in terms of computational efficiency? 4. What are the limitations of the grouped code modeling approach in handling long-range dependencies?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0bcUyy2vdY": {
    "paper_id": "0bcUyy2vdY",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the multi-play multi-armed bandit problem with shareable arm capacities (MP-MAB-SAC), focusing on sample complexity, regret bounds, and a data-efficient algorithm. The authors provide tighter theoretical guarantees compared to prior work, including a minimax lower bound for sample complexity, instance-dependent and independent regret bounds, and an algorithm (PC-CapUL) that achieves these bounds. The work is motivated by resource allocation challenges in LLM inference and edge computing."
          },
          "strengths": {
            "value": "The paper makes significant theoretical contributions by closing gaps in sample complexity and regret bounds for MP-MAB-SAC. The proposed algorithm, PC-CapUL, effectively balances exploration and exploitation through prioritized coordination of UCB/LCB. The analysis is rigorous, leveraging techniques like Le Cam's method. The problem formulation is well-motivated by real-world applications, and the paper clarifies key distinctions from prior work, such as deterministic vs. stochastic arm capacities."
          },
          "weaknesses": {
            "value": "The empirical validation is limited, with only basic experiments provided and no error bars or detailed analysis in the main text. The application examples (e.g., LLM inference) lack concrete quantitative details. The comparison with Wang et al. (2022a) requires further clarification, particularly regarding the fairness of regret bound comparisons. Some notational inconsistencies and ambiguities in the original paper (e.g., unclear definitions of UE/IE) were noted, though the rebuttal addresses these."
          },
          "questions": {
            "value": "1. How does the proposed model handle scenarios where arm capacities are highly imbalanced or when the number of arms $K$ is large? 2. What are the practical implications of the regret bounds in the context of LLM inference, where $c$ (movement cost) might vary significantly? 3. The rebuttal mentions additional experiments on estimator convergence—could these results be included in the final version? 4. How does the algorithm's performance depend on the choice of $c$ (movement cost), and what guarantees exist for its robustness?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper studies the multi-play multi-armed bandit problem with scarce shareable arm capacities (MP-MAB-SAC), focusing on sample complexity, regret bounds, and a data-efficient algorithm. The authors propose a new reward model that isolates capacity information in the mean, derive tighter theoretical guarantees, and introduce PC-CapUL to balance exploration and exploitation. They address gaps in prior work by closing the sample complexity gap and improving regret bounds."
          },
          "strengths": {
            "value": "The paper makes significant theoretical contributions, including a tighter sample complexity lower bound independent of arm capacity and novel regret bounds. The proposed algorithm PC-CapUL is well-justified with theoretical guarantees. The problem formulation is relevant to real-world applications like LLM inference serving, and the paper addresses a clear gap in prior work by redefining the reward model to isolate capacity information. The structure is logical, and the theoretical analysis is rigorous."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient experimental details, such as error bars in figures and concrete examples of LLM applications. The comparison with Wang et al. (2022a) is partially addressed in the rebuttal but requires deeper clarification on why their regret bounds are less informative. The dependency of the sample complexity on μ_k and σ^2 is critical but not fully contextualized. The notation and proof references (e.g., Lemma 14.2) need stricter adherence to avoid confusion."
          },
          "questions": {
            "value": "1. How does the new reward model (equation 5) compare to conventional linear bandits in terms of practical implementation? 2. Can the authors provide more concrete examples of LLM inference serving applications to strengthen the motivation? 3. What is the exact mechanism for ensuring 'prioritized coordination' in PC-CapUL, and how does it avoid over-exploration? 4. How sensitive is the algorithm to the movement cost parameter c, and what guarantees exist for its robustness?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper addresses the multi-play multi-armed bandit problem with shareable arm capacities (MP-MAB-SAC), focusing on sample complexity, regret bounds, and a proposed algorithm (PC-CapUL). The authors claim to close the sample complexity gap from prior work and provide new instance-independent regret bounds. They also propose a novel algorithm for data-efficient exploration, validated through numerical experiments."
          },
          "strengths": {
            "value": "The paper makes significant theoretical contributions by establishing tight sample complexity and regret bounds, which address gaps in prior work. The proposed algorithm (PC-CapUL) is well-motivated and theoretically analyzed. The problem formulation is relevant to real-world applications like LLM inference serving. The paper also provides a clear comparison with existing literature, highlighting improvements in bounds."
          },
          "weaknesses": {
            "value": "The experimental validation is limited, with insufficient details on the LLM application and no error bars in the figures. The theoretical analysis assumes a simplified reward model that may not capture all real-world complexities. The comparison with Wang et al. (2022a) requires more rigorous justification, particularly regarding the fairness of bounds. The notation and references in the original paper had issues, though the rebuttal addresses some of these."
          },
          "questions": {
            "value": "1. How does the algorithm handle scenarios where N is significantly larger than the sum of arm capacities? 2. What is the practical significance of the instance-independent regret bounds in real-world settings? 3. How does the proposed method scale with large K or T? 4. Can the authors provide additional empirical evidence to support the data efficiency claims?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "0e2pcSxQJS": {
    "paper_id": "0e2pcSxQJS",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces PN-GAIL, a novel imitation learning method that leverages non-optimal information from imperfect demonstrations by having the discriminator assess both positive and negative risks. It theoretically shows that PN-GAIL avoids non-optimal data while mimicking imperfect demonstrations and demonstrates superior performance on six control tasks compared to baselines like 2IWIL and GAIL."
          },
          "strengths": {
            "value": "The paper presents a novel framework (PN-GAIL) that addresses a critical gap in imitation learning by incorporating negative risk assessment from imperfect demonstrations. The theoretical analysis rigorously demonstrates how PN-GAIL deviates from non-optimal data, a key contribution. The experiments on six control tasks provide empirical validation, and the rebuttal clarifies critical methodological details (e.g., definition of δ, BSC vs. SC). The paper also improves presentation with revised figures and explanations."
          },
          "weaknesses": {
            "value": "The theoretical analysis lacks depth in connecting the discriminator's risk assessment to policy optimality. The experiments focus on control tasks but do not generalize to complex real-world scenarios. The semi-supervised confidence classifier's training methodology is under-specified, and the ablation studies on $n_u$ and $n_c$ are limited. The rebuttal addresses some issues, but the paper still lacks a comprehensive comparison with ranking-based methods and a thorough analysis of parameter sensitivity."
          },
          "questions": {
            "value": "1. How is the semi-supervised confidence classifier trained, and what are its specific advantages over existing methods? 2. Why does GAIL predict a confidence score of 5.0 for certain data points, and how does PN-GAIL avoid this? 3. Are there cases where PN-GAIL fails, and what are the limitations of its risk assessment approach? 4. How does the method scale to high-dimensional or continuous state-action spaces?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes PN-GAIL, a novel imitation learning method that leverages non-optimal information from imperfect demonstrations by incorporating both positive and negative risk assessments in the discriminator. It addresses limitations of existing methods like 2IWIL, which are susceptible to biases in imperfect data. The approach requires minimal labeled confidence scores and is theoretically analyzed to avoid non-optimal demonstrations while learning optimal policies. Experimental results on six control tasks demonstrate superior performance compared to baselines."
          },
          "strengths": {
            "value": "The paper identifies a critical gap in existing imitation learning methods when dealing with imperfect demonstrations, which is a significant practical problem. The theoretical analysis provides a clear justification for why PN-GAIL avoids non-optimal data, offering a novel perspective on discriminator training. The experimental validation on multiple tasks demonstrates practical utility. The rebuttal clarifies key technical points (e.g., definition of δ, BSC vs. SC), improving the paper's rigor."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough ablation study to isolate the impact of the positive-negative risk formulation. The experimental results do not compare against recent state-of-the-art methods beyond 2IWIL and IC-GAIL, which limits the assessment of PN-GAIL's competitiveness. The theoretical analysis assumes idealized conditions (e.g., small covariances) that may not hold in practice. The paper also does not address how PN-GAIL scales to high-dimensional or continuous state spaces."
          },
          "questions": {
            "value": [
              "How does the positive-negative risk formulation specifically improve performance compared to 2IWIL? Are there quantitative measures of how much the discriminator's bias is reduced?",
              "What is the exact mechanism by which PN-GAIL avoids non-optimal demonstrations? Does this rely on the confidence classifier's accuracy, and how is this validated?",
              "The rebuttal mentions that BSC outperforms SC, but the paper's experiments only compare PN-GAIL with variants like PN-GAIL\\BSC. How do these results generalize to other imitation learning frameworks?",
              "The paper claims to require only a small subset of labeled confidence scores. What is the empirical threshold for 'small' and how does this affect performance in real-world scenarios with sparse labels?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes PN-GAIL, a novel imitation learning method that leverages non-optimal information from imperfect demonstrations by incorporating both positive and negative risks into the discriminator. It addresses limitations of existing methods like 2IWIL, which can be biased by preferences in imperfect data. Theoretical analysis and experiments on six control tasks demonstrate improved performance over baselines."
          },
          "strengths": {
            "value": "The paper addresses a critical real-world problem where demonstrations are often imperfect, which is underexplored in imitation learning. The method introduces a novel framework within GAIL to handle non-optimal data, combining theoretical analysis with empirical validation. The rebuttal clarifies key definitions and experimental comparisons, improving the paper's transparency. The focus on balancing positive and negative risks in the discriminator is a promising direction for robust imitation learning."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how PN-GAIL explicitly avoids non-optimal demonstrations, despite claiming theoretical guarantees. The experimental comparisons (e.g., PN-GAIL vs. PN-GAIL\\BSC) are not fully convincing, as the rebuttal's explanations remain vague. The impact of hyperparameters like $n_u$ and $n_c$ is not thoroughly explored, and the justification for setting $\\alpha = \\beta$ in Algorithm 1 is insufficient. The theoretical analysis in Theorem 4.2 is underdeveloped, with unclear connections to the proposed method."
          },
          "questions": {
            "value": "1. How does PN-GAIL explicitly differentiate between optimal and non-optimal demonstrations during training? 2. The rebuttal mentions PN-GAIL\\BSC uses SC classification, but the distinction between BSC and SC remains unclear. 3. Why does GAIL predict a confidence score of 5.0 in the example? Is this a result of the weighting mechanism or a numerical artifact? 4. What are the specific conditions under which PN-GAIL\\BSC outperforms SC, and how generalizable is this improvement? 5. How sensitive is the method to the ratio of $n_u$ to $n_c$, and what is the minimal dataset size required for stable performance?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0fwJMANq9P": {
    "paper_id": "0fwJMANq9P",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces Hercules and Hercules-P, algorithms for heuristic generation using Large Language Models (LLMs). Hercules employs Core Abstraction Prompting (CAP) to generate specific search directions by abstracting core components from elite heuristics, while Hercules-P integrates Performance Prediction Prompting (PPP) to predict heuristic fitness values based on semantic similarity, reducing resource costs. The work demonstrates improvements over state-of-the-art methods through extensive experiments."
          },
          "strengths": {
            "value": "The paper addresses two critical challenges in LLM-based heuristic generation: unspecific search directions and resource-intensive evaluation. CAP's zero-shot abstraction method is novel, and PPP introduces a first-of-its-kind performance prediction approach for HG tasks. The theoretical analysis of CAP's effectiveness via information gain and comprehensive experiments across multiple tasks and LLMs strengthen the contribution. Ablation studies further validate the proposed mechanisms."
          },
          "weaknesses": {
            "value": "The paper lacks detailed explanations of how CAP abstracts core components without examples, which is critical for reproducibility. The theoretical proof of CAP's effectiveness (Appendix A) is not sufficiently elaborated in the main text. The experiments, while extensive, could include more baseline comparisons (e.g., against non-LLM-based methods) and larger-scale COPs. The term 'proprietary' in 'proprietary CAP algorithm' is ambiguously defined, and the paper could clarify how CAP differs from prior work like Ye et al. (2024a)."
          },
          "questions": {
            "value": "1. How does CAP ensure meaningful abstraction of core components without examples, and what guarantees its effectiveness? 2. What is the theoretical basis for the information gain proof in Appendix A, and how does it directly relate to reducing unspecificity? 3. How does PPP handle heuristics with complex or non-trivial semantic structures? 4. Are there scenarios where PPP's predictions fail, and how does ConS mitigate such cases? 5. How does the paper differentiate CAP from prior work like Ye et al. (2024a) in terms of novelty and technical contribution?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Hercules and Hercules-P, algorithms for heuristic generation in combinatorial optimization problems (COPs) using Large Language Models (LLMs). The key contributions include Core Abstraction Prompting (CAP) to generate specific search directions and Performance Prediction Prompting (PPP) to predict heuristic performance, reducing computational costs. The authors claim superior performance over state-of-the-art methods and validate their approach through extensive experiments."
          },
          "strengths": {
            "value": "The paper demonstrates originality in proposing CAP and PPP, with PPP being a novel LLM-based performance predictor for heuristic generation. The theoretical analysis of CAP using information gain and the ablation studies on key components (e.g., EXEMPLAR, ConS) add depth. The experiments span multiple tasks, COPs, and LLMs, showcasing the methods' generalizability. The clarity of figures and structured presentation of algorithms also enhance readability."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with baseline methods beyond the claims of 'state-of-the-art' performance. The theoretical proof of CAP's effectiveness is brief and relies on assumptions that require further justification. The PPP mechanism's dependence on example quality (via EXEMPLAR) is not thoroughly analyzed, and the rebuttal does not clarify how EXEMPLAR ensures diverse, high-quality examples. Additionally, the paper does not address potential limitations of zero-shot CAP in complex or domain-specific COPs."
          },
          "questions": {
            "value": "1. How does the EXEMPLAR mechanism specifically select high-quality examples for PPP, and what metrics are used to evaluate their quality? 2. Are there scenarios where CAP's zero-shot abstraction might fail to capture critical components of elite heuristics? 3. How does the paper ensure that the semantic similarity-based predictions in PPP are robust across diverse COPs? 4. What are the exact computational savings achieved by Hercules-P, and how do they scale with problem size?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes Hercules and Hercules-P, algorithms for heuristic generation in combinatorial optimization problems (COPs). Hercules uses Core Abstraction Prompting (CAP) to generate specific search directions by abstracting core components from elite heuristics, while Hercules-P introduces Performance Prediction Prompting (PPP) to predict heuristic performance based on semantic similarity, reducing resource costs. The work claims state-of-the-art performance and resource efficiency."
          },
          "strengths": {
            "value": "The paper addresses two critical challenges in LLM-based heuristic generation: unspecific search directions and resource-intensive evaluation. CAP's zero-shot abstraction of core components is novel, and PPP's semantic similarity-based prediction is a first-of-its-kind approach for HG. Theoretical analysis of CAP's effectiveness via information gain and extensive experiments across multiple tasks and LLMs demonstrate practical value. Ablation studies validate the contributions of key mechanisms."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with prior work on the exact nature of CAP's novelty relative to existing reflection prompting (RP). The theoretical proof of CAP's effectiveness relies on Appendix A, which is not accessible here, raising questions about its rigor. The PPP method's reliance on semantic similarity lacks concrete implementation details (e.g., similarity metrics). Experiments do not thoroughly address edge cases (e.g., rare semantic equivalence scenarios). The term 'proprietary' is ambiguously defined, and the paper could better clarify how CAP differs from Ye et al. (2024a)."
          },
          "questions": {
            "value": "1. How exactly does CAP's zero-shot abstraction process work? What specific mechanisms ensure the extraction of core components without examples? 2. What metrics/techniques does PPP use to quantify semantic similarity between heuristics? 3. Are there limitations to PPP's performance when examples lack diversity or representativeness? 4. How does the rank-based selection mechanism compare to random selection in terms of empirical performance? 5. Can the authors provide additional evidence that CAP's improvements are not merely incremental over RP?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "0gGPVbRqOE": {
    "paper_id": "0gGPVbRqOE",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes Splitted Wavelet Differential Inclusion (SWDI), a novel method for neural signal processing that addresses the limitations of traditional wavelet shrinkage by distinguishing between strong and weak signal components. SWDI uses an ℓ₂ splitting mechanism to estimate both the strong signal (with bias-free recovery) and the weak signal (via ℓ₂ shrinkage), demonstrated on Parkinson's disease (PD) signal analysis."
          },
          "strengths": {
            "value": "The paper introduces a novel approach by combining differential inclusion with an ℓ₂ splitting mechanism, addressing a gap in wavelet shrinkage methods that neglect weak signals. The theoretical analysis includes closed-form solutions and claims of improved estimation. The application to PD signal processing is timely and relevant, with experimental results showing improved correlation with medication effects. The structure is clear, and the problem formulation is well-motivated."
          },
          "weaknesses": {
            "value": "The paper lacks comparison with recent state-of-the-art methods beyond traditional wavelet shrinkage (e.g., Bayesian or deep learning approaches). The theoretical analysis, particularly Proposition 4.1, is incomplete and lacks rigorous justification. The experiments are limited to a single application (PD) without broader validation. The handling of non-orthogonal wavelet matrices is not thoroughly explained, and computational efficiency or scalability is not discussed. The connection to differential inclusion literature is superficial, with limited references to related work."
          },
          "questions": {
            "value": "1. How does SWDI compare to Bayesian shrinkage or deep learning-based methods for neural signal processing? 2. What guarantees exist for the convergence or stability of the differential inclusion dynamics? 3. How does the ℓ₂ splitting mechanism handle non-orthogonal wavelet transforms, as mentioned in the paper? 4. Are the theoretical claims (e.g., bias-free estimation of strong signals) validated with concrete error bounds or empirical evidence? 5. What are the computational costs of the proposed method compared to existing techniques?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes Splitted Wavelet Differential Inclusion (SWDI), a novel method for neural signal processing that aims to simultaneously estimate both strong and weak signals in wavelet-based denoising. By introducing an ℓ₂ splitting mechanism within a differential inclusion framework, the authors claim to achieve bias-free estimation of strong signals and improved recovery of weak signals compared to traditional wavelet shrinkage methods."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in existing wavelet shrinkage methods by explicitly considering both strong and weak signal components, which is particularly relevant for neuroscience applications like Parkinson's disease analysis. The theoretical analysis provides clear mathematical formulations, including closed-form solutions for the differential inclusion. The method's novelty lies in combining differential inclusion with ℓ₂ splitting, offering a fresh perspective on signal estimation. The application to PD and identification of non-burst activity demonstrates practical relevance."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficient, with limited details on datasets, baselines, and quantitative metrics. The claims of 'improved accuracy' lack concrete evidence, such as comparison with state-of-the-art methods or statistical significance tests. The theoretical guarantees are mathematically sound but may not be fully grounded in practical scenarios. The paper also fails to clarify how the differential inclusion dynamics are discretized and implemented, which is critical for reproducibility."
          },
          "questions": {
            "value": [
              "What specific datasets and metrics were used to evaluate the method's performance? How does it compare to established wavelet shrinkage techniques like SureShrink or Bayesian shrinkage?",
              "How is the ℓ₂ splitting mechanism implemented in practice, and what are the computational costs compared to existing methods?",
              "The paper mentions 'additional findings of tonic activity in PD'—what statistical evidence supports this claim, and how was it validated?",
              "How does the method handle non-orthogonal wavelet transforms, as stated in the preliminary section?",
              "What is the theoretical justification for the early stopping mechanism in the differential inclusion dynamics?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes Splitted Wavelet Differential Inclusion (SWDI), a novel method for neural signal processing that addresses the limitations of traditional wavelet shrinkage by distinguishing between strong and weak signal components. The method uses an $\\ell_2$ splitting mechanism within a differential inclusion framework to separately estimate strong signals (with high coefficients) and weak signals (with small coefficients), aiming to improve reconstruction accuracy. Theoretical analysis and experiments on Parkinson's disease data are presented to demonstrate its effectiveness."
          },
          "strengths": {
            "value": "The paper identifies a critical gap in existing wavelet shrinkage methods, which often neglect weak signals despite their potential importance in neuroscience. The theoretical contributions include propositions on bias-free estimation for strong signals and a closed-form solution path. The application to Parkinson's disease, where weak signals (non-burst activity) are shown to correlate with medication effects, highlights practical relevance. The method's dual-parameter approach and connection to differential inclusion provide a novel perspective on signal estimation."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficient, with no details on datasets, baselines, or metrics for comparison. The theoretical analysis is incomplete, as key proofs (e.g., Proposition 4.1) are cut off, and the paper lacks discussion of computational efficiency. The novelty over existing methods like SureShrink or Bregman Iteration is not clearly established. Additionally, the handling of non-orthogonal wavelet matrices is not thoroughly addressed, which limits generalizability."
          },
          "questions": {
            "value": "1. What specific datasets and metrics were used to evaluate the method's performance? How does it compare to state-of-the-art wavelet shrinkage techniques? 2. Can the authors clarify the theoretical guarantees for the closed-form solution and its convergence properties? 3. How does the method handle non-orthogonal wavelet transforms, and what are its computational costs? 4. Are there any ablation studies to validate the necessity of the $\\ell_2$ splitting mechanism?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0koPj0cJV6": {
    "paper_id": "0koPj0cJV6",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces a black-box watermarking scheme for language models that requires only sequence sampling capabilities, offering distortion-free properties, key chaining, and theoretical performance guarantees. It provides formal analysis of detection accuracy and robustness under specific assumptions."
          },
          "strengths": {
            "value": "The work addresses a critical practical problem (black-box watermarking) with a novel approach that avoids white-box requirements. The theoretical contributions include distortion-free guarantees and detection performance bounds. The paper's clarity is improved through algorithmic details, and the practical implications for third-party detection are well-motivated. The recursive watermarking framework enables multi-layered detection, which is a significant innovation."
          },
          "weaknesses": {
            "value": "The paper's presentation is overly terse, with insufficient explanations of key concepts like PRF, CDF, and n-gram handling. The theoretical analysis relies on strong assumptions (e.g., independence of n-grams) that lack empirical validation. The experimental section is incomplete, making it difficult to assess the method's real-world effectiveness. The connection between the continuous distribution assumption and discrete token scoring is under-specified, raising concerns about practical implementation."
          },
          "questions": {
            "value": "1. How does the continuous distribution assumption (F) interact with discrete token scoring in practice? 2. What empirical evidence supports the independence of n-grams across sequences? 3. Can the authors provide additional details on the experimental setup and baselines to validate their claims? 4. How does the method handle model-specific tokenization schemes that may affect n-gram extraction?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a black-box watermarking scheme for language models that requires only sequence sampling access, offering distortion-free properties, key-based chaining, and theoretical guarantees. The approach leverages pseudorandom functions and statistical hypothesis testing for detection, with proofs of fidelity to the model's distribution and performance bounds."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in black-box watermarking, offering a novel solution with strong theoretical foundations. The distortion-free property and key-based chaining are significant innovations. Theoretical analysis includes rigorous proofs of fidelity and detection performance, while the practical implementation considerations (e.g., handling n-grams, hash collisions) demonstrate careful design. The work's relevance to real-world applications (e.g., third-party detection) enhances its significance."
          },
          "weaknesses": {
            "value": "The paper's presentation is overly terse, with key algorithmic details relegated to the appendix, limiting accessibility. Theoretical analysis assumes idealized conditions (e.g., independent n-grams) that may not hold in practice. Experimental validation is incomplete (content cut off), making it difficult to assess practical efficacy. Some formalism (e.g., n-gram definitions, PRF details) lacks clarity, requiring readers to infer details from the rebuttal."
          },
          "questions": {
            "value": "1. How does the scheme handle varying model sizes or architectures? 2. What are the computational costs of the watermarking process? 3. How does the method perform under adversarial attacks (e.g., paraphrasing)? 4. Are the theoretical assumptions (e.g., independent n-grams) validated empirically? 5. Will the promised Python implementation in the appendix address the clarity concerns about PRF and n-gram definitions?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a black-box watermarking scheme for language models that requires only sequence sampling, claims distortion-free properties, and provides theoretical guarantees. It leverages secret keys for chaining and demonstrates performance improvements over white-box methods through experiments."
          },
          "strengths": {
            "value": "The paper's main strength lies in its novel black-box approach, which addresses a critical limitation of existing watermarking schemes. The theoretical analysis, including the distortion-free property and ROC-AUC bounds, is rigorous. The recursive watermarking feature and practical implementation considerations (e.g., handling of n-grams) add value. The authors also address security risks associated with white-box access."
          },
          "weaknesses": {
            "value": "The paper's presentation is overly terse, with critical details relegated to the Appendix, which may hinder readability. The theoretical assumptions (e.g., independence of n-grams) lack thorough justification, and the experiments are limited to a single model and dataset. The comparison with prior work is not comprehensive, and the practical implications of the assumptions (e.g., continuous F) are underexplored. The rebuttal addresses some issues but does not fully resolve concerns about clarity and generalizability."
          },
          "questions": {
            "value": "1. How robust are the theoretical guarantees to violations of the independence assumptions? 2. What are the practical implications of using a continuous distribution F in discrete token spaces? 3. How does the method scale to larger models and diverse datasets beyond MISTRAL-7B? 4. Can the authors provide additional empirical validation against a broader set of baselines? 5. How is the recursive watermarking scheme evaluated in terms of detectability and security?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0mJZplhexS": {
    "paper_id": "0mJZplhexS",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces the Little-Big algorithm, a two-pass approach for accelerating image classifiers by using a lightweight 'little' model to pre-screen samples and only routing low-confidence predictions to a 'big' model. This method achieves significant MACs reductions across multiple model families (e.g., 76% for EfficientViT-L3-384) without accuracy loss, motivated by human vision's dual-processing pathways."
          },
          "strengths": {
            "value": "The paper presents a novel, model-agnostic approach with strong empirical results. Its simplicity and practicality are notable, as it requires no model modifications. The motivation from human vision provides a compelling theoretical foundation. Experiments are comprehensive, covering diverse architectures (CNNs, transformers, hybrids) and scales. The clarity of explanations and figures enhances readability."
          },
          "weaknesses": {
            "value": "The paper lacks analysis on optimal Little model selection and threshold tuning, which are critical for performance. While the rebuttal mentions exhaustive evaluation of 1,953 model pairs, this is not discussed in the original manuscript, raising questions about reproducibility. The method's generalizability to tasks beyond image classification (e.g., segmentation) is underexplored. Theoretical guarantees for accuracy preservation are absent, and ablation studies on confidence thresholds are missing."
          },
          "questions": {
            "value": "1. How does the choice of Little model (e.g., EfficientNet vs. DeiT) impact performance, and what criteria should users employ for selection? 2. What is the sensitivity of results to threshold values, and how should users determine optimal thresholds in practice? 3. How does the method perform on tasks beyond image classification, such as semantic segmentation or zero-shot classification (as mentioned in the rebuttal)? 4. Are there theoretical bounds or guarantees for maintaining accuracy when using Little-Big pairs?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes the Little-Big algorithm, a model-agnostic two-pass approach that uses a lightweight 'little' model to preprocess samples and only sends low-confidence (difficult) samples to a 'big' model. The method achieves significant MACs reductions (up to 81%) across multiple vision model families without accuracy loss, motivated by human vision's dual-path processing mechanism."
          },
          "strengths": {
            "value": "The paper demonstrates a simple yet effective solution to model compression with strong empirical results. The motivation from human vision provides novel biological context. The experiments are comprehensive, covering diverse model architectures and scales. The algorithm's model-agnostic nature and practical deployment benefits are significant contributions. The clarity of problem formulation and technical exposition is excellent."
          },
          "weaknesses": {
            "value": "The paper lacks ablation studies on threshold selection and confidence calibration. The assumption that low confidence = hard samples isn't rigorously validated across all tasks. The comparison with established compression methods (pruning, quantization) is superficial. The theoretical analysis of why low-confidence samples are correctable is limited. The extension to non-image tasks mentioned in the rebuttal isn't in the original submission."
          },
          "questions": {
            "value": "How sensitive is the method to threshold selection across different datasets/tasks? What's the impact of using different little models on the same big model? Can the approach be theoretically grounded in information theory or decision boundaries? How does it handle concept drift in real-world deployments? What's the latency benefit beyond MACs reduction?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces the Little-Big algorithm, a two-pass approach for accelerating image classifiers by using a lightweight 'little' model to preprocess samples and only passing difficult ones to a larger 'big' model. This method achieves significant reductions in multiply-accumulate operations (MACs) across various model families without sacrificing accuracy."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by proposing a simple yet effective model-agnostic approach to inference efficiency. The experimental validation is thorough, showing substantial MACs reductions across multiple architectures (e.g., 76% for EfficientViT-L3-384). The clarity of the methodology and figures is excellent, and the significance of the work lies in its practical applicability for large model compression."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons with established model compression techniques like pruning or quantization, which limits the assessment of its relative effectiveness. The choice of 'little' models is user-dependent but not systematically analyzed. The generalization to tasks beyond image classification is not sufficiently explored, and the theoretical analysis of the method's guarantees is minimal. Additionally, the paper does not address potential limitations in scenarios where the 'little' model's confidence estimates are unreliable."
          },
          "questions": {
            "value": "1. How does the Little-Big method compare to existing compression techniques (e.g., pruning, quantization) in terms of efficiency and accuracy trade-offs? 2. What criteria should users employ to select optimal 'little' models for different applications? 3. Are the reported results generalizable to tasks like semantic segmentation or natural language processing? 4. How sensitive is the method to the choice of confidence threshold, and what strategies exist for optimizing it?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "0nJt9aVGtl": {
    "paper_id": "0nJt9aVGtl",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces WaveDiffusion, a novel framework that reframes Full Waveform Inversion (FWI) as a joint diffusion process in a shared latent space. The method combines a dual autoencoder with vector quantization (VQ) to create a common latent space for seismic data and velocity maps, followed by a diffusion model that generates physically consistent pairs by refining latent representations. The approach enables simultaneous generation of seismic-velocity pairs that inherently satisfy the governing PDE without explicit constraints."
          },
          "strengths": {
            "value": "The paper presents a novel joint diffusion process in a shared latent space, which is a significant advancement for FWI. The methodology is well-structured, with clear explanations of the dual autoencoder and diffusion steps. Experiments on the OpenFWI dataset demonstrate high-fidelity, PDE-compliant outputs. The authors address practical applications through a one-in-two-out autoencoder configuration, showing flexibility. The work offers a fresh perspective on FWI by integrating diffusion models with physical constraints."
          },
          "weaknesses": {
            "value": "The dual autoencoder architecture is not novel, as highlighted by the reviewer, though the authors correctly emphasize the diffusion process as the core contribution. The paper lacks detailed analysis of how the diffusion model scores latent space points based on PDE deviation. Experimental comparisons with state-of-the-art methods (e.g., InversionNet) are limited, and the one-in-two-out results mentioned in the rebuttal are not integrated into the main text. The quantitative validation of PDE adherence could be more thorough."
          },
          "questions": {
            "value": "1. How is the diffusion model's scoring mechanism for PDE deviation implemented? The paper mentions it but does not elaborate on the technical details. 2. The rebuttal references experiments with a one-in-two-out autoencoder, but these results are not included in the main text. Could the authors clarify how these experiments validate the method's practical applicability? 3. The paper claims generated pairs satisfy the PDE without constraints, but how is this rigorously quantified beyond L2 distance metrics?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "WaveDiffusion introduces a novel framework for Full Waveform Inversion (FWI) by reframing it as a joint diffusion process in a shared latent space. The method uses a dual autoencoder with vector quantization to create a shared latent space for seismic data and velocity maps, followed by a diffusion model that generates physically consistent pairs. The diffusion process scores latent representations based on their deviation from the governing PDE, enabling refinement toward valid solutions without explicit constraints."
          },
          "strengths": {
            "value": "The paper presents a creative integration of diffusion models and autoencoders for FWI, offering a new perspective on solving PDEs. The method's ability to generate physically consistent seismic-velocity pairs without explicit PDE constraints is novel. Experiments on OpenFWI demonstrate high fidelity and diversity, with clear ablation studies showing the diffusion process's role in refining solutions. The architecture's flexibility (e.g., one-in-two-out configuration) enhances practical applicability. Clarity is maintained through structured sections, figures, and mathematical formulations."
          },
          "weaknesses": {
            "value": "The dual autoencoder with shared latent space is not novel, as highlighted by the reviewer and acknowledged by the authors. While the diffusion component is original, the paper lacks comprehensive comparisons to state-of-the-art FWI methods (e.g., InversionNet) and detailed ablation studies on diffusion hyperparameters. The evaluation metrics (FID, L2 distance) are insufficient to fully assess physical consistency. The rebuttal's experimental links are not integrated into the paper, raising concerns about reproducibility. The method's limitations (e.g., computational cost, robustness to noise) are not discussed."
          },
          "questions": {
            "value": "1. How does the diffusion model explicitly encode PDE constraints into its scoring function? 2. What are the quantitative results comparing WaveDiffusion to traditional FWI and other ML-based methods on OpenFWI? 3. How does the one-in-two-out configuration perform in terms of computational efficiency and physical consistency compared to the dual autoencoder? 4. Are there cases where the generated pairs fail to satisfy the PDE, and how are these handled? 5. How sensitive is the method to hyperparameters in the diffusion process?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces WaveDiffusion, a novel framework for Full Waveform Inversion (FWI) that reframes the problem as a joint diffusion process in a shared latent space. It combines a dual autoencoder with vector quantization (VQ) to create a shared latent representation of seismic data and velocity maps, followed by a diffusion model that generates physically consistent pairs by iteratively refining latent representations. The method claims to satisfy the governing PDE without explicit constraints."
          },
          "strengths": {
            "value": "The paper presents a creative approach to FWI by integrating diffusion models with latent space representations, offering a new perspective on solving PDEs. The experimental results on the OpenFWI dataset demonstrate high-fidelity and diverse outputs that adhere to physical constraints. The methodology is clearly explained with structured figures and a logical flow. The focus on joint generation, rather than conditional generation, addresses a gap in prior work."
          },
          "weaknesses": {
            "value": "The core novelty of the work hinges on the joint diffusion process, but the dual autoencoder with a shared latent space is not novel, as highlighted by the rebuttal and prior work (e.g., paired autoencoders for inverse problems). The paper lacks thorough comparisons with state-of-the-art methods, including diffusion-based approaches and traditional FWI. The mechanism by which the diffusion model scores latent space points based on PDE deviation is not sufficiently detailed. Ablation studies on the diffusion process's effectiveness are missing. The rebuttal's experiments (e.g., one-in-two-out configuration) should be integrated into the main paper for completeness."
          },
          "questions": {
            "value": [
              "How does the VQ layer in the dual autoencoder specifically contribute to the PDE adherence of generated pairs?",
              "Can the authors clarify the exact scoring mechanism used by the diffusion model to prioritize latent points that satisfy the PDE?",
              "What baseline methods (e.g., traditional FWI, other diffusion models) were compared against in the experiments, and how does WaveDiffusion outperform them?",
              "The rebuttal mentions a one-in-two-out configuration, but the paper does not elaborate on its implementation or results. How does this configuration affect the model's performance on standard FWI tasks?",
              "How are the FID scores and PDE deviation metrics correlated? Are there trade-offs between fidelity and physical consistency?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0owyEm6FAk": {
    "paper_id": "0owyEm6FAk",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates the security risks of LoRA adapters in the share-and-play ecosystem, demonstrating how backdoors can be injected into LoRA modules and distributed at scale. The authors propose a training-free attack mechanism where a backdoor-infected LoRA can be merged with multiple task-enhancing LoRAs while retaining both malicious and benign capabilities."
          },
          "strengths": {
            "value": "The paper addresses a novel and critical security gap in the LoRA ecosystem, which has not been previously explored. It provides a clear threat model, detailed experimental analysis of backdoor injection, and practical examples of malicious scenarios. The work highlights the unique risks posed by the open-source nature of LoRA sharing and offers actionable insights for the community. The technical depth in explaining LoRA merging and backdoor mechanisms is commendable."
          },
          "weaknesses": {
            "value": "The paper lacks thorough experimental validation across diverse downstream tasks and real-world scenarios. The backdoor injection mechanism relies on a simplified 'FF-only merging' recipe, but the technical details of how this ensures robustness across different LoRAs are under-explained. Additionally, the paper does not adequately address detection strategies for such attacks, which is critical for practical impact. The rebuttal clarifies some motivations, but the original submission could have better contextualized the urgency of the threat."
          },
          "questions": {
            "value": "1. How does the FF-only merging recipe specifically enable the coexistence of benign and malicious capabilities in merged LoRAs? 2. What are the limitations of the current backdoor detection methods in the LoRA ecosystem, and how could they be improved? 3. Are there empirical results demonstrating the effectiveness of the attack across different LoRA architectures or model sizes?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates the security risks of LoRA adapters in the share-and-play ecosystem, demonstrating how backdoors can be injected into LoRA modules and propagated across multiple downstream tasks through a training-free merging mechanism. The authors highlight the potential for large-scale malicious distribution of compromised LoRAs, emphasizing the need for heightened security awareness in open-source LLM communities."
          },
          "strengths": {
            "value": "The paper identifies a novel and timely security risk in the rapidly growing LoRA ecosystem, which has not been previously explored in the literature. It provides a clear threat model and mechanism for backdoor injection, supported by theoretical analysis. The motivation is compelling, as the share-and-play model's popularity creates a unique attack surface. The work also contributes to the understanding of LoRA compositionality and its implications for security."
          },
          "weaknesses": {
            "value": "The experimental evaluation is limited in scope and depth, with only two backdoor examples and minimal quantitative results. The paper lacks systematic analysis of trade-offs between backdoor effectiveness and downstream task performance. The proposed attack mechanism relies on a specific FF-only merging recipe, but the paper does not thoroughly explain why this approach is effective or how it differs from existing LoRA composition methods. The absence of detection or mitigation strategies weakens the practical impact of the work."
          },
          "questions": {
            "value": "1. How was the FF-only merging recipe discovered, and what specific properties make it suitable for large-scale backdoor propagation? 2. Are the two backdoor examples representative of broader attack scenarios, or are they tailored to specific use cases? 3. What are the limitations of the training-free merging approach in real-world deployment? 4. How can the community detect such backdoors given the stealthy nature of the attack? 5. Are there alternative LoRA composition methods that could mitigate this vulnerability?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper identifies a novel security risk in the LoRA ecosystem, where attackers can inject backdoors into LoRA adapters that retain both malicious and benign capabilities when merged with task-specific LoRAs. The authors demonstrate the feasibility of this 'LoRA-as-an-Attack' under the share-and-play paradigm, highlighting vulnerabilities in community-shared adapters."
          },
          "strengths": {
            "value": "The paper introduces a novel threat model for LoRA-based systems, addressing an underexplored attack surface in open-source ecosystems. The problem statement is timely and relevant given the popularity of LoRA in community-driven workflows. The experimental analysis, while limited, provides initial evidence of backdoor persistence across merged LoRAs. The motivation is compelling, emphasizing the risks of decentralized LoRA distribution without integrity checks."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficient, with minimal quantitative results on backdoor effectiveness and downstream task performance. The paper lacks comparison with existing backdoor detection methods or mitigation strategies. The attack mechanism relies on a specific 'FF-only merging' recipe, but the paper does not thoroughly analyze its generalizability. The rebuttal addresses some concerns about real-world relevance but fails to resolve critical gaps in scalability, detection, and practical implications."
          },
          "questions": {
            "value": "1. How were the backdoor datasets constructed, and what guarantees exist that they reflect real-world attack scenarios? 2. What is the exact mechanism enabling 'LoRA once, backdoor everywhere'—does it depend on specific LoRA configurations or training hyperparameters? 3. How would the proposed attack scale to diverse downstream tasks and LoRA merging techniques beyond the FF-only approach? 4. What are the limitations of the current evaluation metrics in capturing real-world backdoor efficacy?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0rS9o1uKqu": {
    "paper_id": "0rS9o1uKqu",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces TLDR, a network inversion approach to reconstruct training-like data from vision classifiers with convolutional layers. The method leverages classifier confidence, robustness to perturbations, and gradient properties to guide a conditioned generator in producing semantically similar data to the training set. Experiments on standard datasets demonstrate the feasibility of reconstruction even with regularization techniques."
          },
          "strengths": {
            "value": "The paper presents a novel extension of network inversion to convolutional neural networks (CNNs) with multi-class tasks, addressing a gap in prior work focused on fully connected layers. The method incorporates multiple complementary signals (confidence, robustness, gradients) and emphasizes practical constraints (no pre-training, auxiliary data, or gradient access). The experimental validation on diverse datasets and the clear theoretical motivation for the approach are notable strengths."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous comparisons with existing inversion methods, particularly those in the related work section. The ablation study and quantitative metrics (e.g., SSIM) are only mentioned in the rebuttal, suggesting insufficient detail in the original submission. The theoretical justification for combining the proposed losses and their relative importance is underdeveloped. Additionally, the practical challenges of weight sharing in CNNs (highlighted in the rebuttal) are not adequately addressed in the analysis."
          },
          "questions": {
            "value": "1. How does TLDR compare in performance to existing inversion methods (e.g., those using gradient-based optimization or SAT solvers) under similar constraints? 2. What are the specific limitations of the approach when applied to larger datasets like Tiny-ImageNet, and how does the method scale? 3. How are the hyperparameters for the loss functions tuned, and what is their impact on reconstruction quality? 4. The rebuttal mentions 'prior knowledge' in the form of variational loss—how is this prior derived, and what assumptions does it rely on?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces TLDR, a network inversion method to reconstruct training-like data from vision classifiers. The approach uses a conditioned generator guided by classifier properties like confidence, robustness, and gradient behavior. It extends prior work to CNNs and multi-class settings without pre-training or auxiliary data."
          },
          "strengths": {
            "value": "The paper addresses a critical privacy concern in ML model sharing, which is highly significant. The methodology combines multiple signals (confidence, robustness, gradients) to guide inversion, showing creativity. The extension to CNNs and multi-class tasks is novel compared to prior work focused on fully connected layers. The theoretical framework is well-structured, and the empirical validation on standard datasets demonstrates practical relevance."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient experimental rigor, such as quantitative metrics (e.g., SSIM) and ablation studies to validate the proposed losses. The comparison to prior work is unclear, particularly in distinguishing TLDR from existing methods that also avoid pre-training or auxiliary data. The explanation of the generator's conditioning mechanism and loss functions is vague, reducing clarity. The claim about CNNs being harder to invert due to weight sharing is not thoroughly justified."
          },
          "questions": {
            "value": "1. How does TLDR specifically differ from the NeurIPS 2022 work [A] in terms of methodology and assumptions? 2. What metrics were used to evaluate the semantic similarity of reconstructed data, and how do they compare to baselines? 3. How was the generator's diversity ensured without explicit prior knowledge of the data distribution? 4. Why is gradient-based reconstruction feasible in CNNs despite weight sharing, which the authors claim complicates inversion?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces TLDR, a network inversion method to reconstruct training-like data from vision classifiers with convolutional layers. The approach leverages classifier properties such as confidence, robustness to perturbations, and gradient behavior, combined with a conditioned generator and multiple loss functions, to produce semantically similar data to the original training set."
          },
          "strengths": {
            "value": "The paper addresses a critical privacy concern in ML model sharing, which is highly relevant. The extension of inversion techniques to CNNs with regularization (e.g., dropout, batch normalization) represents a significant technical challenge. The method's use of multiple loss functions (cross-entropy, KL divergence, cosine similarity, feature orthogonality) and conditioning mechanisms demonstrates thoroughness. The novelty lies in applying inversion to realistic, large-scale CNNs rather than simplified settings, as highlighted in the rebuttal."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficient, with limited quantitative metrics (e.g., SSIM) and no ablation studies on loss components. The paper lacks comparisons to state-of-the-art inversion methods, making it hard to assess relative performance. The claim of reconstructing 'training-like data' without prior knowledge or gradients is underexplored, and the computational complexity of the method is not discussed. The rebuttal acknowledges these gaps, suggesting the revised version will address them."
          },
          "questions": {
            "value": "1. How does the method handle varying architectures beyond CNNs? 2. What are the computational costs and scalability of the approach? 3. How does the generator avoid producing random noise instead of semantically meaningful data? 4. Are the reconstructed samples quantitatively evaluated against training data using metrics like FID or SSIM? 5. How does the method ensure diversity in generated samples without auxiliary datasets or pre-training?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0sJ8TqOLGS": {
    "paper_id": "0sJ8TqOLGS",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces SPARK, a framework to evaluate large language models' (LLMs) ability to identify inconsistencies in problem framing using the Hierarchical Three-Space Theory. The authors modify existing datasets to create benchmarks, measuring problem-solving and challenge rates. They test multiple models and hypotheses (e.g., SSI, PSS) to analyze LLMs' critical thinking capabilities across domains."
          },
          "strengths": {
            "value": "The paper presents a rigorous theoretical foundation rooted in the Three-Space Theory, offering a structured approach to evaluate critical thinking. The framework is comprehensive, covering diverse domains (math, science, reading) and hypotheses about problem-solving strategies, domain generalization, and robustness to misinformation. The experimental design is detailed, with clear metrics (problem-solving rate, challenge rate) and controlled dataset modifications. The work addresses a critical gap in evaluating LLMs' ability to critique problem setups, which is vital for real-world applications."
          },
          "weaknesses": {
            "value": "The dataset modifications (e.g., hidden correct answers, missing information) are simplistic and may not capture real-world inconsistencies effectively. The challenge rate metric relies on assumptions about model behavior (e.g., inherent challenge tendencies) that are not thoroughly validated. The rebuttal clarifies the calculation of challenge rates, but the methodology for isolating inconsistency detection from other factors (e.g., model architecture, prompting) remains under-explained. Additionally, the paper lacks ablation studies to assess the individual impact of each hypothesis."
          },
          "questions": {
            "value": "1. How do the authors ensure that the challenge rate is not confounded by factors like model architecture or prompting techniques? 2. Are there ablation studies to validate the effectiveness of individual hypotheses (e.g., SSI, PSS)? 3. How do the authors handle cases where models challenge problem setups due to biases or other non-critical thinking factors? 4. What evidence supports the claim that critical thinking reflects a 'higher-order' aspect of LLM behavior, distinct from hallucination?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces SPARK, a framework to evaluate large language models' (LLMs) ability to identify inconsistencies in problem framing using the Hierarchical Three-Space Theory. The authors modify existing datasets to create benchmarks that test critical thinking through tasks like missing information, hidden correct answers, and misleading hints. They propose five hypotheses (SSI, PSS, ADA, RMI, KBC) and evaluate state-of-the-art LLMs on these tasks, highlighting limitations in critical thinking and exploring mitigation strategies."
          },
          "strengths": {
            "value": "The paper presents a structured theoretical framework (Three-Space Theory) grounded in cognitive science, offering a novel lens for evaluating critical thinking in LLMs. The SPARK framework introduces clear hypotheses and metrics (problem-solving rate, challenge rate) to systematically assess model behavior. The experimental design is comprehensive, covering diverse domains (math, science, reading comprehension) and exploring factors like model architecture, prompting strategies, and in-context learning. The work addresses an under-explored aspect of AI evaluation: detecting flaws in problem setups rather than just solving well-defined tasks."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with existing benchmarks for critical thinking, making it difficult to assess the novelty and effectiveness of SPARK. The modified datasets (e.g., hidden correct answers, misleading hints) are simplistic and may not reflect real-world inconsistencies, which are often more nuanced. The evaluation metrics (e.g., challenge rate) are not thoroughly justified, and their calculation relies heavily on appendix details. The rebuttal clarifies some methodological points but does not fully address concerns about the real-world validity of the modified datasets or the statistical significance of results."
          },
          "questions": {
            "value": "1. How do the authors justify the simplicity of their modified datasets compared to real-world problem inconsistencies? 2. Are the challenge rate and problem-solving rate metrics robust to variations in model architecture or prompting strategies? 3. What statistical tests were used to validate the significance of the observed differences in model performance across hypotheses? 4. How do the authors address potential biases in their manual curation of held-out datasets for evaluating correctness and challenge rates?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces SPARK, a framework to evaluate large language models' (LLMs) ability to identify inconsistencies in problem framing using the Hierarchical Three-Space Theory. The authors modify existing datasets to create benchmarks that test critical thinking by introducing missing information, misleading cues, and flawed problem setups. They propose five hypotheses (SSI, PSS, ADA, RMI, KBC) and evaluate LLMs' performance on tasks spanning mathematics, science, and reading comprehension. The experiments highlight limitations in LLMs' critical thinking, particularly in recognizing inconsistencies, and explore mitigation strategies like modified prompting and fine-tuning."
          },
          "strengths": {
            "value": "The paper presents a novel framework (SPARK) grounded in a well-established theoretical model (Three-Space Theory), offering a structured approach to evaluate critical thinking in LLMs. The methodology for creating benchmarks by modifying datasets is creative and addresses a gap in existing evaluation methods. The work's focus on problem framing inconsistencies aligns with real-world challenges, and the proposed metrics (problem-solving rate and challenge rate) provide quantifiable measures for critical thinking. The paper also acknowledges limitations and explores mitigation strategies, demonstrating a comprehensive approach."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient experimental details to validate the proposed metrics. For example, the calculation of 'challenge rate' is unclear, and the rebuttal's explanation relies on an appendix, which may not be accessible during review. The experiments do not include ablation studies or comparisons with strong baselines, making it difficult to assess the impact of specific design choices (e.g., dataset modifications). Additionally, the paper does not address how the framework scales to more complex or realistic real-world scenarios, despite acknowledging that their setup is simplified. The choice of models (e.g., Llama-3.1, Mistral, GPT-4o) is not justified, and the results are not contextualized against prior work on critical thinking in LLMs."
          },
          "questions": {
            "value": "1. How are the 'problem-solving rate' and 'challenge rate' calculated in practice? The paper refers to an appendix, but the definitions and implementation details are not clear in the main text. 2. The rebuttal claims that the framework controls for knowledge insufficiency, but how are 'well-defined questions' and 'modified questions' selected to ensure this? 3. What statistical significance tests were performed to validate the results, and how do the findings compare to existing benchmarks for critical thinking in LLMs? 4. The paper mentions 'domain-general' critical thinking but does not provide quantitative evidence of cross-domain transferability. 5. How do the proposed mitigation strategies (e.g., modified prompting) compare to existing techniques for improving LLM robustness?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0sary0UZn5": {
    "paper_id": "0sary0UZn5",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper investigates the rank properties of attention matrices in Transformers, identifying a low-rank barrier and model-reduction effect. Through extensive experiments on synthetic and real-world datasets, and theoretical analysis, the authors show that attention ranks increase with head dimension $d_h$ but eventually saturate at approximately $0.63n$. Theoretical bounds and empirical validations are provided, with implications for understanding Transformer efficiency and capacity."
          },
          "strengths": {
            "value": "The paper presents rigorous theoretical analysis with clear mathematical bounds on attention ranks, supported by comprehensive experiments across diverse settings. The empirical results are well-structured, and the work contributes to understanding Transformer architectural limitations. The clarity of the problem formulation and the detailed ablation studies enhance the paper's quality. The theoretical insights into low-rank barriers and model-reduction effects are novel and significant for efficiency-oriented model design."
          },
          "weaknesses": {
            "value": "The experiments primarily focus on random data and weights, raising questions about generalizability to real-world scenarios. While the rebuttal addresses this by adding NLP experiments, the theoretical assumptions (e.g., exact orthogonality) remain restrictive. The connection between rank saturation and practical model performance (e.g., accuracy, efficiency) is not sufficiently explored. Additionally, the paper's abstract and introduction may overstate the generality of findings, requiring clearer disclaimers about the scope of the analysis."
          },
          "questions": {
            "value": "1. How do the theoretical results on random data translate to real-world datasets with complex dependencies? 2. What specific practical challenges (e.g., computational efficiency, model accuracy) are addressed by the low-rank barrier and model-reduction insights? 3. Could the model-reduction effect be leveraged to design more efficient Transformers without sacrificing performance? 4. How sensitive are the theoretical bounds to deviations from the assumed orthogonality conditions?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates the rank properties of attention matrices in Transformers, identifying a low-rank barrier and model-reduction effect. Through extensive experiments and theoretical analysis, the authors demonstrate that attention ranks increase with head dimension $d_h$ but eventually saturate at approximately $0.63n$, where $n$ is the sequence length. They provide mathematical bounds and validate their findings across diverse model configurations and data distributions."
          },
          "strengths": {
            "value": "The paper makes significant contributions by systematically analyzing the rank dynamics of attention matrices, combining empirical evidence with rigorous theoretical analysis. The experimental design is thorough, covering varied model dimensions, sequence lengths, and data distributions. Theoretical results, such as the $0.63n$ upper bound, offer novel insights into Transformer architecture. The clarity of presentation, including detailed tables and figures, enhances readability."
          },
          "weaknesses": {
            "value": "The experiments primarily use randomly initialized models and synthetic data, limiting generalizability to real-world scenarios. The theoretical analysis assumes exact orthogonality of inputs, which may not hold in practice. The connection between rank saturation and practical applications (e.g., efficiency gains) is underexplored. While the rebuttal addresses some concerns, the paper lacks sufficient validation on large-scale real-world datasets, particularly in NLP tasks."
          },
          "questions": {
            "value": "1. How do the findings apply to Transformers trained on real-world data, where input distributions and training dynamics differ from random initialization? 2. What are the practical implications of the low-rank barrier for model design, such as trade-offs between efficiency and expressiveness? 3. Can the model-reduction effect be leveraged to optimize computational resources without sacrificing performance? 4. The theoretical assumptions (e.g., exact orthogonality) require further justification for real-world validity."
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates the rank properties of attention matrices in Transformers, identifying a 'low-rank barrier' and 'model-reduction effect' where attention ranks increase with head dimension $d_h$ but eventually saturate. The authors combine empirical experiments on synthetic and real-world data with theoretical analysis to show that attention ranks are bounded by $\\approx 0.63n$ and saturate when $d_h = \\Omega(\\log n)$. They argue these findings shed light on Transformer efficiency and capacity."
          },
          "strengths": {
            "value": "The paper makes a strong case for the theoretical and empirical analysis of attention rank dynamics. The experiments are comprehensive, covering varied model configurations, data distributions, and sequence lengths. The theoretical results, while under idealized assumptions, provide a rigorous foundation for the observed phenomena. The clarity of the problem formulation and structure is commendable, and the paper addresses a critical gap in understanding Transformer efficiency."
          },
          "weaknesses": {
            "value": "The primary limitation is the reliance on synthetic data and random weight initialization, which may not generalize to real-world scenarios. While the authors address this in the rebuttal by adding NLP experiments, the real-world validation remains limited. The theoretical analysis assumes orthogonality, which is not guaranteed in practice, and the connection between attention rank and practical performance (e.g., accuracy, efficiency) is underdeveloped. The paper also lacks ablation studies on how rank saturation affects downstream tasks."
          },
          "questions": {
            "value": "1. How do the observed rank properties directly impact Transformer performance or efficiency in practical applications? 2. What evidence supports the generalization of theoretical results to non-orthogonal, real-world data? 3. Could the model-reduction effect be leveraged to design more efficient Transformers, and if so, how? 4. How do the NLP experiments on IMDB compare to the image-based results in terms of rank saturation trends?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0uRc3CfJIQ": {
    "paper_id": "0uRc3CfJIQ",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes ORSO, an algorithm for online reward selection and policy optimization in reinforcement learning. By framing reward function selection as an online model selection problem, ORSO automatically identifies high-performing shaping rewards with provable regret guarantees. The method demonstrates superior data efficiency and computational performance compared to prior approaches, achieving results comparable to manually engineered rewards."
          },
          "strengths": {
            "value": "Originality is strong through the novel online model selection framework for reward design. Theoretical contributions include provable regret bounds, while experiments across diverse tasks validate effectiveness. The clarity of the method's two-phase structure (reward generation and online selection) and the practical demonstration of computational efficiency are notable. The significance lies in addressing the critical challenge of reward shaping in RL with minimal human intervention."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed analysis of how ORSO's performance varies with different reward function sets, despite the rebuttal's partial response. The theoretical guarantees rely on strong assumptions (e.g., monotonic dominance of the optimal learner), which may limit applicability. While the rebuttal provides empirical evidence for reward function selection, it does not fully address how ORSO adapts to non-stationary or poorly designed reward spaces. The connection between the D³RB algorithm and ORSO's regret bounds requires deeper clarification."
          },
          "questions": {
            "value": [
              "How does ORSO handle scenarios where none of the candidate reward functions are near-optimal? The rebuttal mentions iterative improvement, but the mechanism for dynamically adding new candidates is unclear.",
              "The theoretical analysis assumes a monotonic dominance condition for the optimal learner. How robust is this assumption in practice, especially with non-stationary environments?",
              "The rebuttal's table for the Ant task shows ORSO selects top reward functions, but how does this generalize to other tasks? Are there cases where ORSO's selection is less reliable?",
              "What are the computational trade-offs between different selection strategies (e.g., ε-greedy vs. Exp3) in ORSO? The paper mentions empirical validation but lacks a systematic comparison."
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes ORSO, an algorithm for online reward selection and policy optimization in reinforcement learning. It frames reward function selection as an online model selection problem, using techniques from multi-armed bandits to efficiently identify high-performing shaping rewards. ORSO claims to reduce data and computational requirements while achieving performance comparable to manually designed rewards."
          },
          "strengths": {
            "value": "The paper presents a novel approach by formalizing reward selection as an online decision-making problem with provable regret guarantees. The methodology is theoretically grounded, and the experiments demonstrate significant improvements in data efficiency and performance across continuous control tasks. The clarity of the algorithm description and the structured presentation of results are notable strengths."
          },
          "weaknesses": {
            "value": "The theoretical analysis relies on strong assumptions (e.g., existence of a dominant learner) that may not hold in practice. The experimental validation, while extensive, lacks a detailed analysis of how ORSO's performance varies with different reward function sets. The rebuttal addresses some concerns but provides limited evidence for scalability and robustness to suboptimal candidates. The paper also does not thoroughly discuss the limitations of the assumed reward generator $ G $."
          },
          "questions": {
            "value": [
              "How does ORSO handle scenarios where none of the candidate reward functions are near-optimal? The rebuttal mentions iterative improvement, but the mechanism and effectiveness of this process are unclear.",
              "The theoretical guarantees depend on the assumption of a monotonic dominant learner. How general is this assumption, and what happens if it is violated?",
              "The experimental analysis of reward function selection (e.g., Table 1) focuses on a single task (Ant). More ablation studies across diverse environments would strengthen the claims.",
              "The paper mentions a reward generator $ G $ but does not elaborate on its design or how it impacts the quality of candidate reward functions. What are the implications for real-world applications?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes ORSO, an algorithm for online reward selection and policy optimization in reinforcement learning. It frames reward function selection as an online model selection problem, using techniques from multi-armed bandits to balance exploration and exploitation. ORSO claims to improve data efficiency, reduce computational costs, and outperform prior methods like EUREKA and manually designed rewards."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in RL—reward shaping—by introducing a novel online decision-making framework. The theoretical guarantees with regret bounds are rigorous, and the empirical results demonstrate significant improvements in data efficiency and performance. The method's ability to automate reward selection without human intervention is a strong practical contribution. The code availability and clear problem formulation further enhance its value."
          },
          "weaknesses": {
            "value": "The theoretical analysis assumes a monotonic dominant learner, which may not hold in practice. The experiments, while promising, focus on a single task (Ant) in the rebuttal, limiting generalizability. The paper lacks ablation studies on key parameters (e.g., selection strategies, reward function diversity) and does not thoroughly analyze how the reward generator $G$ impacts performance. The comparison with prior methods is insufficiently detailed, and the non-stationary nature of reward utilities is not fully addressed."
          },
          "questions": {
            "value": [
              "How does ORSO handle scenarios where the assumed monotonic dominant learner (Assumption 4.2) does not hold? Are there empirical validations of this assumption?",
              "The rebuttal provides results for the Ant task, but how do these generalize to other continuous control tasks? Additional experiments on diverse domains would strengthen the claims.",
              "What is the role of the reward generator $G$ in the method? How is it designed, and how sensitive is ORSO to its choice?",
              "The paper mentions 'provable regret guarantees,' but the analysis assumes stationary reward distributions. How does ORSO address the non-stationarity of reward utilities during training?",
              "The ablation studies on selection strategies (e.g., $\\epsilon$-greedy vs. Exp3) are limited. Could the authors provide more insights into the trade-offs between different strategies?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "0vtftmYQGV": {
    "paper_id": "0vtftmYQGV",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes SNAP-TTA, a sparse test-time adaptation (STTA) framework designed to reduce latency for edge devices by minimizing adaptation frequency and data usage. The method combines Class and Domain Representative Memory (CnDRM) for efficient sampling and Inference-only Batch-aware Memory Normalization (IoBMN) for dynamic normalization during inference. It demonstrates competitive accuracy with adaptation rates as low as 0.01 while maintaining compatibility with resource-constrained hardware."
          },
          "strengths": {
            "value": "The paper presents a novel approach to STTA by addressing the critical trade-off between latency and accuracy in edge computing. The CnDRM component innovatively combines class and domain representation for efficient sampling, while IoBMN provides a lightweight normalization strategy. The methodology is well-structured, with clear technical contributions and practical relevance. The experiments validate the effectiveness of SNAP-TTA across multiple TTA algorithms and edge devices, showcasing its adaptability. The rebuttal further strengthens the evaluation by adding latency data on additional hardware, demonstrating the framework's broad applicability."
          },
          "weaknesses": {
            "value": "While the rebuttal addresses some concerns, the paper initially lacked comprehensive evaluations across diverse edge devices and TTA algorithms. The theoretical justification for why CnDRM and IoBMN achieve such strong performance remains underdeveloped. The analysis of adaptation rate limits (e.g., minimum viable rate for MCUs) and comparisons with existing sparse adaptation methods is insufficient. Additionally, the memory usage analysis, though partially addressed in the rebuttal, requires more quantitative details to fully assess scalability on ultra-low-resource devices."
          },
          "questions": {
            "value": [
              "What are the theoretical guarantees for the effectiveness of CnDRM's dual criteria (class and domain representation) in low-data regimes?",
              "How does SNAP-TTA compare to other sparse adaptation methods (e.g., periodic updates, pruning-based approaches) in terms of accuracy-latency trade-offs?",
              "What is the minimum viable adaptation rate for SNAP-TTA on MCUs, and how does it impact model stability?",
              "Are there scenarios where IoBMN's batch-aware normalization could introduce new biases or instability in the presence of extreme domain shifts?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes SNAP-TTA, a sparse test-time adaptation (TTA) framework designed to reduce latency on edge devices by minimizing adaptation frequency and data usage. The method combines Class and Domain Representative Memory (CnDRM) for efficient sampling and Inference-only Batch-aware Memory Normalization (IoBMN) for dynamic normalization during inference. The approach maintains competitive accuracy with adaptation rates as low as 0.01, demonstrating effectiveness across multiple TTA algorithms and edge devices."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in real-world deployment of TTA methods by focusing on latency constraints for edge devices. The proposed framework (SNAP-TTA) introduces two novel components (CnDRM and IoBMN) that balance accuracy and efficiency through sparse adaptation. The rebuttal significantly strengthens the evaluation by adding experiments across three additional edge devices (Jetson Nano, Raspberry Pi Zero 2W), demonstrating consistent latency reduction. The methodology is well-structured, with clear ablation studies and theoretical justification for key design choices. The practical relevance to latency-sensitive applications (e.g., autonomous driving) is emphasized."
          },
          "weaknesses": {
            "value": "While the rebuttal improves the evaluation, the initial experiments lacked depth in analyzing the trade-offs between adaptation rates and accuracy. The theoretical foundation for selecting domain-representative samples (e.g., Wasserstein distance-based criteria) could be more rigorously justified. The memory management details are primarily in the appendix, and the paper does not fully address how CnDRM adapts to extreme domain shifts. Additionally, the comparison with existing STTA methods is limited, and the paper does not explore the impact of hyperparameter choices (e.g., confidence threshold, momentum parameter) in depth."
          },
          "questions": {
            "value": "How does the adaptation rate (e.g., 0.01 vs. 0.5) affect performance in scenarios with rapidly changing domains? What are the specific computational constraints of MCU platforms (e.g., Cortex-M) that SNAP-TTA must adhere to, and how does the framework handle them? How does CnDRM ensure robustness when the initial model's predictions are highly uncertain (e.g., in noisy or out-of-distribution data)? The paper should clarify how the confidence threshold and domain centroid updates are calibrated in practice."
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes SNAP-TTA, a sparse test-time adaptation framework designed for edge devices with limited computational resources. It introduces two components: Class and Domain Representative Memory (CnDRM) for efficient sampling and Inference-only Batch-aware Memory Normalization (IoBMN) for dynamic normalization during inference. The method reduces adaptation frequency and data usage, achieving latency reductions while maintaining competitive accuracy across multiple TTA algorithms."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in deploying TTA on edge devices, offering a practical solution for latency-sensitive applications. The methodology combines novel sampling strategies (CnDRM) with efficient normalization (IoBMN) to achieve sparse adaptation. The rebuttal strengthens the evaluation by adding latency experiments on multiple edge devices, demonstrating broad compatibility. The focus on real-world constraints and the clear problem formulation contribute to its significance."
          },
          "weaknesses": {
            "value": "The original submission lacked comprehensive latency evaluations across diverse edge devices and TTA algorithms. While the rebuttal adds experiments on three devices, the paper still does not provide a thorough comparison with existing sparse TTA methods or a detailed analysis of computational complexity. The memory usage analysis is incomplete, and the paper does not fully clarify how SNAP-TTA differs from prior work on sparse adaptation. The theoretical justification for the Wasserstein distance-based domain sampling is also underdeveloped."
          },
          "questions": {
            "value": [
              "How does SNAP-TTA compare to existing sparse TTA approaches in terms of adaptation strategy and performance trade-offs?",
              "What are the exact memory and computational costs of CnDRM and IoBMN on MCUs, and how do they scale with different batch sizes or model architectures?",
              "The paper claims compatibility with MCUs like Cortex-M, but what specific optimizations enable this? Are there constraints on model size or TTA algorithm complexity?",
              "The Wasserstein distance criterion for domain representation is introduced without extensive ablation studies or justification for its superiority over alternative metrics."
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "0y3hGn1wOk": {
    "paper_id": "0y3hGn1wOk",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces FIUBENCH, a benchmark for evaluating unlearning in Vision Language Models (VLMs) under the 'Right to be Forgotten' setting. The work formalizes VLM unlearning as forgetting image-paired private knowledge, constructs a synthetic Fictitious Facial Identity VQA dataset, and proposes a two-stage evaluation pipeline with privacy attacks. Experiments on baseline methods reveal limitations in unlearning performance, emphasizing the need for robust evaluation metrics."
          },
          "strengths": {
            "value": "The paper addresses a novel and timely problem by focusing on VLM unlearning, which is underexplored compared to LLMs. The dataset construction is meticulous, with synthetic faces paired with fictional private data and a two-stage evaluation pipeline to control knowledge exposure. The inclusion of privacy attacks (membership inference and adversarial extraction) as evaluation metrics is a significant contribution. The formalization of VLM unlearning as forgetting image-paired knowledge, rather than visual attributes, is conceptually sound and addresses a critical gap in the literature."
          },
          "weaknesses": {
            "value": "The synthetic nature of the dataset may limit generalizability to real-world scenarios. The paper lacks comparison with existing VLM unlearning benchmarks, and the evaluation of baseline methods is limited to a single VLM architecture (e.g., LLaVA). The rebuttal clarifies early stopping criteria and GPT-Eval scaling, but the robustness of metrics like 'truth ration' remains unclear. Additionally, the paper does not thoroughly address how the Fictitious Facial Identity VQA dataset avoids prior knowledge leakage in VLMs, despite the 3.4% bias observed in LLaVA."
          },
          "questions": {
            "value": "1. How does the synthetic dataset's design mitigate risks of prior knowledge leakage in VLMs, given the 3.4% overlap detected in LLaVA? 2. Are the privacy attacks (membership inference, adversarial extraction) specifically tailored to VLMs, or are they adaptations of LLM-focused methods? 3. How do the results generalize across different VLM architectures (e.g., multimodal vs. unimodal)? 4. What are the computational costs of the two-stage pipeline, and how scalable is it for large VLMs? 5. How does the paper differentiate between 'forgetting' private knowledge and 'masking' it, given the potential for adversarial attacks to recover hidden information?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces FIUBENCH, a benchmark for evaluating unlearning in Vision Language Models (VLMs) under the 'Right to be Forgotten' setting. The benchmark includes a synthetic Fictitious Facial Identity VQA dataset, a two-stage evaluation pipeline (learning + unlearning), and robust privacy attack metrics. The authors evaluate four baseline unlearning methods and highlight limitations in their effectiveness, emphasizing the need for privacy-aware evaluations."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in VLM unlearning research by formalizing privacy-focused unlearning tasks and proposing a novel benchmark. The two-stage evaluation pipeline with controlled information exposure and the inclusion of membership inference and adversarial privacy attacks demonstrate rigorous methodological design. The synthetic dataset construction process is detailed, and the formal definition of VLM unlearning provides a clear framework for future work. The rebuttal clarifies the benchmark's purpose and addresses key concerns about evaluation metrics and baseline comparisons."
          },
          "weaknesses": {
            "value": "The synthetic dataset's limited real-world relevance and potential biases (e.g., 3.4% privacy leakage) raise concerns about generalizability. The evaluation metrics focus on specific privacy attack scenarios but may not fully capture real-world privacy risks. The baseline methods are limited in number and scope, and the comparison between different VLMs (e.g., LLaVA-Phi vs. LLama-Vision) reveals inconsistencies that suggest the benchmark's robustness could be improved. The paper lacks ablation studies on the impact of dataset construction choices (e.g., K-means clustering, UMAP reduction)."
          },
          "questions": {
            "value": "1. How does the 3.4% privacy leakage in the dataset affect the validity of the unlearning evaluation? 2. Are the synthetic facial identities and private backgrounds sufficiently representative of real-world privacy risks? 3. Could the two-stage evaluation pipeline be adapted to other types of VLM unlearning tasks beyond facial identity? 4. How do the authors plan to expand the benchmark to include more diverse unlearning scenarios and VLM architectures?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces FIUBENCH, a benchmark for evaluating unlearning in Vision Language Models (VLMs) under the Right to be Forgotten framework. The authors formalize VLM unlearning as forgetting image-paired private knowledge, construct a synthetic Fictitious Facial Identity VQA dataset, and propose a two-stage evaluation pipeline with privacy attacks. They evaluate baseline unlearning methods and highlight trade-offs between model utility and forget quality."
          },
          "strengths": {
            "value": "The paper's strengths include a novel and structured approach to VLM unlearning, the creation of a specialized benchmark with synthetic data, and the inclusion of privacy attacks for robust evaluation. The two-stage pipeline (learning/unlearning) and formal definition of VLM unlearning address gaps in prior work. The empirical analysis of baseline methods provides actionable insights into unlearning limitations."
          },
          "weaknesses": {
            "value": "The synthetic dataset's real-world applicability is questionable, as it relies on fictional identities and may not capture complex privacy scenarios. The evaluation lacks comparisons with other benchmarks or state-of-the-art methods. The paper does not thoroughly address how the two-stage pipeline generalizes to non-facial data. Additionally, the synthetic data's potential biases and leakage risks are underexplored despite the authors' brief analysis."
          },
          "questions": {
            "value": "1. How representative are the synthetic faces and private data in the Fictitious Facial Identity VQA dataset compared to real-world privacy risks? 2. What are the limitations of using GPT-4o-generated VQA pairs for evaluation, and how do they affect the benchmark's validity? 3. How scalable is the two-stage pipeline for large-scale VLMs with diverse modalities beyond facial identities? 4. Could the early stopping criterion (loss threshold of -20) introduce bias in comparing unlearning methods?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "10JOlFIPjt": {
    "paper_id": "10JOlFIPjt",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces NEMO, a multimodal contrastive learning framework for classifying cell types and brain regions from electrophysiological data. NEMO jointly embeds autocorrelation (ACG) images and extracellular waveforms (EAPs) of neurons, achieving state-of-the-art performance on two opto-tagged datasets and the IBL Brain-wide Map dataset."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by applying contrastive learning to neurophysiological data with two distinct modalities (ACG and EAP). The methodology is rigorous, with careful preprocessing, data augmentation, and a clear contrastive objective. The experiments are well-designed, showing significant improvements over baselines like PhysMAP and VAEs. The paper also addresses practical challenges like label scarcity and provides detailed analysis of performance metrics. The presentation is clear, with informative figures and supplementary materials."
          },
          "weaknesses": {
            "value": "The paper could better address the limitations of its approach, such as the lack of comparison with other multimodal methods beyond PhysMAP and VAEs. The justification for using ACG and EAP as modalities is somewhat superficial, and the paper does not fully explore the relationship between these modalities and cell-type/brain-region classification. Additionally, the paper's claims about generalizability are weakened by not testing on datasets like S1, A1, or V1/Hippocampus used in PhysMAP. The analysis of class imbalance in brain region classification is also underdeveloped."
          },
          "questions": {
            "value": [
              "The paper mentions that NEMO outperforms SimCLR on the IBL dataset, but it is unclear how the noise characteristics of IBL differ from the UHD dataset. Could the authors elaborate on this?",
              "The authors note that they did not test NEMO on the S1, A1, and V1/Hippocampus datasets used in PhysMAP. What are the challenges in adapting NEMO to these datasets, and how might this affect performance?",
              "The paper uses single-channel templates for some datasets and multi-channel for others. How does this choice impact the model's ability to capture spatial information, and what are the trade-offs?",
              "The rebuttal mentions statistical significance for NEMO's improvements, but the paper's original submission did not include detailed p-values or effect sizes. Could the authors provide more granular statistical analysis in the final version?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces NEMO, a multimodal contrastive learning framework for classifying neuronal cell-types and brain regions using electrophysiological data. It combines autocorrelation of spiking activity and extracellular waveforms, pre-trained on unlabeled data and fine-tuned for downstream tasks. NEMO achieves state-of-the-art performance on two opto-tagged datasets and the IBL Brain-wide Map dataset."
          },
          "strengths": {
            "value": "NEMO addresses a critical gap in neuroscience by enabling cell-type and brain region classification without histological data, which is both novel and impactful. The method's integration of multiple modalities (autocorrelation and waveforms) is well-justified, and the experiments are thorough, with strong comparisons to baselines like PhysMAP and VAEs. The paper's clarity is commendable, with detailed technical descriptions and clear evaluation metrics."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous statistical validation for the claimed improvements, particularly for smaller gains. While the rebuttal includes t-tests, results are mixed (e.g., NEMO is not significantly better than SimCLR on some metrics). The generalizability to other datasets (e.g., S1, A1, V1/Hippocampus) is untested, and the handling of class imbalance in metrics like F1 and balanced accuracy is unclear. The choice of data augmentations and encoder architectures could be better justified."
          },
          "questions": {
            "value": "1. How were class imbalances in the datasets addressed, and how might they affect the reported metrics? 2. What specific challenges arise when applying NEMO to datasets with different channel configurations (e.g., multi-channel vs. single-channel templates)? 3. How does the performance on the IBL dataset compare to other benchmark datasets, and what factors might influence this? 4. Could the authors elaborate on the theoretical justification for using CLIP-inspired objectives in this context?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces NEMO, a multimodal contrastive learning framework for classifying cell types and brain regions from electrophysiological data. NEMO jointly embeds spiking activity autocorrelations and extracellular waveforms, demonstrating state-of-the-art performance on opto-tagged datasets and the IBL Brain-wide Map dataset. The method leverages a CLIP-inspired contrastive objective and shows strong results in label-limited regimes."
          },
          "strengths": {
            "value": "The paper presents a novel application of contrastive learning to neurophysiological data, addressing a critical gap in scalable cell-type and brain region classification. The methodology is well-structured, with clear technical details on preprocessing, data augmentations, and encoder architectures. Empirical results are robust, with NEMO outperforming existing methods on multiple benchmarks. The inclusion of code and project resources enhances reproducibility and practical utility. The rebuttal also addresses statistical significance concerns, strengthening the validity of the claims."
          },
          "weaknesses": {
            "value": "The experiments are limited to specific datasets (NP Ultra, C4 cerebellum, IBL), and the paper does not evaluate NEMO on the S1, A1, and V1/Hippocampus datasets used in PhysMAP, which could affect generalizability. While the rebuttal provides significance tests, the original paper lacked detailed comparisons to unimodal baselines like SimCLR beyond the IBL dataset. The justification for multimodal learning over unimodal approaches is somewhat superficial, and the paper could better address potential limitations of the CLIP-inspired objective in neurophysiological contexts. Additionally, the paper's claims about the necessity of multimodal learning could be more rigorously supported with ablation studies."
          },
          "questions": {
            "value": "1. How does NEMO perform on the S1, A1, and V1/Hippocampus datasets used in PhysMAP, and what are the challenges in adapting the method to these domains? 2. What ablation studies were conducted to validate the necessity of multimodal learning versus unimodal approaches? 3. How does the CLIP-inspired contrastive objective handle modality-specific noise in electrophysiological data, and what are the trade-offs compared to other contrastive frameworks? 4. Can the method be extended to handle other modalities, such as peri-stimulus time histograms (PSTHs), as mentioned in the related work?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "13G5KXm98a": {
    "paper_id": "13G5KXm98a",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a novel confidence decision boundary visualization method for active learning (AL) using Voronoi tessellation and ridge confidence. The approach aims to capture the cumulative effects of AL strategies on model training, providing deeper insights into sampling behaviors and decision boundary dynamics. The method is evaluated on MNIST and CIFAR-10, demonstrating its ability to visualize how different query strategies select samples and impact model performance."
          },
          "strengths": {
            "value": "The paper presents a creative combination of Voronoi tessellation and confidence metrics to address a critical gap in AL visualization. The methodology is well-structured, with clear definitions of key concepts like 'predicted ridge confidence.' The experiments are comprehensive, comparing multiple AL strategies and providing visual insights into their behaviors. The use of a separate visualization model to fix feature representations for consistent analysis is a notable strength. The paper also acknowledges limitations of prior work and provides a clear rationale for the proposed approach."
          },
          "weaknesses": {
            "value": "The paper lacks a direct comparison with existing visualization techniques, making it difficult to assess the novelty and effectiveness of the proposed method. The experiments are limited to two datasets (MNIST and CIFAR-10), and the scalability to more complex, high-dimensional, or multi-class datasets is not thoroughly explored. The explanation of how the visualization model is trained and its impact on results is insufficient. Additionally, the paper does not address potential computational challenges when applying the method to large-scale datasets."
          },
          "questions": {
            "value": "1. How does the Voronoi tessellation-based approach handle high-dimensional data, and what are the specific limitations when applied to datasets with more than 20 classes? 2. What ablation studies were conducted to validate the effectiveness of the 'predicted ridge confidence' metric? 3. How does the training of the visualization model affect the interpretation of the results, and are there any biases introduced by this fixed feature map approach? 4. Could the authors provide more details on how the method scales to larger datasets or real-world applications?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces a confidence decision boundary visualization method for active learning (AL) using Voronoi tessellation and ridge confidence. This approach aims to capture the dynamic behavior of AL query strategies by analyzing boundary regions in reduced-dimensional spaces, with experiments on MNIST and CIFAR-10 highlighting differences in sampling behaviors and model uncertainty."
          },
          "strengths": {
            "value": "The paper presents a novel visualization technique combining Voronoi tessellation with ridge confidence to analyze AL query strategies, addressing a gap in existing methods that fail to capture iterative model updates. The method's focus on boundary regions and confidence metrics provides actionable insights into sampling dynamics. The experiments on two benchmark datasets demonstrate practical utility, and the rebuttal clarifies key implementation details (e.g., fixed vs. dynamic features) to strengthen the validity of results."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparison with existing AL visualization techniques, limiting the assessment of its novelty and effectiveness. The experimental validation is limited to two datasets, and the scalability to larger/complex datasets (e.g., more than 20 classes) is underexplored. The confidence metric (Equation 2) is not sufficiently justified, and the computational cost of Voronoi tessellation for high-dimensional data remains unclear. The rebuttal addresses some concerns but does not fully resolve questions about generalizability and theoretical foundations."
          },
          "questions": {
            "value": "1. How does the method handle high-dimensional data beyond 2D? What are the limitations of t-SNE-based dimensionality reduction on Voronoi tessellation accuracy? 2. Can the confidence metric (Equation 2) be theoretically validated, or is it primarily heuristic? 3. How does the proposed method compare to established AL visualization techniques (e.g., decision boundary plotting, uncertainty maps) in terms of interpretability and practical utility? 4. What are the specific computational bottlenecks when applying this method to large-scale datasets?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a Voronoi tessellation-based confidence decision boundary visualization method to analyze active learning (AL) query strategies. The approach uses Voronoi cells to partition reduced-dimensional feature spaces and evaluates ridge confidence to highlight regions of the decision boundary with varying uncertainty. The method aims to provide deeper insights into how different AL strategies select samples and how these selections impact model training over time."
          },
          "strengths": {
            "value": "The paper presents a novel visualization framework that addresses a gap in understanding the cumulative effects of AL strategies. The use of Voronoi tessellation to partition feature spaces and quantify ridge confidence is creative and offers a fresh perspective on decision boundary analysis. The experiments on MNIST and CIFAR-10 demonstrate practical insights into query strategy behaviors, such as how uncertainty types evolve and how diversity methods interact with model biases. The method's ability to highlight regions of high/low confidence provides actionable information for AL practitioners."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous comparisons with existing visualization techniques, making it unclear how the proposed method improves upon prior work. The experiments focus on standard datasets (MNIST, CIFAR-10) without testing on more complex or high-dimensional data. The use of a fixed visualization model for analysis may not capture dynamic changes during active learning, and the paper does not address how the method scales to datasets with many classes. Additionally, the confidence metric is defined theoretically but lacks quantitative validation beyond visual inspection."
          },
          "questions": {
            "value": "1. How does the method handle high-dimensional data where Voronoi tessellation becomes computationally infeasible? 2. What metrics were used to quantitatively evaluate the effectiveness of the visualization in capturing AL strategy dynamics? 3. How does the fixed visualization model affect the interpretation of results in scenarios where the model's feature space evolves significantly during training? 4. Could the proposed approach be adapted to handle datasets with more than 20 classes, as suggested in the rebuttal?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "15lk4nBXYb": {
    "paper_id": "15lk4nBXYb",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes CCM-DiT, a method for camera-pose controllable video generation using DiT-based models. It introduces a sparse motion encoding module and LoRA fine-tuning to embed camera-pose sequences into the temporal attention layers of DiT, aiming to improve trajectory and object consistency in long video generation."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in controllable video generation by focusing on camera-pose sequences, which is important for creative applications. The methodology leverages established techniques like LoRA and VAE, demonstrating practicality. The experiments on RealEstate10K dataset show competitive performance, and the paper provides clear technical details on the modules. The significance of enabling fine-grained camera control in DiT-based models is well justified."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of the sparse motion encoding and LoRA components. The camera-pose sequence extraction process is not thoroughly explained, and the comparison with prior work (e.g., Tora, VD3D) is insufficient. The experiments focus on short/medium-length videos, leaving the scalability to longer sequences unexplored. The novelty claim is weakened by the similarity to existing methods like Tora and VD3D, which also use motion-guided injection."
          },
          "questions": {
            "value": "1. How are camera-pose sequences extracted from the RealEstate10K dataset? Are they manually annotated or automatically estimated? 2. What is the exact mechanism of the sparse motion encoding module, and how does it differ from prior work like Tora? 3. Why does the paper claim SOTA performance when the comparison with DiT-based methods (e.g., OpenSora, VD3D) is limited? 4. How does the method handle camera motions not present in the training data? 5. What is the computational overhead of the proposed modules compared to baseline models?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes CCM-DiT, a method for camera-pose controllable video generation using DiT-based frameworks. The approach integrates sparse motion encoding and temporal attention injection modules to embed camera-pose sequences into DiT, enabling precise control over video generation. Experiments on RealEstate10K demonstrate improvements in long video generation tasks compared to LDM-based methods."
          },
          "strengths": {
            "value": "The paper addresses a clear gap in controllable video generation by focusing on camera-pose sequences, which is underexplored in DiT-based methods. The use of LoRA for efficient fine-tuning and sparse motion encoding shows practical innovation. The methodology is well-structured with clear modules (sparse motion encoding and temporal attention injection). The experiments provide quantitative results on long video generation, which is a significant challenge in the field. The paper also contextualizes its work within relevant prior art, including MotionCtrl and VD3D."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of individual components (e.g., sparse motion encoding vs. LoRA). The comparison with recent DiT-based methods beyond OpenSora and VD3D is limited. The CamMC metric for camera-pose consistency is not sufficiently explained, and its validity is unclear. The experiments focus on a single dataset (RealEstate10K), raising questions about generalization. The theoretical justification for the sparse motion encoding's effectiveness is weak, and the paper does not address potential limitations in handling complex camera motions."
          },
          "questions": {
            "value": "1. How is the CamMC metric calculated, and why is it suitable for evaluating camera-pose consistency? 2. What ablation studies were conducted to validate the contributions of the sparse motion encoding and LoRA modules? 3. How does the method handle camera motions beyond the tested scenarios (e.g., non-rigid or highly dynamic movements)? 4. Are there any quantitative comparisons with other DiT-based methods (e.g., CogVideoX, SnapVideo) that could strengthen the claims? 5. What are the computational costs and inference time of CCM-DiT compared to baseline methods?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes CCM-DiT, a method for camera-pose controllable video generation using DiT frameworks. It introduces sparse motion encoding and LoRA fine-tuning to embed camera-pose sequences into temporal attention layers, achieving improved trajectory consistency and long-video generation performance."
          },
          "strengths": {
            "value": "Originality: The combination of sparse motion encoding with DiT-based video generation is novel, particularly for camera-pose control. Quality: The method leverages OpenSora's ST-DiT framework and includes technical details on VAE training and LoRA adaptation. Clarity: The architecture overview and key equations are well-structured. Significance: Camera-pose control is a critical gap in video generation, and the method addresses this with practical applications in content creation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art DiT-based methods (e.g., VD3D, EasyAnimate) beyond vague claims of 'SOTA' performance. Ablation studies on the sparse motion encoding module and LoRA components are missing, making it hard to assess their individual contributions. The camera-pose consistency metric (CamMC) is not explicitly defined or validated. Experimental results on long videos (72 frames) are under-described, with no analysis of failure cases or limitations. The VAE training process and loss function details are insufficiently explained."
          },
          "questions": {
            "value": [
              "What specific metrics (e.g., FVD, CLIPSIM) demonstrate SOTA performance compared to DiT-based methods like VD3D or OpenSora?",
              "How is the CamMC metric calculated, and what baseline values does it achieve compared to existing methods?",
              "Are there ablation studies showing the impact of sparse motion encoding vs. direct motion matrix input, and LoRA vs. full fine-tuning?",
              "How does the method generalize to camera movements beyond the 17-frame segments used in training?",
              "What is the computational cost (e.g., VRAM usage, inference speed) compared to OpenSora, and how does LoRA affect training efficiency?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1762Fbr4HK": {
    "paper_id": "1762Fbr4HK",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces dynamic SINDy, a method combining variational inference with SINDy to identify non-autonomous dynamical systems with time-varying coefficients. It uses a VAE to generate ODE coefficients, enabling uncertainty quantification and handling noisy, non-stationary data. The approach is validated on synthetic datasets (harmonic oscillators, Lorenz system) and real neuronal activity data from C. elegans."
          },
          "strengths": {
            "value": "The paper's originality lies in extending SINDy to non-autonomous systems via VAEs, addressing a critical gap in existing methods. The methodology is well-structured, with clear integration of probabilistic modeling and sparse regression. Experiments are comprehensive, covering diverse synthetic and real-world scenarios. The work's significance is underscored by its potential to advance interpretable modeling of complex dynamical systems."
          },
          "weaknesses": {
            "value": "The paper lacks comparison with recent methods for non-autonomous system identification, such as temporal SINDy or hybrid neural-ODE approaches. The synthetic data assumes known basis functions, which may not generalize to real-world scenarios. The uncertainty quantification relies on VAE assumptions that are not thoroughly validated. Additionally, the computational scalability of the method for high-dimensional systems remains unexplored."
          },
          "questions": {
            "value": "How does dynamic SINDy handle high-dimensional systems with complex, unknown basis functions? What are the limitations of the VAE-based uncertainty quantification in capturing rare or extreme events? Could the method be adapted to handle discontinuous or hybrid dynamical systems, as mentioned in the introduction?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces dynamic SINDy, a method combining variational inference with SINDy to identify non-autonomous dynamical systems with time-varying coefficients. It uses a deep generative model (VAE) to infer sparse ODE coefficients, enabling uncertainty quantification and handling of noisy, non-stationary data. The approach is validated on synthetic datasets (non-autonomous oscillators, Lorenz system) and real neuronal activity data from C. elegans."
          },
          "strengths": {
            "value": "The paper addresses a relevant gap in system identification for non-autonomous systems, which is underexplored compared to stationary cases. The integration of VAEs with SINDy for uncertainty quantification is novel and aligns with the growing interest in probabilistic modeling. The methodology is well-structured, with clear problem formulation and systematic experiments. The real-world application to C. elegans data demonstrates practical relevance, and the paper provides a comprehensive comparison with existing methods like switching SLDS and group-sparse regression."
          },
          "weaknesses": {
            "value": "The experimental validation is limited in scope, with most results based on synthetic data and a single real-world dataset. The C. elegans analysis lacks detailed statistical validation and comparison with baselines. The comparison with existing methods is superficial, focusing only on two approaches without thorough ablation studies or analysis of hyperparameter sensitivity. The uncertainty quantification component is acknowledged as incomplete, with the paper noting the need for further work on standard deviation estimation. The paper also lacks a discussion of computational efficiency and scalability to high-dimensional systems."
          },
          "questions": {
            "value": [
              "How was the C. elegans neuronal activity data specifically preprocessed and validated? What statistical measures were used to assess the model's performance on real-world data?",
              "The paper mentions comparisons with switching SLDS and group-sparse regression, but the results are not detailed. Could the authors provide a more thorough analysis of these comparisons, including quantitative metrics and failure cases?",
              "The uncertainty quantification section highlights limitations in standard deviation estimation. What specific aspects of the VAE architecture or hyperparameters contribute to these limitations, and how might they be addressed?",
              "The paper does not discuss the computational cost or scalability of dynamic SINDy. How does the method perform on high-dimensional or long-sequence datasets compared to existing approaches?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces dynamic SINDy, a method combining variational inference with SINDy to identify non-autonomous dynamical systems with time-varying coefficients. It uses a VAE to generate ODE coefficients, enabling uncertainty quantification and handling noisy, non-stationary data. The approach is validated on synthetic datasets (e.g., harmonic oscillators, Lorenz system) and neuronal activity data from C. elegans."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in system identification for non-autonomous systems, which is underexplored in prior work. The integration of VAEs with SINDy for uncertainty quantification is novel and aligns with the growing interest in probabilistic modeling. The methodology is well-motivated, and the synthetic experiments demonstrate the method's ability to recover time-varying coefficients. The application to real-world neuronal data shows potential for broader impact."
          },
          "weaknesses": {
            "value": "The experimental validation is limited in scope and depth. The comparison with existing methods (e.g., switching linear dynamical systems, group sparse regression) is superficial, with no quantitative metrics or ablation studies. The synthetic datasets (e.g., harmonic oscillators, Lorenz system) are simplistic, and the real-world application to C. elegans lacks detailed analysis of how the method outperforms baselines. The uncertainty quantification is mentioned but not rigorously validated, and the paper does not address computational scalability or robustness to high-dimensional data."
          },
          "questions": {
            "value": "1. How does dynamic SINDy compare quantitatively to existing methods like Koopman operator-based approaches or dynamic mode decomposition? 2. What are the limitations of the VAE architecture in capturing complex time-varying patterns (e.g., chaotic systems)? 3. How does the method scale to high-dimensional systems or large datasets? 4. Are the uncertainty estimates (e.g., standard deviation of coefficients) statistically validated against ground truth?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "17idjbdHVW": {
    "paper_id": "17idjbdHVW",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes DVRGTFW, a decentralized stochastic Frank-Wolfe algorithm combining variance reduction, gradient tracking, and multi-consensus to achieve improved IFO and communication complexity bounds for constrained optimization. It claims tighter convergence rates compared to existing methods in both convex and non-convex settings."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in decentralized optimization with a novel algorithm design that integrates multiple advanced techniques. The theoretical contributions include improved IFO complexity bounds (e.g., $\\tilde{\\mathcal{O}}(n + \\sqrt{\\frac{n}{m}}L\\varepsilon^{-1})$) and near-optimal communication complexity. The empirical results validate the claims, and the paper provides a comprehensive comparison with prior work in Table 1. The method's integration of gradient tracking and variance reduction is well-motivated."
          },
          "weaknesses": {
            "value": "The convergence analysis lacks sufficient detail in the paper, making it hard to verify the novel claims. The rebuttal clarifies some aspects (e.g., gradient variance assumptions), but the original manuscript could have better explained the proof techniques. The experimental section is limited, with no comparison to all baseline methods (e.g., I-PDS). The practical implementation details (e.g., hyperparameter tuning) are under-specified. The paper's novelty compared to existing decentralized FW methods requires stronger justification."
          },
          "questions": {
            "value": "1. How does the proposed analysis differ from Wai et al. (2017) and Jiang et al. (2022)? Provide a clear technical distinction. 2. What are the exact constants in the IFO/communication bounds, and how do they scale with $m$ and $n$? 3. Why is the non-convex communication complexity $\\mathcal{O}(\\frac{L^2 \\varepsilon^{-2}}{\\sqrt{1 - \\lambda_2(W)}})$, and how does this compare to the lower bound $\\Omega(\\frac{L \\varepsilon^{-2}}{\\sqrt{1 - \\lambda_2(W)}})$? 4. How does the FastMix subroutine handle heterogeneous data distributions in practice?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes a decentralized variance reduction gradient tracking Frank-Wolfe (DVRGTFW) algorithm for constrained finite-sum optimization. It claims improved incremental first-order oracle (IFO) complexity bounds in both convex and non-convex settings, along with near-optimal communication complexity for non-convex cases. The method combines variance reduction, gradient tracking, and multi-consensus techniques to achieve tighter theoretical guarantees compared to existing decentralized FW methods."
          },
          "strengths": {
            "value": "The paper presents a novel convergence analysis that diverges from prior decentralized FW methods, achieving tighter IFO complexity bounds. The integration of variance reduction and gradient tracking techniques is well-justified. The theoretical results are comprehensive, addressing both convex and non-convex scenarios. The paper also provides a detailed comparison with existing methods in a tabular format, highlighting the superiority of the proposed approach."
          },
          "weaknesses": {
            "value": "The experimental section is underdeveloped, lacking specific details about datasets, baselines, and evaluation metrics. The 'FastMix' subroutine is described algorithmically but not theoretically justified beyond its implementation. The assumption of bounded gradient variance is not thoroughly addressed, despite the rebuttal's claims. The interplay between gradient tracking and variance reduction mechanisms requires more explicit analysis."
          },
          "questions": {
            "value": "1. What specific datasets and baselines were used in the experiments? 2. How is the 'FastMix' subroutine theoretically analyzed for convergence? 3. Can the authors clarify the handling of gradient variance assumptions in the presence of heterogeneous data? 4. What is the practical impact of the claimed IFO complexity improvements in real-world scenarios?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes DVRGTFW, a decentralized stochastic Frank-Wolfe algorithm combining variance reduction, gradient tracking, and multi-consensus to address constrained optimization. It claims improved IFO complexity bounds and near-optimal communication complexity in convex and non-convex settings compared to prior work."
          },
          "strengths": {
            "value": "The paper presents a novel algorithm with rigorous theoretical analysis, addressing key challenges in decentralized optimization. The comprehensive comparison in Table 1 highlights competitive IFO and communication complexity bounds. The integration of variance reduction and gradient tracking techniques demonstrates creative problem-solving. The paper also provides a detailed literature review and contextualizes its contributions effectively."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficient, lacking specific datasets, baselines, and quantitative comparisons. The convergence analysis for non-convex cases relies on assumptions (e.g., bounded gradient variance) that require clearer justification. The rebuttal does not fully resolve concerns about the proof's complexity and the role of gradient tracking in handling heterogeneity. The paper's claims about 'best-known' bounds need stronger empirical support."
          },
          "questions": {
            "value": [
              "Please provide detailed experimental results, including specific datasets, baseline comparisons, and metrics (e.g., convergence speed, communication cost).",
              "Clarify how the gradient tracking technique avoids reliance on gradient variance assumptions, particularly in non-convex settings.",
              "Explain the practical implications of the claimed near-optimal communication complexity in real-world decentralized systems.",
              "Detail the proof steps for the non-convex convergence analysis, especially the handling of stochastic gradients and linear oracle errors."
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "1AYrzmDK4V": {
    "paper_id": "1AYrzmDK4V",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces the 'smoothing attack' to bypass statistical watermarking in large language models (LLMs). The attack leverages a weaker reference model to smooth out distribution shifts caused by watermarks, enabling undetectable text generation while maintaining quality. The authors demonstrate its effectiveness across multiple watermarking schemes on Llama2-7b and OPT-1.3b, showing superiority over existing methods."
          },
          "strengths": {
            "value": "The paper addresses a critical practical threat model, where attackers lack access to strong models, making the attack scenario highly realistic. The methodology is novel, with a clear framework for smoothing distribution shifts without requiring knowledge of the watermarking mechanism. Experiments are comprehensive, covering diverse watermarking strategies and models. The work highlights a fundamental limitation of statistical watermarking, advancing the field's understanding of robustness. The presentation is structured, with clear definitions and illustrative figures."
          },
          "weaknesses": {
            "value": "The paper's claim of universal applicability to all statistical watermarking schemes is slightly overstated, as distortion-free watermarking (which does not alter token distributions) is not addressed in detail. While the rebuttal clarifies this, the original manuscript could have explicitly acknowledged this limitation. Additionally, the quality evaluation relies heavily on perplexity (PPL), which may not fully capture text quality; the authors later added BLEU metrics, but this remains a secondary focus. The baseline comparison with naive smoothing is addressed in the rebuttal, but the original paper lacks detailed analysis of this approach."
          },
          "questions": {
            "value": "1. How does the attack perform against distortion-free watermarking schemes, which the rebuttal notes are trivial to bypass? The paper should clarify whether this is a distinct category or an edge case. 2. What is the optimal value of K (top-K sampling) for the attack, and how does it affect performance? 3. Are there scenarios where the smoothing attack might inadvertently degrade text quality, and how is this mitigated?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces the 'watermark smoothing attack,' a method to bypass statistical watermarking in language models by leveraging a weaker reference model to smooth out distribution shifts caused by watermarks. The attack maintains text quality while making the watermark undetectable, demonstrating vulnerabilities in existing watermarking techniques under practical attack scenarios."
          },
          "strengths": {
            "value": "The paper presents a novel, practical attack framework that challenges the robustness of statistical watermarking without requiring access to strong models, which is a significant contribution. The approach is generalizable across watermarking schemes, and the experiments on Llama2-7b and OPT-1.3b show strong results. The problem statement and methodology are clearly articulated, with detailed analysis of significance levels and token distribution shifts."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive baseline comparisons, such as the naive smoothing baseline suggested in the rebuttal, which could have strengthened the evaluation. While the rebuttal addresses some concerns, the original work does not fully explore distortion-free watermarking schemes or provide sufficient metrics (e.g., BLEU) to evaluate text quality. The choice of z-score thresholds and their impact on results is not sufficiently explained."
          },
          "questions": {
            "value": "How does the attack perform against distortion-free watermarking schemes, which the authors acknowledge are trivial to bypass? What are the limitations of the method when the reference model is not significantly weaker than the target model? How does the attack handle cases where the green token assignment is context-dependent (e.g., KGW watermark)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a 'smoothing attack' to bypass statistical watermarking in language models by leveraging a weaker reference model. The attack smooths the distribution shifts caused by watermarks, enabling text generation with high quality while evading detectors. Experiments show effectiveness against eight watermarking strategies, even with weaker models like TinyLlama-1.3b."
          },
          "strengths": {
            "value": "The paper presents a novel, practical attack method that challenges the robustness of statistical watermarking without requiring strong reference models. The experiments are comprehensive, covering multiple watermarking schemes and models. The approach is generalizable, applicable to any statistical watermarking method without needing prior knowledge of the green list. The rebuttal clarifies key concerns, strengthening the paper's validity."
          },
          "weaknesses": {
            "value": "The paper's claim of universality over all statistical watermarking schemes is not fully substantiated, as distortion-free watermarks (which do not alter distributions) are trivial to attack under the threat model. The rebuttal's justification for rejecting a naive smoothing baseline (e.g., combining top-K probabilities) is weak, as the authors have access to such probabilities. The choice of z-score thresholds and their impact on results is not sufficiently explained, and the handling of different tokenizers in distribution combination remains under-clarified."
          },
          "questions": {
            "value": "1. How does the attack perform against distortion-free watermarking schemes beyond the trivial case? 2. Why is the naive smoothing baseline (using top-K probabilities) not considered a valid comparison, given the authors' access to such data? 3. Can the authors provide additional analysis on the relationship between $S_t$ and TVD for different watermarking parameters? 4. How do the results generalize to other models or watermarking configurations not tested in the paper?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "1D3TjFidCS": {
    "paper_id": "1D3TjFidCS",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces LogLU, a novel activation function designed to address the dead neuron and vanishing gradient problems in deep neural networks. It claims to improve convergence and accuracy by combining linear and logarithmic behaviors, validated through experiments on benchmark datasets and the XOR problem."
          },
          "strengths": {
            "value": "The paper provides a clear mathematical formulation of LogLU, including differentiability analysis and theoretical guarantees. The experimental results on Caltech 101 and Imagenette datasets demonstrate performance improvements over existing activation functions. The XOR problem demonstration highlights LogLU's ability to model non-linearities with minimal complexity. The comparison with established functions like ReLU, Leaky ReLU, and ELU is well-structured."
          },
          "weaknesses": {
            "value": "The experimental validation is limited to InceptionV3, and the paper lacks ablation studies or comparisons across diverse architectures. The claims about computational efficiency are not thoroughly justified, and the derivative analysis for negative inputs (1/(1 - x)) could lead to numerical instability for large negative values. The paper does not address how LogLU handles more complex non-linear tasks beyond XOR. The visual comparisons (e.g., Figure 1, 3) are referenced but not provided, reducing transparency."
          },
          "questions": {
            "value": "1. How does LogLU perform on architectures other than InceptionV3? 2. What statistical significance tests were conducted to validate the performance improvements? 3. How does LogLU handle extreme negative inputs, given its derivative 1/(1 - x)? 4. Are there ablation studies to isolate LogLU's impact on convergence? 5. How does LogLU compare to Swish/Mish in terms of training dynamics and gradient flow?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces LogLU, a novel activation function designed to address issues like dead neurons and vanishing gradients in deep neural networks. It claims to solve the XOR problem with a single neuron and demonstrates improved performance on benchmark datasets compared to existing activation functions such as ReLU, Leaky ReLU, ELU, Swish, and Mish."
          },
          "strengths": {
            "value": "The paper provides a rigorous mathematical analysis of LogLU's differentiability and non-linearity, which is a strong foundation for its claims. The XOR experiment is a compelling demonstration of LogLU's ability to model non-linear relationships with minimal complexity. The experimental evaluation on benchmark datasets (Caltech 101 and Imagenette) shows consistent improvements in accuracy and loss reduction compared to baseline activation functions. The paper also addresses computational efficiency, suggesting LogLU's potential for practical deployment."
          },
          "weaknesses": {
            "value": "The experiments lack depth, such as ablation studies to isolate LogLU's impact or analysis of training dynamics (e.g., convergence speed, gradient magnitudes). The paper does not address numerical stability concerns for extreme negative inputs (e.g., log(0) or near-zero values). The comparison to existing functions like ELU and Swish is superficial, and the paper fails to discuss potential trade-offs (e.g., computational cost of the logarithm). Additionally, the theoretical claims about mitigating exploding gradients are not empirically validated."
          },
          "questions": {
            "value": "1. How does LogLU handle numerical instability for very negative inputs (e.g., x << -1)? 2. What ablation studies were conducted to confirm LogLU's contribution to the observed performance gains? 3. How does LogLU's computational overhead compare to simpler functions like ReLU or Leaky ReLU? 4. Are there specific scenarios where LogLU underperforms compared to other activation functions? 5. Can the authors provide training curves or gradient flow analysis to support claims about convergence improvements?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces LogLU, a novel activation function designed to address issues like dead neurons and vanishing gradients in deep neural networks. It claims to improve convergence and accuracy, particularly in solving the XOR problem with fewer neurons and demonstrating superior performance on benchmark datasets like Caltech 101 and Imagenette."
          },
          "strengths": {
            "value": "The paper provides a clear mathematical formulation of LogLU, including its differentiability and non-linearity analysis. The theoretical analysis of gradient behavior is thorough, and the XOR experiment demonstrates the function's ability to model non-linear relationships. The comparison with existing activation functions on benchmark datasets adds practical relevance."
          },
          "weaknesses": {
            "value": "The empirical validation lacks depth, with insufficient details on hyperparameters, training procedures, and statistical significance of results. The XOR experiment uses fixed weights without explaining how they were optimized. The computational efficiency claims are not supported by concrete runtime data. The paper also fails to address potential limitations, such as performance in tasks beyond image classification or sensitivity to hyperparameters."
          },
          "questions": {
            "value": "1. How were the weights in the XOR experiment determined? Were they learned through training or manually set? 2. Were the benchmark experiments conducted with consistent hyperparameters across all activation functions? 3. What statistical tests were used to validate the performance improvements? 4. How does LogLU perform in tasks beyond image classification, such as NLP or reinforcement learning? 5. What is the computational cost of the logarithmic operation compared to other activations?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      }
    ]
  },
  "1DIdt2YOPw": {
    "paper_id": "1DIdt2YOPw",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper explores uncertainty-based abstention in large language models (LLMs) to improve reliability, safety, and reduce hallucinations. The authors propose two uncertainty measures—statistical uncertainty (e.g., negative log-likelihood, predictive entropy, semantic entropy) and In-Dialoage Uncertainty (InDU), which quantifies hedging language (e.g., 'I don't know'). Experiments across correctness, hallucination, and safety tasks demonstrate that abstaining based on these measures improves performance metrics like correctness (up to 8% improvement), hallucination reduction (50% decrease), and safety (70-99% reduction in unsafe responses)."
          },
          "strengths": {
            "value": "The paper addresses a critical practical problem in LLM deployment: reliability, safety, and hallucinations. It introduces InDU, a novel verbalized uncertainty measure that aligns with human communication patterns, and validates its effectiveness. The methodology is thorough, with experiments across multiple datasets and models (Llama2, Vicuna). The use of open-source models and clear metrics (AUROC, AUARC) enhances reproducibility. The paper also contextualizes its work within existing literature and highlights the importance of uncertainty-aware abstention in real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with alternative uncertainty estimation methods (e.g., Bayesian neural networks or ensemble-based approaches). The InDU measure relies on a predefined hedge word list, but the paper does not discuss its comprehensiveness or robustness to domain shifts. The computational overhead claims are not quantified, and the experiments focus on specific models (Llama2, Vicuna) without generalization to other architectures. Additionally, the paper does not explore why certain uncertainty measures (e.g., semantic entropy) outperform others in specific tasks."
          },
          "questions": {
            "value": "1. How is InDU quantified? Does the hedge word list account for context-specific hedging (e.g., domain-specific jargon)? 2. Are the results sensitive to the choice of threshold for abstention? 3. How do the uncertainty measures perform on non-English LLMs or multilingual tasks? 4. What is the exact computational overhead of implementing InDU compared to statistical measures? 5. Are the safety improvements robust across different adversarial datasets or only specific ones like AutoDAN and AttaQ?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper explores the use of uncertainty-based abstention in large language models (LLMs) to enhance reliability, safety, and reduce hallucinations. The authors introduce two uncertainty measures—statistical uncertainty (e.g., negative log-likelihood, predictive entropy, semantic entropy) and In-Dialoage Uncertainty (InDU), which quantifies hedge words in responses. They demonstrate that abstaining based on these measures improves correctness, reduces hallucinations by 50%, and boosts safety by 70-99% across Llama2 models with and without RLHF finetuning."
          },
          "strengths": {
            "value": "The paper addresses critical real-world challenges in LLM reliability, including safety, correctness, and hallucinations, which are highly relevant for deployment. The proposed In-Dialoage Uncertainty (InDU) measure is novel and aligns with human-like expressions of uncertainty, offering a practical, unobtrusive method for chat-based applications. The methodology is well-structured, with clear definitions of statistical uncertainty metrics and rigorous evaluation across three distinct scenarios. The results are promising, showing significant improvements in key metrics, and the paper provides a comprehensive comparison of models with and without RLHF."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing uncertainty estimation methods, such as those using explicit prompting or finetuning, which limits the context for evaluating the novelty of InDU. The statistical uncertainty measures (e.g., predictive entropy) are well-defined but not thoroughly analyzed for limitations, such as their performance across different model sizes or architectures. The hedge word list for InDU is not described in detail, raising questions about its robustness and generalizability. The computational overhead claim is not supported with empirical data, and the paper does not address trade-offs between abstention rates and model utility. Additionally, the experiments focus on Llama2 models, leaving open the question of scalability to other architectures."
          },
          "questions": {
            "value": "1. How do the proposed uncertainty measures compare to existing methods that explicitly prompt or finetune models for uncertainty estimation? 2. What is the composition of the hedge word list used for InDU, and how was it validated for capturing uncertainty? 3. Are the statistical uncertainty metrics (e.g., predictive entropy) sensitive to model size or architecture, and how does this affect their effectiveness? 4. What empirical evidence supports the claim of 'almost no additional computational overhead'? 5. How does the abstention threshold impact the trade-off between correctness, hallucination reduction, and safety, and are there scenarios where this approach might fail?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates uncertainty-based abstention in large language models (LLMs) to improve reliability, reduce hallucinations, and enhance safety. The authors introduce two uncertainty measures—statistical uncertainty (e.g., negative log-likelihood, predictive/semantic entropy) and In-Dialoage Uncertainty (InDU), which quantifies hedge words in responses. They demonstrate that abstaining based on these measures improves correctness, reduces hallucinations by 50%, and boosts safety by 70-99%, with minimal computational overhead."
          },
          "strengths": {
            "value": "The paper's originality lies in applying uncertainty-based abstention to LLMs for safety and reliability, particularly through the novel InDU metric. The experimental design is rigorous, covering multiple datasets and tasks (correctness, hallucinations, safety). The clarity of methodology and results is strong, with detailed statistical analyses. The significance is high, as addressing LLM reliability is critical for real-world deployment. The paper also highlights practical benefits of RLHF in preserving uncertainty awareness."
          },
          "weaknesses": {
            "value": "The paper lacks comparison with existing uncertainty quantification methods in LLMs, which limits the assessment of novelty. The InDU metric relies on a predefined hedge word list, which may not capture all forms of uncertainty. The experiments focus on Llama2 models, limiting generalizability. The claim about 'almost no additional computational overhead' is not substantiated with concrete metrics. Additionally, the paper does not address how abstention thresholds are determined or validated across tasks."
          },
          "questions": {
            "value": "1. How does the paper address the potential bias in the hedge word list used for InDU? 2. What ablation studies were conducted to validate the effectiveness of statistical vs. InDU measures? 3. How are abstention thresholds calibrated across different tasks (correctness, hallucinations, safety)? 4. Are the results generalizable to other LLM architectures beyond Llama2? 5. What is the trade-off between abstention rate and model utility (e.g., response coverage)?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "1FY1apsMxc": {
    "paper_id": "1FY1apsMxc",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces PromptGFM, a Graph Foundation Model (GFM) that leverages Large Language Models (LLMs) as Graph Neural Networks (GNNs) through graph vocabulary learning. The framework consists of two modules: (1) a Graph Understanding Module that replicates GNN workflows in the language space using prompts, and (2) a Graph Inference Module that establishes a language-based graph vocabulary to resolve modality incompatibility. The method demonstrates strong performance in node classification and link prediction, with transferability across datasets."
          },
          "strengths": {
            "value": "The paper presents a novel approach to integrating LLMs and GNNs by addressing the limitations of decoupled architectures and OOV tokens. The methodology is original in replicating GNN workflows via prompts, and the experiments are comprehensive, covering multiple tasks and datasets. The clarity of the writing and figures is strong, and the significance of advancing GFMs is well-articulated. The contributions highlight a fresh paradigm for GNN-LLM integration and graph-text alignment."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the effectiveness of individual components (e.g., the Graph Understanding Module vs. the Graph Inference Module). The comparison with existing methods is insufficient, particularly regarding the novelty of 'LLM as GNN' versus prior work like LLaGA or InstructGraph. The theoretical justification for the prompt-based GNN is weak, relying more on empirical results than formal analysis. Additionally, the computational efficiency and scalability of the framework are underexplored, especially given the increased resource demands of LLMs."
          },
          "questions": {
            "value": [
              "How does the computational cost of PromptGFM compare to traditional GNNs, and what optimizations are employed to handle large graphs?",
              "Are there ablation studies demonstrating the individual contributions of the Graph Understanding and Inference Modules?",
              "What specific metrics or experiments validate the 'expressiveness' and 'scalability' of the language-based graph vocabulary?",
              "How does the framework handle graphs with no textual attributes, given the focus on text-attributed graphs?",
              "What are the limitations of the language-based vocabulary in terms of domain adaptability or handling highly heterogeneous graphs?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes PromptGFM, a Graph Foundation Model (GFM) that integrates Large Language Models (LLMs) and Graph Neural Networks (GNNs) by replicating GNN workflows in the language space through prompts. The method includes a Graph Understanding Module for message-passing simulation and a Graph Inference Module with a universal language-based graph vocabulary to address OOV token limitations and enable cross-task transferability."
          },
          "strengths": {
            "value": "The paper introduces a novel paradigm for GNN-LLM integration by explicitly replicating GNN operations in the language space, addressing modality incompatibility through a universal graph vocabulary. The methodology demonstrates strong empirical performance on node classification and link prediction tasks, with notable zero-shot transferability. The work is well-structured, with clear problem formulation and technical contributions. The focus on practical alignment over strict mathematical proofs is justified given the experimental validation."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive ablation studies to isolate the impact of key components like the graph vocabulary or prompt design. Experimental comparisons with state-of-the-art GNN-LLM integration methods (e.g., LLaGA, InstructGraph) are insufficient, making it difficult to assess relative novelty. The computational efficiency and scalability of the approach remain unclear, particularly for large graphs. The claim of being the 'first true implementation of LLM as GNN' requires stronger justification against prior work."
          },
          "questions": {
            "value": [
              "How does the method handle dynamic or evolving graphs with changing structures?",
              "What specific prompt engineering strategies are used to simulate GNN message passing, and how are they validated?",
              "Are there limitations to the types of graphs (e.g., directed vs. undirected) or tasks this approach can handle?",
              "How does the universal vocabulary scale to unseen nodes or graphs outside the training distribution?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces PromptGFM, a Graph Foundation Model (GFM) that leverages Large Language Models (LLMs) to replicate Graph Neural Network (GNN) workflows in the language space. The method consists of two modules: (1) a Graph Understanding Module that uses prompts to simulate GNN operations like neighbor sampling and aggregation, and (2) a Graph Inference Module that establishes a language-based graph vocabulary to enable transferable, scalable representations. The authors claim superior performance in node classification and link prediction, along with strong cross-dataset transferability."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel paradigm of using LLMs as GNNs through prompt-based replication of GNN workflows, which differs from prior GNN-LLM integration methods. Quality: The methodology is structured, with clear separation of modules and alignment with GNN principles. Clarity: The paper provides a detailed technical description of the prompt design and vocabulary construction. Significance: The work addresses critical challenges in graph-text alignment and modality incompatibility, with potential implications for cross-domain GFM development."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficiently detailed. The paper lacks specific metrics, baseline comparisons, and ablation studies to substantiate claims of superiority. The mechanism for resolving OOV token issues remains vague, with no quantitative analysis of the language-based vocabulary's expressiveness or scalability. The rebuttal clarifies the focus on text-attributed graphs, but the paper's broader applicability is not thoroughly justified. The computational efficiency and scalability of the LLM-based approach compared to traditional GNNs are not addressed."
          },
          "questions": {
            "value": "1. How is the language-based graph vocabulary evaluated for expressiveness and transferability? Are there quantitative metrics or case studies demonstrating its superiority over OOV token approaches? 2. What specific prompts are used for neighbor sampling and aggregation? How are these prompts validated to ensure they capture structural information accurately? 3. How does the method handle graphs with no textual attributes, given the focus on text-attributed graphs? 4. What are the computational costs and inference speed of PromptGFM compared to traditional GNNs and other GFM approaches? 5. How is the multi-instruction fine-tuning framework implemented, and what strategies are used to prevent catastrophic forgetting across diverse tasks?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1Ffzgglq2I": {
    "paper_id": "1Ffzgglq2I",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a framework called Binary Reward Labeling (BRL) to bridge the gap between preference-based reinforcement learning (PBRL) and standard reward-based offline RL. The key idea is to transform preference feedback into scalar rewards, enabling the use of existing offline RL algorithms. The method minimizes information loss during this transformation and demonstrates empirical effectiveness on D4RL benchmarks."
          },
          "strengths": {
            "value": "The paper presents a novel and general framework for PBRL, addressing a critical gap in the literature. The theoretical analysis connects PBRL techniques to standard offline RL, and the empirical results show competitive performance against existing methods. The clarity of the problem formulation and the practical implementation of BRL are strong points. The work also highlights the importance of minimizing information loss during reward labeling, which is a significant contribution to the field."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough discussion of the limitations of the binary labeling approach, particularly in cases where trajectories overlap. The theoretical analysis of the optimization problem is minimal, and the connection to existing PBRL techniques is not fully elaborated. The experiments, while promising, could include more ablation studies and comparisons with recent PBRL methods. Additionally, the assumption that the link function is unknown but not required by the method needs further justification."
          },
          "questions": {
            "value": "How does the binary labeling approach handle overlapping trajectories, which are common in practice? What are the theoretical guarantees for the optimality of the binary labeling solution? Can the framework be extended to handle multiple preference labels per trajectory pair, as in some existing works? How does the choice of link function (e.g., sigmoid) affect the performance of BRL?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes a general framework called Binary Reward Labeling (BRL) to bridge the gap between preference-based and reward-based offline reinforcement learning (PBRL). The framework converts preference feedback into scalar rewards via binary labeling, enabling the use of existing reward-based offline RL algorithms. The authors theoretically analyze the connection between BRL and existing PBRL methods, and empirically demonstrate that BRL achieves competitive performance on standard benchmarks compared to recent PBRL baselines."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in offline PBRL by proposing a generalizable framework that leverages existing reward-based algorithms. The theoretical analysis connects BRL to existing PBRL techniques, highlighting its potential for improving efficiency. The empirical results on D4RL benchmarks show strong performance, often matching or exceeding state-of-the-art PBRL methods. The framework's simplicity and compatibility with standard RL algorithms are significant advantages."
          },
          "weaknesses": {
            "value": "The theoretical analysis of BRL's optimality under general conditions is incomplete, particularly regarding the role of the link function and its assumptions. The paper lacks a detailed discussion on handling distribution mismatch in preference-based settings. The optimization problem in equation (1) is not thoroughly analyzed for computational complexity or scalability. The experiments focus on specific environments and may not generalize to more complex or diverse settings. The comparison to reward modeling techniques is not fully justified in all cases."
          },
          "questions": {
            "value": "1. How does the choice of link function (e.g., sigmoid vs. other monotonic functions) impact BRL's performance, and what assumptions are made about its form? 2. Can the authors provide a more detailed analysis of the optimization problem in equation (1), including convergence guarantees or computational feasibility? 3. What are the limitations of BRL in scenarios with overlapping trajectories or noisy preference labels? 4. How does the framework address the inherent information loss in preference feedback compared to direct reward signals?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes a general framework called Binary Reward Labeling (BRL) to bridge the gap between offline preference-based reinforcement learning (PBRL) and standard reward-based offline RL. The key idea is to transform preference feedback into scalar rewards via binary labeling, enabling the use of existing reward-based offline RL algorithms. The authors theoretically analyze the connection between BRL and PBRL techniques, and empirically demonstrate that their method outperforms existing PBRL baselines on standard benchmarks."
          },
          "strengths": {
            "value": "The paper introduces a novel framework (BRL) for PBRL, addressing a critical gap in the literature. The theoretical analysis connects BRL to existing PBRL methods, and the empirical results show competitive performance against reward-based algorithms. The clarity of the presentation is strong, with well-structured sections and illustrative figures. The significance of bridging PBRL and reward-based RL is high, as it enables leveraging mature reward-based RL techniques for preference feedback scenarios."
          },
          "weaknesses": {
            "value": "The paper lacks thorough comparisons with recent PBRL methods beyond the mentioned baselines. The theoretical analysis of information loss during binary labeling is insufficient, with limited discussion on how the method handles overlapping trajectories or non-sigmoid link functions. The experiments focus on D4RL benchmarks but do not explore diverse environments or edge cases. The distinction between BRL and reward modeling is not clearly justified, and the practical implementation details (e.g., handling trajectory overlaps) are under-specified."
          },
          "questions": {
            "value": "1. How does BRL handle trajectory overlaps, which could complicate reward labeling? 2. What are the limitations of the binary labeling approach when the link function deviates from the assumed form (e.g., non-sigmoid)? 3. Are there ablation studies demonstrating the effectiveness of binary labeling versus other labeling strategies? 4. How does the framework scale to high-dimensional or continuous action spaces? 5. What are the computational costs of the optimization problem in Equation (1) compared to existing PBRL methods?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1H90Gb9rJ9": {
    "paper_id": "1H90Gb9rJ9",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper presents a deterministic algorithm for lossless optimization of neural networks (NNs) representing Boolean networks (BNs), reducing neurons and connections by up to 70% and 60%, respectively. It introduces an objective-aware algorithm leveraging NPN classification to accelerate optimization and demonstrates significant speedups compared to baseline methods."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in NN-based technology mapping, offering practical solutions with clear experimental validation. The use of NPN classification for shared representations is innovative, and the results demonstrate substantial improvements over state-of-the-art methods. The methodology is well-structured, with detailed theoretical foundations and empirical analysis."
          },
          "weaknesses": {
            "value": "The theoretical analysis of the $\\ell_1$-relaxation's impact on $\\ell_0$-norm minimization is limited, and scalability to very large BNs remains underexplored. The paper could benefit from a more accessible introduction and deeper discussion of the limitations of the proposed methods. The experimental section lacks comparisons with alternative optimization strategies for larger-scale problems."
          },
          "questions": {
            "value": "1. How generalizable is the NPN classification approach to non-Boolean network domains? 2. What are the specific constraints or assumptions limiting scalability to larger BNs? 3. How do the authors reconcile the $\\ell_1$-relaxation's suboptimality in some cases with the claim of lossless optimization? 4. Are there theoretical guarantees for the optimality of the proposed architecture-aware algorithm?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper presents methods to optimize neural network (NN) representations of Boolean networks (BNs) by reducing neurons and connections while preserving functional equivalence. It introduces a deterministic algorithm and an objective-aware approach leveraging NPN classification to accelerate optimization, achieving significant compression and speedup compared to state-of-the-art methods."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in lossless NN optimization for Boolean functions, which is underexplored in existing literature. The proposed methods are novel, with clear experimental validation showing up to 70% reduction in connections and 60% in neurons. The theoretical foundations (e.g., MP representation, NPN classification) are well-established, and the practical relevance to circuit simulation and neurosymbolic systems is highlighted. The paper also provides a structured analysis of optimization subproblems and their aggregation."
          },
          "weaknesses": {
            "value": "The theoretical analysis of the $\\ell_1$-relaxation for MMP optimization is incomplete, leaving open questions about its guarantees. The scalability to larger BNs is not thoroughly addressed, and the paper lacks a detailed discussion of limitations for higher-input Boolean functions. The clarity for a broader audience is insufficient, with dense technical language and limited high-level examples. The rebuttal acknowledges these gaps but does not fully resolve them, particularly regarding $\\ell_0$-norm minimization and generalization to arbitrary BNs."
          },
          "questions": {
            "value": "1. How generalizable are the NPN classification-based optimizations to arbitrary Boolean functions beyond the tested cases? 2. What are the concrete limitations of the $\\ell_1$-relaxation in terms of suboptimality for larger BNs? 3. How does the proposed method handle BNs with non-uniform depth or complex inter-depth dependencies? 4. Could the authors provide additional empirical evidence on the robustness of their approach across diverse BN architectures?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a deterministic algorithm for lossless optimization of neural networks (NNs) representing Boolean networks (BNs), focusing on reducing neurons and connections while preserving functional equivalence. It proposes an objective-aware algorithm leveraging NPN classification to accelerate optimization, achieving significant reductions in size and speedups compared to state-of-the-art methods."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by applying NPN classification to NN optimization for BNs, a novel approach. The experimental results are compelling, showing up to 70% fewer connections and 60% fewer neurons, along with substantial speedups. The methodology is well-structured, and the problem's industrial relevance is clearly justified. The paper provides thorough definitions and contextualizes its contributions effectively."
          },
          "weaknesses": {
            "value": "The theoretical analysis of the L1 relaxation for L0-norm minimization is insufficient, and the scalability to larger BNs remains underexplored. The paper lacks detailed comparisons with alternative optimization techniques, and the computational complexity of NPN classification is not fully analyzed. While the rebuttal addresses some concerns, the theoretical guarantees and broader applicability of the method remain unclear."
          },
          "questions": {
            "value": "1. How does the NPN classification algorithm handle BNs with varying input sizes or complex structures beyond the examples provided? 2. What are the exact computational bottlenecks in the current approach, and how might they be further optimized? 3. How does the proposed method generalize to non-Boolean network domains or different NN architectures? 4. Can the theoretical guarantees of lossless optimization be strengthened, particularly for larger BNs?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "1Iq1qIsc2s": {
    "paper_id": "1Iq1qIsc2s",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates the use of Rotary Position Embeddings (RoPE) as a replacement for Relative Positional Biases (RPB) in vision transformers, particularly in the context of fused attention implementations. The authors demonstrate that RoPE avoids the backward pass inefficiencies of RPB, achieves competitive accuracy, and provides speedups with their optimized CUDA implementation."
          },
          "strengths": {
            "value": "The paper provides a thorough analysis of the limitations of RPB in fused attention, which is a critical practical issue. The empirical results across multiple vision models (ViT, Swin, NAT) show consistent improvements with RoPE. The fast RoPE implementation and ablation studies on dimensionality reduction are valuable contributions. The clarity of explanations, figures, and comparative analysis are strong."
          },
          "weaknesses": {
            "value": "The paper lacks ablation studies on all design choices mentioned in Section 3.3.2, such as different 2D RoPE variants. Experiments are limited to image classification, excluding detection/segmentation tasks. The theoretical justification for RoPE's superiority over RPB is somewhat superficial. The rebuttal acknowledges limited scalability analysis for larger models and resolutions."
          },
          "questions": {
            "value": "1. How does RoPE's performance scale with extremely large models (e.g., 100M+ parameters) and high-resolution inputs? 2. Could the authors provide more details on the Axial RoPE implementation's efficiency gains? 3. Why are certain 2D RoPE variants (e.g., AS2DRoPE) not compared directly? 4. What are the exact trade-offs between RoPE's dimensionality reduction (k_rope) and model accuracy?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates the use of Rotary Position Embeddings (RoPE) as a replacement for Relative Positional Biases (RPB) in vision transformers, particularly in the context of fused attention implementations. The authors demonstrate that RoPE avoids the performance bottlenecks of RPB in fused attention by decoupling positional bias from attention weights, and they present empirical results showing competitive accuracy and speed improvements across multiple vision transformer architectures."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in vision transformers: the incompatibility of attention weight biases with fused attention. The empirical analysis across diverse models (ViT, Swin, NAT) and tasks is thorough. The implementation of Axial RoPE and analysis of hyperparameters like $k_{rope}$ show practical relevance. The clear distinction between RoPE and RPB, along with visual explanations, enhances readability. The rebuttal clarifies that RoPE works with both fused and unfused attention, broadening its applicability."
          },
          "weaknesses": {
            "value": "The paper lacks a comprehensive comparison with other position embedding methods (e.g., APE, 2D RoPE variants). Experiments are limited to image classification, omitting tasks like detection/segmentation. Ablation studies on design choices in Section 3.3.2 are incomplete. The novelty of applying RoPE to vision models is not sufficiently differentiated from prior work in LLMs. The theoretical justification for RoPE's superiority over RPB remains underexplored."
          },
          "questions": {
            "value": "1. How does the proposed RoPE implementation differ from existing 2D RoPE variants (e.g., AS2DRoPE) in terms of architecture or performance? 2. What are the theoretical guarantees that RoPE's decoupling from attention weights leads to better scalability than RPB? 3. Can the authors provide insights into how RoPE's performance scales with extremely large models (e.g., 100B+ parameters)? 4. How do the results generalize to non-image modalities (e.g., video, audio)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper investigates the use of Rotary Position Embeddings (RoPE) as a replacement for Relative Positional Biases (RPB) in vision transformers, particularly in the context of fused attention. It demonstrates that RoPE avoids the backward pass bottlenecks of RPB, achieves competitive accuracy, and improves inference speed with a custom CUDA implementation. The work also analyzes the impact of hyperparameters like $k_{rope}$ and evaluates RoPE across multiple vision model architectures."
          },
          "strengths": {
            "value": "The paper addresses a critical practical challenge in fused attention implementations, where RPB introduces backward pass inefficiencies. Empirical validation across diverse vision models (ViT, Swin, NAT) with varying architectures and sizes demonstrates consistent improvements. The analysis of RoPE's scalability, the proposed Axial RoPE implementation, and the ablation studies on $k_{rope}$ show thoroughness. The clarity of figures (e.g., attention weight bias vs. RoPE) and the structured methodology enhance readability."
          },
          "weaknesses": {
            "value": "The experiments are limited to image classification tasks, omitting other vision tasks like detection or segmentation. The ablation studies on hyperparameters (e.g., shared angles, $k_{rope}$) are partial, with some results only added in the rebuttal. The paper does not deeply analyze why RoPE outperforms RPB, relying on assertions rather than mechanistic insights. The novelty is somewhat diluted by RoPE's prior use in LLMs, and the related work section lacks citations for recent vision-specific RoPE extensions."
          },
          "questions": {
            "value": "1. How does the custom CUDA implementation of RoPE achieve speedups compared to RPB? What specific optimizations are involved? 2. Why do some models (e.g., NAT, DiNAT) show no significant changes with RoPE, while others (e.g., Swin, ViT) do? 3. Are there trade-offs between using more hidden dimensions for RoPE and computational cost? 4. How does the paper address the potential limitations of RoPE in long-context scenarios compared to RPB?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1Iu2Yte5N6": {
    "paper_id": "1Iu2Yte5N6",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper investigates the order sensitivity of in-context learning (ICL) in decoder-only LLMs by analyzing prompt embedding spaces. It identifies a 'clustering property' where prompts with the same first and last demonstrations form clusters, attributed to causal attention masks and positional encoding. The authors propose Cluster-based Search, a method that reduces time complexity for demonstration selection while maintaining performance."
          },
          "strengths": {
            "value": "The paper makes a significant contribution by uncovering a novel clustering property in LLM embeddings, supported by thorough empirical analysis (UMAP, K-Means, partial derivatives) and theoretical insights. The proposed Cluster-based Search method addresses a practical challenge in ICL, offering a substantial efficiency gain. The work is well-structured, with clear explanations of both theoretical and empirical findings. The focus on causal attention and positional encoding provides new understanding of LLM behavior."
          },
          "weaknesses": {
            "value": "The theoretical analysis relies on idealized assumptions (e.g., unit-sphere embeddings) that may not hold in practice. While the rebuttal clarifies the asymmetric importance of first vs. last demonstrations, the paper initially lacks sufficient justification for why first-demonstration clustering is stronger. The empirical validation for last-demonstration clustering is less robust, with some visualizations suggesting weaker effects. The paper could better address how clustering interacts with different LLM architectures and tasks."
          },
          "questions": {
            "value": "1. How does the clustering property hold when the first and last demonstrations are identical? 2. What specific mechanisms explain the asymmetric clustering effect between first and last demonstrations? 3. Are the observed clustering patterns consistent across different LLM architectures (e.g., GPT-2 vs. Llama)? 4. How does the method handle cases where the optimal demonstration order differs from the clustering-based selection?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 5
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates the order sensitivity of in-context learning (ICL) in decoder-only LLMs by analyzing prompt embeddings. The authors identify a 'clustering property' where prompts with the same first and last demonstrations form clusters in the embedding space, attributed to causal attention masks and positional encoding. They propose Cluster-based Search, a method to accelerate demonstration selection and ordering, reducing time complexity from factorial to quadratic while maintaining performance."
          },
          "strengths": {
            "value": "The paper makes a novel contribution by linking ICL order sensitivity to the structural properties of LLMs (causal attention, positional encoding). Empirical analyses (UMAP, K-Means, partial derivatives) are thorough and consistent. The proposed Cluster-based Search addresses a practical bottleneck in ICL, demonstrating significant speed improvements. Theoretical insights, while abstract, provide a foundation for understanding clustering mechanisms. The work is well-motivated and addresses a critical gap in ICL research."
          },
          "weaknesses": {
            "value": "The theoretical analysis lacks rigorous proof, relying on abstract equations without clear connections to empirical results. The clustering property's dependence on specific LLM architectures (e.g., causal attention) is not fully explored. Experiments with larger demonstration sets (k=16/10) are limited, and the rebuttal's additional analysis (e.g., attention weights) was not in the original paper. The claim about 'both first and last demonstration clustering' is overstated, with the rebuttal clarifying first-demonstration clustering is stronger. The paper could better address edge cases (e.g., non-causal models)."
          },
          "questions": {
            "value": "1. How generalizable is the clustering property across non-causal architectures or tasks with different token distributions? 2. Can the theoretical analysis be strengthened with concrete proofs or simulations? 3. What are the limitations of Cluster-based Search in tasks where demonstration order has minimal impact? 4. How do positional encodings interact with causal masks in other LLM variants (e.g., with and without absolute positional encoding)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates the clustering property in the prompt embedding space of decoder-only LLMs, where prompts with the same first and last demonstrations form clusters. The authors theoretically and empirically analyze this phenomenon, attributing it to causal attention masks and positional encoding. They propose Cluster-based Search, a method that accelerates demonstration selection and ordering by leveraging these clusters, reducing time complexity from factorial to quadratic."
          },
          "strengths": {
            "value": "The paper makes a novel contribution by identifying the clustering property in LLM embeddings and linking it to order sensitivity in ICL. The empirical validation is thorough, combining UMAP visualization, K-Means clustering, and partial derivative analysis. The theoretical analysis provides a foundation for understanding the clustering mechanism, and the practical application of Cluster-based Search demonstrates significant efficiency gains. The rebuttal effectively addresses concerns about the strength of first-demonstration clustering and clarifies the methodological refinements."
          },
          "weaknesses": {
            "value": "The empirical analysis is limited to specific LLMs and tasks, and the generalizability of the clustering property across diverse models and domains remains unproven. The theoretical model assumes idealized conditions (e.g., unit-sphere embeddings) that may not reflect real-world scenarios. The paper initially overemphasized the role of both first and last demonstrations, though the rebuttal mitigates this by refining the focus to first-demonstration clustering. The impact of clustering on different task types (classification vs. reasoning) is not fully explored."
          },
          "questions": {
            "value": "1. How does the clustering property vary across different LLM architectures and tasks beyond the ones tested? 2. What are the limitations of the theoretical analysis under non-idealized assumptions (e.g., non-unit-sphere embeddings)? 3. How does the proposed method handle cases where the clustering effect is weak or absent? 4. What is the relationship between the observed attention weight patterns and the clustering property in later layers?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1Njl73JKjB": {
    "paper_id": "1Njl73JKjB",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces a principled method to evaluate sparse autoencoders (SAEs) by comparing them to supervised feature dictionaries, using the Indirect Object Identification (IOI) task as a case study. The authors propose a framework to assess SAEs' disentanglement and control over model behavior through three tests: approximation, sparse controllability, and causal faithfulness. They demonstrate that SAEs trained on task-specific or full-distribution data capture interpretable features comparable to supervised dictionaries, while also highlighting qualitative phenomena like feature splitting and magnitude preferences."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in evaluating SAEs by proposing a novel, task-specific framework grounded in supervised feature dictionaries. The methodology is rigorous, with clear experimental design and application to a well-studied task (IOI). The authors emphasize scalability and generalizability, acknowledging limitations while providing actionable directions for future work. The integration of theoretical insights (e.g., linear representation hypothesis) with empirical validation strengthens the contribution. The rebuttal effectively addresses many technical concerns, improving clarity and depth."
          },
          "weaknesses": {
            "value": "The framework's reliance on manually defined task attributes (e.g., IO, S, Pos) limits its generalizability to tasks without clearly identifiable ground-truth features. The experiments are narrowly focused on the IOI task, with insufficient exploration of broader applicability. The paper lacks comparisons to alternative SAE evaluation methods (e.g., geometric measures, toy models) and does not fully address how the supervised dictionaries themselves are validated. The rebuttal improves clarity on the F1 score and necessity/sufficiency formulas, but the original paper's technical depth on these points was lacking."
          },
          "questions": {
            "value": "1. How does the framework handle tasks where ground-truth attributes are ambiguous or multi-faceted? 2. What are the computational costs of identifying task-specific attributes, and how do they scale with task complexity? 3. Are the supervised dictionaries themselves robust to variations in the underlying data distribution? 4. How do the observed phenomena (feature splitting, magnitude preferences) impact downstream tasks beyond the IOI task? 5. Could the framework be adapted to evaluate SAEs in non-linguistic domains?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces a principled framework to evaluate sparse autoencoders (SAEs) for interpretability by comparing them to supervised feature dictionaries derived from task-relevant attributes. The approach is applied to the Indirect Object Identification (IOI) task using GPT-2 Small, demonstrating that SAEs can capture interpretable features and achieve performance comparable to supervised methods. The work also highlights qualitative phenomena in SAE training, such as feature splitting and magnitude biases."
          },
          "strengths": {
            "value": "The paper presents a novel and structured method for evaluating SAEs, addressing a critical gap in interpretability research. The IOI task case study is well-executed, with clear experimental design and analysis. The authors emphasize the importance of task-specific supervision, which provides a meaningful benchmark for SAEs. The methodology is technically sound, and the paper is well-organized with clear sections and figures. The contributions advance the understanding of SAEs in realistic linguistic tasks."
          },
          "weaknesses": {
            "value": "The experiments are limited to the IOI task, and the generalizability to other tasks or models is not thoroughly demonstrated. The paper could benefit from more direct comparisons with alternative SAE evaluation methods. While the rebuttal addresses some issues (e.g., moving formulas to the main text), the original work lacked detailed ablation studies or analysis of edge cases. The reliance on manually defined attributes for supervision introduces a potential bottleneck, though the authors acknowledge this as a limitation."
          },
          "questions": {
            "value": "1. How does the framework handle tasks where task-relevant attributes are not explicitly defined or are ambiguous? 2. What are the computational costs of building supervised dictionaries for more complex or high-dimensional tasks? 3. How do the authors ensure that the supervised dictionaries do not inadvertently capture spurious correlations rather than meaningful features? 4. Can the framework be adapted to evaluate SAEs in non-linguistic domains, such as vision or reinforcement learning?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a principled framework for evaluating sparse autoencoders (SAEs) by comparing them to supervised feature dictionaries on specific tasks, using the Indirect Object Identification (IOI) task as a case study. It demonstrates that SAEs can capture interpretable features and that certain variants (e.g., Gated SAEs) perform competitively with supervised methods. The work emphasizes disentanglement, control, and causal consistency of features."
          },
          "strengths": {
            "value": "The paper's main strength lies in its novel, task-specific evaluation framework that addresses a critical gap in SAE evaluation. The methodology is well-structured, with clear experiments on the IOI task and comparisons between SAE variants. The theoretical grounding in the linear representation hypothesis and the use of logit difference metrics for causal analysis are robust. The paper also provides qualitative insights into SAE training dynamics (e.g., feature splitting)."
          },
          "weaknesses": {
            "value": "The evaluation is limited to the IOI task, with insufficient discussion on generalizability to other tasks or domains. The comparison with existing SAE evaluation metrics (e.g., reconstruction error, sparsity) is superficial, and the paper does not thoroughly address why supervised feature dictionaries are superior. The theoretical justification for the supervised dictionary approach lacks depth, and the computational scalability of the method is not discussed. Additionally, the paper's claims about 'causal faithfulness' of features require stronger empirical validation."
          },
          "questions": {
            "value": "1. How do the supervised feature dictionaries compare to other SAE evaluation metrics (e.g., reconstruction accuracy, sparsity) in terms of reliability and task relevance? 2. Are there scenarios where SAEs fail to capture features that the supervised dictionaries successfully identify, and what are the underlying reasons? 3. What are the computational costs of the proposed framework, and how does it scale to larger models or more complex tasks? 4. How does the framework handle tasks with ambiguous or overlapping feature definitions, where supervised dictionaries may not be straightforward to construct?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "1NprT9Kz0d": {
    "paper_id": "1NprT9Kz0d",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "TexTailor introduces a method for generating view-consistent 3D textures from text descriptions by addressing texture shifts across viewpoints through resampling in a non-Markovian diffusion process, a performance preservation loss, and adaptive camera positioning. The approach leverages depth-aware diffusion models and demonstrates superior results on standard datasets."
          },
          "strengths": {
            "value": "The paper tackles a critical challenge in 3D texture synthesis—view inconsistency—by proposing a novel resampling scheme adapted from 2D image inpainting. The integration of a performance preservation loss and adaptive viewpoint refinement addresses key limitations of prior work. The experiments are comprehensive, comparing against multiple state-of-the-art methods and validating the effectiveness of the proposed techniques. The method’s practicality is reinforced by its compatibility with existing diffusion models like Stable Diffusion with ControlNet."
          },
          "weaknesses": {
            "value": "The paper lacks a clear distinction between TexTailor’s resampling approach and similar methods (e.g., TexFusion, TexGen), which the rebuttal clarifies but the original text does not adequately address. The third paragraph of the introduction’s discussion on explicit vs. implicit representations is unclear, potentially weakening the motivation for the work. Additionally, the over-smoothed appearance of results in Fig. 1b requires further explanation, particularly how the update process in Sec. 2.2 mitigates this issue."
          },
          "questions": {
            "value": "1. How does the resampling scheme in TexTailor differ quantitatively from TexFusion and TexGen, especially in terms of sampling iterations and texture consistency metrics? 2. Can the authors provide additional examples or ablation studies to validate the effectiveness of the performance preservation loss in preventing catastrophic forgetting? 3. What specific geometric constraints cause over-smoothing in certain viewpoints, and how does the adaptive viewpoint refinement mitigate this in complex shapes?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "TexTailor introduces a method for generating view-consistent 3D textures from text descriptions by addressing limitations in existing diffusion-based approaches. The key innovations include a resampling scheme within a non-Markovian diffusion process, a performance preservation loss to mitigate overfitting with limited data, and adaptive camera position refinement based on mesh geometry. Experiments show improvements over state-of-the-art methods on Objaverse and ShapeNet car datasets."
          },
          "strengths": {
            "value": "The paper presents a novel application of resampling techniques from 2D image inpainting to 3D texture synthesis, leveraging the non-Markovian DDIM process for efficiency. The adaptive viewpoint refinement addresses a critical limitation of fixed camera positions in prior work. The performance preservation loss is a thoughtful solution to the challenge of training on small datasets. The experimental results demonstrate quantitative improvements in LPIPS and FID metrics, and the method's ability to generate coherent textures across viewpoints is well-supported by qualitative examples."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the contributions of the resampling scheme, performance preservation loss, and adaptive viewpoint refinement. The comparison with existing methods is limited to a subset of datasets, and it is unclear how TexTailor performs against all relevant baselines. The theoretical analysis of why the resampling approach reduces texture shifts is minimal. Additionally, the paper does not address potential limitations in handling highly complex geometries or extreme viewpoint changes. The connection between the third paragraph of the introduction and the main contribution is unclear, as noted in the rebuttal."
          },
          "questions": {
            "value": "1. How is the adaptive camera position refinement algorithm implemented? What metrics or criteria are used to determine when and how to adjust viewpoints? 2. Are there ablation studies demonstrating the individual contributions of the resampling scheme, performance preservation loss, and viewpoint adaptation? 3. How does the method handle textures for objects with highly occluded or non-convex geometries? 4. The rebuttal clarifies the distinction between TexFusion and TexGen, but the paper's novelty claims could be strengthened by a more detailed comparison with these works. 5. What is the computational cost of the resampling process compared to baseline methods?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "TexTailor introduces a novel method for generating view-consistent 3D textures from text descriptions by addressing limitations in existing diffusion-based approaches. It employs a resampling scheme within a non-Markovian diffusion process (DDIM), a performance preservation loss to mitigate overfitting with limited data, and adaptive viewpoint refinement based on mesh geometry. The method demonstrates superior performance on Objaverse and ShapeNet car datasets compared to prior work."
          },
          "strengths": {
            "value": "The paper presents a technically sound approach with clear motivation for addressing texture degradation across viewpoints. The resampling strategy extends 2D image inpainting techniques to 3D texture synthesis, which is a novel contribution. The adaptive viewpoint refinement addresses a critical limitation of fixed camera positions. The experiments show measurable improvements in LPIPS and FID metrics, and the integration of ControlNet and DDIM is well-justified. The paper also provides a detailed analysis of the gradual texture shift phenomenon."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient ablation studies to isolate the impact of individual components (e.g., resampling, performance preservation loss, adaptive viewpoints). The performance preservation loss is not clearly defined or evaluated in terms of its contribution to preventing overfitting. The comparison to recent methods like TexFusion and TexGen is insufficiently detailed in the original manuscript, requiring clarification in the rebuttal. The experimental validation is limited to two datasets, and the paper does not address failure cases or edge scenarios. The theoretical analysis of how resampling improves texture consistency is minimal."
          },
          "questions": {
            "value": "1. How is the performance preservation loss formulated mathematically, and what ablation studies demonstrate its effectiveness? 2. What specific metrics or qualitative results show the benefit of adaptive viewpoint refinement compared to fixed viewpoints? 3. Why does the paper focus on explicit mesh representations when implicit methods like DMTet already enable geometry extraction? 4. How does the resampling scheme in DDIM differ from existing methods in terms of implementation and theoretical guarantees? 5. Are there quantitative results showing the reduction in texture shift across viewpoints?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1OyE9IK0kx": {
    "paper_id": "1OyE9IK0kx",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates the challenges of achieving faithful Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) by evaluating three strategies: activation editing, fine-tuning, and in-context learning (ICL). The authors demonstrate that these techniques offer limited success in improving faithfulness, with only marginal gains in controlled scenarios. They also analyze the inherent difficulties in eliciting faithful CoT reasoning and highlight the need for new methodologies."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem—faithfulness in LLMs—which is essential for high-stakes applications like healthcare. The methodology is rigorous, combining established metrics (e.g., early answering) with systematic experimentation across multiple strategies. The clarity of the problem statement, technical descriptions, and empirical analysis is strong. The significance of the findings lies in their implication that current approaches are insufficient, which motivates future research on trustworthy reasoning."
          },
          "weaknesses": {
            "value": "The paper lacks in-depth analysis of why the tested strategies failed, which limits the interpretability of results. While the rebuttal mentions that explanations for success/failure were omitted from the main text, this omission weakens the paper’s contribution. The experiments are limited to a narrow set of benchmarks, and the authors acknowledge that expanding to more domains would strengthen the work. Additionally, the faithfulness metric (early answering) is not thoroughly compared to alternatives, leaving questions about its validity and generalizability."
          },
          "questions": {
            "value": "1. How do the authors reconcile the limited success of activation editing with their claim that it 'demonstrates limited success in amplifying faithful behavior'? 2. What specific limitations of the early answering metric (e.g., sensitivity to prompt design) were not addressed in the paper? 3. How do the authors plan to validate their findings across additional domains or tasks in future work? 4. Can the authors provide more details on the 'fundamental uniqueness' of finetuning for faithfulness, as mentioned in the rebuttal?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates the challenges of achieving faithful Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) by evaluating three strategies: activation editing, fine-tuning, and in-context learning (ICL). The authors find that these methods offer only marginal improvements in faithfulness, highlighting the inherent difficulty of aligning LLM-generated explanations with their internal reasoning processes. They also propose a framework for measuring faithfulness using the early answering metric and discuss the limitations of existing approaches."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem: the trustworthiness of LLMs in high-stakes applications. It provides a systematic evaluation of existing techniques for improving CoT faithfulness, which is novel in its focus on this specific aspect of LLM transparency. The methodology is rigorous, with clear definitions of faithfulness and well-structured experimental designs. The paper also contributes to the literature by identifying the limitations of current approaches, which is valuable for guiding future research. The use of the early answering metric from Lanham et al. (2023) is a significant strength, as it offers a quantifiable way to assess faithfulness."
          },
          "weaknesses": {
            "value": "The paper's empirical analysis is limited to a narrow set of benchmarks, which reduces the generalizability of its findings. While the authors acknowledge this in their rebuttal, the lack of diverse datasets and tasks weakens the impact of their conclusions. Additionally, the explanations for why the proposed methods fail are insufficiently detailed in the main text, with key insights relegated to the appendix. The paper also does not thoroughly address the nuances of the faithfulness metric used, despite the rebuttal's clarification. Finally, the absence of a clear roadmap for future work on improving faithfulness limits the paper's practical utility."
          },
          "questions": {
            "value": [
              "How do the authors plan to expand their experiments to other domains and datasets in future work, and what specific challenges might arise in doing so?",
              "What are the key insights from the analysis of ICL and fine-tuning limitations that could inform the design of new methods for improving faithfulness?",
              "The rebuttal mentions a coding framework for assessing faithfulness—how accessible and well-documented is this framework for the broader research community?",
              "How does the authors' use of the early answering metric compare to alternative faithfulness measures, and what are the trade-offs of this choice?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates the difficulty of achieving faithful Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) by evaluating three strategies—activation editing, fine-tuning, and in-context learning (ICL). The authors find limited success in improving faithfulness, highlighting the inherent challenges in aligning CoT explanations with the model's internal reasoning processes. They propose a systematic analysis of faithfulness using the early answering metric and identify gaps in existing techniques."
          },
          "strengths": {
            "value": "The paper's originality lies in its systematic evaluation of existing techniques for improving faithfulness, a critical yet underexplored aspect of LLM transparency. The methodology is rigorous, leveraging established metrics like the early answering test and providing detailed experimental setups. The clarity of the writing is strong, with clear definitions of concepts and structured sections. The significance is high, as the work addresses a pressing need for trustworthy reasoning in high-stakes applications."
          },
          "weaknesses": {
            "value": "The paper's scope is limited by the use of a narrow set of benchmarks, which may reduce the generalizability of findings. The analysis of why existing strategies fail is superficial, with the authors acknowledging that deeper insights were omitted from the main text. The reliance on a single faithfulness metric (early answering) without thorough discussion of its limitations is a drawback. Additionally, the paper lacks concrete examples of how the proposed strategies could be improved, leaving the path forward somewhat vague."
          },
          "questions": {
            "value": "1. How do the authors plan to address the limitations of the early answering metric in future work? 2. What specific insights did the experiments reveal about the inherent challenges in achieving faithfulness, beyond the general claim of 'inherent difficulty'? 3. Can the authors provide more details on the 'fundamental uniqueness' of fine-tuning for faithfulness, as mentioned in the rebuttal? 4. How might the findings be validated across additional domains or tasks to strengthen the paper's impact?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1R5BcYS8EC": {
    "paper_id": "1R5BcYS8EC",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces SysCaps, a framework for enhancing simulation surrogates using natural language descriptions (system captions) to improve accessibility and performance. It proposes a lightweight multimodal model combining text and time-series inputs, leverages LLMs to generate synthetic captions from simulation metadata, and demonstrates improved accuracy and generalization on real-world energy system simulators."
          },
          "strengths": {
            "value": "The work presents a novel application of language interfaces in surrogate modeling for complex energy systems, addressing a critical gap in accessibility for non-experts. The multimodal architecture is well-structured, and the use of LLMs for synthetic caption generation is innovative. Experiments on real-world datasets (buildings and wind farms) show competitive performance, with evidence of generalization benefits. The paper also highlights potential for language-driven design exploration, a promising direction for future work."
          },
          "weaknesses": {
            "value": "The paper lacks thorough comparisons with state-of-the-art regression models beyond one-hot baselines, which limits the assessment of novelty. The evaluation of non-expert usability remains superficial, as no user studies were conducted. Technical details about the multimodal fusion mechanism and the LLM caption generation pipeline are under-specified. The claim about 'semantically related generalization' lacks quantitative validation. The rebuttal addresses some concerns but leaves open questions about model robustness to long captions and the impact of attribute selection."
          },
          "questions": {
            "value": "1. How do the results compare to advanced regression models (e.g., neural networks, hybrid models) beyond one-hot encodings? 2. What metrics quantify the 'semantic generalization' benefits claimed? 3. How does the model handle noisy or incomplete LLM-generated captions? 4. Are there ablation studies on the impact of different text encoders (e.g., BERT vs. other architectures)? 5. What is the computational cost of the LLM-based caption generation pipeline in real-world deployment?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces 'SysCaps,' a framework for enhancing simulation surrogates of complex energy systems (CES) by integrating natural language descriptions (system captions) with time-series data. The approach leverages large language models (LLMs) to generate synthetic captions from simulation metadata, combining them with a lightweight multimodal model for improved regression accuracy and generalization. Experiments on building and wind farm simulators demonstrate superior performance over traditional methods and highlight capabilities like language-driven design exploration."
          },
          "strengths": {
            "value": "The paper presents a novel application of language interfaces to surrogate modeling, addressing accessibility for non-experts. The use of LLMs to synthesize high-quality captions from metadata is a significant contribution. The experiments are grounded in real-world CES simulators, and the open-source release fosters reproducibility. The multimodal architecture, which fuses text embeddings with time-series data, is well-motivated and technically sound. The paper also emphasizes practical benefits like semantic generalization and prompt-based regularization."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous comparison with state-of-the-art multimodal models for time-series regression, which limits the assessment of the proposed architecture's novelty. The evaluation of LLM-generated captions vs. key-value templates is superficial, with insufficient analysis of their relative strengths. The absence of user studies or qualitative feedback from non-experts weakens claims about accessibility. Additionally, the handling of numeric attributes (e.g., bucketing) and the impact of caption length on performance require deeper exploration. The rebuttal addresses some issues, but core limitations in evaluation depth remain."
          },
          "questions": {
            "value": "1. How does the proposed multimodal model compare to existing architectures like Time-LLM or contrastive pretraining methods in terms of performance and efficiency? 2. What specific advantages do LLM-generated captions offer over key-value templates in downstream tasks beyond accuracy (e.g., interpretability, robustness)? 3. How generalizable is the SysCaps framework to other domains beyond energy systems? 4. What are the limitations of the current tokenization strategy for key-value templates, and how might it affect model performance?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces SysCaps, a framework that uses natural language descriptions (system captions) to interface with simulation surrogates for complex energy systems. The approach combines text embeddings from LLMs with time series data to improve accuracy and generalization in surrogate modeling, demonstrated on building and wind farm simulators."
          },
          "strengths": {
            "value": "The paper's originality lies in applying language interfaces to surrogate modeling, a novel approach for improving accessibility and generalization. The lightweight multimodal architecture and LLM-based SysCap generation are well-structured. Experiments on real-world simulators demonstrate improved accuracy over baselines, and the open-source release facilitates future work. The paper also highlights potential for language-driven design exploration."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive baseline comparisons (e.g., neural network-based methods) and detailed analysis of LLM-generated caption quality. Generalization experiments rely on synthetic data, and the absence of user studies limits claims about accessibility for non-experts. The model's scalability to longer captions and the exact role of text embeddings in generalization remain underexplored. The comparison to related work (e.g., Time-LLM) is insufficient."
          },
          "questions": {
            "value": "1. How does the model handle long captions beyond the 'medium' length it was trained on? 2. What specific metrics were used to evaluate the quality of LLM-generated SysCaps? 3. Why not compare with neural network-based baselines (e.g., transformers) for time series regression? 4. How are the 'key-value template' and 'natural language' SysCaps fundamentally different in terms of model performance? 5. What ablation studies were conducted to isolate the impact of text embeddings on generalization?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1S7kpbfgq9": {
    "paper_id": "1S7kpbfgq9",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces Normalized Space Alignment (NSA), a novel metric for comparing neural network representations by combining local and global structural discrepancies. NSA consists of Local NSA (LNSA) and Global NSA (GNSA), which respectively analyze local intrinsic dimensionality and pairwise distance similarities. The method is presented as a differentiable loss function and analytical tool for tasks like dimensionality reduction and adversarial robustness analysis."
          },
          "strengths": {
            "value": "The paper offers a well-structured theoretical foundation for NSA, with rigorous mathematical formulations and clear definitions of LNSA and GNSA. The computational complexity analysis is thorough, and the authors address key challenges in representation learning (e.g., scalability, parameter tuning). The rebuttal strengthens the work by adding diverse datasets and experiments, demonstrating NSA's generalizability across domains. The method's dual role as a similarity index and loss function is novel and broadly applicable."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons to relevant baselines in critical applications like link prediction and adversarial analysis, which limits the evaluation of NSA's practical efficacy. While the rebuttal adds contextual benchmarks, it does not address how NSA outperforms existing methods in these tasks. The theoretical guarantees for NSA's convergence and robustness under varying conditions remain underexplored. Additionally, the paper does not fully clarify how NSA handles significant dimensional mismatches between representations."
          },
          "questions": {
            "value": "1. How does NSA specifically outperform existing metrics like CKA or RTD in tasks such as link prediction or adversarial analysis? 2. What are the theoretical limitations of NSA when dealing with highly non-Euclidean or sparse data? 3. How does the choice of $k$ in LNSA affect performance, and what guidelines are recommended for selecting $k$ in different applications? 4. Can NSA be adapted to handle dynamic or streaming data scenarios?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces Normalized Space Alignment (NSA), a metric for comparing and aligning neural network representations by combining local and global structural analysis. NSA leverages Local Intrinsic Dimensionality (LID) for local alignment and Representational Similarity Matrices (RSMs) for global alignment, aiming to address challenges in representation learning such as structural integrity preservation and robustness analysis."
          },
          "strengths": {
            "value": "The paper presents a novel framework (NSA) that integrates established concepts (LID, RSMs) into a unified metric with clear theoretical foundations. The methodology is computationally efficient (quadratic complexity) and demonstrates versatility across tasks like dimensionality reduction and adversarial analysis. The rebuttal addresses key concerns by expanding datasets, adding ablation studies, and clarifying NSA's role as a metric vs. loss function. Theoretical guarantees on mini-batch convergence and scalability are notable strengths."
          },
          "weaknesses": {
            "value": "The paper initially lacked rigorous comparisons to relevant baselines (e.g., CKA, RTD) and detailed ablation studies. While the rebuttal adds some comparisons, the link prediction experiments still conflate NSA's role as a dimensionality reduction tool with its use as a loss function. The empirical validation on large-scale datasets (e.g., ImageNet) remains limited, and the paper does not fully clarify how NSA's local analysis (via LID) complements global RSM-based approaches. Theoretical proofs for certain properties (e.g., triangle inequality for GNSA) are deferred to appendices without explicit verification."
          },
          "questions": {
            "value": "1. How does NSA's local analysis (LNSA) specifically improve upon existing neighborhood-based metrics? 2. What are the exact conditions under which NSA's mini-batch convergence guarantees hold for large-scale datasets? 3. How does the paper address potential biases in the choice of datasets (e.g., COIL-20 vs. ImageNet) for generalizability claims? 4. Can the authors provide additional evidence of NSA's effectiveness in adversarial robustness analysis compared to specialized defense mechanisms?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Normalized Space Alignment (NSA), a metric for comparing neural network representations by combining Local NSA (LNSA) and Global NSA (GNSA). LNSA uses Local Intrinsic Dimensionality (LID) to measure local structure preservation, while GNSA leverages Representational Similarity Matrices (RSMs) for global discrepancies. NSA is presented as a differentiable, computationally efficient tool for tasks like dimensionality reduction and adversarial analysis."
          },
          "strengths": {
            "value": "The paper provides a clear theoretical foundation for NSA, integrating LID and RSMs with rigorous mathematical definitions. The computational efficiency claims are supported by complexity analysis. The versatility of NSA is demonstrated across multiple applications, and the rebuttal addresses concerns about dataset diversity and scalability. The paper’s structure and technical depth are strong."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons to established metrics like CKA or RTD in key tasks, which undermines claims of superiority. While the rebuttal clarifies NSA’s role as a tool rather than a direct competitor, the experimental sections still need stronger baselines. Ablation studies and detailed analysis of hyperparameter sensitivity are insufficient. The theoretical guarantees for mini-batch convergence are mentioned but not thoroughly validated."
          },
          "questions": {
            "value": "1. How does NSA compare to CKA/RTD in specific tasks like link prediction or adversarial robustness? 2. What are the limitations of NSA’s mini-batch approximation for large-scale datasets? 3. Can the authors provide more details on the ablation studies mentioned in the rebuttal? 4. How generalizable is NSA to non-Euclidean or high-dimensional spaces beyond the experiments shown?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1S8ndwxMts": {
    "paper_id": "1S8ndwxMts",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper systematically evaluates metrics for assessing protein generative models, focusing on quality, diversity, and distributional similarity. The authors analyze the behavior of metrics like pLDDT, perplexity, and distributional similarity measures under controlled perturbations and real-world scenarios, identifying challenges such as sample size dependencies and computational trade-offs. They provide practical recommendations for robust evaluation practices."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in the protein design field by rigorously analyzing evaluation metrics. It combines synthetic and real-world experiments, offering actionable insights. The methodology is thorough, with clear categorization of metrics (quality, diversity, distributional similarity). The discussion of limitations and practical recommendations enhances its utility for researchers. The bidirectional sequence-structure mapping framework for defining 'good' proteins is novel and well-justified."
          },
          "weaknesses": {
            "value": "The experiments on real-world generative models are limited, with no comparison of state-of-the-art models like AlphaFold or ESMFold. The synthetic datasets lack diversity (e.g., no multi-domain proteins or complex structural perturbations). Computational efficiency analysis is superficial, with no quantitative benchmarks for metrics like scPerplexity. The rebuttal clarifies some methodological choices but does not address the lack of ablation studies on metric parameters. The paper also does not explore metric robustness to out-of-distribution data beyond the described cluster corruption scenarios."
          },
          "questions": {
            "value": "1. How were the synthetic perturbations (e.g., amino acid substitutions) validated to reflect realistic model training dynamics? 2. Why was ProGen2-base chosen for perplexity calculations over other models like ESM-2? 3. Could the authors provide computational cost benchmarks for scPerplexity across different model sizes? 4. How generalizable are the recommendations to non-structural metrics (e.g., functional validity)? 5. Were there any unexpected interactions between metrics that warranted further investigation?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper conducts a systematic analysis of evaluation metrics for protein generative models, focusing on quality, diversity, and distributional similarity. The authors investigate metrics like pLDDT, perplexity, and Fréchet distance under synthetic perturbations and real-world models, identifying challenges such as sample size dependencies and computational efficiency trade-offs. They provide practical recommendations for robust evaluation."
          },
          "strengths": {
            "value": "The paper thoroughly examines key evaluation metrics, covering quality, diversity, and distributional similarity. It combines synthetic and real-world experiments to validate metrics under controlled and practical conditions. The focus on practical recommendations addresses a critical gap in the field. The methodology is rigorous, with clear categorization of metrics and systematic analysis of their behavior."
          },
          "weaknesses": {
            "value": "The paper lacks a comprehensive comparison with existing evaluation frameworks, leaving unclear how its findings align with prior work. The synthetic datasets, while controlled, may not fully capture real-world protein complexity. The justification for selecting specific metrics (e.g., pLDDT over alternatives) is insufficient. Computational efficiency trade-offs are mentioned but not quantified in detail for practical deployment scenarios."
          },
          "questions": {
            "value": "1. How do the authors' recommendations address gaps in existing evaluation practices compared to prior work? 2. Could the synthetic datasets be extended to include more biologically relevant perturbations (e.g., structural constraints)? 3. What empirical evidence supports the claim that scPerplexity is computationally expensive, and how does this impact real-world applications?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper conducts a systematic analysis of evaluation metrics for protein generative models, focusing on quality, diversity, and distributional similarity. It evaluates metrics under synthetic and real-world conditions, identifies challenges like sample size dependencies, and provides practical recommendations for robust evaluation."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in the lack of standardized metrics for protein generative models, offering a structured framework for evaluation. It systematically examines multiple metric categories (quality, diversity, distributional similarity) with controlled experiments and real-world data. The proposed desirable properties for metrics (robustness, interpretability, efficiency) are well-reasoned, and the practical recommendations are valuable for the field."
          },
          "weaknesses": {
            "value": "The paper lacks a comprehensive comparison of metrics' effectiveness in different scenarios. The synthetic data experiments, while controlled, may not fully capture real-world complexities. The computational trade-offs for metrics like scPerplexity are not thoroughly discussed. The diversity metric (Cluster Density) is briefly described without detailed analysis of its limitations or validation. The recommendations are general and could benefit from more specific guidance for different model types."
          },
          "questions": {
            "value": "How do the authors ensure that synthetic data reflects real protein distributions? Are there limitations in the choice of metrics examined? How do the recommendations address computational efficiency for large-scale applications? What are the specific trade-offs between metrics in terms of accuracy and runtime?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1VwWi6zbxs": {
    "paper_id": "1VwWi6zbxs",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a new metric called τJp, derived from the product of task vectors and the Jacobian of pre-trained models, to address interference in task arithmetic. The authors propose regularization to minimize τJp, demonstrating its effectiveness in reducing coefficient tuning and improving task performance. They validate their approach through experiments on vision and NLP tasks, showing scalability in incremental learning scenarios."
          },
          "strengths": {
            "value": "The paper provides a clear theoretical framework linking τJp to weight disentanglement, supported by experimental validation. The regularization approach is practical and addresses a key limitation of task arithmetic. The work extends prior linearization methods by offering a causal explanation for interference, and the experiments demonstrate reproducibility across diverse tasks. The paper also highlights real-world applicability by leveraging publicly available models."
          },
          "weaknesses": {
            "value": "The novelty claim is partially weakened by similarities to Ortiz-Jimenez et al. (2023), which also emphasized linearization. While the authors address this by highlighting τJp's causal role, the theoretical justification for τJp's specific formulation remains underdeveloped. The hyperparameter selection for regularization is inadequately detailed, and the paper lacks a thorough comparison with alternative regularization strategies. The connection between τJp and NTK regime assumptions requires deeper analysis."
          },
          "questions": {
            "value": "1. How does τJp's formulation specifically capture the causal mechanisms of interference compared to other Jacobian-based metrics? 2. What are the limitations of the NTK regime assumptions in real-world non-linear fine-tuning scenarios? 3. How does the proposed regularization interact with other task arithmetic techniques (e.g., task analogies) not covered in the paper? 4. Could the τJp metric be generalized to other model architectures beyond Vision Transformers?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces a new metric called τJp, derived from the product of task vectors and the Jacobian of pre-trained models, to quantify interference in task arithmetic. It proposes regularization to minimize τJp, aiming to improve weight disentanglement and reduce the need for coefficient tuning in model editing. The work also demonstrates practical benefits in incremental learning and with pre-trained models."
          },
          "strengths": {
            "value": "The paper makes a clear theoretical contribution by linking τJp to weight disentanglement in the NTK regime, providing a causal mechanism for task arithmetic interference. The experimental validation shows a correlation between τJp and performance metrics, and the practical applications (e.g., incremental learning, pre-trained models) highlight its real-world relevance. The work builds on prior NTK and linearization studies while offering a novel regularization framework."
          },
          "weaknesses": {
            "value": "The novelty is partially overlapping with Ortiz-Jimenez et al. (2023), as the paper relies on similar linearization principles without sufficiently distinguishing its methodological contributions. The theoretical explanation for why τJp minimization improves performance remains underdeveloped, and the regularization's effectiveness compared to existing methods lacks rigorous comparison. Hyperparameter selection (e.g., λ) is not thoroughly justified, and the paper does not address potential trade-offs between regularization and model plasticity."
          },
          "questions": {
            "value": "1. How does the τJp regularization specifically differ from prior approaches that use orthogonality constraints (e.g., [2])? 2. What are the limitations of using a unified λ across tasks, and how sensitive is performance to this choice? 3. How does the regularization affect long-term fine-tuning dynamics or adaptability to new tasks? 4. Are there cases where τJp increases despite regularization, and what causes this?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a new metric called τJp, derived from the product of task vectors and the Jacobian of pre-trained models, to quantify interference in task arithmetic. The authors propose regularization to minimize τJp, reducing interference and eliminating the need for coefficient tuning. They validate this approach on vision models, demonstrating improved weight disentanglement and practical benefits for real-world applications."
          },
          "strengths": {
            "value": "The paper presents a novel metric (τJp) with theoretical grounding in the NTK regime, linking it to weight disentanglement. The regularization approach is experimentally validated on multiple tasks, showing reduced interference and improved performance. The work addresses practical challenges in task arithmetic, such as coefficient tuning and scalability in incremental learning. The connection to prior work on linearization (e.g., Ortiz-Jimenez et al.) is acknowledged, and the authors provide empirical evidence of τJp's correlation with accuracy and disentanglement error."
          },
          "weaknesses": {
            "value": "The paper's novelty is somewhat limited compared to prior work (e.g., Ortiz-Jimenez et al. 2023), which also emphasized linearization. While the authors claim a causal mechanism, the theoretical justification for τJp's role in interference remains underdeveloped. The hyperparameter selection for regularization is not thoroughly explained, and the sensitivity analysis of λ is lacking. The paper also does not fully address how τJp regularization interacts with model plasticity during fine-tuning, which could affect generalization. Additionally, the experimental scope is narrow, focusing on vision models without broader validation."
          },
          "questions": {
            "value": "1. How does the proposed τJp regularization compare theoretically to existing methods like kernel localization in Ortiz-Jimenez et al.? 2. What is the exact mechanism by which τJp reduction improves task arithmetic, and how does this differ from prior work? 3. Can the authors provide more details on the hyperparameter selection process and the robustness of λ across tasks? 4. How does the regularization affect the model's ability to adapt to new tasks, and what are the trade-offs? 5. Are the empirical results generalizable to other architectures or modalities beyond vision models?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1X1R7P6yzt": {
    "paper_id": "1X1R7P6yzt",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes DGPPo, a framework for multi-agent safe optimal control that combines discrete graph control barrier functions (DGCBF) with proximal policy optimization. It addresses challenges like unknown dynamics, input constraints, and changing neighborhoods by learning both a DGCBF and a safe policy simultaneously. The method is validated across multiple simulation environments, demonstrating high task performance and safety rates with consistent hyperparameters."
          },
          "strengths": {
            "value": "The paper makes significant contributions to multi-agent safe control by integrating discrete-time CBFs with reinforcement learning (RL) without requiring a nominal policy. The theoretical analysis, including Theorem 2 on constraint-value functions as DCBFs, provides a novel foundation for safety guarantees. The empirical evaluation is comprehensive, showing robust performance across diverse environments. The work addresses critical gaps in existing methods, such as handling unknown dynamics and input constraints, and the proposed DGCBF extension is theoretically grounded."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with recent multi-agent RL safety methods, such as those using Lagrange multipliers or CMDP formulations, which could contextualize its advantages. The theoretical justification for the strict constraint (11b) remains incomplete, relying heavily on empirical validation. While the rebuttal clarifies some aspects (e.g., score function vs. policy gradients), the connection between DGCBF and DCBF in practice is not fully elaborated. Additionally, the paper could better address the scalability of DGCBF to large MAS."
          },
          "questions": {
            "value": "1. How does DGPPo handle the trade-off between safety and task performance in scenarios where constraints are highly conflicting? 2. What are the specific limitations of DGCBF compared to DCBF in terms of computational complexity or adaptability to dynamic environments? 3. Can the framework be extended to continuous-action spaces, and how would that affect the safety guarantees? 4. How does the use of deterministic rollouts for constraint-value functions impact the method's robustness to partial observability?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces DGPPo, a framework for multi-agent safe optimal control that simultaneously learns a discrete graph CBF (DGCBF) and a distributed safe policy. It addresses challenges like unknown dynamics, input constraints, and changing neighborhoods by combining reinforcement learning (RL) with DCBFs. The method is validated across multiple simulation environments, demonstrating high task performance and safety rates with consistent hyperparameters."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in multi-agent systems (MAS) by integrating RL with control barrier functions (CBFs) for safety. The theoretical contributions include proofs for DCBF construction and safety guarantees, which are novel. The experiments are comprehensive, covering diverse environments, and the framework's hyperparameter robustness is empirically validated. The clarity of the problem formulation and the structured presentation of the methodology are strong."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed analysis of how DCBF nonlinearity is handled in practice, despite acknowledging its challenges. The comparison with recent multi-agent RL safety methods is limited, and the theoretical justification for the strict constraint (11b) remains underdeveloped. The distinction between policy gradients and score function gradients in the method is not sufficiently clarified in the original paper, though the rebuttal addresses this. The adaptability of DGCBF to dynamic neighborhoods is not explicitly discussed."
          },
          "questions": {
            "value": "1. How does the DGCBF dynamically adapt to changing agent neighborhoods during training? 2. What specific mechanisms ensure the DCBF conditions are met despite nonlinearity and unknown dynamics? 3. Are there cases where the strict constraint (11b) might hinder performance, and how are these mitigated? 4. How does the framework handle partial observability beyond the defined local observation function?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes DGPPo, a framework for multi-agent safe optimal control under unknown discrete-time dynamics, input constraints, and changing neighborhoods. It introduces discrete graph CBFs (DGCBFs) and combines them with proximal policy optimization to learn safe, high-performance policies without requiring a pre-existing nominal policy. The method is validated across multiple simulation environments with consistent hyperparameters."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in multi-agent systems: safety under uncertain dynamics and partial observability. Its originality lies in integrating discrete CBFs with reinforcement learning (RL) to eliminate reliance on known dynamics or pre-specified nominal policies. The theoretical contributions include proving that constraint-value functions of policies are DCBFs, enabling end-to-end learning. The empirical validation is thorough, demonstrating competitive task performance and safety rates across diverse environments. The clarity of problem formulation and connection to prior work are strong."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with alternative RL-based safe control methods (e.g., CMDP or Lagrangian approaches) that do not rely on CBFs, limiting the understanding of DGPPo's relative advantages. The safety guarantees are partially theoretical, with some claims (e.g., generalizability of DGCBF) requiring stronger proofs. The modified constraint (11b) is criticized as overly strict, and while the rebuttal provides empirical evidence, the theoretical justification remains incomplete. The clarity of Theorem 3 and its practical implications is insufficient, as noted in the rebuttal."
          },
          "questions": {
            "value": "1. How does DGPPo compare to non-CBF-based safe RL methods (e.g., CMDP or Lagrangian approaches) in terms of safety and performance? 2. What are the exact conditions under which the DGCBF guarantees safety, and how does this generalize to unseen environments? 3. Can the strictness of constraint (11b) be relaxed while maintaining safety, and what are the trade-offs? 4. How does the use of score function gradients (as opposed to policy gradients) affect the stability and convergence of the algorithm?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "1d8Egv45of": {
    "paper_id": "1d8Egv45of",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces a self-explaining multi-view deep learning architecture that generates task-relevant, human-interpretable masks for cardiovascular signal stratification. The method uses semantic segmentation to create complementary views of input signals (ECG/PPG) and demonstrates superior explainability compared to post-hoc methods like LIME and SHAP, while achieving competitive performance on clinical tasks."
          },
          "strengths": {
            "value": "The paper presents a novel approach to self-explaining models with clear clinical relevance. The multi-view architecture with semantic segmentation masks offers a structured way to generate interpretable features. The experiments on three clinically relevant tasks (classification and regression) demonstrate practical applicability. The rebuttal addresses key concerns by adding quantitative comparisons with LIME/SHAP and improving figure clarity, enhancing the paper's credibility."
          },
          "weaknesses": {
            "value": "The paper lacks a user study with clinicians to validate the clinical relevance of the generated masks. The embedding and decision networks remain non-interpretable, limiting transparency despite the semantic segmentation. The ablation study focuses narrowly on the number of views, omitting critical components like the mask network's design. While the rebuttal updates Table 1, some SOTA methods cited are outdated, and the paper does not fully address how it differs from general XAI methods."
          },
          "questions": {
            "value": "1. How does the method specifically address clinical requirements beyond using ECG/PPG signals? 2. Can the authors clarify the alignment between semantic masks and domain knowledge with more concrete examples? 3. What are the limitations of the current embedding/decision networks, and how might they be mitigated? 4. How were the SOTA methods in Table 1 selected to ensure fairness in comparison?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes a self-explaining multi-view deep learning architecture that generates interpretable semantic masks for cardiovascular signal analysis. The method uses complementary semantic views to highlight task-relevant patterns in ECG/PPG signals, demonstrating superior explainability compared to post-hoc methods like LIME and SHAP, while achieving competitive performance on clinical tasks."
          },
          "strengths": {
            "value": "The paper addresses a critical need for interpretability in healthcare AI, with strong clinical relevance. The multi-view architecture introduces a novel approach to generating task-specific explanations through semantic segmentation. The experiments are comprehensive, comparing with established XAI methods and demonstrating both quantitative and qualitative improvements. The methodology is well-structured, and the paper is clearly written with appropriate contextualization of prior work."
          },
          "weaknesses": {
            "value": "The paper lacks thorough analysis of the embedding/decision networks' interpretability, which are crucial for transparency. The ablation study is limited to the number of views, not architectural components. The comparison with SOTA methods includes older works, though the authors justify their relevance. No user studies with clinicians validate the clinical utility of the explanations. Figure 2 is insufficiently detailed, and some equations have redundant notation."
          },
          "questions": {
            "value": "1. How does the method specifically differ from general XAI approaches in addressing clinical signal interpretation? 2. What mechanisms could improve the transparency of the embedding/decision networks? 3. Why were older SOTA methods chosen, and how do they align with current benchmarks? 4. How were the semantic states validated against clinical knowledge? 5. What are the limitations of the current architecture in real-world deployment?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a self-explaining multi-view deep learning architecture that generates task-relevant, human-interpretable masks for cardiovascular signal stratification (ECG/PPG). The method creates complementary semantic views through a mask network, which are optimized during training to highlight informative patterns in the input signals. The proposed approach is evaluated on three clinical tasks, showing superior explainability compared to post-hoc methods like LIME and SHAP, with competitive performance against state-of-the-art models."
          },
          "strengths": {
            "value": "The paper presents a novel multi-view framework for self-explaining models, with clear application to clinically relevant signals. The method's ability to generate complementary semantic views that align with domain knowledge is a significant contribution. The rebuttal addresses key concerns by adding quantitative comparisons with LIME/SHAP and clarifying the efficiency advantage over post-hoc methods. The focus on real-world clinical tasks enhances the paper's practical relevance."
          },
          "weaknesses": {
            "value": "The paper lacks thorough comparison with existing self-explaining models (e.g., physics-informed networks or attention-based methods). The embedding/decision networks' lack of transparency remains unresolved, despite the rebuttal's acknowledgment. The ablation study is limited to the number of views, omitting critical components like the mask network's design. No user studies with clinicians validate the clinical relevance of the generated explanations. The experimental setup for some SOTA methods (e.g., QPPG/ERMA) is not fully justified as 'state-of-the-art' despite their age."
          },
          "questions": {
            "value": "1. How does the proposed method specifically address the unique challenges of clinical signals (e.g., noise, variability) compared to general XAI approaches? 2. Can the authors provide quantitative metrics on the efficiency gains of their method vs. post-hoc approaches? 3. What are the limitations of the current embedding/decision networks in terms of interpretability, and how might they be addressed? 4. How was the alignment between semantic views and domain knowledge quantitatively validated? 5. Why were only two views tested, and what are the theoretical justifications for this choice?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1dDxMPJy4i": {
    "paper_id": "1dDxMPJy4i",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces NEDAG-GP, a nonparametric method for continuous DAG learning that integrates realistic expert knowledge and accurately models edge strengths using Gaussian Processes (GPs). The approach addresses three key principles: incorporating expert knowledge, reducing parametric assumptions, and improving interpretability through accurate weight formulations."
          },
          "strengths": {
            "value": "The paper excels in originality by combining nonparametric GP methods with continuous structure learning, a novel integration that addresses gaps in prior work. The method's systematic incorporation of diverse expert knowledge (required edges, initial graphs, topological orderings) is well-justified. The theoretical foundation for edge strength as the expected L2-norm of partial derivatives is rigorous, and experiments demonstrate superior performance on synthetic and real-world datasets, including Gene Regulatory Networks. The clarity of the problem formulation and the structured presentation of contributions are commendable."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed comparison with recent nonparametric methods in continuous DAG learning, which limits the assessment of NEDAG-GP's novelty. The computational complexity of the GP-based approach is not thoroughly analyzed, which is critical for real-world applications. Additionally, the handling of conflicting expert knowledge (e.g., contradictory required/forbidden edges) is not addressed. The theoretical justification for the GP-derived edge strength formulation could be strengthened with more concrete examples or empirical validation of the expected L2-norm calculation."
          },
          "questions": {
            "value": "1. How does NEDAG-GP handle conflicting expert knowledge, such as contradictory required/forbidden edges? 2. What is the computational complexity of the GP-based method compared to parametric alternatives, and how does it scale to large datasets? 3. Are the GP hyperparameters (e.g., amplitude, length scale) learned jointly with the DAG structure, or are they fixed? 4. How is the DAG constraint enforced during optimization, and what guarantees exist that the final solution is a valid DAG?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces NEDAG-GP, a nonparametric method for continuous DAG structure learning that integrates expert knowledge (e.g., required edges, topological orderings) and uses Gaussian Processes (GPs) to accurately model edge strengths. It addresses limitations in existing continuous learning methods by formulating edge weights through GP-derived expected L2-norms of partial derivatives, improving interpretability and structure accuracy."
          },
          "strengths": {
            "value": "Originality is evident in combining nonparametric GPs with expert knowledge in continuous DAG learning, addressing gaps in weight formulation and knowledge integration. The method's theoretical grounding in GP properties (e.g., closed-form edge strength computation) and empirical validation on synthetic and real-world datasets (e.g., Gene Regulatory Networks) demonstrate quality. Clarity is maintained through structured sections and references to prior work. Significance lies in advancing interpretable causal discovery for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed derivation of the GP weight formulation, particularly the mathematical justification for using expected L2-norms. Experimental comparisons are limited to a single real-world dataset (GRNs), with insufficient baselines (e.g., no comparison to recent nonparametric methods). The integration of expert knowledge is not thoroughly analyzed for varying levels of noise or incompleteness. Theoretical guarantees for edge strength accuracy are sparse, and scalability to high-dimensional data is unaddressed."
          },
          "questions": {
            "value": "1. Can the authors clarify the derivation of Equation 7 and its connection to GP hyperparameters? 2. What specific real-world datasets (beyond GRNs) were used, and how do they compare to existing benchmarks? 3. How does NEDAG-GP handle incomplete or conflicting expert knowledge? 4. Are there theoretical bounds on the accuracy of GP-derived edge strengths compared to ground truth? 5. What are the computational costs of the GP-based method compared to parametric alternatives?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces NEDAG-GP, a nonparametric method for directed acyclic graph (DAG) learning that integrates expert knowledge and uses Gaussian Processes (GPs) to accurately model edge strengths. It addresses challenges in continuous structure learning by incorporating realistic domain knowledge (e.g., required edges, topological orderings) and formulating interpretable weights through additive GPs with RBF kernels. Experiments on synthetic and real-world datasets (e.g., gene regulatory networks) demonstrate improved structure accuracy and edge strength estimation compared to existing methods."
          },
          "strengths": {
            "value": "The paper makes a novel contribution by combining nonparametric methods with continuous DAG learning, addressing gaps in prior work. The integration of realistic expert knowledge (e.g., required edges, partial topological orderings) is systematic and well-justified. The theoretical analysis of GP-based edge strength formulation is rigorous, leveraging the closed-form properties of additive GPs with RBF kernels. The method's interpretability is emphasized through transparent weight definitions, which align with the goal of making structure learning decisions understandable to domain experts. The experiments are comprehensive, covering both synthetic and real-world scenarios, and the paper highlights the practical relevance of its approach."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art methods in terms of computational efficiency and scalability, particularly for large graphs. While the real-world dataset (e.g., gene regulatory networks) is mentioned, the paper does not provide sufficient details about its characteristics, size, or how it was preprocessed. The theoretical analysis of edge strength accuracy relies on assumptions that may not hold in complex, high-dimensional settings. Additionally, the paper does not thoroughly address potential limitations of GP-based methods, such as sensitivity to hyperparameter choices or the computational cost of GP inference. The rebuttal partially addresses some of these points but does not fully resolve concerns about scalability and real-world validation."
          },
          "questions": {
            "value": "1. How does NEDAG-GP handle computational complexity for large-scale graphs, and what are its scalability limitations compared to parametric methods like NOTEARS-MLP? 2. Can the authors provide more details about the real-world gene regulatory network dataset used in experiments, including its size, structure, and how it was annotated with expert knowledge? 3. How robust is the GP-based edge strength formulation to misspecified hyperparameters (e.g., amplitude, length scale), and what strategies are recommended for hyperparameter tuning? 4. The paper claims improved interpretability, but how do the authors quantify the transparency of model decisions compared to baseline methods? 5. Are there cases where the integration of expert knowledge (e.g., partial topological orderings) could conflict with the learned structure, and how does NEDAG-GP resolve such conflicts?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "1durmugh3I": {
    "paper_id": "1durmugh3I",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces a knowledge distillation approach for machine learning force fields (MLFFs) by aligning energy Hessians between a general-purpose foundation model (teacher) and a smaller, specialized model (student). This method enables faster inference while maintaining physical constraints like energy conservation, demonstrated across multiple MLFFs and datasets with up to 20× speedup."
          },
          "strengths": {
            "value": "The work addresses a critical gap in MLFFs by combining efficiency with physical consistency. The Hessian-based distillation is novel compared to prior feature-matching approaches. Comprehensive experiments across diverse models (e.g., MACE, JMP) and tasks (MD simulations, geometry optimization) validate the method's effectiveness. The rebuttal clarifies baseline comparisons and hyperparameter tuning, strengthening the claims. Theoretical grounding in energy conservation and practical implementation details (e.g., Hessian subsampling) are well-articulated."
          },
          "weaknesses": {
            "value": "The computational cost of Hessian calculations, even with subsampling, remains a concern, particularly for large systems. The paper does not thoroughly analyze how subsampling strategies impact generalization to unseen chemical spaces. The theoretical justification for prioritizing Hessians over other derivatives (e.g., forces) is limited. While the rebuttal improves baseline comparisons, the n2n method still lags behind the proposed approach, suggesting potential gaps in the evaluation framework."
          },
          "questions": {
            "value": "How does the Hessian subsampling strategy affect the student model's ability to generalize to out-of-distribution chemical systems? What are the specific computational bottlenecks when scaling to larger models (e.g., 10^7 parameters), and how does the method compare to alternative approximations? The paper mentions in-place operations complicating differentiation—how do these constraints influence the practical adoption of the method in real-world MLFF frameworks?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes a knowledge distillation method for Machine Learning Force Fields (MLFFs) by aligning the energy Hessians of a 'teacher' foundation model with those of a 'student' model. The approach aims to create faster, specialized MLFFs while preserving physical constraints like energy conservation, demonstrated through experiments on multiple datasets and models with up to 20× speed improvements."
          },
          "strengths": {
            "value": "The paper introduces a novel, architecture-agnostic distillation method leveraging energy Hessians, a critical but underexplored aspect for physical consistency in MLFFs. The experiments are comprehensive, showing significant speedups and improved performance over baselines. The practical focus on molecular dynamics and other downstream tasks, along with the open-source implementation, strengthens its impact. The rebuttal addresses concerns about baseline tuning and Hessian computation, improving the paper's credibility."
          },
          "weaknesses": {
            "value": "The theoretical justification for why Hessian alignment improves physical consistency and performance is limited. The paper lacks a detailed comparison with alternative distillation methods beyond the n2n baseline, and the ablation studies could explore more hyperparameter sensitivity. The reliance on precomputed Hessians may introduce biases, and the paper does not fully address how the method scales to very large systems or higher-order derivatives."
          },
          "questions": {
            "value": "How does the method generalize to MLFF architectures without conservative force parameterization? What is the optimal Hessian subsampling strategy for different system sizes? Can the approach be extended to higher-order derivatives without prohibitive computational costs? How does the Hessian distillation loss interact with other physical constraints beyond energy conservation?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a knowledge distillation approach for machine learning force fields (MLFFs) by aligning energy Hessians between a foundation model (teacher) and a smaller, specialized student model. This method aims to improve inference speed while maintaining physical consistency, such as energy conservation in molecular dynamics simulations. The approach is demonstrated across multiple MLFFs and datasets, achieving up to 20× faster inference without sacrificing accuracy."
          },
          "strengths": {
            "value": "The paper presents a novel method for distilling MLFFs using energy Hessians, which is a unique and theoretically grounded approach. The experimental validation is comprehensive, covering multiple models, datasets, and tasks. The architecture-agnostic nature of the method is a significant strength. The rebuttal addresses baseline tuning concerns, showing improved performance for n2n, and provides ablation studies on hyperparameters like KD weight. The integration of finite differences for Hessian computation is a practical contribution."
          },
          "weaknesses": {
            "value": "The paper lacks explicit experiments demonstrating how Hessian distillation directly enforces energy conservation in molecular dynamics simulations, despite this being a key claim. The comparison with baselines, while improved in the rebuttal, still does not fully address the limitations of existing methods. The justification for excluding higher-order derivatives is superficial, and the connection between Hessian alignment and physical constraints needs more theoretical or empirical support. The paper also does not thoroughly analyze why student models outperform the teacher on the original objective."
          },
          "questions": {
            "value": "1. How does Hessian distillation specifically ensure energy conservation in MD simulations? Are there direct experiments measuring this? 2. The rebuttal mentions that the student outperforms the teacher on the original objective—what factors (e.g., data subsampling, regularization) drive this improvement? 3. The paper claims the method is architecture-agnostic, but are there scenarios where this might not hold? 4. How sensitive is the method to the choice of Hessian subsampling strategy, and what are the trade-offs between sampling size and performance? 5. What are the limitations of using finite differences for Hessian computation, and how do they affect long-term simulation stability?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "1eQT9OzfNQ": {
    "paper_id": "1eQT9OzfNQ",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes Activation Beacon, a method for long context compression in transformer-based LLMs. It compresses activations (keys/values) directly using beacon tokens, enabling efficient and flexible compression through progressive chunking, training with compression-based auto-regression, and dynamic compression ratio sampling. Experiments show performance comparable to baselines with 2x speedup and 8x memory reduction."
          },
          "strengths": {
            "value": "Originality: Introduces beacon tokens to compress activations directly, avoiding soft token limitations. Quality: Comprehensive experiments on long-context tasks (document understanding, Needle-in-a-Haystack) with clear metrics. Clarity: Structured methodology with detailed technical descriptions. Significance: Addresses critical efficiency bottlenecks in LLMs for long contexts, with practical benefits for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks explicit comparisons of beacon token vs. soft token compression mechanisms, which are critical to understanding the novelty. While the rebuttal clarifies differences in optimization objectives (KV activations vs. output embeddings), the original manuscript could have emphasized this distinction more. Additionally, the experimental analysis of ablation studies (e.g., beacon token placement) is limited to a single table without deeper exploration of trade-offs between compression ratios and performance."
          },
          "questions": {
            "value": "1. How does the direct compression of KV activations (beacon tokens) compare to soft token approaches in terms of information retention? 2. What are the limitations of the progressive chunking approach (e.g., potential information loss at chunk boundaries)? 3. The rebuttal mentions ablation studies showing +15% performance improvement with beacon token placement—could this be generalized to other compression methods? 4. How does the model handle dynamic compression ratios during inference when the input length varies significantly from training?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Activation Beacon, a method for long context compression in transformer-based LLMs. The approach compresses activations (keys/values) at each layer using beacon tokens, enabling efficient and flexible compression. Key innovations include progressive chunk-based compression, activation reuse, and training with compression-based auto-regression. Experiments show significant speed and memory improvements while maintaining performance on long-context tasks."
          },
          "strengths": {
            "value": "The paper presents a novel activation-based compression framework that addresses key limitations of existing soft-token approaches. The methodology is well-structured with clear technical contributions, including progressive chunking, activation reuse, and dynamic compression ratio training. Experiments demonstrate strong empirical results, with 2x inference speedup and 8x memory reduction. The paper's clarity is high, with detailed figures, equations, and comparisons to prior work. The significance of long-context compression for real-world applications is well-motivated."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous theoretical analysis explaining why activation compression outperforms soft-token methods. While the rebuttal clarifies differences between beacon and soft tokens, the paper doesn't fully articulate the fundamental advantages of activation-based compression. The ablation studies are limited (e.g., only a +15% improvement from beacon placement is mentioned). Experimental details like exact window sizes and their impact on results are ambiguously presented. The paper also doesn't address potential limitations in extreme compression scenarios or scalability to even longer contexts."
          },
          "questions": {
            "value": [
              "How does the beacon token placement strategy generalize across different types of long-context tasks (e.g., document understanding vs. reasoning)?",
              "What is the theoretical basis for the superior optimization efficiency of activation compression compared to soft-token methods?",
              "Are there any empirical comparisons between Activation Beacon and sparse attention approaches in terms of trade-offs between speed, memory, and accuracy?",
              "How does the progressive compression workflow handle dynamic multi-turn interactions beyond the examples provided in the experiments?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Activation Beacon, a method for long context compression in transformer-based LLMs. It compresses activations (keys and values) directly instead of using soft tokens, employs a progressive chunking strategy with beacon tokens, and trains the model through compression-based auto-regression. The approach claims to reduce computational and memory costs while maintaining performance on long-context tasks."
          },
          "strengths": {
            "value": "Originality is strong in targeting activation compression rather than soft tokens, a novel approach to address efficiency bottlenecks. The method's progressive chunking and beacon token placement offer flexibility and fine-grained control. Experimental results demonstrate significant speed and memory improvements, with claims of comparable performance to baselines on challenging tasks. The paper's structure and equations are well-organized, and the problem statement is clearly motivated."
          },
          "weaknesses": {
            "value": "The experimental validation lacks sufficient detail on baseline comparisons (e.g., how ICAE/AutoCompressor are evaluated). The paper does not thoroughly address how beacon tokens' activations are accumulated across chunks or potential information loss. The adaptive vs. uniform compression ratio justification in the rebuttal suggests the original paper was unclear. The ablation study (Table 4) is mentioned but not analyzed in depth. The paper also does not clarify how the method handles contexts longer than the chunk size."
          },
          "questions": {
            "value": "How are beacon token activations accumulated across chunks without information leakage? What is the exact mechanism for handling contexts longer than the chunk size? How does the method compare to sparse attention approaches in terms of trade-offs between efficiency and accuracy? The rebuttal clarifies compression ratio settings, but the original paper should explicitly state these. The paper should also elaborate on how beacon tokens differ from soft tokens in terms of training dynamics and optimization."
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1fC4ytCAgb": {
    "paper_id": "1fC4ytCAgb",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes a self-conditioned diffusion model (SCD) for consistent human image and video synthesis. The method frames the task as a spatially-conditioned inpainting problem, using a single denoising network to preserve appearance consistency between reference and target images through causal feature interactions. Key contributions include spatial conditioning, causal interaction mechanisms, and a two-stage generation process."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in human-centric generation by proposing a novel spatial conditioning framework. The method's integration of causal feature interactions within a unified denoising network is technically innovative. The experiments demonstrate competitive performance against existing methods, and the paper provides clear motivation and contextualization within the diffusion model literature. The visual results (e.g., Fig. 1) effectively illustrate the method's capabilities."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to quantify the contribution of individual components (e.g., causal interaction, two-stage process). The comparison with state-of-the-art methods is limited to qualitative results, with no quantitative metrics provided. The causal feature interaction mechanism is described conceptually but lacks technical specifics (e.g., how 'querying' is implemented). The experimental section appears incomplete, with Figure 3(b) cut off and no discussion of failure cases or limitations."
          },
          "questions": {
            "value": "1. How does the spatial conditioning strategy compare to other conditioning methods (e.g., CLIP embeddings, ControlNet)? 2. Can the authors provide technical details about the causal interaction framework (e.g., architectural modifications, training objectives)? 3. Are there quantitative results demonstrating the effectiveness of the two-stage process? 4. How does the method handle extreme pose variations or occlusions? 5. What is the computational efficiency compared to Reference-Net-based approaches?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes a self-conditioned diffusion (SCD) model for consistent human image and video synthesis. It frames the task as a spatially-conditioned inpainting problem, using a single denoising network with causal feature interaction to preserve appearance consistency between reference and generated images. The method decomposes generation into reference feature extraction and conditioned target generation, aiming to reduce domain gaps and improve efficiency."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in human-centric synthesis: preserving appearance consistency while generating novel poses. The spatial conditioning approach leverages pretrained diffusion models' inherent inpainting capabilities, offering a novel way to align reference and target features. The causal interaction framework is a thoughtful design to protect reference details. The method's efficiency through single-network training and decomposition into two stages is promising. The experiments demonstrate competitive performance against existing methods, suggesting practical relevance."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of key components (e.g., causal interaction, spatial conditioning). The experimental evaluation is limited, with insufficient quantitative metrics (e.g., FID, LPIPS) and qualitative comparisons to baseline methods. The explanation of how spatial conditioning interacts with the denoising network's architecture is vague, leaving questions about the mechanism. The rebuttal addresses some concerns but does not fully resolve ambiguities about the causal interaction's implementation or the method's generalization to diverse scenarios."
          },
          "questions": {
            "value": "1. How are the quantitative metrics (e.g., FID, LPIPS) calculated for the experiments, and how do they compare to baselines? 2. What is the exact mechanism of the causal feature interaction framework, and how is it implemented within the U-Net? 3. Are there limitations to the method's performance when handling complex backgrounds or occlusions? 4. How does the spatial conditioning strategy handle cases where the reference and target poses differ significantly in scale or perspective?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes a self-conditioned diffusion (SCD) model for consistent human image and video synthesis. The method frames the task as a spatially-conditioned inpainting problem, using a unified denoising network to preserve appearance consistency between reference and target images. It introduces a causal feature interaction framework and decomposes the generation process into two stages for efficiency."
          },
          "strengths": {
            "value": "The paper presents a novel approach by leveraging the denoising network itself for spatial conditioning, reducing domain gaps between reference and target features. The causal interaction framework ensures reference details are protected during generation, and the two-stage decomposition improves computational efficiency. The method demonstrates strong generalization to unseen identities and poses without per-instance fine-tuning, showing competitive results against existing methods."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the causal interaction framework and two-stage decomposition. Experimental comparisons are limited, with insufficient quantitative metrics (e.g., FID, LPIPS) and qualitative analysis of failure cases. The claims about generalization to unseen identities are not thoroughly supported by data. The method's robustness to varying pose complexities or occlusions is not addressed."
          },
          "questions": {
            "value": "1. How is the causal feature interaction implemented technically? Are there specific architectural changes to the U-Net? 2. What ablation studies confirm the effectiveness of the two-stage decomposition? 3. How does the method handle extreme pose variations or occlusions in the target image? 4. Are there quantitative comparisons with baseline methods on standard datasets (e.g., DeepFashion, COCO)?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1jcnvghayD": {
    "paper_id": "1jcnvghayD",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a variational Bayesian last layer (VBLL) approach for Bayesian optimization (BO), combining the uncertainty quantification of Gaussian processes (GPs) with the scalability of Bayesian neural networks (BNNs). The method leverages variational inference for the last layer of a neural network, enabling efficient online training through a connection to GP conditioning. The authors demonstrate competitive performance on tasks with complex correlations and show that VBLLs outperform GPs and BNNs on certain benchmarks."
          },
          "strengths": {
            "value": "The paper presents a novel connection between Bayesian last layer models and GP conditioning, providing a theoretically grounded approach for efficient BO. The method's scalability and ability to handle non-Euclidean correlations are significant strengths. The experiments are well-designed, with clear comparisons to baselines, and the paper addresses practical concerns like online training and hyperparameter tuning. The clarity of the presentation, including figures and equations, enhances readability."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient comparison with GP models using D-scaled hyperpriors, which could provide deeper insights into the advantages of VBLLs. The theoretical analysis of the VBLL's robustness to noise and its superiority over Laplace approximations is not fully supported in the initial submission, though the rebuttal addresses this. The experimental section could better contextualize the results, such as by explicitly discussing the trade-offs between VBLLs and GPs in high-dimensional settings. Some claims about early stopping and model reinitialization are overstated and require more nuanced justification."
          },
          "questions": {
            "value": "1. How does the VBLL's performance compare to GP models with D-scaled hyperpriors on high-dimensional tasks beyond the experiments presented? 2. Can the authors provide more details on the hyperparameter sensitivity analysis for the noise covariance prior and network architecture? 3. What are the specific advantages of VBLLs over other BNN variants (e.g., infinite-width BNNs) in terms of computational efficiency and predictive accuracy? 4. How does the VBLL's online training procedure handle concept drift or non-stationary environments?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Variational Bayesian Last Layer (VBLL) models as surrogate models for Bayesian optimization (BO), combining the uncertainty quantification of Gaussian Processes (GPs) with the scalability of Bayesian Neural Networks (BNNs). The method leverages variational inference for the last layer of a neural network, enabling efficient online training through a connection to GP conditioning. The authors demonstrate competitive performance on diverse tasks, including those with complex correlations, and propose an online training algorithm that interleaves full model training and last-layer updates."
          },
          "strengths": {
            "value": "The paper presents a novel connection between VBLLs and GP conditioning, offering a theoretically grounded approach to BO. The methodology is well-structured, with clear contributions to scalability and uncertainty quantification. The experiments cover a wide range of problem types, including discrete and multi-objective tasks, and the rebuttal addresses key concerns about noise sensitivity and comparisons to GP baselines. The paper also provides detailed analysis of hyperparameters and training efficiency."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient comparison with state-of-the-art BNNs and GP variants beyond the rebuttal's added experiments. The original submission did not thoroughly address computational complexity or scalability on very large datasets. While the rebuttal improves noise sensitivity analysis, the initial claims about LLLA's sensitivity were under-supported. Additionally, the paper could better clarify how VBLLs handle high-dimensional, non-Euclidean data compared to GPs."
          },
          "questions": {
            "value": [
              "How does the VBLL's performance scale with extremely high-dimensional data compared to GPs and other BNNs?",
              "What are the specific limitations of VBLLs in tasks with highly non-stationary or structured correlations?",
              "Can the authors provide a detailed analysis of the computational complexity of the proposed online training algorithm?",
              "How does the variational posterior in VBLLs compare to other BNN methods in terms of uncertainty calibration on real-world BO tasks?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a Bayesian optimization (BO) surrogate model called Variational Bayesian Last Layer (VBLL) that combines the strengths of Gaussian Processes (GPs) and Bayesian Neural Networks (BNNs). By connecting VBLL training to GP conditioning, the authors propose an online optimization algorithm that interleaves model training and recursive updates. The method is evaluated on diverse tasks, showing competitive performance on complex correlation structures and matching GP performance on benchmarks."
          },
          "strengths": {
            "value": "The paper makes a compelling contribution by rigorously connecting VBLL training to GP conditioning, offering a novel theoretical framework for BO surrogates. The experimental design is comprehensive, covering diverse tasks (discrete inputs, multi-objective problems) and comparing against multiple baselines. The method's efficiency is highlighted through practical considerations like early stopping and hyperparameter robustness. Theoretical insights into the equivalence between variational inference and recursive Bayesian linear regression are particularly strong."
          },
          "weaknesses": {
            "value": "The original paper lacked sufficient experimental evidence for key claims, such as the sensitivity of Laplace approximations to noise and the effectiveness of D-scaled GP priors. While the rebuttal addresses some of these gaps with additional experiments, the analysis of noise robustness (e.g., Figure 12) remains somewhat inconclusive. The runtime comparison and early stopping justification were underdeveloped, and some claims about 'optimal' early stopping were overly strong. The connection to real-world applications and broader implications of the method's parametric form is underexplored."
          },
          "questions": {
            "value": "1. The rebuttal clarifies that VBLLs show better consistency under noise than LLLA, but the exact mechanism behind this robustness remains unclear. How does the parametric form of VBLLs inherently mitigate noise sensitivity compared to non-parametric GPs? 2. The revised experiments with D-scaled GP priors suggest that VBLLs outperform GPs on Pestcontrol, but the paper does not explicitly address why GP kernels fail for this task. What specific limitations of GP kernels does VBLL overcome? 3. The authors mention a 'D-scaled hyperprior' but do not provide a detailed comparison of its impact on high-dimensional problems. How does this prior interact with the VBLL's feature learning process?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "1lB5ErmIY0": {
    "paper_id": "1lB5ErmIY0",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper investigates diverging preferences in human-labeled datasets for LLMs, proposing a taxonomy of 10 disagreement categories across four classes (task underspecification, response style, refusals, errors). It demonstrates that standard reward models fail to distinguish between high-agreement and diverging preferences, leading to biased alignment. The authors introduce distributional reward models and methods to identify diverging preferences in evaluations."
          },
          "strengths": {
            "value": "The paper's originality lies in its comprehensive taxonomy of disagreement sources, which systematically categorizes human preferences beyond prior work. The methodology is rigorous, with detailed experiments on two large datasets and clear comparisons to baselines. The significance is high, as it addresses a critical gap in LLM alignment by highlighting how reward modeling biases models toward singular perspectives. The clarity of the problem statement and experimental design is strong, with well-structured analysis of both reward modeling and evaluation methods."
          },
          "weaknesses": {
            "value": "The taxonomy's validity is not thoroughly validated—while the authors describe 10 categories, they do not provide inter-annotator agreement metrics for the taxonomy itself. The experimental analysis of reward models lacks depth, particularly in explaining why distributional models outperform baselines (e.g., no ablation studies on key design choices). The rebuttal addresses some concerns (e.g., high-agreement ties), but questions remain about the impact of interval mapping choices on results. The paper also does not discuss how their methods scale to larger or more diverse datasets."
          },
          "questions": {
            "value": "1. How was the taxonomy validated? Are there inter-annotator agreement metrics for the 10 categories? 2. What is the theoretical justification for modeling rewards as distributions rather than point estimates? 3. How sensitive are the results to the choice of interval mapping for 'ties' and 'preferences'? 4. Can the proposed methods be generalized to non-English or culturally specific datasets? 5. What are the computational costs of the distributional reward model compared to standard approaches?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates diverging preferences in human-labeled datasets for LLMs, proposing a taxonomy of 10 disagreement categories across four classes (task underspecification, response style, refusals, and errors). The authors demonstrate that standard reward models fail to distinguish between unanimous and majority preferences, leading to biased alignment. They introduce distributional reward models and methods to identify diverging preferences in evaluations."
          },
          "strengths": {
            "value": "The paper's originality lies in its comprehensive taxonomy of disagreement sources and its critical analysis of reward modeling limitations. The experiments are methodologically sound, using real-world datasets and comparing multiple models. The clarity of the problem statement and the significance of addressing pluralistic alignment in LLMs are strong. The work bridges a critical gap in understanding human preferences and their impact on model training."
          },
          "weaknesses": {
            "value": "The taxonomy lacks depth in explaining how specific categories interact or influence model behavior. The proposed distributional reward models are not thoroughly validated with ablation studies or comparisons to alternative approaches. The rebuttal acknowledges limitations in mapping intervals for reward differences, suggesting the results might be sensitive to hyperparameters. The paper also does not address how its findings generalize across domains or languages."
          },
          "questions": {
            "value": "1. How do the distributional reward models handle specific disagreement types (e.g., 'verbosity' vs. 'safety')? 2. Are there cases where the taxonomy might misclassify disagreements? 3. How sensitive are the results to the chosen reward difference intervals? 4. Can the proposed methods be adapted to other evaluation benchmarks beyond LLM-as-Judge?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper investigates diverging preferences in human-labeled datasets for reward modeling and LLM evaluations. It introduces a taxonomy of 10 disagreement categories across four classes, demonstrating that most disagreements stem from individual preferences rather than errors. The authors show that standard reward models fail to distinguish between diverging and high-agreement preferences, proposing distributional reward models to address this. They also highlight issues in LLM-as-Judge evaluations where diverging preferences are misclassified."
          },
          "strengths": {
            "value": "The paper offers originality through its detailed taxonomy of disagreement sources and a novel problem formulation about the limitations of existing reward modeling. The experiments are methodologically rigorous, with thorough analysis of datasets and clear metrics (e.g., AUROC improvements). The significance is high, as the work addresses critical challenges in LLM alignment and evaluation. The clarity of the writing and structure is strong, with well-organized sections and visualizations."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficient in several areas. The proposed distributional reward models are only evaluated on AUROC, without comparison to state-of-the-art methods or ablation studies. The mapping intervals for reward differences are arbitrary, and the paper lacks discussion on how alternative interval definitions might affect results. The analysis of LLM-as-Judge evaluations is superficial, with limited exploration of real-world implications. The rebuttal addresses some issues (e.g., High-Agreement Ties), but core weaknesses in experimental depth remain unresolved."
          },
          "questions": {
            "value": "1. How do the proposed distributional reward models generalize to other datasets or tasks beyond the ones tested? 2. What are the practical implications of the mapping intervals for real-world deployment? 3. How does the paper address the potential bias introduced by the specific choice of datasets (e.g., MultiPref-Disagreements and HelpSteer2-Disagreements)? 4. Are there quantitative results comparing the distributional approach to alternative methods (e.g., Bayesian models or ensemble approaches)?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1oIXRWK2WO": {
    "paper_id": "1oIXRWK2WO",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a learning-to-optimize framework for mixed-integer non-linear programming (MINLP) by proposing differentiable correction layers to handle integer variables while preserving gradient information. The approach combines soft constraint penalties with a self-supervised training method, enabling rapid solution generation for large-scale MINLPs. The authors demonstrate the effectiveness of their method on diverse problem classes, including convex and non-convex objectives with integer/mixed-integer variables."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in learning-to-optimize by extending it to MINLPs, a domain with limited prior work. The differentiable correction layers (RC and LT) are novel and effectively enable gradient-based training for integer variables. The self-supervised approach eliminates the need for labeled data, enhancing scalability. Experiments show superior performance on large-scale problems compared to traditional solvers, with clear empirical validation of the method's feasibility and efficiency."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with existing MINLP solvers, particularly exact methods like SCIP or heuristic approaches tailored to MINLP. While the rebuttal adds MILP experiments, the original work focuses primarily on convex/non-convex MINLPs, leaving gaps in understanding performance on real-world instances (e.g., MINLPLib). The handling of equality constraints is limited, and the theoretical analysis of constraint satisfaction remains underdeveloped. Additionally, the paper does not fully address the trade-off between penalty weights and solution quality in the main text."
          },
          "questions": {
            "value": "1. How do the proposed correction layers compare to existing methods for handling integer variables in neural networks (e.g., Gumbel-Softmax)? 2. What are the specific challenges in extending this approach to equality constraints, and how might they be addressed? 3. Are there theoretical guarantees for the feasibility of solutions generated by the framework? 4. How does the method scale to even larger problem sizes beyond the tested 200×200 instances?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a learning-to-optimize (L2O) framework for mixed-integer non-linear programming (MINLP) by proposing two differentiable correction layers (Rounding Classification and Learnable Threshold) to handle integer variables while preserving gradient information. The method uses a self-supervised approach with a soft penalty for constraint violations, demonstrating effectiveness on large-scale MINLP instances where traditional solvers struggle."
          },
          "strengths": {
            "value": "The paper addresses a novel and important problem (L2O for MINLP), which is underexplored compared to continuous or MILP settings. The differentiable correction layers are innovative, enabling gradient-based optimization for integer outputs. The self-supervised training paradigm avoids reliance on labeled data, enhancing scalability. The experiments show strong performance on large-scale problems, and the clear problem formulation and methodology contribute to the paper's readability and impact."
          },
          "weaknesses": {
            "value": "The experiments are limited to synthetic or specific problem classes (e.g., Rosenbrock) without real-world MINLPLib instances, which weakens the practical relevance. The paper lacks thorough comparisons with existing MINLP solvers and does not address equality constraints, which are critical in many applications. The analysis of hyperparameters (e.g., penalty weights) is superficial, and the theoretical guarantees of the correction layers are underdeveloped. The rebuttal addresses some issues (e.g., MILP experiments, penalty weight tuning), but the lack of real-world validation remains a gap."
          },
          "questions": {
            "value": [
              "How does the method handle equality constraints, which are common in practical MINLP problems? Are there theoretical or empirical limitations in this regard?",
              "What are the specific challenges in applying this approach to real-world instances from MINLPLib, and how could the framework be adapted to address them?",
              "The rebuttal mentions that larger problems require higher penalty weights, which may lead to overfitting. How does the framework generalize to unseen problem sizes or distributions?",
              "Are there theoretical guarantees for the feasibility of solutions generated by the correction layers, or is their performance purely empirical?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a learning-to-optimize framework for mixed-integer non-linear programming (MINLP) by proposing two differentiable correction layers (Rounding Classification and Learnable Threshold) to handle integer variables. The method combines a self-supervised approach with a soft penalty for constraint violations, enabling efficient solution generation for parametric MINLPs. Experiments demonstrate superior performance on large-scale problems compared to traditional solvers."
          },
          "strengths": {
            "value": "The paper makes a significant contribution by extending learning-to-optimize to MINLPs, a novel direction in the field. The proposed differentiable correction layers address the non-differentiability of integer variables, enabling gradient-based training. The self-supervised approach avoids reliance on labeled data, enhancing scalability. The experimental results on diverse problem classes, including large-scale instances, show the method's effectiveness. The rebuttal strengthens the claims by adding MILP experiments and addressing penalty weight analysis."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with existing MINLP solvers or learning-based methods like SurCO, which limits the evaluation of competitiveness. The experiments primarily use synthetic data, with limited testing on real-world instances (e.g., MINLPLib). The handling of equality constraints is acknowledged as a challenge, and the paper does not address this limitation. The penalty weight analysis in the rebuttal is a post-hoc addition, suggesting insufficient exploration in the original work."
          },
          "questions": {
            "value": [
              "How does the proposed method compare to specialized MINLP solvers (e.g., SCIP, Gurobi) on standard benchmarks?",
              "What are the limitations of the current approach in handling equality constraints, and how might they be addressed?",
              "Could the self-supervised framework be adapted to incorporate domain-specific knowledge for real-world applications?",
              "How sensitive is the performance to hyperparameter choices (e.g., penalty weight λ, network architecture)?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "1qGkuxI9UX": {
    "paper_id": "1qGkuxI9UX",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces DITTO, a method for aligning language models (LLMs) using a small number of user demonstrations. By treating these demonstrations as preferred over model outputs and intermediate checkpoints, DITTO generates preference data for training via algorithms like DPO. The approach is evaluated on author-specific writing tasks and a user study, showing superior performance over baselines like SFT and few-shot prompting."
          },
          "strengths": {
            "value": "The paper's originality lies in directly leveraging demonstrations for preference data, avoiding the need for large datasets or complex reward functions. The methodology is theoretically grounded in online imitation learning, with a clear algorithm and comprehensive experiments. The user study and lexical analysis add practical relevance, while the comparison to prior work (e.g., Constitutional AI) highlights DITTO's efficiency. The paper's contributions address a critical gap in personalized LLM alignment."
          },
          "weaknesses": {
            "value": "The paper's limitations section could be more detailed, particularly regarding DITTO's inability to generalize complex reasoning skills. The user study (N=16) is small, and the rebuttal's new experiment on RLHF confounding effects lacks depth in the original manuscript. The theoretical analysis of generalization beyond demonstrations is brief, and the paper could clarify how DITTO handles tasks requiring multi-step reasoning. Additionally, the comparison to Constitutional AI is somewhat superficial."
          },
          "questions": {
            "value": "1. How does DITTO handle tasks requiring complex reasoning, and what are the specific limitations? 2. The rebuttal mentions a new experiment on RLHF confounding effects—can the authors elaborate on its design and findings? 3. Why does 1-shot learning require at least three demonstrations, and how does this relate to the model's generalization? 4. The paper claims DITTO outperforms few-shot prompting, but how does it address the challenge of aligning style with minimal examples? 5. Could the authors provide more details on the 'odd linguistic quirks' in DITTO outputs and their implications?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces DITTO, a framework for aligning language models (LLMs) to specific user or task requirements using a small number of demonstrations (<10). DITTO leverages online imitation learning principles to generate preference comparisons between LLM outputs and expert demonstrations, enabling efficient alignment via preference optimization (e.g., DPO). The method is evaluated on author-specific writing tasks and a user study, showing significant improvements over few-shot prompting, SFT, and self-play methods."
          },
          "strengths": {
            "value": "The paper presents a novel approach to LLM alignment by directly using demonstrations as feedback, avoiding the need for large datasets or complex prompts. The theoretical connection to online imitation learning provides a strong foundation. Experiments are comprehensive, covering multiple domains and a user study with 16 participants. The clarity of the algorithm description, figures, and ablation studies is excellent. The significance of addressing personalized alignment with minimal data is high, and the paper contributes a practical framework for real-world applications."
          },
          "weaknesses": {
            "value": "The user study has a small sample size (N=16), limiting statistical generalizability. The paper does not thoroughly address potential confounding effects of RLHF or the robustness of DITTO to noisy demonstrations. The theoretical analysis is limited, with minimal discussion of convergence guarantees or the impact of the KL-divergence constraint. The comparison to Constitutional AI is brief, and the paper lacks ablation studies on key hyperparameters (e.g., number of demonstrations, sampling frequency). The claim that DITTO generalizes beyond demonstrations is not fully supported by experiments."
          },
          "questions": {
            "value": "1. How does DITTO handle cases where the initial SFT step is insufficient, and what is the exact threshold for the number of demonstrations required (e.g., why 3 samples as suggested in the rebuttal)? 2. The user study's small sample size raises concerns about statistical validity—how are the results interpreted in this context? 3. The paper mentions that DITTO's preference pairs are constructed from model outputs and demonstrations, but what criteria are used to ensure these comparisons are meaningful? 4. How does DITTO differ from prior work on model editing or self-play methods in practice, beyond the theoretical framework? 5. What are the limitations of the KL-constrained formulation, and how might it affect performance in tasks requiring diverse outputs?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces DITTO, a method for aligning language models (LLMs) to specific user or task preferences using a small number of demonstrations (<10). DITTO generates preference comparison data by treating user demonstrations as superior to LLM outputs and intermediate checkpoints, then trains the model using preference optimization (e.g., DPO). The approach is evaluated on author-specific writing tasks and a user study, showing significant improvements over baselines like SFT and few-shot prompting."
          },
          "strengths": {
            "value": "The paper's originality lies in directly leveraging demonstrations for preference data, avoiding the need for pairwise comparisons or principles. The method is theoretically grounded in online imitation learning, and experiments include both benchmark datasets and a user study with 16 participants. The clarity of the algorithm description and figures (e.g., Figure 1) is strong. The significance of enabling efficient LLM customization with minimal data is well-articulated."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies on the number of demonstrations required for effective alignment, despite the rebuttal's claim that 3 demonstrations are optimal. The user study's small sample size (N=16) limits generalizability. The comparison to Constitutional AI is superficial, and the rebuttal's critique of LLM-generated comparisons being out-of-distribution is not fully addressed in the paper. The method's ability to handle complex reasoning tasks remains unexplored, though this is noted in the revised limitations."
          },
          "questions": {
            "value": "1. How does DITTO handle tasks requiring complex reasoning, and what are the limitations of its current design? 2. What is the exact mechanism for determining the optimal number of demonstrations (e.g., why 3 vs. 1)? 3. How does DITTO scale with larger demonstration sets, and what are the diminishing returns? 4. Can the paper provide more details on the 'replay' comparisons and their impact on overfitting? 5. How does the method address potential biases in user demonstrations?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "1qP3lsatCR": {
    "paper_id": "1qP3lsatCR",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces NetMoE, a framework for accelerating MoE training by dynamically adjusting sample placement to minimize All-to-All communication costs. It leverages data and network locality, formulates the problem as a combinatorial optimization task, and proposes a two-stage polynomial-time algorithm. Experiments on 32 GPUs show a 1.67x speedup over existing methods."
          },
          "strengths": {
            "value": "The paper presents a novel approach by combining data locality in expert routing with network locality, addressing a gap in prior work. The problem formulation is rigorous, with a clear cost model for All-to-All communication. The algorithm design is practical, with a polynomial-time solution that aligns with real-world training constraints. The experiments are comprehensive, demonstrating significant efficiency gains on large-scale hardware. The writing is structured and well-supported by figures and tables."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed comparison with model-centric approaches, such as dynamic expert placement, beyond the rebuttal's claims. The assumption that inter-node communication dominates may not hold in all scenarios, and the rebuttal's analysis of intra-node communication volume requires further validation. The use of the Kuhn-Munkres (KM) algorithm, despite its cubic complexity, is justified but could benefit from a deeper discussion of its scalability in large-scale settings. The paper also does not fully explore the interaction between NetMoE and existing techniques like gradient accumulation."
          },
          "questions": {
            "value": "1. How does the two-stage optimization framework handle varying network topologies or heterogeneous bandwidths? 2. Can the authors provide a theoretical analysis of the KM-based algorithm's convergence or suboptimality guarantees? 3. What are the specific trade-offs between intra-node and inter-node communication in the proposed approach, and how do they scale with different batch sizes? 4. How does NetMoE integrate with gradient accumulation techniques, and what are the implications for training stability?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces NetMoE, a framework for accelerating MoE training by dynamically rearranging training samples to minimize All-to-All communication costs. The approach leverages data locality in expert routing and network locality across devices, formulating the problem as a combinatorial optimization task solved via a polynomial-time algorithm. Experiments on 32 GPUs show a 1.67× improvement in training efficiency compared to existing methods."
          },
          "strengths": {
            "value": "The paper presents a novel approach by addressing data locality from the sample perspective, which is unexplored in prior work. The problem formulation is clear, with a well-structured algorithm and experiments validating the claims. The integration of network locality with expert routing is a significant contribution. The paper also provides detailed notation and communication cost modeling, enhancing clarity."
          },
          "weaknesses": {
            "value": "The paper lacks depth in analyzing how data locality is quantified or measured. While the rebuttal addresses some concerns about inter-node communication, the experimental analysis of communication volume trade-offs remains limited. The Kuhn-Munkres algorithm's cubic complexity is acknowledged, but the paper could better justify its practicality for large-scale MoE training. Additionally, the scalability of the method to larger systems (e.g., beyond 32 GPUs) is not discussed."
          },
          "questions": {
            "value": "1. How is the 'locality' in expert routing quantified or measured? 2. What are the limitations of the Kuhn-Munkres algorithm when scaling to larger models or more devices? 3. How does NetMoE handle cases where intra-node communication volume becomes a bottleneck despite the 4× bandwidth advantage? 4. Are there any trade-offs between communication reduction and computational overhead introduced by sample reassignment?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces NetMoE, a framework that accelerates MoE training by dynamically reordering training samples to minimize All-to-All communication costs. It formulates the problem as an integer programming task and proposes a polynomial-time algorithm leveraging the Kuhn-Munkres (KM) method, achieving a 1.67x speedup on 32 GPUs."
          },
          "strengths": {
            "value": "NetMoE introduces a novel approach by focusing on data sample placement rather than expert routing, addressing a critical bottleneck in MoE training. The problem formulation as a combinatorial optimization problem is theoretically sound. The experimental results demonstrate significant efficiency gains, and the paper is well-structured with clear figures and tables. The contribution fills a gap in the literature by combining data and network locality for communication optimization."
          },
          "weaknesses": {
            "value": "The paper lacks detailed justification for the integer programming formulation and the polynomial-time solution. The KM algorithm's application to the problem is not clearly explained, and the theoretical guarantees for the algorithm's efficiency remain unclear. The experimental comparisons are limited to existing MoE frameworks, with no baseline against other communication optimization techniques. The rebuttal addresses some concerns but does not fully resolve ambiguities about the algorithm's complexity and practical implementation."
          },
          "questions": {
            "value": "1. How exactly does the Kuhn-Munkres algorithm address the combinatorial optimization problem? What are the specific constraints and objective functions? 2. The paper mentions splitting the problem into two stages—what are the criteria for this decomposition, and how does it ensure optimality? 3. Are there ablation studies to validate the effectiveness of the two-stage approach or the KM-based solution? 4. How do the experimental results compare with state-of-the-art methods that optimize communication through routing topology or model architecture changes?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "1qq1QJKM5q": {
    "paper_id": "1qq1QJKM5q",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces COMET, a deep learning method that induces a modular, sparse architecture using fixed random projections and k-winner-take-all capping. This approach creates overlapping experts whose activation depends on input similarity, addressing limitations of existing sparse neural networks such as representation collapse and reliance on explicit task IDs. Experiments demonstrate effectiveness across multiple tasks and architectures."
          },
          "strengths": {
            "value": "COMET's originality lies in its biologically inspired fixed routing mechanism, which avoids trainable gates and enables dynamic, input-dependent expert overlap. The paper's methodology is rigorous, with clear connections to neuroscience and sparse computation literature. The experimental validation across diverse tasks (image classification, language modeling, regression) and architectures (vision transformers, MLPs) highlights its generalizability. The contribution addresses critical challenges in sparse neural networks, offering a scalable, flexible alternative to existing methods."
          },
          "weaknesses": {
            "value": "The original paper lacked sufficient methodological details in Section 3, making it difficult to fully understand how biological principles directly influenced COMET's design. While the rebuttal addresses this by adding a preliminary section on MoE and input-dependent masking, the connection between biological mechanisms (e.g., random projections, k-winner-take-all) and specific design choices remains under-specified. Additionally, the paper's claims about interpretability were not supported by quantitative metrics, which the authors acknowledge and plan to remove. The absence of code in the original submission also raised reproducibility concerns, though this was partially addressed in the rebuttal."
          },
          "questions": {
            "value": "1. How exactly do the biological mechanisms (random projections, k-winner-take-all) translate into the design of COMET's routing network? Specifically, what empirical evidence supports the claim that these mechanisms produce sparse representations with input-dependent overlap? 2. The paper mentions 'improved interpretability' but provides no quantitative metrics. Will the revised version clarify or remove this claim? 3. What are the exact implementation details of the routing network (e.g., hyperparameters, layer-wise configurations) that enable the exponential number of experts? 4. How does COMET's performance scale with increasing model size, and what are the computational trade-offs compared to standard MoE?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces COMET, a deep learning method that addresses sparsity and modularity challenges in neural networks by using fixed, biologically inspired random projections and k-winner-take-all capping. This approach creates conditionally overlapping experts that adapt to input similarity, avoiding trainable gating functions and explicit task IDs. The method is validated on multiple tasks, showing improved performance without increasing trainable parameters."
          },
          "strengths": {
            "value": "COMET's originality lies in its biologically inspired fixed routing mechanism, which avoids trainable gates and enables scalable, overlapping experts. The paper's clarity improves with the rebuttal's planned additions on MoE foundations and design differences. The experimental scope across diverse tasks (image classification, language modeling, regression) demonstrates broad applicability. The focus on reducing interference and improving generalization addresses key limitations in existing sparse methods."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodological exposition in Section 3, particularly the specific biological influences on design choices. While the rebuttal addresses this, the original submission was unclear on how random projections and k-winner-take-all directly inform COMET's architecture. Interpretability claims are unsupported by metrics, and the absence of code in the original paper hindered reproducibility. Theoretical justification for the effectiveness of fixed routing remains underdeveloped."
          },
          "questions": {
            "value": "1. How exactly do the biological mechanisms (random projections, k-winner-take-all) translate into COMET's design? Are there quantitative analyses of input similarity-driven expert overlap? 2. What are the specific limitations of existing fixed-routing methods that COMET overcomes? 3. How does COMET's performance scale with larger model sizes or more complex tasks? 4. Can the authors provide ablation studies to isolate the impact of fixed routing vs. other components?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces COMET, a deep learning method that leverages fixed random projections and k-winner-take-all capping to create overlapping experts in neural networks. It addresses challenges in sparse neural networks, such as representation collapse and redundant computation, by using biologically inspired routing mechanisms. The approach claims to improve generalization and learning efficiency without requiring input/task IDs or trainable gates."
          },
          "strengths": {
            "value": "The paper presents a novel approach to sparse neural networks with biologically inspired design, addressing key limitations of existing methods. The method's reliance on fixed routing instead of trainable gates is a significant innovation. The experiments demonstrate applicability across diverse tasks and architectures, and the authors provide a clear theoretical motivation linking their design to biological principles. The rebuttal addresses methodological gaps, improving clarity and context."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art sparse methods, making it difficult to assess the magnitude of improvements. The experimental results do not include ablation studies or analysis of the impact of key components like the k-winner-take-all mechanism. The claim about improved interpretability is not supported by quantitative metrics, and the paper's writing initially obscured the relationship between biological inspiration and technical design. The code availability, while improved, is still supplementary rather than integrated into the main submission."
          },
          "questions": {
            "value": [
              "How does COMET's performance compare to other sparse methods like MoE or dynamic networks on standardized benchmarks?",
              "What ablation studies validate the necessity of the k-winner-take-all mechanism versus alternative sparsification strategies?",
              "Can the authors provide quantitative evidence of the claimed improvements in generalization and learning efficiency?",
              "How does the input-dependent sparsity in COMET affect model robustness to distributional shifts or adversarial examples?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "204sPiwBbB": {
    "paper_id": "204sPiwBbB",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Training with Annotations (TWA), a finetuning method for machine translation that leverages span-level error annotations from MQM data. TWA uses a weighted unlikelihood loss for error spans and cross-entropy for non-error spans, demonstrating improvements over baselines like Supervised FineTuning (SFT) and Direct Preference Optimization (DPO) on English-German and Chinese-English translation tasks."
          },
          "strengths": {
            "value": "The paper presents a novel approach to utilizing fine-grained span-level annotations, which is underexplored in machine translation. The method is theoretically grounded, with clear motivation for the loss functions. Experiments are well-designed, comparing TWA against relevant baselines and demonstrating consistent improvements. The clarity of the figures and explanations is strong, and the work addresses an important gap in leveraging detailed human feedback for model training."
          },
          "weaknesses": {
            "value": "The generalizability of TWA to other domains (e.g., QA) is not thoroughly addressed, despite the authors' claims. The baseline models (e.g., SFT) are weaker than the WMT submissions, which may bias results. The paper lacks ablation studies on the severity weighting mechanism and does not fully compare TWA with SFT on reference data. Additionally, the computational efficiency and scalability of TWA remain unexplored."
          },
          "questions": {
            "value": [
              "How does TWA handle different error categories (e.g., fluency vs. accuracy) in the annotations?",
              "What ablation results are available for the severity weighting mechanism (e.g., removing the -|w| term)?",
              "Can TWA be adapted to tasks where span-level annotations are less structured or not available?",
              "How does the performance of TWA scale with larger datasets or more complex models?",
              "What is the impact of ignoring 'off-trajectory' tokens on downstream tasks beyond translation quality?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Training with Annotations (TWA), a finetuning method for machine translation models that leverages span-level error annotations from MQM data. TWA applies a weighted unlikelihood loss to error spans and cross-entropy to non-error spans, while considering sequence trajectory. Experiments show improvements over baselines like SFT and DPO on English-German and Chinese-English translation tasks."
          },
          "strengths": {
            "value": "The paper addresses a meaningful gap in leveraging fine-grained annotations for MT training, which is novel compared to sequence-level approaches. The method is simple yet effective, with clear motivation and well-structured experiments. The ablation studies and figures provide transparency into the mechanism. The work contributes to the growing field of using human feedback for language model improvement, with potential applications beyond MT."
          },
          "weaknesses": {
            "value": "The paper lacks comparison with related span-level methods like FG-RLHF or FUDGE, which could provide context for TWA's novelty. The baseline models (e.g., SFT) are weaker than WMT submissions, potentially inflating TWA's relative gains. Generalization to other domains like QA is not thoroughly discussed. The experiments on stronger models (e.g., M2M100) are missing, and the ablation studies are limited in scope."
          },
          "questions": {
            "value": [
              "How does TWA compare to FG-RLHF or FUDGE in terms of performance and efficiency?",
              "What is the impact of different severity weight scaling factors on TWA's effectiveness?",
              "Can TWA be adapted to QA tasks with span-level annotations, and what challenges might arise?",
              "How sensitive is TWA to the quality of the span annotations, and what happens with noisy annotations?",
              "What are the computational costs of TWA compared to standard SFT or DPO?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces Training with Annotations (TWA), a fine-tuning method for machine translation models that leverages span-level error annotations from MQM data. TWA differentiates between error spans (using unlikelihood loss) and non-error spans (using cross-entropy loss) to improve translation quality. Experiments show TWA outperforms baselines like Supervised FineTuning and Direct Preference Optimization on English-German and Chinese-English tasks."
          },
          "strengths": {
            "value": "The paper presents a novel approach by directly utilizing span-level annotations, which is underexplored in MT literature. The method is conceptually simple yet effective, with clear ablation studies and comparisons to related work (e.g., FG-RLHF, FUDGE). The experimental setup is well-documented, and the motivation for span-level annotations (e.g., detailed error information, practical data collection) is compelling. The paper also addresses limitations of prior methods, such as the need for online annotator models or auxiliary reward models."
          },
          "weaknesses": {
            "value": "The baseline models are not strong enough to convincingly demonstrate TWA's efficacy, as the authors' base model lags significantly behind WMT submissions. The generalizability to domains like QA is underexplored, despite the rebuttal citing Wu et al. (2023). The paper lacks ablation studies on hyperparameters (e.g., severity weight scaling, span grouping strategies). Additionally, the comparison with SFT on reference data is incomplete, and the mechanism of TWA's success (noise removal vs. negative signal guidance) remains ambiguous without further analysis."
          },
          "questions": {
            "value": "1. How does TWA perform when applied to stronger base models (e.g., M2M100, NLLB)? The rebuttal mentions improved results with a weaker model, but this needs validation on state-of-the-art architectures. 2. What is the impact of different severity weight scaling factors (e.g., -0.1, -1, -5) on performance? 3. Can TWA be adapted to other tasks beyond MT (e.g., QA) without significant modifications? 4. How does the span grouping strategy (based on weights) affect training dynamics? 5. Why does TWA outperform SFT on references in COMET but not Metric-X? Are these metrics capturing different aspects of quality?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "246rHKUnnf": {
    "paper_id": "246rHKUnnf",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces EXPLOREToM, a framework for generating adversarial theory of mind (ToM) data using A* search over a domain-specific language. It generates complex, diverse stories to stress-test LLMs, demonstrating that state-of-the-art models like GPT-4o and Llama-3.1-70B perform poorly on this data. The approach enables fine-tuning improvements on benchmarks and provides insights into model shortcomings such as state tracking failures."
          },
          "strengths": {
            "value": "The paper makes a novel contribution by addressing the lack of robust ToM evaluation benchmarks through adversarial data generation. The A* search methodology for story structure optimization is methodologically sound, and the empirical results showing low model accuracy on generated data are compelling. The framework's ability to separate social reasoning from lexical cues and its potential for conceptual analysis (e.g., state tracking, knowledge asymmetry) are significant strengths. The clarity of the approach and the structured presentation of the three-step generation process are notable."
          },
          "weaknesses": {
            "value": "The paper lacks quantitative metrics to evaluate the complexity of generated stories, which is critical for understanding their difficulty. While the rebuttal mentions cost analysis and dataset statistics, these details are not sufficiently integrated into the main text. The story context generation is described as simplistic, and the paper does not fully address how this limits real-world applicability. Additionally, the experiments focus on a narrow set of benchmarks (e.g., ToMi, MMLU), and the generalization to other tasks or domains is underexplored. The paper also does not provide a thorough comparison with existing adversarial benchmarks."
          },
          "questions": {
            "value": "1. How does the paper quantitatively define and measure the complexity of generated ToM stories? The rebuttal mentions a cost analysis, but the paper itself does not clarify how story complexity correlates with model performance. 2. The story context generation is described as 'simple'—how does this affect the framework's ability to model real-world social interactions? 3. What are the limitations of the domain-specific language in capturing nuanced ToM scenarios? 4. How does the framework ensure that generated stories avoid unintended biases or artifacts that could skew model evaluations?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces EXPLOREToM, a framework for generating adversarial theory of mind (ToM) data using A* search over a domain-specific language. The method creates complex, diverse stories to stress-test LLMs, demonstrating that state-of-the-art models like GPT-4o and Llama-3.1-70B perform poorly. The approach also enables fine-tuning improvements on ToMi benchmarks and reveals insights into LLMs' limitations in state tracking and belief reasoning."
          },
          "strengths": {
            "value": "The paper's originality lies in its adversarial generation framework, which systematically addresses gaps in ToM evaluation. The method is methodologically sound, combining A* search with domain-specific language design to ensure plausible, challenging scenarios. Experiments show significant results, including a 27-point accuracy boost on ToMi after fine-tuning. The clarity is strong, with detailed examples and structured sections. The significance is high, as ToM is critical for social intelligence, and the framework offers a scalable solution to a pressing problem."
          },
          "weaknesses": {
            "value": "The story context generation in Section 2.1 is simplistic and may lack real-world complexity, a limitation acknowledged by the authors. The evaluation of story complexity relies on the number of people/actions, but the rebuttal's explanation of performance trends with increased participants is not fully detailed in the paper. The paper also lacks thorough ablation studies on the impact of specific actions (e.g., asymmetric belief updates) and does not address how the framework scales to non-English or culturally specific scenarios. The cost analysis in the rebuttal is promising but needs more concrete data in the main text."
          },
          "questions": {
            "value": "1. How does EXPLOREToM handle cultural or contextual nuances that might affect ToM reasoning in real-world scenarios? 2. What specific ablation studies were conducted to isolate the impact of asymmetric belief updates versus other action types? 3. The rebuttal mentions 'o1-preview evaluation'—what metrics were used, and how do they compare to standard benchmarks? 4. How does the framework ensure diversity in story generation beyond the sampled actions, especially for edge cases? 5. Are there plans to open-source the domain-specific language and action definitions for community extension?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces EXPLOREToM, a framework for generating adversarial theory of mind (ToM) data using A* search over a domain-specific language. It claims to create challenging stories that stress-test LLMs' ability to track mental states, showing significant performance drops on state-of-the-art models. The approach also enables training data creation and insights into model failures like state tracking issues."
          },
          "strengths": {
            "value": "The paper's adversarial generation framework is novel, leveraging A* search and domain-specific language to create complex, diverse ToM scenarios. Strong empirical results show low model accuracy on generated data, highlighting evaluation limitations. The method's ability to disentangle social reasoning from lexical cues is a key strength. The work also provides actionable insights into model shortcomings, such as unreliable state tracking."
          },
          "weaknesses": {
            "value": "The story context generation is criticized as overly simplistic, with limited generalizability to real-world scenarios. The paper lacks detailed analysis of why model performance increases with more people/actions in experiments. The A* search heuristics (g(s) and h(s)) are under-described, hindering reproducibility. The evaluation on MMLU/Multi3Woz is limited, and the rebuttal's cost analysis is not fully integrated into the paper. The framework's scalability to other action sets remains unproven."
          },
          "questions": {
            "value": "1. How does EXPLOREToM handle real-world complexity beyond the sampled contexts? 2. Why does model performance increase with more people/actions in Fig. 3? 3. What specific design choices define g(s) and h(s) in the A* search? 4. How are the 'interestingness' criteria for questions formally quantified? 5. What are the exact metrics for evaluating story complexity beyond count-based measures?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "25j2ZEgwTj": {
    "paper_id": "25j2ZEgwTj",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper analyzes the learning dynamics of gradient descent (GD) in two-layer ReLU neural networks, focusing on how GD balances features and achieves global convergence. The authors propose a three-phase analysis (alignment after weak recovery, tangential growth, and local convergence) and prove a global convergence rate of O(T^{-3}) under specific assumptions. They also demonstrate that GD implicitly biases solutions toward a 'balanced' l2-norm configuration."
          },
          "strengths": {
            "value": "The paper's originality lies in extending prior work to the case where both the number of teacher (k) and student (m) neurons are constants, rather than focusing on single-neuron or exact-parameterization settings. The three-phase analysis framework provides a structured approach to understanding GD dynamics. The technical contributions include novel dynamical system analysis for tangential components and refined alignment analysis. The results are significant for understanding implicit biases and training dynamics in neural networks, with clear theoretical rigor and well-defined assumptions."
          },
          "weaknesses": {
            "value": "The analysis relies on restrictive assumptions, such as orthogonal teacher neurons with equal norms, which may limit generalizability. The convergence rate's optimality is not discussed, and the paper lacks empirical validation to support theoretical claims. The informal theorem statement and reliance on high-probability guarantees without explicit sample complexity bounds weaken the practical relevance. Additionally, the handling of non-orthogonal teacher neurons or more complex architectures remains unaddressed, leaving open questions about scalability."
          },
          "questions": {
            "value": [
              "How do the orthogonal teacher neuron assumptions impact the practical applicability of the results? Can the analysis be extended to non-orthogonal settings?",
              "The paper mentions a 'balanced' l2-norm solution but does not explicitly define or quantify what 'balanced' means. How is this concept formalized in the analysis?",
              "Are there empirical experiments to validate the convergence rate and implicit bias claims? If not, how does the theoretical analysis address the gap in empirical verification?",
              "The informal theorem states a polynomial-time convergence rate, but the sample/time complexity is noted as suboptimal. What are the key barriers to achieving tighter bounds?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper analyzes the dynamics of gradient descent (GD) for training two-layer ReLU neural networks in a regression setting, where the target function is a sum of k ReLU neurons. The authors propose a three-phase analysis (alignment after weak recovery, tangential growth, and local convergence) to establish global convergence at a rate of O(T^{-3}) and demonstrate an implicit bias toward balanced l2-norm solutions. The work extends prior results to the case where m, k = O(1), addressing interactions between multiple teacher and student neurons through refined alignment analysis and a new dynamical system framework."
          },
          "strengths": {
            "value": "The paper makes a significant contribution by extending prior work on GD dynamics to the multi-neuron setting (m, k = O(1)), which is a non-trivial generalization. The three-phase analysis framework provides a structured understanding of training dynamics, and the technical innovations in handling teacher-student interactions (e.g., the dynamical system analysis for tangential components) are novel. The clarity of the problem formulation, notation, and theoretical results is strong, with careful attention to assumptions and their implications."
          },
          "weaknesses": {
            "value": "The assumptions (e.g., orthogonal teacher neurons, specific initialization conditions) significantly limit the generality of the results. The convergence rate of O(T^{-3}) is not optimal, and the paper does not address non-orthogonal teacher neurons or more complex architectures. The lack of empirical validation and detailed discussion on the practical implications of the implicit bias weaken the impact. Additionally, the weak recovery assumption may be restrictive, and the balance condition at initialization is not thoroughly justified."
          },
          "questions": {
            "value": "1. How do the theoretical results hold when teacher neurons are not orthogonal? Can the analysis be extended to non-orthogonal settings? 2. What empirical evidence supports the claimed convergence rate and implicit bias? 3. How sensitive are the results to the balance condition at initialization? 4. Are the assumptions (e.g., small step-size, specific initialization) necessary, or can they be relaxed? 5. How does the implicit bias toward balanced l2-norm solutions affect generalization in practical scenarios?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper analyzes the training dynamics of gradient descent (GD) for two-layer ReLU neural networks in a regression setting, focusing on the case where the number of student neurons (m) and teacher neurons (k) are constants. The authors propose a three-phase analysis (alignment after weak recovery, tangential growth, and local convergence) to establish global convergence at a rate of O(T^{-3}) and demonstrate that GD implicitly balances student neurons to achieve a minimum 'balanced' l2-norm solution."
          },
          "strengths": {
            "value": "The paper presents a novel three-phase analysis framework for understanding GD dynamics in two-layer ReLU networks, which extends prior work on single-neuron and exact-parameterization settings. The theoretical contributions include a rigorous global convergence rate and insights into implicit bias toward balanced l2-norm solutions. The technical analysis of tangential components via dynamical systems is a significant methodological advancement. The problem formulation and assumptions are clearly defined, contributing to the paper's theoretical depth."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation, which is critical for establishing the practical relevance of the theoretical findings. The assumptions (e.g., orthogonal teacher neurons, specific initialization) are extremely restrictive and may limit generalizability. The convergence rate of O(T^{-3}) is not optimal, and the weak recovery condition is highly technical, potentially limiting accessibility. The analysis does not address non-orthogonal teacher neurons or more complex architectures, which are common in practice."
          },
          "questions": {
            "value": "1. How do the results generalize to non-orthogonal teacher neurons or real-world data distributions? 2. What are the practical implications of the 'balanced' l2-norm solution for generalization and model performance? 3. Can the weak recovery assumption be relaxed while maintaining the theoretical guarantees? 4. Are the derived sample/time complexities tight, and how do they compare to empirical observations?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "26oSbRRpEY": {
    "paper_id": "26oSbRRpEY",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces StreamingT2V, an autoregressive method for generating long videos (up to 2 minutes) from text prompts. It addresses challenges in maintaining temporal consistency and motion dynamics by proposing three components: a Conditional Attention Module (CAM) for short-term memory, an Appearance Preservation Module (APM) for long-term memory, and a randomized blending approach for video enhancement. The method aims to overcome issues like video stagnation and inconsistent transitions in existing approaches."
          },
          "strengths": {
            "value": "The paper tackles a critical problem in text-to-video generation: producing long, consistent videos. The proposed modules (CAM and APM) are well-motivated and address specific limitations of prior work. The method's modular design allows for flexibility, and the experiments demonstrate improvements over baselines in motion quality and consistency. The problem statement is clear, and the contributions are structured to advance the field of autoregressive video generation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of CAM, APM, and the blending approach. Quantitative metrics (e.g., FID, LPIPS, or motion quality scores) are not provided, relying instead on qualitative comparisons. The claims about surpassing competitors are not fully substantiated without direct comparisons to state-of-the-art models. The randomized blending approach is described but not thoroughly analyzed for its impact on final video quality. Additionally, the paper does not address potential computational costs or scalability of the method."
          },
          "questions": {
            "value": "1. How do CAM and APM specifically improve consistency compared to existing conditioning mechanisms? 2. What quantitative metrics were used to evaluate motion dynamics and temporal coherence? 3. Are there comparisons with recent state-of-the-art models (e.g., SVD, T2V0, or MTVG) on long video generation? 4. How does the randomized blending approach affect computational efficiency and video quality? 5. What are the limitations of the method in terms of video length, resolution, or prompt complexity?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces StreamingT2V, an autoregressive method for generating long videos (up to 2 minutes) with seamless transitions. It proposes two key modules: a Conditional Attention Module (CAM) for short-term memory and an Appearance Preservation Module (APM) for long-term memory, along with a randomized blending approach for video enhancement. The method aims to address video stagnation and inconsistent transitions in existing text-to-video models."
          },
          "strengths": {
            "value": "The paper presents a novel approach to long video generation, addressing a critical gap in existing methods. The CAM and APM modules are well-motivated and effectively tackle short- and long-term memory challenges. The method is structured clearly, with detailed explanations of components and a logical flow. The experiments demonstrate improvements in motion consistency and quality compared to baselines. The significance of generating high-quality, extended videos aligns with practical applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to quantify the contribution of individual components (CAM, APM, blending). The evaluation metrics are limited to qualitative comparisons, with no quantitative measures for motion quality, consistency, or computational efficiency. The integration of pre-trained models (e.g., Modelscope) is not thoroughly explained, and the paper does not address potential limitations in handling complex or diverse text prompts. The randomized blending approach's mechanism and effectiveness require further clarification."
          },
          "questions": {
            "value": "How do the CAM and APM modules interact with the pre-trained Video-LDM model during training? What metrics are used to quantify 'high motion amount' and 'seamless transitions'? Are there cases where the method fails, and how are they handled? How does the randomized blending approach affect computational cost and scalability? What is the role of the anchor frame in APM, and how is it selected?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes StreamingT2V, an autoregressive method for generating long videos (up to 2 minutes) with seamless transitions. Key components include a Conditional Attention Module (CAM) for short-term memory, an Appearance Preservation Module (APM) for long-term memory, and a randomized blending approach for enhancement. The method aims to address issues of video stagnation and inconsistency in existing text-to-video diffusion models."
          },
          "strengths": {
            "value": "The paper introduces novel modules (CAM and APM) that address specific challenges in long video generation. The approach is well-structured, with clear motivation for addressing limitations in prior work. The authors highlight practical applications for long video synthesis, which is a significant gap in current text-to-video models. The method's ability to extend existing models without additional training is a notable advantage."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficient. The paper lacks quantitative metrics (e.g., FID, IS) comparing StreamingT2V to baselines. The claims about avoiding video stagnation and achieving 'high motion' are not supported by concrete evidence. The randomized blending approach is described but not thoroughly analyzed. The paper also does not address potential limitations, such as computational costs or failure cases. The novelty of CAM and APM is not clearly distinguished from prior work (e.g., ControlNet, CLIP-based conditioning)."
          },
          "questions": {
            "value": "1. How are CAM and APM implemented technically? Are their components (e.g., feature extractors, attention mechanisms) described in sufficient detail? 2. What quantitative metrics demonstrate StreamingT2V's superiority over existing methods? 3. How does the randomized blending approach ensure seamless transitions? Are there ablation studies validating its effectiveness? 4. How does the paper address the computational feasibility of generating 1200-frame videos with the proposed method? 5. What specific limitations of prior work (e.g., SVD, T2V0) does StreamingT2V overcome, and how are these limitations quantified?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "27SSnLl85x": {
    "paper_id": "27SSnLl85x",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces Rectified Linear Networks (ReLNs) as a theoretical framework to analyze the training dynamics of ReLU networks by establishing an equivalence with Gated Deep Linear Networks (GDLNs). It demonstrates that ReLU networks exhibit an inductive bias towards structured mixed-selective latent representations, which becomes more pronounced with additional contexts and hidden layers. The work provides analytical derivations and experiments on tasks like XoR to validate these claims."
          },
          "strengths": {
            "value": "The paper's originality lies in connecting ReLU networks to GDLNs, offering a novel theoretical lens for studying feature learning. The mathematical derivations are rigorous, and the experiments on controlled tasks (e.g., XoR) provide clear insights into the dynamics of ReLU networks. The significance of the work is in addressing a critical gap in understanding feature learning in finite ReLU networks, which are central to deep learning. The clarity of the presentation is strong, with well-structured sections and illustrative figures."
          },
          "weaknesses": {
            "value": "The theoretical framework relies on strong assumptions, such as the mutual diagonalizability of dataset correlation matrices (Assumption 2.1.1), which may not hold in real-world scenarios. The paper lacks extensive empirical validation across diverse tasks, limiting the generalizability of its claims. The practical utility of the ReLN framework for complex datasets and architectures is not thoroughly explored, and the rebuttal acknowledges this as a limitation. Additionally, the connection between the theoretical analysis and the observed inductive bias requires more explicit justification."
          },
          "questions": {
            "value": "1. How does the framework handle violations of Assumption 2.1.1 (non-diagonalizable datasets), and what are the implications for real-world applications? 2. What are the practical challenges in identifying gates for complex datasets, and how can the framework be adapted to address them? 3. Can the inductive bias towards structured mixed selectivity be validated on tasks beyond XoR and contextual control? 4. How does the ReLN framework reconcile the observed discrepancies between ReLU networks and GDLNs in terms of weight initialization and dynamics?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Rectified Linear Networks (ReLNs) as a theoretical framework to analyze feature learning in finite ReLU networks by establishing an equivalence with Gated Deep Linear Networks (GDLNs). The authors demonstrate that ReLU networks exhibit an inductive bias towards structured mixed-selective latent representations, which become more pronounced with additional contexts and hidden layers. Key contributions include deriving training dynamics for ReLU networks via ReLNs and showing how node reuse accelerates learning."
          },
          "strengths": {
            "value": "The paper's originality lies in connecting ReLU networks to GDLNs, offering a novel theoretical lens for studying feature learning. The clarity of definitions (e.g., ReLN, GDLN formalism) and figures (e.g., Figure 1, 2) is strong, aiding comprehension. The significance is high, addressing a critical gap in understanding finite ReLU networks. The experiments on XoR tasks and analysis of depth/context effects provide concrete validation of the theory, though limited in scope."
          },
          "weaknesses": {
            "value": "The assumptions of diagonalizable dataset correlation matrices and singular vector alignment (Assumption 2.1) are critical but underexplored in terms of real-world validity. The experiments focus on synthetic tasks (e.g., XoR) with limited generalizability. The practical utility of the framework for complex datasets remains unclear, and the rebuttal acknowledges this as a limitation. The connection between theoretical insights and broader implications for deep learning practice is not fully elaborated."
          },
          "questions": {
            "value": "1. How robust is the ReLN framework to violations of the diagonalizability assumption in real-world datasets? 2. Can the framework be extended to handle more complex, high-dimensional tasks beyond synthetic examples? 3. What are the practical implications of structured mixed-selective representations for tasks like multi-task learning or continual learning? 4. How do the authors address the potential for alternative gate-finding methods to affect the equivalence between ReLNs and ReLU networks?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Rectified Linear Networks (ReLNs) as a framework to analyze the training dynamics of finite ReLU networks by establishing an equivalence with Gated Deep Linear Networks (GDLNs). It demonstrates that ReLU networks exhibit an inductive bias towards structured mixed-selective latent representations, which becomes more pronounced with additional contexts and hidden layers. The work bridges theoretical analysis with empirical validation on a contextual task, highlighting how node reuse and learning speed shape feature learning."
          },
          "strengths": {
            "value": "The paper's originality lies in connecting ReLU networks to GDLNs, offering a novel theoretical lens for studying feature learning. The theoretical derivations are rigorous, with clear connections to existing linear network paradigms. The experiments on a contextual task provide concrete evidence for the proposed inductive bias. The presentation is well-structured, with illustrative figures and a clear exposition of the ReLN framework. The significance is high, as it addresses a critical gap in understanding finite ReLU networks, which are central to deep learning."
          },
          "weaknesses": {
            "value": "The assumptions about dataset correlation matrices being mutually diagonalizable and singular vector alignment are strong and may limit the framework's applicability to real-world data. The empirical validation is limited to a single task (XOR-like contextual setup), raising questions about generalizability. The claim that every ReLU network has a corresponding ReLN lacks sufficient empirical support, and the paper does not thoroughly address edge cases where the equivalence might fail. The rebuttal acknowledges some of these limitations but does not fully resolve concerns about the framework's robustness."
          },
          "questions": {
            "value": "1. How robust are the assumptions about dataset correlation matrices and singular vector alignment in practical scenarios? 2. Are there specific conditions under which the ReLN equivalence breaks down, and how can the framework be adapted? 3. Can the framework be validated on more complex, real-world datasets beyond the XOR-like task? 4. How does the ReLN approach scale to deeper or wider networks, and what are the computational constraints?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "293V3bJbmE": {
    "paper_id": "293V3bJbmE",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces HEL MET, a comprehensive benchmark for evaluating long-context language models (LCLMs) across seven application-centric categories. It addresses limitations of existing benchmarks by incorporating controllable context lengths (up to 128K tokens), model-based evaluation metrics, and robust few-shot prompting. The authors demonstrate that HEL MET provides more reliable model rankings and identifies key insights, such as the poor predictive power of synthetic tasks and the performance gap between open-source and closed-source models."
          },
          "strengths": {
            "value": "The paper's strengths lie in its comprehensive and diverse benchmark design, covering critical LCLM capabilities like retrieval-augmented generation, summarization, and many-shot in-context learning. The inclusion of controllable context lengths and model-based evaluation metrics addresses significant gaps in existing benchmarks. The experimental analysis of 59 models across multiple tasks is thorough, and the paper provides actionable recommendations for model development. The rebuttal effectively addresses concerns about domain specificity and closed-source model reliance, reinforcing the benchmark's practicality."
          },
          "weaknesses": {
            "value": "The benchmark's coverage of highly specialized domains (e.g., legal/medical) is limited, despite the inclusion of a legal summarization dataset. The reliance on closed-source models like GPT-4 for evaluation, while justified as standard practice, raises concerns about reproducibility. The paper lacks detailed ablation studies to quantify HEL MET's improvements over existing benchmarks. Additionally, the model-based evaluation metrics are not sufficiently explained, and the human evaluation details in the rebuttal are insufficiently integrated into the main text."
          },
          "questions": {
            "value": "1. How does the model-based evaluation metric differ from existing metrics like ROUGE, and what specific improvements does it offer? 2. Can the authors provide more concrete evidence of HEL MET's reliability compared to benchmarks like RULER or ∞BENCH, beyond the qualitative observations in Figure 1? 3. How generalizable are the findings about open-source vs. closed-source model performance to other domains or model architectures? 4. What are the computational costs of the model-based evaluation, and how does it scale to larger models?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces HEL MET, a comprehensive benchmark for evaluating long-context language models (LCLMs) across seven diverse, application-centric categories. It addresses limitations of existing benchmarks by incorporating controllable context lengths (up to 128K tokens), model-based evaluation metrics, and robust prompts. The authors analyze 59 LCLMs, revealing that synthetic tasks like NIAH poorly predict downstream performance, and open-source models lag behind closed-source models in complex tasks."
          },
          "strengths": {
            "value": "The paper's strengths include a novel, application-driven benchmark with diverse tasks (e.g., RAG, citation generation, passage re-ranking) that addresses critical gaps in existing evaluations. The focus on controllable context lengths and model-based metrics improves reliability. The empirical analysis of 59 LCLMs provides actionable insights into model performance trends. The work is well-structured, with clear comparisons to prior benchmarks and a strong emphasis on practical evaluation scenarios."
          },
          "weaknesses": {
            "value": "The benchmark's applicability to highly specialized domains (e.g., medical or legal texts) is not thoroughly validated, despite including a legal summarization task. The reliance on closed-source models for some evaluations (e.g., GPT-4) raises concerns about fairness and generalizability. The rebuttal mentions human evaluation of metrics, but specific agreement rates and discrepancies between human and model-based judgments are not detailed. Additionally, the paper lacks ablation studies to quantify the impact of individual design choices (e.g., controllable lengths, prompts) on evaluation reliability."
          },
          "questions": {
            "value": "1. How does HEL MET perform in domains beyond legal and general text, such as medical or financial documents? 2. What are the exact human-model agreement rates for the model-based evaluation metrics, and how were discrepancies resolved? 3. Are there ablation studies demonstrating the contribution of specific components (e.g., controllable lengths, prompts) to the benchmark's effectiveness? 4. How does the paper address potential biases in using closed-source models for evaluation, given their dominance in certain tasks?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces HEL MET, a comprehensive benchmark for evaluating long-context language models (LCLMs) across seven diverse application-centric categories. It addresses limitations of existing benchmarks by incorporating controllable context lengths (up to 128K tokens), model-based evaluation metrics, and robust few-shot prompting. The authors analyze 59 LCLMs, highlighting discrepancies between synthetic tasks and real-world performance, and emphasize the importance of holistic evaluation."
          },
          "strengths": {
            "value": "The paper provides a well-structured, diverse benchmark addressing critical gaps in existing evaluations. The inclusion of controllable context lengths, model-based metrics, and application-focused tasks demonstrates strong originality. The empirical analysis of 59 models is thorough, and the findings on synthetic task limitations and open-source model gaps are significant. The work's practical relevance for model development is clear."
          },
          "weaknesses": {
            "value": "The benchmark's coverage of domain-specific tasks (e.g., medical, financial) is limited, with only one legal-focused dataset (Multi-LexSum) mentioned. The reliance on closed-source models like GPT-4 for evaluation raises reproducibility concerns, despite the authors' arguments about their superiority. The paper lacks direct ablation studies or comparisons showing HEL MET's improvements over prior benchmarks. The rebuttal's claims about human evaluation are partially addressed but lack detailed metrics."
          },
          "questions": {
            "value": "1. How does HEL MET handle domain-specific tasks beyond legal documents? The rebuttal cites correlations between general and specific domains, but empirical validation is needed. 2. What evidence supports the claim that closed-source models are better judges than open-source ones? 3. Are the model-based evaluation metrics (e.g., key-point generation) rigorously validated against human judgments? 4. How do HEL MET's improvements directly impact model rankings compared to existing benchmarks?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "2AWZTv6kgV": {
    "paper_id": "2AWZTv6kgV",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces Projected Neural Differential Equations (PNDEs), a method for enforcing hard constraints in neural differential equations (NDEs) by projecting the learned vector field onto the tangent space of a constraint manifold. The approach ensures constraint satisfaction at inference time, demonstrated on chaotic systems and power grid models with improved stability and fewer hyperparameters compared to existing methods."
          },
          "strengths": {
            "value": "The paper presents a novel method (PNDEs) that rigorously enforces constraints via differential geometry, offering a principled generalization of prior work. The mathematical derivation is sound, with clear theoretical guarantees (e.g., Proposition 1). The experiments on challenging domains (chaotic dynamics, power grids) demonstrate practical efficacy. The paper is well-structured, with clear figures and contextualization of related work. The contribution addresses a critical gap in constraint enforcement for safety-critical systems."
          },
          "weaknesses": {
            "value": "The paper lacks detailed discussion on the computational cost of the projection operator, particularly for high-dimensional or non-smooth manifolds. While the experiments compare PNDEs to SNDEs, they do not thoroughly analyze scalability or robustness to noisy/missing constraints. The theoretical analysis assumes smooth manifolds, but practical scenarios may involve non-algebraic constraints or singularities. The rebuttal addresses some concerns but does not fully resolve questions about computational efficiency or edge cases."
          },
          "questions": {
            "value": "1. How is the projection operator implemented in practice, especially for non-Euclidean or high-dimensional manifolds? 2. What are the limitations of the method when constraints are non-algebraic (e.g., differential constraints)? 3. How does the computational overhead of PNDEs compare to SNDEs or other constraint enforcement methods? 4. Are there scenarios where the projection could fail (e.g., singular Jacobians of g(u))? 5. How sensitive is the method to hyperparameter choices beyond the claimed 'fewer hyperparameters'?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces Projected Neural Differential Equations (PNDEs), a method for enforcing hard constraints in neural differential equations (NDEs) by projecting the learned vector field onto the tangent space of a constraint manifold. The approach ensures solutions remain on the manifold, satisfying constraints exactly, and is evaluated on chaotic systems and power grid models, showing improved accuracy and stability compared to baselines like stabilized NDEs (SNDEs)."
          },
          "strengths": {
            "value": "The paper presents a novel and theoretically grounded approach to enforcing hard constraints in NDEs, leveraging differential geometry for rigorous guarantees. The proof of Proposition 1 demonstrates that PNDEs strictly satisfy constraints, a key advantage over soft constraints. The method's generality—handling diverse constraints without coordinate-specific formulations—is a significant strength. Experiments on challenging tasks like chaotic dynamics and power grids highlight practical relevance. The writing is clear, with well-structured derivations and illustrative figures."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with alternative projection-based methods (e.g., constraint layers in climate models or symplectic integrators). The experimental evaluation is limited in scope, with insufficient quantitative metrics (e.g., error rates, stability margins) and no ablation studies on hyperparameters. The projection operator's computational complexity and numerical stability are not discussed, nor is the method's scalability to high-dimensional manifolds. The claim of 'fewer hyperparameters' is not substantiated with empirical evidence."
          },
          "questions": {
            "value": "1. How does the projection operator handle non-smooth or non-regular constraint manifolds? 2. What is the computational overhead of the projection step compared to baseline methods? 3. Are there specific constraint types (e.g., non-holonomic) that PNDEs cannot handle? 4. How does the method perform on long-term integration tasks where numerical errors might accumulate? 5. Can the approach be extended to stochastic differential equations or hybrid systems?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Projected Neural Differential Equations (PNDEs), a method for enforcing hard constraints on neural differential equations (NDEs) by projecting the learned vector field onto the tangent space of a constraint manifold. The approach ensures that solutions remain on the manifold, satisfying constraints exactly, and is evaluated on chaotic systems and power grid models."
          },
          "strengths": {
            "value": "The paper presents a theoretically sound method with a clear mathematical foundation, including a proof that PNDEs strictly satisfy constraints. The novelty lies in the projection-based enforcement of hard constraints, which differs from prior 'soft' constraint approaches. The experiments on challenging systems (e.g., power grids) and the claim of reduced hyperparameter sensitivity are significant. The writing is clear, and the figures effectively illustrate the core idea."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient empirical comparison with state-of-the-art methods beyond SNDEs, such as physics-informed neural networks or other projection-based approaches. The ablation studies are minimal, and the paper does not address how the projection affects training dynamics or computational cost. The theoretical analysis assumes smooth manifolds, but the method's robustness to non-smooth or time-varying constraints is unexplored. Additionally, the claim of 'fewer hyperparameters' is not quantified or compared to existing methods."
          },
          "questions": {
            "value": "1. How does the projection operator handle non-smooth or time-varying constraints? 2. What specific hyperparameters are reduced compared to SNDEs, and how was this quantified? 3. Are there cases where the projection could introduce numerical instabilities or bias the learned dynamics? 4. How does the method scale to high-dimensional or complex constraint manifolds? 5. Could the projection step significantly increase computational overhead, and how was this mitigated?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "2Chkk5Ye2s": {
    "paper_id": "2Chkk5Ye2s",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes the Mixture-UCB algorithm for selecting optimal mixtures of generative models to improve evaluation scores like FID and KID. The authors formulate the problem as a quadratic optimization task and frame it as a multi-armed bandit (MAB) problem, proving a regret bound and demonstrating empirical improvements over single-model selection strategies."
          },
          "strengths": {
            "value": "The paper addresses a novel and important problem of combining generative models, which is underexplored in the literature. The theoretical contributions include a regret bound for the Mixture-UCB algorithm and a clear connection between quadratic evaluation scores and bandit optimization. The experiments are thorough, showing consistent improvements across multiple metrics and datasets. The motivation is well-justified, and the paper effectively bridges concepts from MAB and generative modeling."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed comparison with existing ensemble methods or prior work on model combination, which limits the assessment of novelty. The theoretical analysis of sample dependencies in the bandit setting is somewhat hand-wavy, relying on mutual information arguments without rigorous justification. The practical implementation details of Mixture-UCB-OGD are sparse, and the generalization of results to other domains (e.g., text) is not thoroughly discussed. Additionally, the claim that FID/KID are convex functions requires more concrete analysis."
          },
          "questions": {
            "value": "1. How does Mixture-UCB compare to existing ensemble methods (e.g., model averaging, stacking) in terms of performance and efficiency? 2. Can the theoretical analysis of sample dependencies be made more rigorous, particularly the handling of non-iid samples in the bandit framework? 3. What are the computational limitations of Mixture-UCB when scaling to a large number of models? 4. Are the empirical results robust to variations in hyperparameters or dataset characteristics?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes the Mixture-UCB algorithm for selecting optimal mixtures of generative models using a multi-armed bandit (MAB) framework. The authors demonstrate that mixtures of models can outperform individual models in terms of evaluation scores (FID, KID, RKE) and formulate the problem as a quadratic optimization task. They provide theoretical guarantees, including a regret bound, and validate their approach on image and text generation tasks."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to combining generative models rather than selecting a single best model, addressing an important gap in the literature. The theoretical analysis is rigorous, with a provable regret bound for the Mixture-UCB algorithm. The experiments are well-designed, showing improvements over baseline methods like Vanilla-UCB and One-Arm Oracle. The connection to quadratic optimization and MAB is creative and well-justified. The paper also clearly contextualizes its contributions within existing work on generative model evaluation and bandit algorithms."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough comparison with existing mixture strategies (e.g., weighted averaging or ensemble methods) that do not use bandit-based optimization. The theoretical analysis assumes a specific quadratic form for the loss function, which may not generalize to all evaluation metrics. The experiments focus on image generation but do not explore text-based models in depth, despite the claim of cross-modal applicability. The computational complexity of the algorithm and its scalability to large numbers of models are not adequately discussed. The rebuttal addresses some concerns about dimensionality but leaves open questions about practical implementation details."
          },
          "questions": {
            "value": "1. How does the Mixture-UCB algorithm handle non-convex or non-quadratic loss functions that may arise in real-world applications? 2. What are the limitations of the quadratic assumption in the theoretical analysis, and how might the algorithm be adapted to broader classes of evaluation metrics? 3. How does the algorithm scale with the number of models (m)? Does the regret bound depend on m in a way that could limit practical deployment? 4. The rebuttal mentions avoiding the curse of dimensionality, but the paper does not explicitly discuss how the algorithm handles high-dimensional data (e.g., high-resolution images). 5. Are there any empirical comparisons with non-bandit-based mixture strategies (e.g., uniform averaging, model stacking) that could provide stronger evidence of the algorithm's superiority?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes Mixture-UCB, a multi-armed bandit algorithm for selecting optimal mixtures of generative models to improve evaluation scores like FID and KID. It formulates the problem as a quadratic optimization task and provides theoretical guarantees, including a regret bound. Experiments show that mixtures outperform individual models on benchmark datasets."
          },
          "strengths": {
            "value": "The paper introduces a novel application of bandit algorithms to the problem of combining generative models, which is a significant contribution. The theoretical analysis includes a regret bound for the Mixture-UCB algorithm, and the experiments demonstrate empirical improvements in diversity and quality metrics. The paper also clarifies the connection between quadratic optimization and kernel-based evaluation scores, which is a strong theoretical insight."
          },
          "weaknesses": {
            "value": "The experiments are limited to a small set of models and datasets (e.g., FFHQ), which may not generalize to other scenarios. The theoretical analysis of sample dependencies assumes independence, which may not hold in practice. The paper does not thoroughly compare Mixture-UCB to alternative methods for mixture selection, such as exhaustive search or heuristic approaches. Additionally, the practical implications of the regret bound (e.g., convergence speed) are not clearly discussed."
          },
          "questions": {
            "value": [
              "How does the performance of Mixture-UCB scale with the number of models (m)? Are there empirical results for larger m?",
              "What is the computational complexity of solving the convex quadratic program at each iteration, and how does it affect the algorithm's practicality?",
              "The paper claims that mixtures outperform individual models, but are there cases where this does not hold? How are such edge cases handled?",
              "The theoretical analysis assumes independence between samples, but the rebuttal mentions using mutual information to bound dependencies. How sensitive is the regret bound to violations of this assumption?",
              "How does Mixture-UCB compare to non-bandit-based approaches (e.g., grid search, random search) for mixture selection in terms of efficiency and performance?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "2ET561DyPe": {
    "paper_id": "2ET561DyPe",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces Few-Class Arena (FCA), a benchmark for evaluating vision models in few-class scenarios (2-10 classes). It addresses the gap between many-class benchmarks (e.g., ImageNet) and real-world applications, which often involve fewer classes. FCA includes a scalable data-loading approach, a similarity-based difficulty measure (SimSS), and extensive experiments across 10 models and 10 datasets. The work highlights that sub-models (trained on few-class subsets) outperform full models in the few-class regime, with insights into scaling laws and dataset difficulty."
          },
          "strengths": {
            "value": "The paper makes a compelling case for addressing the few-class regime, a critical but underexplored area. The benchmark is comprehensive, with 1591 training runs and analysis across diverse models and datasets. The SimSS measure, leveraging CLIP and DINoV2, shows strong correlation with model performance (Pearson ≥0.88). The experiments reveal novel insights, such as the violation of scaling laws for sub-models in few-class settings. The work is well-structured, with clear motivation and practical utility for practitioners."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology for few-class subset generation (e.g., class distribution, overlap, or sampling strategies). The SimSS measure's integration into the benchmark and its practical impact on model selection are not thoroughly explained. While the rebuttal adds YOLOv8 experiments for object detection, these are limited in scope and do not fully validate generalization to other tasks. The claims about scaling laws and dataset difficulty require more rigorous statistical analysis. Additionally, the paper does not address potential biases in subset selection or the computational cost of sub-model training."
          },
          "questions": {
            "value": "1. How are few-class subsets generated? Are classes selected uniformly, or are there strategies to balance difficulty or diversity? 2. How does SimSS influence model selection in FCA? Is it used for dataset prioritization, architecture design, or other tasks? 3. What are the computational costs of training sub-models, and how does FCA mitigate them? 4. How generalizable are the findings to other tasks (e.g., segmentation) beyond the limited YOLOv8 experiments? 5. Are there ablation studies to validate the effectiveness of SimSS compared to existing difficulty metrics?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Few-Class Arena (FCA), a benchmark for evaluating efficient vision models in the few-class regime (2-10 classes). The authors propose a similarity-based difficulty measure (SimSS) using CLIP and DINoV2, conduct extensive experiments on ten datasets with ten models, and demonstrate that sub-models outperform full models in few-class scenarios. They also address generalization to object detection and segmentation through additional experiments."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in vision research by focusing on the few-class regime, which is underrepresented in existing benchmarks. The FCA benchmark is well-structured, with a scalable data loading approach and a novel similarity-based difficulty measure (SimSS) that shows strong correlation with model performance. The experiments are comprehensive, covering multiple models and datasets, and the rebuttal provides additional validation for generalization to object detection and segmentation. The clarity of the problem statement and the practical utility of the benchmark are notable strengths."
          },
          "weaknesses": {
            "value": "The novelty of FCA is somewhat unclear, as it builds on existing benchmarks and methods without a clear distinction from prior work. The similarity measure (SimSS) leverages pre-trained models (CLIP, DINoV2) rather than introducing a novel algorithm, which limits its originality. The experiments on object detection and segmentation are limited in scope and lack detailed analysis. The paper also does not sufficiently compare FCA to existing few-class benchmarks or address potential biases in dataset subset selection. The theoretical justification for the observed trends (e.g., sub-models outperforming full models) is underdeveloped."
          },
          "questions": {
            "value": "1. How does FCA differ from existing few-class benchmarks, and what specific gaps does it address? 2. What is the exact contribution of SimSS compared to prior similarity-based difficulty measures? 3. Are the object detection experiments sufficiently rigorous, and how do they validate the generalization of FCA to other tasks? 4. How are dataset subsets selected, and what safeguards are in place to avoid bias in the experiments? 5. What are the limitations of using pre-trained models (CLIP, DINoV2) for similarity measurement, and how might this affect the difficulty score?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Few-Class Arena (FCA), a benchmark for evaluating vision models in few-class scenarios (2-10 classes). It addresses the gap between many-class benchmarks (e.g., ImageNet) and real-world applications with limited classes. The work includes a scalable data loading method, a similarity-based difficulty measure (SimSS), and extensive experiments across 10 models and 10 datasets. Key findings show sub-models outperform full models in few-class regimes and highlight the importance of dataset similarity in model selection."
          },
          "strengths": {
            "value": "Originality is strong in addressing the underexplored few-class regime, with a novel benchmark and SimSS measure. The experiments are comprehensive, covering 10 models and 10 datasets across 2-1000 classes. Clarity is good, with detailed methodology and figures. The significance is high, as few-class scenarios are critical for real-world applications, and the benchmark fills a critical gap in the literature."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with existing few-class benchmarks or methodologies, making it hard to assess novelty. The SimSS measure relies on CLIP/DINoV2, but the paper does not thoroughly validate how generalizable this is across datasets. The extension to tasks like object detection is mentioned but not deeply analyzed. Additionally, the claim about scaling laws in few-class regimes needs more rigorous statistical support."
          },
          "questions": {
            "value": "1. How does SimSS compare to existing difficulty metrics in terms of correlation strength and robustness across datasets? 2. What are the limitations of using CLIP/DINoV2 for similarity measurement in the few-class regime? 3. Can the findings on sub-models be generalized to other tasks beyond image classification, as hinted in the rebuttal? 4. How were hyperparameters for SimSS determined, and what ablation studies were conducted?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "2HN97iDvHz": {
    "paper_id": "2HN97iDvHz",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces an LLM-based predictive scheduling system for data centers, aiming to reduce energy consumption and queuing delays by leveraging LLMs to predict task metrics (execution time, energy) from source code. It combines these predictions with a real-time scheduling algorithm, reporting 32% energy reduction and 30% waiting time reduction through a collaboration with a data center."
          },
          "strengths": {
            "value": "The paper presents a novel application of LLMs to data center scheduling, addressing a critical sustainability challenge. The methodological contributions include a generalizable predictive model and a data-driven scheduling framework. The practical deployment results are promising, and the paper highlights the potential for extending predictions to metrics like carbon emissions. The architecture is well-structured, and the comparison to prior methods emphasizes the advantages of LLM-based representations."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as specific LLM architectures, hyperparameters, or baseline comparisons. The claimed energy reduction is not contextualized against state-of-the-art methods. The generalization to new metrics (e.g., carbon emissions) is speculative without empirical evidence. The collaboration with a data center is mentioned but not elaborated on, raising questions about the real-world applicability and scalability of the approach."
          },
          "questions": {
            "value": "1. What specific LLM architecture and training details were used? 2. How were the baseline scheduling methods selected, and what metrics were used for comparison? 3. Can the authors provide additional data on the data center's infrastructure and the validation of their energy consumption metrics? 4. How was the 32% energy reduction statistically validated, and what is the significance of this result relative to existing solutions?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces an LLM-based predictive scheduling system for data centers, aiming to improve operational efficiency and reduce environmental impact by predicting resource requirements (execution time, energy consumption) from source code and optimizing GPU allocation. The approach claims to generalize across diverse tasks and achieve significant energy and delay reductions."
          },
          "strengths": {
            "value": "The paper presents a novel application of LLMs for data center resource prediction, addressing a critical sustainability challenge. The methodological contributions include a unified predictive model and automated scheduling system, which could generalize across task types. The practical deployment results (32% energy reduction, 30% delay decrease) demonstrate real-world relevance. The paper also highlights the advantages of LLMs over traditional handcrafted features, such as better generalization and data efficiency."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as baseline comparisons, statistical significance tests, or analysis of prediction accuracy metrics. The claim of generalization to tasks like CNN+LSTM is not empirically supported. The LLM's training process, feature extraction mechanism, and handling of GPU configuration variations are under-explained. The model architecture section is incomplete, and the paper does not address potential limitations (e.g., overfitting, data dependency). The collaboration with a single data center raises questions about scalability and representativeness."
          },
          "questions": {
            "value": "1. How was the LLM trained? What specific features does it extract from source code? 2. What baseline methods were used for comparison, and how do the claimed improvements (32%/30%) compare to existing scheduling algorithms? 3. How does the system handle GPU configuration variations not seen during training? 4. What is the size and diversity of the training dataset, and how does it impact generalization? 5. Are there ablation studies demonstrating the necessity of the LLM component versus traditional methods?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes an LLM-based predictive scheduling system for data centers, aiming to reduce energy consumption and queuing delays by leveraging LLMs to predict execution time, energy consumption, and other sustainability metrics from source code. The approach combines a pre-trained LLM for representation learning with a probe for downstream predictions, followed by a real-time scheduling algorithm. The authors claim a 32% energy reduction and 30% delay reduction through collaboration with a data center."
          },
          "strengths": {
            "value": "The paper presents a novel application of LLMs to data center scheduling, addressing a critical sustainability challenge. The methodology is well-structured, with clear technical details on the LLM-based predictive model and scheduling framework. The practical deployment claims (e.g., 1-second inference) and minimal data requirements are promising. The comparison with prior methods highlights advantages in generalization and flexibility, and the experimental results demonstrate significant improvements over heuristic baselines."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous empirical validation, including insufficient baseline comparisons (e.g., no comparison with state-of-the-art predictive models or traditional scheduling algorithms). The claims about generalization to metrics like carbon emissions and water usage are speculative without supporting data. The training data size (500 examples) is extremely small for LLM-based approaches, raising concerns about overfitting. The collaboration with a data center is not sufficiently detailed, making it hard to assess the real-world applicability of the results. The theoretical analysis of the scheduling algorithm is minimal."
          },
          "questions": {
            "value": "1. How was the data center collaboration conducted? What specific metrics and hardware configurations were used? 2. What baselines were compared against (e.g., existing predictive models or scheduling algorithms)? 3. How does the model handle tasks not seen during training? 4. What is the exact methodology for the sequential decision-making algorithm? 5. Are there any limitations to the LLM's ability to generalize across different code structures or GPU architectures?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "2LHzKdb8Ao": {
    "paper_id": "2LHzKdb8Ao",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the symmetry mismatch issue in robotic learning caused by non-top-down camera views. The authors propose simple image preprocessing techniques (reprojection for RGBD and perspective transform for RGB) to align input transformations with the true O(2) symmetry of the environment, demonstrating improved performance for equivariant policy learning across multiple simulated tasks."
          },
          "strengths": {
            "value": "The paper presents a clear, practical solution to a well-defined problem in robotic learning. The proposed preprocessing methods are simple to implement and effective, with empirical validation across diverse settings (RGBD/RGB, reinforcement/imitation learning). The background and related work sections are comprehensive, and the methodology is rigorously explained. The problem of symmetry mismatch in non-top-down views is both timely and important for real-world deployment."
          },
          "weaknesses": {
            "value": "The experiments are limited to simulated environments without real-world validation, which is a critical gap for robotics applications. The paper does not thoroughly analyze the trade-offs between preprocessing complexity and performance gains. The discussion of occlusion handling is superficial, and the generalizability to more complex or dynamic environments remains unexplored. The claim about applicability to 'many equivariant methods' lacks concrete examples or analysis."
          },
          "questions": {
            "value": "1. How does the performance gap between the proposed method and the ideal top-down view vary across different task complexities or environments? 2. What are the specific limitations of the interpolation-based occlusion handling in highly cluttered scenes? 3. Can the authors provide more details on how their method would integrate with existing equivariant architectures (e.g., SE(2) vs O(2) models)? 4. How do the preprocessing steps affect the sample efficiency of different learning paradigms (e.g., reinforcement learning vs imitation learning)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper addresses symmetry mismatch in robotic learning caused by sideview cameras by proposing image preprocessing techniques to transform input data into top-down views. The methods include reprojecting RGBD images into point clouds and applying perspective transforms to RGB images, aiming to align image transformations with the O(2) symmetry of robotic tasks. The approach is evaluated in simulated environments, showing performance improvements for equivariant learning."
          },
          "strengths": {
            "value": "The paper presents a clear, practical solution to a specific problem in robotic learning with strong theoretical grounding in equivariant networks. The preprocessing steps are simple to implement and applicable to both RGB and RGBD modalities. The empirical analysis demonstrates consistent performance gains across multiple settings, including reinforcement learning and imitation learning. The background and related work sections are well-structured and contextualize the contribution effectively."
          },
          "weaknesses": {
            "value": "The experiments are limited to simulated environments, with no real-world validation, which is a critical gap for robotics applications. The paper does not thoroughly analyze how the preprocessing steps interact with different equivariant architectures or address limitations in complex, cluttered scenes. The handling of occlusions and distortions remains suboptimal, and the method's generalizability to other symmetry groups or multi-view setups is not explored. Additionally, ablation studies on the impact of individual preprocessing components are missing."
          },
          "questions": {
            "value": [
              "How do the proposed preprocessing steps affect different types of equivariant networks (e.g., SO(2) vs. SE(2))? Are there scenarios where they could degrade performance?",
              "What are the specific challenges of applying these methods in real-world settings beyond depth noise, such as dynamic environments or sensor noise?",
              "How does the method handle multi-view RGBD inputs, and could it be extended to leverage multiple camera perspectives?",
              "Are there cases where the perspective transform or reprojected images introduce artifacts that negatively impact policy learning?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper addresses symmetry mismatch in robotic learning caused by non-top-down camera views by proposing simple image preprocessing steps. The authors reproject RGBD images to top-down views using depth information and apply perspective transforms to RGB images. They demonstrate improved performance across various equivariant learning settings, including reinforcement and imitation learning."
          },
          "strengths": {
            "value": "The work presents a novel and practical solution to a specific problem in robotic learning, addressing symmetry mismatches that arise from sideview cameras. The preprocessing methods are simple to implement and applicable to both RGB and RGBD modalities. The empirical analysis is comprehensive, showing consistent performance gains across multiple environments and learning algorithms. The paper also provides a clear theoretical foundation for equivariant networks and their limitations in non-top-down settings."
          },
          "weaknesses": {
            "value": "The experiments are limited to simulated environments, with no real-world validation, which is a critical gap for robotic applications. The paper does not thoroughly address how preprocessing artifacts (e.g., occlusion interpolation) affect model performance, especially in complex scenes. The comparison with alternative approaches like multi-view RGBD or point cloud-based methods is superficial. The contribution's novelty is somewhat limited, as the preprocessing steps are not fundamentally new but rather applied to a specific problem."
          },
          "questions": {
            "value": [
              "How do the preprocessing steps affect the model's generalization to unseen environments or tasks not tested in the experiments?",
              "What are the limitations of the occlusion interpolation approach in highly cluttered or dynamic scenes?",
              "How does the computational cost of the preprocessing steps impact real-time robotic applications?",
              "Could the proposed method be adapted to other symmetry groups beyond O(2), such as SE(2) or SO(3)?",
              "How does the performance of the preprocessing steps compare to more advanced inpainting techniques (e.g., learned models) in real-world scenarios?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "2MqyCIxLSi": {
    "paper_id": "2MqyCIxLSi",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Generalized Combinatorial Complex Networks (GCCNs), a framework for Topological Deep Learning (TDL) that systematically transforms graph neural networks (GNNs) into TDL models. It proposes TopoTune, a software library for implementing GCCNs, and demonstrates their superiority over existing Combinatorial Complex Neural Networks (CCNNs) through experiments on multiple topological domains."
          },
          "strengths": {
            "value": "The paper presents a novel and principled framework (GCCNs) that addresses a critical gap in TDL standardization. The methodological contribution is significant, as it enables systematic generalization of GNNs to higher-order topological domains. The implementation of TopoTune as a lightweight PyTorch module enhances accessibility. The experiments are comprehensive, showing consistent performance improvements over CCNNs with reduced model complexity. The paper also clearly contextualizes its work within open problems in TDL."
          },
          "weaknesses": {
            "value": "The initial expressivity proof (Proposition 3) was weak and lacked theoretical depth. While the rebuttal provides a stronger argument using a k-WL analog, the connection to practical TDL benefits remains underexplored. The comparison to existing TDL methods like hypergraph GNNs is insufficient, particularly regarding how GCCNs differ in preserving topological symmetries. The paper could also clarify how GCCNs avoid the limitations of graph-expansion approaches (e.g., loss of rank information) in more detail."
          },
          "questions": {
            "value": "1. How does the k-WL-based expressivity proof directly translate to practical advantages in TDL tasks? 2. What specific topological properties of GCCNs enable their superior performance compared to CCNNs? 3. How does TopoTune handle computational efficiency for large-scale combinatorial complexes? 4. Are there scenarios where GCCNs might underperform compared to domain-specific TDL models?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Generalized Combinatorial Complex Networks (GCCNs), a framework for systematically transforming graph neural networks (GNNs) into Topological Deep Learning (TDL) models. The authors propose TopoTune, a software tool for building GCCNs, and demonstrate their effectiveness on benchmark datasets. GCCNs are claimed to generalize and outperform existing Combinatorial Complex Neural Networks (CCNNs) while maintaining topological symmetry."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in TDL by proposing a systematic method to generalize GNNs to higher-order topological domains. The introduction of GCCNs as a flexible, principled framework is novel, and the accompanying software (TopoTune) enhances accessibility. The work aligns with open problems in TDL, such as standardization and benchmarking, and provides extensive experiments showing competitive performance. The theoretical analysis of expressivity and equivariance is a significant contribution."
          },
          "weaknesses": {
            "value": "The paper lacks a clear explanation of how GCCNs enable new operations or patterns beyond CCNNs, despite the authors' rebuttal. The expressivity proof, while improved, relies on abstract concepts (e.g., k-WL tests) that require deeper technical justification. Experimental comparisons with CCNNs are limited to specific architectures, and the paper does not thoroughly address computational efficiency or scalability. The connection between GCCNs and existing TDL models (e.g., hypergraph neural networks) is underdeveloped."
          },
          "questions": {
            "value": "1. How do GCCNs enable new message-passing patterns not possible in CCNNs? The rebuttal mentions per-rank neighborhoods, but the paper should clarify this with concrete examples. 2. What are the limitations of the k-WL-based expressivity proof, and how does it translate to practical performance? 3. How does TopoTune integrate with existing TDL libraries like TopoBenchmark, and what specific features make it 'lightweight'? 4. Are there cases where GCCNs fail to outperform CCNNs, and why?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Generalized Combinatorial Complex Networks (GCCNs) as a framework to systematically generalize Graph Neural Networks (GNNs) into Topological Deep Learning (TDL) models. It proposes TopoTune, a lightweight software library for building GCCNs, and demonstrates their effectiveness on benchmark datasets, claiming superior performance over existing CCNNs with reduced complexity."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in TDL by providing a principled framework for building topological models, enhancing accessibility and standardization. The introduction of GCCNs as a flexible generalization of CCNNs is innovative, and the accompanying software (TopoTune) is a practical contribution. The formalization of neighborhoods and augmented Hasse graphs is thorough, and the experiments show promising results, particularly in capturing higher-order interactions."
          },
          "weaknesses": {
            "value": "The expressivity proof for GCCNs over CCNNs remains insufficiently rigorous, despite the authors' revisions. The experiments, while showing improvements, lack depth in comparing against alternative TDL approaches (e.g., hypergraph GNNs). The paper does not fully clarify how GCCNs avoid the limitations of graph-expansion methods (e.g., loss of topological symmetry). Additionally, the claim that GCCNs outperform CCNNs in 'many cases' is not supported by concrete, detailed examples or ablation studies."
          },
          "questions": {
            "value": "1. How exactly do the new neighborhood structures in GCCNs enable superior performance compared to CCNNs? Provide specific examples where GCCNs outperform CCNNs due to this flexibility. 2. What are the limitations of TopoTune in terms of scalability or supported topological domains? 3. How does the proposed expressivity proof (via k-WL) compare to existing TDL expressivity analyses? 4. Are there scenarios where GCCNs might underperform compared to other TDL methods, and how are these handled?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "2NqrA1wYi6": {
    "paper_id": "2NqrA1wYi6",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the lack of a unified methodology for evaluating memory in reinforcement learning (RL) agents by formalizing definitions of memory types (e.g., declarative vs. procedural, short-term vs. long-term) inspired by cognitive science. It introduces a classification of tasks requiring memory (Memory Decision-Making and Meta-RL), proposes an experimental framework for testing memory capabilities, and demonstrates how ignoring their methodology leads to flawed conclusions."
          },
          "strengths": {
            "value": "The paper's originality lies in its formalization of memory concepts from cognitive science, which provides a structured foundation for evaluating RL agents. The methodology is well-structured, with clear definitions and a focus on addressing the critical issue of inconsistent memory evaluation in RL. The clarity of the paper is strong, with examples like the T-Maze environment and visualizations aiding understanding. The significance is high, as the work tackles a fundamental gap in RL research, offering a framework for objective comparison of memory-enhanced agents."
          },
          "weaknesses": {
            "value": "The paper relies on simplified environments (e.g., T-Maze) that may not capture the complexity of real-world tasks, raising questions about generalizability. The focus on declarative memory excludes procedural memory, which is a major area of RL research, and the experimental validation is limited to specific configurations. The definitions of procedural and declarative memory, while improved in the rebuttal, still lack explicit criteria for distinguishing between them in practice. Additionally, the paper does not fully address how its framework integrates with existing memory mechanisms (e.g., transformers, recurrent networks)."
          },
          "questions": {
            "value": "1. How can the proposed framework be extended to evaluate procedural memory, which the authors acknowledge as important but exclude from their focus? 2. What are the limitations of the T-Maze and similar environments in capturing the nuances of memory in complex RL tasks? 3. How does the paper’s methodology handle agents that combine multiple memory types (e.g., short-term and long-term) in practice? 4. Are there plans to validate the framework on more complex or diverse tasks beyond the current experiments?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a formal framework for classifying and evaluating memory in reinforcement learning (RL) agents, distinguishing between declarative/procedural memory and short-term/long-term memory. It proposes a methodology for testing memory capabilities in Memory Decision-Making (Memory DM) tasks and demonstrates the risks of improper experimental design."
          },
          "strengths": {
            "value": "The paper makes a significant contribution by formalizing ambiguous concepts of memory in RL, drawing from cognitive science to create clear definitions. The classification of tasks into Memory DM and Meta-RL, along with precise criteria for declarative/procedural memory, addresses a critical gap in the field. The methodology for evaluating memory is novel and structured, and the paper's organization and clarity are strong. The significance lies in enabling objective comparisons of memory mechanisms across agents."
          },
          "weaknesses": {
            "value": "The experimental validation is limited to simple environments (e.g., T-Maze), which may not reflect real-world complexity. The paper's focus on declarative memory excludes procedural memory, which the rebuttal acknowledges as beyond its scope but raises questions about the completeness of the framework. The definitions for declarative/procedural memory, while adjusted in the rebuttal, remain somewhat abstract and lack concrete examples of how they apply to diverse RL architectures. The methodology's comparison to existing approaches is underdeveloped, and the paper does not address how its framework scales to larger, more complex tasks."
          },
          "questions": {
            "value": "1. How does the proposed methodology handle agents with hybrid memory architectures (e.g., combining short-term and long-term mechanisms)? 2. What are the specific limitations of the current experimental environments in capturing real-world memory challenges? 3. How do the revised definitions of declarative/procedural memory affect the interpretation of existing RL work? 4. Can the framework be extended to evaluate memory in continuous control tasks or multi-agent settings?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper aims to formalize the concept of memory in RL agents by defining types such as long-term/short-term and declarative/procedural memory, inspired by cognitive science. It proposes a classification of memory tasks (Memory DM vs. Meta-RL), a standardized experimental methodology for evaluating memory capabilities, and demonstrates the importance of adhering to this methodology through experiments."
          },
          "strengths": {
            "value": "The paper's strengths lie in its structured theoretical framework, which addresses a critical gap in RL research by providing clear definitions of memory types and their classification. The connection to cognitive science adds conceptual depth, and the proposed experimental methodology offers a systematic approach to evaluating memory. The paper also highlights the limitations of existing practices, which is valuable for the field."
          },
          "weaknesses": {
            "value": "The experimental validation is limited to simple environments, which may not capture the complexity of real-world RL tasks. The definitions of declarative/procedural memory (e.g., based on $n_{envs} \times n_{eps}$) are overly simplistic and lack nuance. The paper does not thoroughly compare its methodology with existing approaches, making it difficult to assess its novelty or practical utility. Additionally, the rebuttal's adjustments to definitions do not fully resolve ambiguities in how memory types are distinguished."
          },
          "questions": {
            "value": "1. How do the authors address the variability in memory mechanisms across different RL agents (e.g., transformers vs. RNNs)? 2. What evidence supports the claim that the proposed methodology is universally applicable across diverse memory types? 3. How do the authors justify the choice of $n_{envs} \times n_{eps}$ as a criterion for procedural vs. declarative memory, given the lack of consensus in the literature? 4. What are the limitations of the T-Maze and similar environments in testing the proposed framework?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "2PRpcmJecX": {
    "paper_id": "2PRpcmJecX",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper presents the first comprehensive finite-time global convergence analysis of policy gradient methods for average reward Markov Decision Processes (MDPs). It eliminates the need for unverified smoothness assumptions, establishes sublinear convergence rates, and provides explicit performance bounds dependent on MDP complexity. The work also extends to discounted reward MDPs and includes empirical validation."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in the literature by focusing on average reward MDPs, which are less studied than discounted counterparts. The theoretical analysis is rigorous, particularly the proof of smoothness without prior assumptions. The explicit dependence of bounds on MDP complexity constants is a novel contribution. The empirical results validate the theoretical findings, and the paper's structure is logically organized."
          },
          "weaknesses": {
            "value": "The related work section requires more thorough discussion, particularly in distinguishing the paper's contributions from existing works like Xiao (2022b) and Grosof et al. (2024). The exclusion of epsilon dependencies for clarity may obscure the practical relevance of the bounds. The convergence analysis for simple MDPs with constant step sizes needs more detailed comparison with existing results. The handling of non-ergodic MDPs is briefly mentioned but not fully explored."
          },
          "questions": {
            "value": "1. How do the authors reconcile their results with the linear convergence rates in Xiao (2022b) under aggressive step sizes, given their focus on constant step sizes? 2. What are the practical implications of the MDP complexity parameters in the bounds, and how do they compare to state-of-the-art metrics? 3. Can the ergodicity assumption be relaxed, and what are the limitations of the proposed MDP transformation technique for non-ergodic cases?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper presents the first finite-time global convergence analysis of policy gradient methods for average reward Markov Decision Processes (MDPs). The authors address the challenge of analyzing policy gradient convergence in average reward settings by eliminating the need for unverified smoothness assumptions, deriving explicit performance bounds, and providing empirical validation. They also extend their analysis to discounted reward MDPs and improve existing bounds."
          },
          "strengths": {
            "value": "The paper's primary strength lies in its novel proof technique that eliminates the smoothness assumption, a critical limitation in prior work. The theoretical analysis is rigorous, with explicit convergence rates and bounds that explicitly depend on MDP complexity rather than state/action space sizes. The paper also provides empirical validation through simulations, demonstrating the impact of MDP structure on convergence. The clarity of mathematical formulations and the structured presentation of assumptions and results further strengthen the paper."
          },
          "weaknesses": {
            "value": "The paper lacks a comprehensive comparison with existing work on discounted reward MDPs, particularly regarding the trade-offs between step size choices and convergence rates. The experimental validation is limited to a simple class of MDPs, which may not fully reflect real-world scenarios. The assumption of irreducibility and aperiodicity in the transition matrix is strong, and the proposed workaround for non-ergodic MDPs is not thoroughly analyzed. Additionally, the paper does not address how the projection technique affects policy exploration or the practical implications of the derived bounds."
          },
          "questions": {
            "value": "1. How does the projection technique used to ensure value function uniqueness impact the policy's exploration capabilities in practice? 2. The paper mentions that the ergodicity assumption can be relaxed with an O(ε) loss of optimality. How sensitive are the theoretical guarantees to this approximation? 3. The convergence rate in Theorem 1 for simple MDPs is linear under constant step sizes. How does this compare to the results in Xiao (2022b) when considering the same step size regime? 4. The paper claims to improve bounds for discounted MDPs but does not provide explicit comparisons with state-of-the-art results. How do the new bounds differ in terms of dependency on MDP parameters?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper presents the first finite-time global convergence analysis of policy gradient methods for average reward Markov Decision Processes (MDPs). It addresses the challenge of extending discounted reward convergence guarantees to average reward settings by proving the smoothness of the average reward without prior assumptions. The authors derive sublinear convergence bounds and provide simulations to validate their results."
          },
          "strengths": {
            "value": "The paper's primary strength lies in its novel theoretical contribution: proving global convergence for average reward MDPs without relying on unverified smoothness assumptions, which previous work required. The analysis introduces a projection technique to handle the non-uniqueness of value functions in average reward problems. The paper also provides explicit performance bounds with dependencies on MDP complexity, offering deeper insights than discounted reward frameworks. The experimental validation, while brief, supports the theoretical claims."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing work on discounted reward MDPs, particularly regarding the trade-offs between constant vs. adaptive step sizes. The convergence rate analysis for simple MDPs (linear convergence) is not thoroughly connected to the broader context of policy iteration or natural policy gradient methods. Additionally, the experimental section is minimal, with no quantitative results to substantiate the claimed convergence rates. The ergodicity assumption in Theorem 1 is critical but not fully justified for non-ergodic MDPs, leaving open questions about generalizability."
          },
          "questions": {
            "value": "1. How does the proposed method handle non-ergodic MDPs, and what guarantees exist for such cases? 2. The rebuttal mentions that aggressive step sizes could enable linear convergence, but how does this reconcile with the paper's focus on constant step sizes? 3. What are the practical implications of the MDP complexity parameter in the bounds, and how does it compare to state-of-the-art metrics in discounted reward settings? 4. The paper cites Xiao (2022b) but does not explicitly compare the convergence rates or assumptions. How do the authors' bounds improve upon or differ from Xiao's? 5. The experimental validation is limited—what specific MDPs were used, and how were hyperparameters tuned?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "2QkWSUMQh5": {
    "paper_id": "2QkWSUMQh5",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces edge-based truss robustness measures to analyze the sensitivity of truss decomposition to edge removals. The authors propose a dependency graph among edges, define three metrics (Edge Robustness, Edge Strength, EdgeRank), and demonstrate their effectiveness in improving GNN-based edge classification. The approach is computationally efficient, with a 3.74× speedup over baselines, and shows significant performance gains on real-world datasets."
          },
          "strengths": {
            "value": "Originality: The paper pioneers edge-based truss robustness analysis, addressing a critical gap in understanding truss decomposition's sensitivity. Quality: The methodology is rigorous, combining theoretical insights (dependency graph construction) with efficient algorithms. Clarity: The paper is well-structured, with clear definitions and illustrative examples. Significance: The proposed measures enhance GNN performance for edge classification, particularly for rare classes, and open new avenues for edge-driven graph analysis."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with other edge-centric metrics (e.g., coreness, degree) beyond the ablation study. The dependency graph focuses on single-edge removals, which may not capture complex real-world scenarios. EdgeRank's theoretical foundation is underexplored, and its variability across datasets (as noted in the rebuttal) warrants further investigation. The distinction between the authors' approach and Chen et al.'s (2021) work on community robustness remains unclear, potentially limiting the perceived novelty."
          },
          "questions": {
            "value": "1. How does the dependency graph generalize to multiple-edge removals, and what are the computational implications? 2. What are the theoretical guarantees of the ERC algorithm's efficiency, and how does its complexity scale with graph size? 3. Can EdgeRank be adapted to other graph tasks beyond edge classification, and how does its performance vary across different graph types? 4. How do the proposed measures compare to existing edge-based features (e.g., triangle count, coreness) in terms of discriminative power?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces a dependency graph to model edge robustness in truss decomposition, proposes three edge-based measures (Edge Robustness, Edge Strength, EdgeRank), and integrates them into GNNs for improved edge classification. It demonstrates efficiency and effectiveness on real-world datasets."
          },
          "strengths": {
            "value": "The paper presents novel edge-based robustness measures for truss decomposition, which are theoretically grounded and computationally efficient (3.74× faster than baselines). The integration into GNNs for edge classification shows practical utility, with up to 3.08% F1 score improvement. The dependency graph approach captures nuanced structural properties, and the rebuttal addresses key clarification points. The work is well-structured with clear motivation and empirical validation."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparison with existing edge-based features (e.g., coreness, degree, triangle count) beyond the rebuttal's added runtime analysis. The theoretical analysis of the algorithm's complexity is deferred to the rebuttal, not the main text. The EdgeRank formulation lacks deeper justification, and the paper could better contextualize its novelty relative to prior work on core/tract robustness. Some experimental details (e.g., dataset statistics, ablation studies) are under-specified in the original submission."
          },
          "questions": {
            "value": "How do the proposed measures compare to other edge-based features like node degrees or core numbers in terms of predictive power? What are the limitations of the dependency graph approach (e.g., scalability, sensitivity to graph size)? Could the EdgeRank formulation be further justified through theoretical analysis or alternative interpretations? How do the three measures interact in the GNN framework, and what is their individual contribution to performance gains?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces edge-based robustness measures for truss decomposition, proposing a dependency graph to capture how edge removal affects neighboring edges. It defines three metrics (Edge Robustness, Edge Strength, EdgeRank) and integrates them into GNNs for edge classification, demonstrating improved performance with negligible computational overhead."
          },
          "strengths": {
            "value": "The paper presents a novel approach to edge-level truss robustness, which is underexplored in prior work. The theoretical contributions include efficient algorithms for dependency graph computation. The empirical results show measurable improvements in edge classification tasks, and the integration with GNNs is well-justified. The work addresses a practical gap in understanding truss decomposition's sensitivity to graph perturbations."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of why the proposed metrics outperform baselines, relying heavily on empirical results. Some definitions (e.g., trussness support) are referenced to prior work without sufficient elaboration. The dependency graph's construction and EdgeRank's theoretical foundation require deeper justification. The experiments focus on a limited set of datasets, and the comparison to related work (e.g., Chen et al.) is not fully contextualized."
          },
          "questions": {
            "value": "How do the three metrics (ER, ES, EdgeRank) interact when combined in GNNs? What are the limitations of the dependency graph in capturing edge interactions? Are there scenarios where the proposed measures fail to reflect true robustness? How does the efficiency of the ERC algorithm scale with large graphs?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "2TasVD7FXp": {
    "paper_id": "2TasVD7FXp",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces InvestESG, a multi-agent reinforcement learning (MARL) benchmark to study the impact of ESG disclosure mandates on corporate climate investments. The framework models a social dilemma where companies balance short-term profits with long-term climate risks, while ESG-conscious investors influence corporate behavior through investment decisions. The authors release open-source implementations in PyTorch and JAX, and their experiments demonstrate that ESG-informed investors can drive corporate mitigation efforts, though greenwashing remains a challenge."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem at the intersection of climate policy and machine learning. The MARL framework enables simulation of complex, long-term interactions between corporations and investors, offering insights into social dilemmas that traditional models cannot capture. The open-source release and alignment with real-world empirical data strengthen its practical relevance. The work also connects to established economic literature, highlighting its interdisciplinary value."
          },
          "weaknesses": {
            "value": "The agent count is limited, and the model simplifies investor decisions to binary choices, which may not reflect real-world complexity. The paper could better contextualize its economic foundations by explicitly comparing to existing models and clarifying how it extends them. Some figures (e.g., Figure 7) are ambiguous, and the rebuttal suggests additional experiments (e.g., longer investment timelines) are needed to validate robustness. The claim that results align with real-world data requires stronger evidence."
          },
          "questions": {
            "value": "1. How does the model handle long-term investment timelines, and what evidence supports the assumption of five-year lock-in periods? 2. Why are investor decisions restricted to binary choices, and how might continuous allocation improve realism? 3. How do the authors reconcile the limited agent count with claims about real-world applicability? 4. What specific real-world data supports the alignment of experimental results with empirical findings?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces InvestESG, a multi-agent reinforcement learning (MARL) benchmark to study the impact of ESG disclosure mandates on corporate climate investments. It models a social dilemma where companies balance short-term profits with long-term climate risks, while ESG-conscious investors influence corporate behavior. The authors release open-source implementations and demonstrate that sufficient ESG-investor capital drives corporate mitigation, while greenwashing and information asymmetry pose challenges."
          },
          "strengths": {
            "value": "The paper presents a novel and timely benchmark for studying climate policy through MARL, addressing a critical socio-economic problem. The connection to real-world ESG policies and empirical validation of findings strengthens its significance. The framework extends prior economic models by incorporating dynamic climate evolution, greenwashing, and multi-period interactions. The clear structure and open-source release enhance reproducibility and community engagement."
          },
          "weaknesses": {
            "value": "The experiments are limited in scale (few agents) and lack real-world data integration, which undermines generalizability. The model simplifies investor decisions (binary choices) and assumes static firm characteristics, which may not reflect real-world complexity. The analysis of greenwashing and climate risk dynamics is superficial, and the rebuttal acknowledges these gaps. The paper also lacks detailed comparisons with existing MARL benchmarks tailored to policy analysis."
          },
          "questions": {
            "value": "1. How do the authors plan to address the scalability limitations of their experiments, and what evidence supports the validity of their findings in larger-scale scenarios? 2. What specific real-world data will be integrated to strengthen the empirical alignment of results? 3. How does the binary investor decision mechanism impact the realism of the model, and what are the trade-offs of adopting continuous portfolio allocation? 4. Can the authors clarify the conflicting interpretation of Figure 7b and its implications for climate risk and market wealth?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces InvestESG, a multi-agent reinforcement learning (MARL) benchmark to study the impact of ESG disclosure mandates on corporate climate investments. It models a social dilemma where companies balance short-term profits with long-term climate risks, and investors influence corporate behavior through investment decisions. The benchmark includes simulations of mitigation, greenwashing, and resilience strategies, with experiments showing that ESG-conscious investors and climate risk information drive corporate mitigation."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely policy issue (climate change) using MARL, offering a novel benchmark that bridges social dilemmas and real-world economic dynamics. The experiments align with empirical evidence, and the open-source release in PyTorch/JAX enables scalable simulations. The work highlights MARL's potential for policy design, and the conceptual framework is grounded in finance and economics literature. The clarity of the problem setup and the structured analysis of investor-company interactions are commendable."
          },
          "weaknesses": {
            "value": "The model's simplicity and limited agent numbers (e.g., 25 companies/investors) may not capture real-world complexity. The binary investor decision-making and static ESG score formulation oversimplify real-world behavior. The paper lacks thorough comparison with existing economic models, and some experiments (e.g., Figure 7b) are poorly explained. The rebuttal acknowledges these limitations but does not fully address how the simplified framework affects the validity of results or the generalizability of findings."
          },
          "questions": {
            "value": "1. How does the simplified agent design (e.g., binary investor decisions, static ESG scores) impact the realism of the model? 2. What evidence supports the claim that the benchmark aligns with real-world data, given the limited agent scale? 3. How will the planned larger-scale simulations (25 companies/investors) address the current limitations? 4. Could the confusion in Figure 7b be resolved by clarifying the relationship between climate risk and market wealth metrics? 5. How will the proposed continuous investor decision variables improve the model's fidelity to real-world behavior?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "2VhFZPYqjE": {
    "paper_id": "2VhFZPYqjE",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces CHASE, a framework for generating synthetic, challenging problems for evaluating Large Language Models (LLMs). The approach uses a bottom-up method to build problems from simpler components and decomposes generation into verifiable sub-tasks. The framework is applied to three domains—document-based QA, code completion, and math reasoning—demonstrating that current LLMs struggle with these benchmarks, especially under long-context conditions."
          },
          "strengths": {
            "value": "The paper presents a novel framework for synthetic evaluation data, addressing an underexplored area. The decomposition into verifiable sub-tasks ensures quality and correctness, while the bottom-up approach creates challenging problems. The experiments across diverse domains are thorough, showing significant performance gaps and highlighting the effectiveness of CHASE. The writing is clear, with well-structured figures and a strong focus on practical applications."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed comparison with existing synthetic data methods beyond the mentioned baselines. The scalability of the framework is not fully demonstrated, and the verification process's reliability is under-explained. The paper does not address how to handle potential verifier errors or validate the correctness of sub-task decompositions. The rebuttal clarifies some points but leaves gaps in the technical details of verification and scalability."
          },
          "questions": {
            "value": "How is the verifier's accuracy validated, and what safeguards exist for potential errors? What is the exact process for decomposing tasks into sub-tasks, and how is complexity managed? How does the bottom-up approach ensure real-world relevance beyond synthetic difficulty? What are the limitations of the framework in terms of domain adaptability?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces CHASE, a framework for generating synthetic, challenging evaluation benchmarks for LLMs using a bottom-up approach and decomposition into verifiable sub-tasks. It creates three domains: document-based QA (CHASE-QA), repository-level code completion (CHASE-CODE), and math reasoning (CHASE-MATH). Experiments show that even top LLMs struggle with these benchmarks, highlighting the framework's effectiveness."
          },
          "strengths": {
            "value": "The paper presents a novel framework (CHASE) with clear methodological contributions: bottom-up problem generation and decomposition into verifiable sub-tasks. The experiments are comprehensive, covering diverse domains and demonstrating the effectiveness of the approach. The motivation for synthetic evaluation benchmarks is well-justified, addressing saturation of human-curated datasets. The paper is well-structured with clear figures and detailed implementation descriptions."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with existing synthetic data generation methods (e.g., SWE-bench or Evol-Instruct) in code generation, despite mentioning these in the rebuttal. The scalability of the framework is not thoroughly demonstrated beyond the described benchmarks. The verification process's robustness is not fully explained (e.g., how verifiers handle edge cases). The claims about 'high-quality' data could be strengthened with quantitative metrics on correctness rates."
          },
          "questions": {
            "value": [
              "How does CHASE's approach compare to existing synthetic data methods like SWE-bench or Evol-Instruct in terms of effectiveness and efficiency?",
              "What specific metrics were used to evaluate the correctness of generated data during verification steps?",
              "Can the framework be adapted to other domains beyond the three presented, and what are the limitations of such adaptations?",
              "How does the bottom-up approach ensure that generated problems are not biased toward the generator/verifier models?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces CHASE, a framework for generating synthetic, challenging problems for evaluating Large Language Models (LLMs) across three domains: document-based QA, repository-level code completion, and math reasoning. The approach uses a bottom-up methodology to create problems by iteratively hiding solution components and decomposing generation into verifiable sub-tasks. The authors demonstrate that CHASE benchmarks are challenging for state-of-the-art LLMs, with accuracy ranging from 40-60%, and highlight performance drops when scaling context sizes."
          },
          "strengths": {
            "value": "The paper presents a novel framework (CHASE) for synthetic data generation tailored for evaluation, addressing scalability and quality gaps in existing benchmarks. The bottom-up problem creation and decomposition into verifiable sub-tasks are innovative and well-motivated. The experiments across three diverse domains (QA, code, math) demonstrate the framework's versatility. The focus on long-context reasoning and the observed 70% performance drop when scaling context size are significant contributions. The paper also provides clear comparisons to baselines like Evol-Instruct, showing CHASE's superiority in generating high-quality data."
          },
          "weaknesses": {
            "value": "The paper lacks critical details about the specific LLMs used as the generator (G) and verifier (V), which limits reproducibility and understanding of the framework's dependencies. The verification process is described but not thoroughly explained—how are sub-tasks checked for correctness? The experiments do not include ablation studies to isolate the impact of key components (e.g., bottom-up vs. top-down generation). Additionally, the paper does not address potential biases in the generated data or provide statistical significance for the performance gaps between models. The rebuttal clarifies that the framework is scalable, but the lack of explicit data on dataset sizes (e.g., 10k math problems vs. 150 in CHASE-MATH) raises questions about consistency."
          },
          "questions": {
            "value": "1. What specific LLMs were used as the generator (G) and verifier (V)? How do their capabilities influence the framework's effectiveness? 2. Can the authors provide a detailed description of the verification process for sub-tasks, including how correctness is quantitatively assessed? 3. Why does CHASE-MATH contain only 150 problems, while the rebuttal mentions generating 10k math problems? How does this affect the validity of the experiments? 4. Are there ablation studies showing the impact of the bottom-up approach versus alternative methods? 5. How does the framework ensure diversity and avoid overfitting to specific patterns in the generated data?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "2VmB01D9Ef": {
    "paper_id": "2VmB01D9Ef",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces AutoHijacker, an automated indirect prompt injection attack for black-box LLM agents. It addresses the limitations of existing methods by proposing a batch-based optimization framework and a trainable memory to handle sparse feedback and reduce dependency on continuous querying. The method demonstrates state-of-the-art performance on public benchmarks and a commercial LLM agent platform."
          },
          "strengths": {
            "value": "The paper tackles a critical and practical security issue in LLMs with a novel approach that combines batch optimization and memory mechanisms. The experimental validation on diverse benchmarks and a commercial platform showcases its effectiveness. The framework's multi-agent design (attacker, prompter, scorer) is innovative, and the comparison with 11 baselines and 8 defenses strengthens the claims. The problem formulation and real-world relevance are well-motivated."
          },
          "weaknesses": {
            "value": "The paper lacks ablation studies to isolate the contributions of the batch-based optimization and memory components. The commercial platform attack details are insufficient (e.g., specific injection strategies, success metrics). The generalizability of the method to other models or tasks is not thoroughly discussed. The handling of sparse feedback in the batch framework is not clearly explained. Ethical considerations and mitigation strategies are absent."
          },
          "questions": {
            "value": "1. How was the commercial LLM agent attacked? What specific injection data and success criteria were used? 2. What ablation studies were conducted to validate the batch optimization and memory components? 3. How does the method handle varying levels of feedback sparsity across different tasks? 4. Are there limitations in scenarios where the victim model's foundation LLM is unknown? 5. What ethical safeguards or mitigation strategies are proposed for deploying such attacks?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces AutoHijacker, an automated black-box indirect prompt injection attack framework for LLM agents. It addresses the challenge of sparse feedback in indirect prompt injection by proposing a batch-based optimization framework and a trainable memory mechanism, enabling effective attack generation without continuous querying. The method is evaluated on public benchmarks and a commercial LLM agent, demonstrating superior performance compared to existing baselines."
          },
          "strengths": {
            "value": "The paper makes a significant contribution by addressing the critical gap in automated, black-box prompt injection attacks, which are more realistic than prior white-box or gray-box methods. The use of LLM-as-optimizers with a memory mechanism to handle sparse feedback is novel. The experimental results on benchmarks and a commercial platform show strong performance, and the paper highlights practical security implications for LLM agents. The framework's design is well-motivated by the limitations of existing approaches."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical explanations of key components, such as how the batch-based optimization framework specifically mitigates sparse feedback or how the attack memory is implemented. The evaluation on the commercial platform is under-specified (e.g., no details on the platform's architecture or defense mechanisms). The comparison to baselines is not thoroughly analyzed (e.g., statistical significance of results, ablation studies). Additionally, the paper does not address potential countermeasures or limitations of AutoHijacker, such as robustness to defense mechanisms beyond those tested."
          },
          "questions": {
            "value": "1. How does the batch-based optimization framework explicitly handle sparse feedback? What metrics or mechanisms ensure its effectiveness? 2. Can the authors provide more details on the commercial LLM agent's architecture and the specific defenses it employs? 3. Are there ablation studies demonstrating the necessity of the attack memory and batch optimization components? 4. How does AutoHijacker generalize to other types of LLM agents or tasks beyond document interaction and website browsing?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes AutoHijacker, an automated black-box indirect prompt injection attack method for LLM agents. It addresses limitations of existing handcrafted or white-box attacks by using a batch-based optimization framework and a trainable memory to handle sparse feedback and reduce querying needs. The method demonstrates state-of-the-art performance on benchmarks and a commercial platform."
          },
          "strengths": {
            "value": "Originality is evident in the automated black-box approach, leveraging LLM-as-optimizers with novel batch-based optimization and memory mechanisms. The experiments on public benchmarks and a commercial platform show practical relevance. The paper's focus on real-world scenarios and sparse feedback challenges is significant. The multi-agent framework (attacker, prompter, scorer) represents a creative combination of components."
          },
          "weaknesses": {
            "value": "Key technical details about the attack memory construction, scoring mechanism, and how the batch-based optimization specifically mitigates sparse feedback are under-specified. The commercial platform evaluation lacks transparency (e.g., model specifics, defense configurations). The paper does not address potential limitations in generalizability across different LLM architectures or tasks. Claims about 'state-of-the-art' performance lack comparative analysis of computational efficiency or scalability."
          },
          "questions": {
            "value": [
              "How is the attack memory $\\mathcal{A}$ constructed and updated during training? What criteria determine which attacks are stored?",
              "What specific metrics does the scorer use to evaluate victim responses against the attack goal $G$?",
              "How was the commercial LLM agent platform's environment characterized (e.g., RAG implementation, tool capabilities) to ensure the attack's validity?",
              "Are there ablation studies demonstrating the necessity of the batch-based optimization framework versus single-sample approaches?",
              "How does AutoHijacker handle cases where the victim model's responses are completely unresponsive to injected content?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "2aL6gcFX7q": {
    "paper_id": "2aL6gcFX7q",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper investigates targeted data poisoning attacks on Retrieval-Augmented Generation (RAG) systems, identifying that effective attacks occur along directions of low variance in clean data distributions. It proposes DRS (Directional Relative Shifts) as a defense mechanism and a regularization-based attack algorithm to generate stealthier poisoned data. Experiments across multiple RAG scenarios demonstrate the effectiveness of these methods."
          },
          "strengths": {
            "value": "The paper provides novel theoretical insights into the vulnerability of RAG systems to data poisoning attacks, particularly highlighting the role of data distribution variance. The proposed DRS defense is innovative and supported by extensive experiments across diverse RAG applications. The work addresses a critical safety concern in RAG, which is increasingly used in high-stakes domains. The clarity of problem formulation, methodology, and experimental design is strong, with clear comparisons to existing methods."
          },
          "weaknesses": {
            "value": "The theoretical analysis of why low-variance directions are more susceptible to attacks is somewhat superficial. The paper lacks detailed ablation studies on hyperparameters for DRS and the attack algorithm. The dependency of DRS on the quality and diversity of clean data samples is not thoroughly addressed, despite the rebuttal's acknowledgment of this limitation. Additionally, the paper does not fully explore the practical challenges of deploying DRS in dynamic or evolving data environments."
          },
          "questions": {
            "value": "1. How does the DRS defense perform against non-targeted attacks or attacks with varying poisoning strategies? 2. What are the specific hyperparameters used in the regularization term for the attack algorithm, and how were they tuned? 3. How does the method handle scenarios where the clean data distribution is not static (e.g., real-time updates to the corpus)? 4. Are there cases where the proposed attack algorithm fails to bypass DRS, and what factors influence this?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates targeted data poisoning attacks against Retrieval-Augmented Generation (RAG) systems, identifying that attacks are more effective along directions with small variances in clean data. The authors propose DRS (Directional Relative Shifts) as a defense mechanism and a regularization-based attack algorithm to generate stealthier poisoned data. They validate their methods across multiple RAG scenarios, including Q&A and medical applications."
          },
          "strengths": {
            "value": "The paper offers novel insights into the mechanics of RAG attacks by linking attack effectiveness to data distribution variances, a perspective not previously emphasized. The proposed DRS defense is theoretically grounded in the analysis of embedding spaces, and the attack algorithm introduces a regularization technique to evade detection. The experimental validation across diverse RAG setups demonstrates practical relevance. The writing is clear, and the structure effectively connects theoretical analysis to practical applications."
          },
          "weaknesses": {
            "value": "The theoretical foundation for the variance-based attack analysis is underdeveloped, with limited mathematical rigor to justify why small-variance directions are more susceptible. The DRS defense relies on a clean dataset, but the paper does not address how to obtain or maintain such a dataset in real-world scenarios. The experiments lack detailed comparisons with existing defenses, and the rebuttal highlights limitations in DRS's ability to distinguish semantically similar queries. The attack algorithm's stealthiness claims are not thoroughly validated against state-of-the-art detection methods."
          },
          "questions": {
            "value": "1. How rigorously is the variance-based hypothesis validated? Are there mathematical proofs or empirical analyses to support the claim that small-variance directions are more vulnerable? 2. What are the practical challenges in maintaining a representative clean dataset for DRS in dynamic or large-scale RAG systems? 3. How does the regularization-based attack compare to existing stealthy attack methods in terms of effectiveness and detectability? 4. Can DRS be adapted to handle scenarios where the clean dataset is not available or incomplete?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper investigates targeted data poisoning attacks on Retrieval-Augmented Generation (RAG) systems, providing theoretical insights into their effectiveness and proposing a novel defense (DRS) and a stealthier attack algorithm. The authors analyze how attacks exploit low-variance directions in clean data distributions and validate their methods across multiple RAG scenarios."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem (RAG security) with practical relevance. It offers novel theoretical insights into attack mechanisms, introduces a principled defense (DRS) based on directional variance, and proposes a regularization-based attack that challenges existing defenses. The experimental evaluation is comprehensive, covering diverse RAG applications. The writing is clear, and the contributions are well-structured."
          },
          "weaknesses": {
            "value": "The theoretical analysis of why low-variance directions are exploited lacks depth, with limited mathematical justification. The DRS defense's dependency on clean data size and diversity is not thoroughly addressed, despite the rebuttal's ablation study. The attack algorithm's regularization is not rigorously validated against alternative defenses. The paper's comparison to state-of-the-art methods is insufficient, and the threat model's assumptions (e.g., white-box access) may not reflect real-world constraints. The rebuttal acknowledges limitations in detection rates with small clean datasets, which could undermine practical applicability."
          },
          "questions": {
            "value": "1. How generalizable are the theoretical insights about low-variance directions to other RAG architectures or data modalities? 2. What are the exact trade-offs between DRS's detection rate and the size/diversity of the clean dataset in real-world scenarios? 3. How does the regularization-based attack compare to existing stealthy attack methods in terms of computational cost and practical feasibility? 4. Can the DRS defense be adapted to scenarios where clean data is not available or limited?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "2bIQBDSfRk": {
    "paper_id": "2bIQBDSfRk",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces DenseAttention, a simplified Transformer architecture that claims to achieve linear time and space complexity by removing components like Softmax, LayerNorms, and projection matrices. It leverages matrix multiplication associativity and introduces MaxNormActivation to address numerical stability. The approach is evaluated on long-sequence tasks, showing competitive performance with BERT-large and state-of-the-art results on the LRA benchmark."
          },
          "strengths": {
            "value": "The paper presents a practical solution to the quadratic complexity of Transformers, which is a critical challenge in NLP. The empirical results on long sequences and the BERT-large pre-training demonstrate strong efficiency gains. The introduction of MaxNormActivation and Cosine Relative Positional Embeddings are novel contributions. The work also addresses real-world deployment challenges by enabling training on 16K-token sequences."
          },
          "weaknesses": {
            "value": "The novelty of DenseAttention is questionable, as the core idea of removing Softmax and using matrix associativity overlaps with prior work like Linear Transformers. The theoretical analysis is insufficient, and the paper fails to clearly differentiate its approach from existing methods. The justification for MaxNormActivation over LayerNorm is weak, and the experiments lack comprehensive comparisons with alternatives like FlashAttention or State-Space Models."
          },
          "questions": {
            "value": "How does DenseAttention fundamentally differ from Linear Transformers in terms of architecture and theoretical guarantees? What specific ablation studies demonstrate the necessity of MaxNormActivation over LayerNorm or other normalization techniques? Are the claimed O(N) complexity results validated with detailed computational analysis, including memory usage and hardware-specific optimizations?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces DenseAttention, a simplified Transformer architecture that claims to achieve linear time and space complexity by removing components like Softmax, LayerNorms, and key/value projections. It leverages matrix multiplication associativity and introduces MaxNormActivation for numerical stability. The approach is evaluated on long-sequence tasks, showing competitive performance with BERT-large and state-of-the-art results on the LRA benchmark."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in Transformers—quadratic complexity—and proposes a novel simplification with clear practical benefits. The empirical results on long sequences and the BERT-large pre-training demonstrate significant efficiency gains. The clarity of the architecture description and mathematical formulations is strong, and the significance of improving hardware efficiency for large-scale models is high. The introduction of MaxNormActivation and Cosine RelPE shows creativity in addressing stability and positional encoding challenges."
          },
          "weaknesses": {
            "value": "The paper's novelty is questionable, as similar ideas (e.g., Linear Transformers) have been explored previously. The rebuttal clarifies differences, but the original submission did not sufficiently differentiate DenseAttention from prior work. The mathematical analysis (e.g., Proposition 1) is incomplete or lacks rigor, undermining confidence in the theoretical claims. The experiments lack comparison with state-of-the-art long-context models like Sparse Attention or State-Space Models. The justification for MaxNormActivation over alternatives like LayerNorm is weak, despite the rebuttal's arguments."
          },
          "questions": {
            "value": "1. How does DenseAttention fundamentally differ from Linear Transformers [1] in terms of architecture and theoretical guarantees? 2. What is the exact theoretical basis for the O(N) complexity claim, and why does the paper's analysis of Proposition 1 fall short? 3. Why is MaxNormActivation necessary when l2 normalization (or LayerNorm) could theoretically suffice, given norm equivalence in finite dimensions? 4. How does the paper address the risk of numerical instability in real-world scenarios with fp16/bf16 precision, beyond MaxNormActivation?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces DenseAttention, a simplified Transformer architecture that claims to achieve O(N) time and space complexity by eliminating components like Softmax, LayerNorms, and projection matrices. It leverages matrix multiplication associativity and introduces MaxNormActivation for numerical stability. The method is evaluated on the LRA benchmark and shows competitive performance with BERT-large on long sequences."
          },
          "strengths": {
            "value": "The paper presents a novel approach to reducing Transformer complexity by removing redundant components, which could have significant implications for efficiency. The empirical results on long sequences and the LRA benchmark demonstrate practical utility. The clear explanation of the architectural changes and the introduction of MaxNormActivation as a novel normalization technique are notable strengths. The paper also addresses the challenge of handling extremely long contexts with LocalAttention variations."
          },
          "weaknesses": {
            "value": "The paper's claims about novelty are questionable, as the core mechanism (removing Softmax and using matrix associativity) has been explored in prior work (e.g., Linear Transformers). The rebuttal clarifies differences, but the paper fails to adequately contextualize these distinctions. The justification for MaxNormActivation over alternatives like LayerNorm or L2 normalization is insufficient, with limited empirical evidence. The experimental validation lacks ablation studies on removed components (e.g., Softmax, LayerNorms) and detailed comparisons with state-of-the-art methods beyond FlashAttention. The theoretical analysis of numerical stability is weak, and the paper does not address potential issues with exploding gradients in practice."
          },
          "questions": {
            "value": "1. How does DenseAttention fundamentally differ from Linear Transformers, given the rebuttal's arguments? 2. What ablation studies demonstrate the necessity of removing components like Softmax and LayerNorms? 3. Why is MaxNormActivation more effective than LayerNorm or L2 normalization, and how was this validated? 4. How does the paper address the risk of exploding gradients in the absence of Softmax and reweighting? 5. What are the exact implementation details of Cosine Relative Positional Embeddings and their advantages over RoPE?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "2bWf4M5tRo": {
    "paper_id": "2bWf4M5tRo",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes a novel approach to enhance hallucination detection in large language models (LLMs) by injecting noise into intermediate layers of the model, complementing traditional sampling-based uncertainty estimation. The authors demonstrate that perturbing hidden unit activations increases model uncertainty during hallucinations, leading to improved detection accuracy across multiple datasets and models."
          },
          "strengths": {
            "value": "The paper presents a clear, well-structured problem statement and introduces a novel method that combines noise injection with existing sampling techniques. The experiments are comprehensive, covering multiple datasets (e.g., GSM8K, TriviaQA) and models (e.g., Llama2, Mistral). The use of Answer Entropy as a domain-specific uncertainty metric for reasoning tasks is particularly insightful. The theoretical analysis linking noise injection to model uncertainty is sound, and the rebuttal strengthens the statistical validity of results through bootstrap confidence intervals and t-tests."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with state-of-the-art hallucination detection methods, which limits the assessment of its relative effectiveness. The computational complexity and practical trade-offs of the proposed method are not thoroughly discussed, despite the authors' acknowledgment of this issue. Additionally, while the rebuttal addresses statistical significance, the use of only 5 generations for some metrics may still raise concerns about the reliability of Monte Carlo estimates. The paper also does not fully explore the impact of noise injection on model accuracy or its generalizability to non-reasoning tasks."
          },
          "questions": {
            "value": "1. How does the proposed method compare to existing hallucination detection techniques (e.g., semantic consistency checks, knowledge-based QA)? 2. What are the computational costs of noise injection compared to standard sampling, and how does this affect scalability? 3. Are there specific types of hallucinations (e.g., factual vs. logical) where noise injection is less effective? 4. How does the method perform on tasks beyond mathematical reasoning, such as open-ended dialogue or code generation?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes enhancing hallucination detection in large language models (LLMs) by injecting noise into intermediate layers of the model, complementing traditional sampling-based uncertainty estimation. The authors demonstrate that this approach improves detection accuracy by introducing an additional source of randomness, validated through experiments on datasets like GSM8K and across multiple model architectures."
          },
          "strengths": {
            "value": "The paper makes a clear contribution by introducing a novel method for hallucination detection that addresses limitations of existing sampling-based approaches. The empirical analysis is comprehensive, with experiments on diverse datasets (e.g., GSM8K, TriviaQA) and models (e.g., Llama2, Mistral). The introduction of Answer Entropy as a task-specific uncertainty metric for reasoning tasks is innovative. The figures (e.g., Figure 2, Figure 3) effectively illustrate the complementary effects of noise injection and prediction layer sampling. The paper also acknowledges practical trade-offs between complexity and performance, which adds depth to the discussion."
          },
          "weaknesses": {
            "value": "The paper lacks a rigorous theoretical justification for why noise injection in intermediate layers provides complementary uncertainty estimates. While the experiments show improvements, the mechanisms underlying the observed effects are not thoroughly analyzed. The sensitivity analysis of noise magnitude is useful but does not address how to systematically choose optimal noise parameters across tasks. The paper also does not fully discuss the computational overhead of noise injection or its scalability to larger models. Additionally, the statistical significance of results (e.g., AUROC improvements) is only briefly addressed in the rebuttal, leaving some questions about robustness."
          },
          "questions": {
            "value": "1. How does the paper theoretically justify the complementary relationship between noise injection and prediction layer sampling? 2. What criteria were used to select the specific layers (e.g., 25–40 transformer layers) for noise injection, and how might this choice affect generalizability? 3. How does the Answer Entropy metric handle cases where answers are semantically similar but not identical (e.g., '3' vs. 'three')? 4. What are the computational costs of noise injection, and how do they scale with model size or dataset complexity? 5. How does the method perform on non-reasoning tasks (e.g., open-ended generation) beyond the evaluated datasets?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper explores enhancing hallucination detection in large language models (LLMs) by introducing noise injection into intermediate layers of the model. The authors argue that traditional methods relying on prediction layer sampling are suboptimal and propose combining noise injection with sampling to improve uncertainty estimation. Experiments on GSM8K and other datasets show improved detection accuracy, with empirical validation of complementary effects between the two randomness sources."
          },
          "strengths": {
            "value": "The paper demonstrates clear empirical improvements in hallucination detection through noise injection, supported by experiments across multiple datasets and models. The proposed method addresses a critical problem in LLM safety, and the analysis of complementary effects between sampling and noise injection is novel. The rebuttal provides statistical validation (bootstrap, t-tests) that strengthens the claims. The paper is well-structured with clear figures and theoretical insights."
          },
          "weaknesses": {
            "value": "The experimental scope is limited to mathematical reasoning tasks (GSM8K) and lacks evaluation on diverse task types (e.g., open-ended generation, factual QA). The paper does not thoroughly analyze computational costs or practical deployment challenges. The theoretical justification for intermediate layer perturbation remains superficial, and the sensitivity to noise magnitude and layer selection is underexplored. The rebuttal addresses some issues but leaves gaps in generalizability and ablation studies."
          },
          "questions": {
            "value": "1. How does the choice of intermediate layers (e.g., MLP vs. attention layers) impact performance? 2. What are the trade-offs between noise injection and inference latency? 3. How does the method perform on non-mathematical tasks like factual QA or code generation? 4. Can the approach be adapted to other architectures (e.g., MLLMs, vision-language models)? 5. How sensitive is the method to hyperparameters like noise magnitude and layer ranges?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "2e4ECh0ikn": {
    "paper_id": "2e4ECh0ikn",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a novel evaluation protocol for audio foundation models (FMs) focused on turn-taking dynamics in human-AI conversations. The authors propose a judge model trained on human-human interactions to assess the timing of turn-taking events, conduct a user study comparing dialogue systems, and evaluate audio FMs on curated benchmarks. The work highlights limitations in existing systems' ability to manage conversational flow, backchanneling, and interruptions."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by addressing a critical gap in evaluating conversational AI through timing-centric metrics, which go beyond traditional corpus-level statistics. The methodology is rigorous, with a well-structured user study and benchmarking framework. The clarity of the problem statement and the detailed technical description of the judge model are commendable. The significance of the work lies in its potential to advance the development of more natural and interactive audio FMs."
          },
          "weaknesses": {
            "value": "The paper lacks a direct comparison with prior turn-taking evaluation methods, such as Ekstedt and Skantze (2022), which limits the reader's ability to contextualize the novelty of the proposed protocol. The user study's sample size (authors and colleagues) and limited topic diversity may affect generalizability. The evaluation of audio FMs focuses on a narrow set of models, and the OOD generalization claims are not thoroughly substantiated. The rebuttal clarifies some differences in evaluation focus but the paper could have been clearer on these distinctions."
          },
          "questions": {
            "value": "1. How does the judge model's performance on OOD datasets (e.g., Columbia Games Corpus) compare to its performance on Switchboard? 2. What specific metrics were used to evaluate the audio FMs beyond the proposed timing-centric measures? 3. How were the pseudo-ground-truth labels generated for the user study, and what validation steps were taken to ensure their reliability? 4. Could the authors elaborate on the limitations of the current judge model in handling complex conversational scenarios (e.g., multi-party interactions)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a novel evaluation protocol to assess audio foundation models (FMs) on their turn-taking capabilities in conversational interactions. The authors propose a supervised model trained on human-human conversations to judge the timing of turn-taking events, complemented by a user study on existing dialogue systems and benchmarking of audio FMs. The work highlights limitations in current systems and provides a framework for future research."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in evaluating conversational AI by focusing on turn-taking dynamics, which is often overlooked in standard benchmarks. The proposed protocol introduces timing-centric metrics that go beyond corpus-level statistics, offering deeper insights into interaction quality. The user study provides actionable findings about existing systems, and the open-sourcing of the evaluation platform enhances reproducibility. The methodology is well-structured, with clear connections to prior work and practical relevance."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough comparison with existing metrics, such as those from Ekstedt and Skantze (2022), and does not fully justify why their approach is superior. The evaluation of audio FMs is limited to a narrow set of datasets (e.g., Switchboard), and the judge model's reliability is not rigorously validated. The user study's sample size and diversity are questionable, as participants are primarily authors and colleagues. Additionally, the paper does not address scalability challenges for real-world deployment or multi-language scenarios."
          },
          "questions": {
            "value": "How does the judge model's performance compare to human annotations in terms of reliability? What are the specific limitations of the proposed metrics in edge cases (e.g., ambiguous interruptions)? How scalable is the protocol to non-English or domain-specific conversations? Are there ablation studies demonstrating the necessity of the timing-centric approach over simpler heuristics?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a novel evaluation protocol to assess audio foundation models (FMs) on their turn-taking capabilities in conversational interactions. The authors propose a supervised judge model to predict turn-taking events, conduct user studies on existing dialogue systems, and benchmark audio FMs on curated datasets. They highlight limitations in current systems, such as aggressive interruptions and lack of backchannels, and open-source their evaluation platform."
          },
          "strengths": {
            "value": "The paper's originality lies in its focus on turn-taking dynamics, a critical yet underexplored aspect of conversational AI. The proposed evaluation protocol introduces timing-centric metrics that address gaps in prior corpus-level statistics. The user study provides actionable insights into real-world system limitations, and the open-sourcing of the platform promotes reproducibility. The clarity of the methodology and figures is strong, with clear problem formulation and technical details."
          },
          "weaknesses": {
            "value": "The evaluation protocol's reliance on a judge model raises questions about its ability to capture human-like nuance. The user study's sample size and diversity are limited (primarily authors and colleagues), which may affect generalizability. The comparison with prior work, especially Ekstedt and Skantze (2022), is not sufficiently detailed, and the paper lacks ablation studies to validate the necessity of the proposed metrics. Scalability concerns and the practicality of the judge model's training process remain unaddressed."
          },
          "questions": {
            "value": [
              "How does the judge model's performance on human-human conversations translate to its effectiveness in evaluating AI systems? Are there specific failure modes where the judge model might mislabel turn-taking events?",
              "What is the impact of the limited user study sample (authors and colleagues) on the validity of the insights about system behavior? Could alternative participant recruitment strategies improve generalizability?",
              "The rebuttal emphasizes differences between this work and prior metrics, but how do the proposed metrics quantitatively outperform existing ones in capturing turn-taking quality? Are there controlled experiments to validate this?",
              "How does the paper address the computational cost and practicality of training the judge model for real-world deployment? Are there plans to optimize this process?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "2fojNANZSv": {
    "paper_id": "2fojNANZSv",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes MIXTUREPFN, a method that combines a Mixture of In-Context Prompters (MICP) and Context-Aware Finetuning (CAPFN) to improve the scalability and effectiveness of Prior-Fitted Networks (PFNs) on large tabular datasets. MICP uses K-Means clustering to route test samples to specialized experts, while CAPFN finetunes PFNs via bootstrapping to address distribution shift. The approach achieves state-of-the-art performance across 36 datasets."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in tabular learning—PFN's inefficiency and effectiveness degradation on large datasets. The method introduces a novel combination of clustering-based routing (MICP) and bootstrapped finetuning (CAPFN), with clear theoretical analysis of complexity improvements. The experimental results demonstrate significant performance gains over 19 baselines, including achieving the Condorcet winner status. The work is well-structured, with detailed ablation studies and comparisons to related methods."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient analysis of how MICP handles high-dimensional data or the sensitivity of results to hyperparameters like the number of clusters (K). While the rebuttal mentions feature scalability as a limitation, the paper does not thoroughly discuss this. The experiments on large datasets are limited (only three datasets tested), and the use of target encoding and fixed cluster sizes may mask potential improvements. The theoretical justification for why MICP and CAPFN outperform alternatives is underdeveloped."
          },
          "questions": {
            "value": "1. How does MICP handle high-dimensional feature spaces, and what strategies are recommended for feature selection/compression? 2. What is the sensitivity of performance to the choice of K in K-Means clustering, and how is K determined in practice? 3. Can the authors provide more insight into why the number of experts (K) has a stronger impact on performance than categorical encoding methods, as suggested by ablation studies? 4. How does the method perform on datasets with extreme class imbalance, given the observed correlation (R²=0.249) between performance and balance?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes MIXTUREPFN, a method combining Sparse Mixture of In-Context Prompters (MICP) and Context-Aware Finetuning (CAPFN) to improve the efficiency and effectiveness of Prior-Fitted Networks (PFNs) for tabular data. MICP clusters training data and routes test samples to specialized prompters, while CAPFN addresses distribution shift via bootstrapping. The approach demonstrates superior performance across 36 datasets compared to 19 baselines."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to scaling PFNs for large tabular datasets by addressing efficiency bottlenecks and distribution shift. The experimental results are comprehensive, showing strong performance metrics (mean rank, Condorcet winner). The method's design (MICP/CAPFN) effectively leverages clustering and finetuning, with clear theoretical grounding in ICL and Bayesian inference. The paper also provides detailed complexity analysis and practical implementation insights."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient comparison with closely related work, particularly LoCALPFN, which was submitted after ICLR's deadline but raises questions about novelty. The ablation studies are limited in scope (e.g., focusing on cluster counts rather than architectural components). The analysis of feature scalability limitations is cursory, with minimal discussion of how to address high-dimensional data. Some claims (e.g., 'minimal performance deterioration') lack quantitative justification."
          },
          "questions": {
            "value": "1. How does the paper address the potential overlap with LoCALPFN, given their concurrent development? The rebuttal mentions parallel work, but the paper itself does not adequately contextualize this. 2. What are the specific limitations of MICP when dealing with high-dimensional features (e.g., >1000 features)? The rebuttal notes performance degradation but does not propose solutions. 3. How sensitive is the method to hyperparameter choices (e.g., number of clusters, cluster size) across different dataset characteristics?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes MIXTUREPFN, a method combining Sparse Mixture of In-Context Prompters (MICP) and Context-Aware Finetuning (CAPFN) to address scalability and distribution shift issues in Prior Fitted Networks (PFNs) for tabular data. MICP clusters training data to enable efficient inference, while CAPFN mitigates distribution shift via bootstrapping. The method achieves strong performance on 36 datasets compared to 19 baselines."
          },
          "strengths": {
            "value": "Originality: Introduces Mixture of Experts framework for PFNs, addressing scalability and distribution shift. Quality: Comprehensive experiments across 36 datasets with statistical significance. Clarity: Well-structured paper with clear motivation and technical details. Significance: Advances tabular learning, which remains challenging for deep learning."
          },
          "weaknesses": {
            "value": "The paper lacks comparison with alternative clustering methods (e.g., hierarchical clustering) for MICP. The ablation studies in the rebuttal suggest K is more critical than encoding, but this isn't thoroughly discussed. Experiments on very large datasets (e.g., 1MM samples) are limited, with constraints on cluster size and encoding. The computational cost of the routing mechanism is not analyzed. The relationship to parallel work (LoCALPFN) is not adequately contextualized in the original submission."
          },
          "questions": {
            "value": "1. Why was K-Means chosen over other clustering methods for MICP? How does performance vary with different clustering approaches? 2. The rebuttal shows MICP outperforms LoCALPFN with more experts—does this imply scalability limitations in LoCALPFN's design? 3. How does the method handle high-dimensional data with many features, given the paper notes performance degradation in such cases? 4. What is the exact computational overhead of the routing mechanism compared to baseline methods? 5. How does the paper address the inherent feature scalability limitations of TabPFN, as acknowledged in the rebuttal?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ]
  },
  "2gTEW29qsM": {
    "paper_id": "2gTEW29qsM",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces GIT-STORM, a novel world model that enhances sequence modeling capabilities by replacing the traditional MLP prior with a MaskGIT prior, as well as addressing continuous control tasks through a state mixer function. The method achieves state-of-the-art results on the Atari 100k benchmark and demonstrates effectiveness in continuous action environments via experiments on the DeepMind Control Suite."
          },
          "strengths": {
            "value": "The paper makes a clear contribution by integrating MaskGIT into world models, which offers a novel inductive bias for sequence modeling. The experimental results on Atari 100k and DMC demonstrate practical improvements, and the state mixer for continuous actions addresses a significant gap in prior work. The methodology is well-structured, and the paper provides comprehensive comparisons with existing models. The use of transformers for world models aligns with current trends in sequence modeling."
          },
          "weaknesses": {
            "value": "The paper lacks depth in explaining how the MaskGIT prior specifically improves performance over MLP priors, with limited ablation studies on the impact of discrete representation quality. The claim of being the first to apply transformer-based models to continuous control tasks is overstated, as the rebuttal highlights existing work (e.g., VLA models) that use similar approaches. The model design explanations and figures are confusing, and the paper does not fully address the limitations of the state mixer in complex environments. The rebuttal's additional experiments on action conditioning are not sufficiently detailed in the original manuscript."
          },
          "questions": {
            "value": "1. How does the MaskGIT prior explicitly improve sequence modeling compared to MLP priors? Are there ablation studies on the impact of different masking strategies? 2. The claim of 'first time' applying transformer-based models to continuous control needs stronger justification, especially given existing work like VLA and iVideoGPT. 3. The rebuttal mentions experiments comparing the state mixer with iVideoGPT, but the paper does not include these results. Could the authors provide additional analysis on the effectiveness of the state mixer? 4. The performance on DMC is reported as outperformed by DreamerV3, but the rebuttal suggests this may be due to training scale. Could the authors clarify the experimental setup and hyperparameters for this comparison?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces GIT-STORM, a novel world model that enhances sequence modeling capabilities by replacing the MLP prior in STORM with a MaskGIT prior. The approach is evaluated on RL tasks (Atari 100k) and continuous control tasks (DMC), with claims of improved performance and the first application of transformer-based world models to continuous actions via a state mixer function."
          },
          "strengths": {
            "value": "The paper presents a clear innovation in replacing MLP priors with MaskGIT for world models, demonstrating significant performance gains on Atari 100k. The application to continuous control tasks via a state mixer addresses a gap in prior work. The experiments include comprehensive ablation studies and comparisons against strong baselines like DreamerV3 and IRIS. The method builds on prior work (e.g., TECO) while introducing novel architectural choices."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient ablation studies on the state mixer's design and its impact on continuous control tasks. The model explanation in Figure 1 is ambiguous, with unclear roles for masked inputs and the dot product mechanism. The claim of 'first application to continuous actions' is overstated, as similar approaches exist in VLA and action-conditioned models. The DMC results are not thoroughly analyzed, and the rebuttal's experimental comparisons require more detail."
          },
          "questions": {
            "value": "1. How does the state mixer explicitly handle the interaction between categorical latent states and continuous actions? 2. What ablation studies were performed to validate the necessity of the MaskGIT prior versus other priors? 3. How does the paper address the limitations of the state mixer in high-dimensional continuous spaces? 4. What specific design choices in GIT-STORM enable it to outperform DreamerV3 on DMC despite the rebuttal's claims?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces GIT-STORM, a world model that replaces the MLP prior in STORM with a MaskGIT prior to improve sequence modeling for reinforcement learning (RL) and video prediction. It claims performance gains on Atari 100k and extends transformer-based world models to continuous control tasks via a state mixer function."
          },
          "strengths": {
            "value": "The paper presents a novel combination of MaskGIT priors with world models, demonstrating significant improvements on Atari 100k. The application to continuous control tasks is a meaningful contribution. The experiments include thorough ablation studies and comparisons against baselines like DreamerV3 and IRIS. The work addresses a gap in prior research by enabling transformer-based models for continuous actions."
          },
          "weaknesses": {
            "value": "The model description and figures (e.g., Figure 1) are unclear, with confusion about how masked tokens are used. The claim of 'first-time' application to continuous actions is overstated, as similar approaches exist in VLA and action-conditioned models. The DMC results are ambiguous, as the rebuttal suggests DreamerV3 outperforms GIT-STORM. The paper lacks citations for the dot product weight-tying technique and does not fully address why the state mixer improves performance over simpler baselines."
          },
          "questions": {
            "value": "1. How does the MaskGIT prior specifically improve sequence modeling compared to MLP? What ablation studies support this? 2. Why is the state mixer's contribution not clearly quantified against simpler baselines like action concatenation? 3. The DMC results contradict the paper's claims—what explains this discrepancy? 4. Are there existing works that use weight-tying via dot products in similar contexts? 5. How does the state mixer avoid the limitations of iVideoGPT's approach, as suggested by the rebuttal?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "2hbgKYuao1": {
    "paper_id": "2hbgKYuao1",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces HyMN, a hybrid approach combining Subgraph GNNs and Structural Encodings (SEs) using walk-based centrality measures. The method leverages centrality scores for efficient subgraph selection and as SEs to enhance model expressiveness, achieving competitive performance with reduced computational costs."
          },
          "strengths": {
            "value": "The paper addresses a critical problem of computational complexity in Subgraph GNNs while maintaining expressiveness. The theoretical connection between centrality-based sampling and perturbation analysis is novel. The experiments are comprehensive, covering synthetic and real-world datasets, and demonstrate HyMN's efficiency and effectiveness. The combination of centrality for subgraph selection and as SEs is a creative and well-justified approach."
          },
          "weaknesses": {
            "value": "The theoretical analysis relies on an upper bound that does not guarantee practical effectiveness, and the paper could better justify why walk-based centrality (particularly Subgraph Centrality) is superior to other measures. The experiments lack detailed runtime comparisons with baselines, and the claim of SC's superiority is not sufficiently supported. The paper also does not thoroughly explore how different centrality measures interact with the model's performance across tasks."
          },
          "questions": {
            "value": "How does the proposed centrality-based sampling compare to other sampling strategies in terms of computational efficiency and performance? What are the specific scenarios where SC excels compared to other centrality measures? Can the paper provide ablation studies on the impact of different centrality metrics on model performance? How does HyMN's runtime scale with larger graphs compared to existing methods?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces HyMN, a hybrid approach combining subgraph GNNs with walk-based centrality measures to balance efficiency and expressiveness. The method uses centrality scores for subgraph selection and as structural encodings, leveraging perturbation analysis to justify its design. Experiments demonstrate competitive performance with reduced computational costs compared to existing subgraph GNNs and state-of-the-art models."
          },
          "strengths": {
            "value": "The paper makes a novel contribution by connecting centrality-based sampling to perturbation analysis, providing a theoretical foundation for subgraph selection. The approach effectively reduces computational complexity while maintaining performance, as shown through experiments on synthetic and real-world datasets. The integration of centrality as both a sampling strategy and structural encoding is creative and addresses key limitations of MPNNs. The clarity of the problem statement and the logical flow of the analysis are strong."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with alternative centrality measures (e.g., betweenness, Katz index) to validate the superiority of Subgraph Centrality (SC). While the rebuttal provides additional experiments, the original submission did not thoroughly explore how SC performs relative to other measures across diverse tasks. The ablation studies are limited, and the paper does not address potential limitations of centrality-based sampling in graphs with specific structural properties. The theoretical analysis, while insightful, is not fully leveraged to guide the design of the sampling strategy."
          },
          "questions": {
            "value": "1. How does HyMN perform on graphs with non-homogeneous structures or varying degrees of connectivity? 2. What are the specific scenarios where SC outperforms other centrality measures, and how can the method be adapted to leverage alternative measures? 3. Are the experimental results on real-world datasets sufficiently robust to generalization, or do they depend on specific dataset characteristics? 4. How does the computational efficiency of HyMN scale with larger graphs compared to existing subgraph GNNs?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces HyMN, a method that combines subgraph GNNs with walk-based centrality measures to balance efficiency and expressiveness. By leveraging centrality scores for subgraph selection and as structural encodings, HyMN aims to reduce computational costs while maintaining competitive performance. The approach is validated through experiments on synthetic and real-world datasets."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in subgraph GNNs—high computational costs—by proposing an efficient sampling strategy based on centrality measures. The theoretical analysis linking centrality to perturbation effects is novel, and the experiments demonstrate competitive performance with reduced runtime. The method’s simplicity and lack of learnable components are significant advantages. The rebuttal strengthens the empirical claims with additional results."
          },
          "weaknesses": {
            "value": "The theoretical justification for using centrality-based sampling remains limited, as the upper-bound analysis does not conclusively show why high-centrality nodes are optimal. The comparison with alternative centrality measures is insufficient, and the paper does not adequately acknowledge that other measures might outperform Subgraph Centrality in specific contexts. The computational efficiency claims lack detailed runtime comparisons with baselines. The experiments focus on synthetic tasks and a limited set of real-world datasets, leaving room for broader validation."
          },
          "questions": {
            "value": "1. How does HyMN handle graphs where centrality measures fail to capture relevant structural information? 2. What is the theoretical basis for prioritizing high-centrality nodes over other candidates? 3. Can the authors provide a more comprehensive comparison of different centrality measures across diverse tasks? 4. How does HyMN’s runtime scale with graph size compared to existing methods?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  },
  "2kfpkTD5ZE": {
    "paper_id": "2kfpkTD5ZE",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes FMG, a method that leverages multi-modal foundation models (MMFMs) to induce interpretable domain-specific languages (DSLs) for molecular graphs. By integrating clique tree decomposition with MMFM decision-making, FMG enables data-efficient molecular generation with chemical interpretability through multi-modal alignment and prompt learning."
          },
          "strengths": {
            "value": "The paper introduces a novel approach by combining MMFMs with graph decomposition for DSL induction, addressing the gap in automated, interpretable molecular design. The method's emphasis on interpretability via 'design narratives' and its potential to bypass human expertise are significant contributions. The integration of multi-modal alignment (images and text) and the use of non-expert LLM judges for validation demonstrate creative problem-solving. The theoretical foundation in clique tree decomposition is well-structured and relevant to the task."
          },
          "weaknesses": {
            "value": "The paper lacks concrete experimental validation, with no specific metrics, benchmarks, or comparisons to existing methods. The role of the MMFM in ensuring chemical validity is unclear, and the paper does not address potential biases or errors in MMFM reasoning. The methodology description is vague on critical steps, such as how MMFM selections translate to DSL rules or how the non-expert LLM judge evaluates correctness. The claims about data-efficiency and diversity remain unproven."
          },
          "questions": {
            "value": "1. What specific benchmarks and metrics were used to evaluate FMG's synthesizability, diversity, and data-efficiency? 2. How is the MMFM's chemical validity ensured during selection steps, and what mechanisms prevent erroneous decisions? 3. How does the non-expert LLM judge's evaluation process work, and what criteria are used to assess DSL quality? 4. What are the limitations of the MMFM's zero-shot capabilities in this context, and how are they mitigated? 5. How does FMG scale to complex molecular structures, and what are the computational costs of the tree decomposition process?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes a method called Foundation Molecular Grammar (FMG) that leverages multi-modal foundation models (MMFMs) to automatically induce interpretable domain-specific languages (DSLs) for molecular graph generation. The approach frames DSL construction as a tree decomposition problem, where MMFMs make sequential decisions to merge, select, and prune cliques in a molecular graph, guided by multi-modal prompts and human-like reasoning."
          },
          "strengths": {
            "value": "The paper introduces a novel framework that bridges MMFMs with graph-based DSL induction, addressing the critical challenge of data-efficiency and interpretability in molecular generation. The method's theoretical grounding in clique tree decomposition and its use of MMFMs for decision-making modules demonstrate originality. The clarity of the methodology is strong, with detailed descriptions of the algorithmic steps and multi-modal prompting strategies. The significance is high, as it tackles a key limitation in generative molecular design by enabling automated DSL creation without human intervention."
          },
          "weaknesses": {
            "value": "The paper lacks concrete experimental validation to substantiate claims of 'significant advantages' in synthesizability, diversity, and data-efficiency. Key details about the MMFM's decision-making process (e.g., how it translates reasoning into tree decomposition steps) are abstract and under-specified. The role of non-expert LLM judges in evaluating DSL quality is unclear, and the paper does not address potential limitations, such as scalability to complex molecules or dependency on pretraining data. The connection between the generated DSLs and actual molecule generation is not explicitly demonstrated."
          },
          "questions": {
            "value": [
              "How is the performance of FMG quantitatively evaluated against existing methods? What specific benchmarks and metrics were used?",
              "What is the exact mechanism by which MMFM's reasoning steps are translated into tree decomposition operations? Are there cases where the MMFM's decisions might lead to invalid molecular structures?",
              "How does the method handle molecules with highly complex or non-trivial clique structures that deviate from the assumed decomposition framework?",
              "What is the role of the non-expert LLM judge in the DSL evaluation process? How is their 'correctness and persuasiveness' quantified?",
              "Are the generated DSLs explicitly used to generate new molecules, and if so, how does their quality compare to baselines in terms of chemical validity and novelty?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes FMG, a method to induce interpretable molecular domain-specific languages (DSLs) using multi-modal foundation models (MMFMs). It frames DSL construction as a clique tree decomposition problem, leveraging MMFMs to make decisions during tree decomposition, and uses their reasoning steps to generate interpretable 'design narratives'. The approach aims to improve data-efficiency, synthesizability, and transparency in molecular generation."
          },
          "strengths": {
            "value": "The paper presents a novel approach by combining MMFMs with clique tree decomposition for DSL induction, addressing the lack of interpretable generative models in molecular design. The method's focus on interpretability through 'design narratives' is a significant contribution. The theoretical foundation in graph decomposition and the use of MMFMs for zero-shot reasoning are well-motivated. The paper also highlights practical advantages like data-efficiency, which is critical for domains with limited data."
          },
          "weaknesses": {
            "value": "The paper lacks concrete experimental validation. It mentions benchmarks but provides no details on datasets, baselines, or quantitative metrics for synthesizability, diversity, or data-efficiency. The role of the non-expert LLM judge in evaluating DSL quality is unclear. The method's reliance on MMFMs' zero-shot capabilities is not empirically verified, and the paper does not address potential limitations of the clique tree approach (e.g., computational complexity). The comparison to existing DSL induction methods (e.g., manual or algorithmic) is insufficient."
          },
          "questions": {
            "value": "1. How were the benchmarks for evaluating synthesizability/diversity defined? What datasets were used, and how do they compare to prior work? 2. What specific metrics were used to assess the DSL's interpretability, and how were they validated? 3. How does the non-expert LLM judge's evaluation process work, and what evidence supports its reliability? 4. Are there ablation studies showing the impact of MMFM-driven decisions vs. traditional heuristics? 5. How does FMG handle cases where the MMFM's reasoning is incorrect or inconsistent?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ]
  }
}