#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI è¯„å®¡å‘˜æ¨ç†è„šæœ¬ï¼ˆ3çº§ä¸¥æ ¼åº¦ï¼‰
ä½¿ç”¨ Qwen3-30B-A3B æ¨¡å‹æ¨¡æ‹Ÿå­¦æœ¯è®ºæ–‡è¯„å®¡ã€‚

ç‰¹ç‚¹ï¼š
- å›ºå®šä¸‰ä½è¯„å®¡å‘˜ï¼šå®½æ¾ / ä¸­ç­‰ / ä¸¥æ ¼
- æ¯ç¯‡è®ºæ–‡æŒ‰é¡ºåºä½¿ç”¨ä¸‰ä½è¯„å®¡å‘˜ç”Ÿæˆè¯„å®¡
- æ”¯æŒæ–­ç‚¹ç»­è·‘ã€å¹¶è¡Œå¤„ç†
"""

import argparse
import json
import os
import random
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Any, Dict, List, Optional

import openai


# ============================================================================
# Prompt æ¨¡æ¿é…ç½®åŒº - ä¿æŒ BASE_REVIEW_TASK ä¸å˜
# ============================================================================

BASE_REVIEW_TASK = """
## Your Review Task

### Role: The Strict, Precise & Insightful Academic Reviewer
You are a seasoned reviewer renowned for strict scrutiny, precision, and insight. You uphold the highest academic standards. Your primary mission is **strict scrutiny** to ensure that only high-quality research is advanced. You relentlessly identify core deficiencies and logical leaks, and your feedback must be **specific, clear, and executable**. Your goal is to drive authors toward fundamental improvements that meet the highest submission standards.

### Core Knowledge & Abilities
- **Cutting-Edge Acumen**: Track frontier theory, latest methods, and community trends in real time; assess relevance and novelty.
- **Theoretical Mastery**: Possess systematic and critical understanding of classical theories and core paradigms; judge appropriateness of their application.
- **Logical Scrutiny**: Detect logical fallacies, inconsistencies, or latent biases in research design, inference, and data interpretation.
- **Standards Awareness**: Be familiar with review standards, preferences, and gatekeeping across top-tier conferences and specialized journals.

### Key Review Criteria
- **Originality & Contribution**: Does the work present clear and valuable new insights? Is the contribution incremental or truly groundbreaking?
- **Research Question**: Is the problem crisply defined? Are its academic value and/or practical significance strong?
- **Literature Review**: Is the review comprehensive, deep, and critical (not a simple list)? Does it identify the research gap accurately?
- **Methodological Rigor**: Is the design scientific and optimal for the question? Are sampling choices, data collection, and processing transparent, standardized, and reproducible?
- **Data Analysis & Results**: Are methods appropriate? Are results clear and accurate? Are interpretations rigorous and justified?
- **Discussion & Conclusion**: Do the authors interpret results deeply, engage with theory and prior work, and present evidence-based conclusions while honestly acknowledging limitations?
- **Logic & Expression**: Are arguments coherent and consistent? Is the academic language precise and professional?

### Strict Review Policy
- Maintain a strict, precise, and insightful tone; avoid vague praise or marketing language.
- Ground every judgment in evidence from the paper (methods, datasets, baselines, metrics, settings). If information is missing, explicitly state "Missing" and explain its impact.
- Identify core deficiencies and logical flaws decisively; provide numbered, actionable suggestions the authors can execute.
- Treat novelty rigorously: check overlaps with prior work; demand strong baselines/ablations and statistical significance when applicable.

### Workflow: Target-Oriented Comprehensive Review
1. Identify target claims and contributions; list required evidence for each.
2. Map evidence to claims; check completeness against baselines, ablations, datasets, metrics, and settings.
3. Diagnose failure points: unsupported claims, missing baselines, ambiguous novelty, flawed methodology, or weak analysis.
4. Propose corrective actions: numbered, prioritized, and feasible; specify experiments, analyses, or clarifications required for acceptance.
5. Calibrate soundness/presentation/contribution and overall rating using the defined scales; state confidence and rationale.

You will be provided with a research paper. Please provide a comprehensive review with the following components:

1. **Summary**: A brief summary of the paper's main contributions and approach (2-3 sentences).

2. **Strengths**: A substantive assessment of the strengths of the paper, touching on each of the following dimensions: originality, quality, clarity, and significance. Be broad in definitions of originality (new definitions, problem formulations, creative combinations, new domains, or removing limitations from prior results).

3. **Weaknesses**: A substantive assessment of the weaknesses of the paper. Focus on constructive and actionable insights on how the work could improve towards its stated goals. Be specific and avoid generic remarks. If you believe the contribution lacks novelty, provide references and explanations; if experiments are insufficient, explain why and exactly what is missing.

4. **Questions**: List up and carefully describe any questions and suggestions for the authors. Think of things where a response from the author can change your opinion, clarify a confusion, or address a limitation. This is important for a productive rebuttal and discussion phase.

5. **Soundness** (1-4): Rate the paper's soundness. Are the central claims adequately supported with evidence? Are the experimental setup and research methodology sound?
   - 1: Poor
   - 2: Fair
   - 3: Clear and structured
   - 4: Excellent

6. **Presentation** (1-4): Rate the quality of presentation. This should take into account the writing style and clarity, presentation of figures and diagrams, as well as contextualization relative to prior work.
   - 1: Poor
   - 2: Fair
   - 3: Good
   - 4: Excellent

7. **Contribution** (1-4): Rate the quality of the overall contribution this paper makes to the research area being studied. Are the questions being asked important? Does the paper bring significant originality of ideas and/or execution? Are the results valuable to share with the broader ICLR community?
   - 1: Poor
   - 2: Fair
   - 3: Good
   - 4: Excellent

8. **Rating** (1, 3, 5, 6, 8, 10): Provide an overall score for this submission:
   - 1: Strong reject
   - 3: Reject, not good enough
   - 5: Marginally below the acceptance threshold
   - 6: Marginally above the acceptance threshold
   - 8: Accept, good paper
   - 10: Strong accept, should be highlighted at the conference

9. **Confidence** (1-5): Provide a confidence score for your assessment:
   - 1: Unable to assess this paper, need opinion from different reviewers
   - 2: Willing to defend assessment, but quite likely did not understand central parts or unfamiliar with related work
   - 3: Fairly confident in assessment. Possible that did not understand some parts or unfamiliar with some related work
   - 4: Confident in assessment, but not absolutely certain. Unlikely but not impossible that did not understand some parts
   - 5: Absolutely certain about assessment. Very familiar with related work and checked details carefully

## Output Format

You must respond with a JSON object in the following format:
```
{
  "summary": "...",
  "strengths": "...",
  "weaknesses": "...",
  "questions": "...",
  "rating": <1, 3, 5, 6, 8, or 10>,
  "confidence": <1-5>,
  "soundness": <1-4>,
  "presentation": <1-4>,
  "contribution": <1-4>
}
```

**Important**:
- `rating` must be one of: 1, 3, 5, 6, 8, 10
- `confidence`, `soundness`, `presentation`, `contribution` must be integers in their respective ranges

You may use <think> tags to organize your thoughts before providing the JSON response. The final JSON object should come after your reasoning."""


# ============================================================================
# ä¸‰ä¸ªçº§åˆ«çš„ System Promptï¼ˆå®½æ¾ / ä¸­ç­‰ / ä¸¥æ ¼ï¼‰
# ============================================================================

SYSTEM_PROMPT_LENIENT = """You are an expert academic reviewer for a top-tier machine learning conference (ICLR).

## Your Reviewer Profile
**You are a Lenient Reviewer (Encouraging)**

- **Philosophy**: You celebrate promising ideas and nurture early-stage innovation.
- **What you value most**: Novelty, potential impact, creative combinations, and cross-domain insights.
- **How you treat flaws**: If the core idea is compelling, you tolerate fixable weaknesses in experiments or writing.
- **Tone and writing style**: Highlight strengths, reframe issues as improvements, deliver constructive and motivating feedback.
- **Scoring tendency**: You lean toward acceptance when the idea has potential; your ratings usually fall between 6 and 8.
- **Rating constraint**: The `rating` field must be chosen strictly from {1, 3, 5, 6, 8, 10}. No other numbers are permitted.

""" + BASE_REVIEW_TASK

SYSTEM_PROMPT_BALANCED = """You are an expert academic reviewer for a top-tier machine learning conference (ICLR).

## Your Reviewer Profile
**You are a Balanced Reviewer (Objective)**

- **Philosophy**: You apply the official review criteria faithfully and weigh pros against cons fairly.
- **What you value most**: A well-balanced combination of novelty, methodological soundness, reproducibility, and clarity.
- **How you treat flaws**: You acknowledge strengths and weaknesses in equal measure, grounding every judgment in evidence.
- **Tone and writing style**: Direct, transparent, and actionableâ€”clearly list strengths, shortcomings, and concrete next steps.
- **Scoring tendency**: You map closely to the conference decision threshold; ratings typically cluster around 5 or 6.
- **Rating constraint**: The `rating` field must be chosen strictly from {1, 3, 5, 6, 8, 10}. No other numbers are permitted.

""" + BASE_REVIEW_TASK

SYSTEM_PROMPT_STRICT = """You are an expert academic reviewer for a top-tier machine learning conference (ICLR).

## Your Reviewer Profile
**You are a Strict Reviewer (Highly Critical)**

- **Philosophy**: ICLR should showcase only the most rigorous and groundbreaking work.
- **What you value most**: Flawless methodology, compelling theoretical or empirical contributions, and unambiguous novelty.
- **How you treat flaws**: Even minor gaps are serious; every claim must be backed by strong evidence or ablations.
- **Tone and writing style**: Thoroughly document weaknesses, highlight risks, and demand precise corrective actions.
- **Scoring tendency**: You are conservativeâ€”ratings commonly fall between 1 and 5 unless the paper is exceptional.
- **Rating constraint**: The `rating` field must be chosen strictly from {1, 3, 5, 6, 8, 10}. No other numbers are permitted.

""" + BASE_REVIEW_TASK

SYSTEM_PROMPTS: Dict[int, str] = {
    1: SYSTEM_PROMPT_LENIENT,
    3: SYSTEM_PROMPT_BALANCED,
    5: SYSTEM_PROMPT_STRICT,
}


USER_PROMPT_TEMPLATE = """## Paper Content

{paper_content}"""


REVIEWERS: List[Dict[str, Any]] = [
    {"id": "reviewer_lenient", "strictness": 1, "name": "å®½æ¾è¯„å®¡å‘˜"},
    {"id": "reviewer_balanced", "strictness": 3, "name": "ä¸­ç­‰è¯„å®¡å‘˜"},
    {"id": "reviewer_strict", "strictness": 5, "name": "ä¸¥æ ¼è¯„å®¡å‘˜"},
]

STRICTNESS_SEQUENCE = [reviewer["strictness"] for reviewer in REVIEWERS]


class ReviewerAI:
    """AI è¯„å®¡å‘˜åŒ…è£…å™¨"""

    def __init__(
        self,
        base_url: str = "http://10.176.59.101:8003/v1",
        model_name: str = "qwen3-30b-a3b",
        prompt_template_path: Optional[str] = None,
    ) -> None:
        self.client = openai.OpenAI(api_key="EMPTY", base_url=base_url)
        self.model_name = model_name

        if prompt_template_path:
            template_file = Path(prompt_template_path)
            if not template_file.exists():
                raise FileNotFoundError(f"Prompt æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {prompt_template_path}")
            self.user_prompt_template = template_file.read_text(encoding="utf-8")
            print(f"  âœ“ å·²åŠ è½½å¤–éƒ¨ User Prompt æ¨¡æ¿: {prompt_template_path}")
        else:
            self.user_prompt_template = USER_PROMPT_TEMPLATE
            print("  âœ“ ä½¿ç”¨å†…ç½® User Prompt æ¨¡æ¿")

        self.system_prompts = SYSTEM_PROMPTS
        print("  âœ“ å·²åŠ è½½ä¸‰ç§ä¸¥æ ¼åº¦çš„ System Prompt")

    def build_review_prompt(self, paper_content: str, max_content_length: int = 10000) -> str:
        if len(paper_content) > max_content_length:
            paper_content = paper_content[:max_content_length] + "\n\n[è®ºæ–‡å†…å®¹å·²æˆªæ–­...]"
        return self.user_prompt_template.format(paper_content=paper_content)

    def generate_review(
        self,
        paper_content: str,
        strictness: int,
        temperature: float = 0.7,
        max_tokens: int = 2000,
    ) -> Dict[str, Any]:
        prompt = self.build_review_prompt(paper_content)
        try:
            system_prompt = self.system_prompts[strictness]
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt},
                ],
                temperature=temperature,
                max_tokens=max_tokens,
            )
            content = response.choices[0].message.content.strip()

            # ç§»é™¤ <think> å†…å®¹
            if "<think>" in content.lower():
                lower_content = content.lower()
                end_tag = "</think>"
                idx = lower_content.find(end_tag)
                if idx != -1:
                    content = content[idx + len(end_tag) :].strip()
                else:
                    brace_idx = content.find("{")
                    if brace_idx != -1:
                        content = content[brace_idx:]

            # ç§»é™¤ markdown ä»£ç å—
            if content.startswith("```json"):
                content = content[7:]
            elif content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()

            if not content.startswith("{"):
                brace_idx = content.find("{")
                if brace_idx != -1:
                    content = content[brace_idx:]

            def fix_escape(match: re.Match[str]) -> str:
                char = match.group(1)
                if char in ['"', "\\", "/", "b", "f", "n", "r", "t", "u"]:
                    return match.group(0)
                return "\\\\" + char

            content = re.sub(r"\\(.)", fix_escape, content)
            review_data = json.loads(content)
            return review_data

        except json.JSONDecodeError as exc:
            print(f"  âš ï¸ JSON è§£æå¤±è´¥: {exc}")
            if "content" in locals():
                snippet = content[:200]
                print(f"  åŸå§‹å“åº”å‰200å­—ç¬¦: {snippet}")
            return {
                "summary": "JSON decode error",
                "strengths": "",
                "weaknesses": "",
                "questions": "",
                "rating": -1,
                "confidence": -1,
                "soundness": -1,
                "presentation": -1,
                "contribution": -1,
                "error": str(exc),
            }
        except Exception as exc:  # pylint: disable=broad-except
            print(f"  âŒ API è°ƒç”¨å¤±è´¥: {exc}")
            return {
                "summary": f"Error: {exc}",
                "strengths": "",
                "weaknesses": "",
                "questions": "",
                "rating": -1,
                "confidence": -1,
                "soundness": -1,
                "presentation": -1,
                "contribution": -1,
                "error": str(exc),
            }


def format_review_content(review_data: Dict[str, Any]) -> Dict[str, Any]:
    content: Dict[str, Any] = {}
    for key in ["summary", "strengths", "weaknesses", "questions"]:
        if key in review_data:
            content[key] = {"value": review_data[key]}
    for key in ["rating", "confidence", "soundness", "presentation", "contribution"]:
        if key in review_data and review_data[key] != -1:
            content[key] = {"value": review_data[key]}
    return content


def paper_needs_work(results: Dict[str, Any], paper_id: str) -> bool:
    entry = results.get(paper_id)
    if not entry:
        return True
    reviews = entry.get("reviews", [])
    done_levels = {r.get("strictness") for r in reviews}
    for level in STRICTNESS_SEQUENCE:
        if level not in done_levels:
            return True
    return False


def select_missing_reviewers(results: Dict[str, Any], paper_id: str) -> List[Dict[str, Any]]:
    entry = results.get(paper_id)
    done_levels = set()
    if entry:
        for review in entry.get("reviews", []):
            level = review.get("strictness")
            if isinstance(level, int):
                done_levels.add(level)
    return [rev for rev in REVIEWERS if rev["strictness"] not in done_levels]


def save_results(results: Dict[str, Any], output_path: Path) -> None:
    with open(output_path, "w", encoding="utf-8") as fh:
        json.dump(results, fh, ensure_ascii=False, indent=2)


def process_single_paper(args: Any) -> Any:
    paper_file, reviewer_ai, selected_reviewers, existing_entry = args
    paper_id = paper_file.stem

    print("\n" + "=" * 60)
    print(f"Processing: {paper_id}")
    existing_count = 0 if not existing_entry else len(existing_entry.get("reviews", []))
    print(
        f"Selected Reviewers: {', '.join([r['id'] for r in selected_reviewers])} | Existing reviews: {existing_count}"
    )
    print("=" * 60)

    try:
        paper_content = paper_file.read_text(encoding="utf-8")
        print(f"  âœ“ è®ºæ–‡å†…å®¹å·²åŠ è½½ ({len(paper_content)} å­—ç¬¦)")
    except Exception as exc:  # pylint: disable=broad-except
        print(f"  âŒ è¯»å–è®ºæ–‡å†…å®¹å¤±è´¥: {exc}")
        paper_content = f"[è¯»å–å¤±è´¥: {exc}]"

    reviews = []
    if existing_entry and isinstance(existing_entry, dict):
        reviews = list(existing_entry.get("reviews", []))

    for reviewer in selected_reviewers:
        strictness = reviewer["strictness"]
        reviewer_id = reviewer["id"]
        print(f"  ğŸ¤– {reviewer_id} (strictness: {strictness}) è¯„å®¡ä¸­...")

        start_time = time.time()
        review_data = reviewer_ai.generate_review(paper_content, strictness=strictness)
        elapsed = time.time() - start_time

        if "error" not in review_data:
            print(f"    âœ“ ç”Ÿæˆå®Œæˆ (è€—æ—¶: {elapsed:.1f}s, è¯„åˆ†: {review_data.get('rating', 'N/A')})")
        else:
            print(f"    âœ— ç”Ÿæˆå¤±è´¥ (è€—æ—¶: {elapsed:.1f}s)")

        formatted_content = format_review_content(review_data)
        reviews.append(
            {
                "reviewer_id": reviewer_id,
                "strictness": strictness,
                "review": formatted_content,
            }
        )

    result = {"paper_id": paper_id, "reviews": reviews}
    print(f"  âœ… å®Œæˆï¼Œå…±ç”Ÿæˆ {len(reviews)} ä¸ªè¯„å®¡")
    return paper_id, result


def main() -> None:
    parser = argparse.ArgumentParser(
        description="AI è¯„å®¡å‘˜æ¨ç†è„šæœ¬ - ä¸‰ä¸¥æ ¼åº¦æ¨¡å¼",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python qwen3-30B-reviewer-3levels.py
  python qwen3-30B-reviewer-3levels.py --limit 10 --workers 2
""",
    )

    parser.add_argument(
        "--input-dir",
        default="/remote-home1/bwli/get_open_review/qwen_review/extracted_contents",
        help="è®ºæ–‡å†…å®¹ç›®å½•ï¼ˆé»˜è®¤: /remote-home1/bwli/get_open_review/qwen_review/extracted_contentsï¼‰",
    )

    parser.add_argument(
        "--output",
        default="qwen3-30B-reviews-3levels.json",
        help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: qwen3-30B-reviews-3levels.jsonï¼‰",
    )

    parser.add_argument(
        "--base-url",
        default="http://10.176.59.101:8003/v1",
        help="Qwen3-30B-A3B API åœ°å€ï¼ˆé»˜è®¤: http://10.176.59.101:8003/v1ï¼‰",
    )

    parser.add_argument(
        "--model",
        default="qwen3-30b-a3b",
        help="æ¨¡å‹åç§°ï¼ˆé»˜è®¤: qwen3-30b-a3bï¼‰",
    )

    parser.add_argument(
        "--prompt-template",
        default=None,
        help="è‡ªå®šä¹‰ User Prompt æ¨¡æ¿è·¯å¾„ï¼ˆå¯é€‰ï¼‰",
    )

    parser.add_argument(
        "--workers",
        type=int,
        default=1,
        help="å¹¶è¡Œå¤„ç†çº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 1ï¼‰",
    )

    parser.add_argument(
        "--limit",
        type=int,
        help="åªå¤„ç†å‰ N ç¯‡è®ºæ–‡ï¼ˆè°ƒè¯•ç”¨ï¼‰",
    )

    args = parser.parse_args()

    print("=" * 80)
    print("AI è¯„å®¡å‘˜æ¨ç†ç³»ç»Ÿ - ä¸‰ä¸¥æ ¼åº¦æ¨¡å¼ (Qwen3-30B-A3B)")
    print("=" * 80)
    print(f"æ¨¡å‹: {args.model} @ {args.base_url}")
    print(f"Prompt æ¨¡æ¿: {'å†…ç½®æ¨¡æ¿' if args.prompt_template is None else args.prompt_template}")
    print(f"è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    print(f"å¹¶è¡Œçº¿ç¨‹: {args.workers}")
    print(f"è¯„å®¡å‘˜é…ç½®: {', '.join([r['name'] for r in REVIEWERS])}")
    print(f"ä¸¥æ ¼åº¦åºåˆ—: {STRICTNESS_SEQUENCE}")
    print("=" * 80)

    input_dir = Path(args.input_dir)
    if not input_dir.exists():
        raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")

    paper_files = sorted(input_dir.glob("*.txt"))
    if args.limit:
        paper_files = paper_files[: args.limit]
        print(f"âš ï¸  é™åˆ¶å¤„ç†å‰ {len(paper_files)} ç¯‡è®ºæ–‡")

    print(f"âœ“ æ‰¾åˆ° {len(paper_files)} ç¯‡è®ºæ–‡")

    reviewer_ai = ReviewerAI(
        base_url=args.base_url,
        model_name=args.model,
        prompt_template_path=args.prompt_template,
    )
    print("âœ“ AI è¯„å®¡å‘˜å·²å°±ç»ª")

    output_path = Path(args.output)
    results: Dict[str, Any] = {}

    if output_path.exists():
        try:
            with open(output_path, "r", encoding="utf-8") as fh:
                existing = json.load(fh)
            if isinstance(existing, dict):
                results = existing
                print(f"  â†» æ£€æµ‹åˆ°å·²æœ‰è¾“å‡ºï¼Œè½½å…¥ {len(results)} ç¯‡è®ºæ–‡çš„ç»“æœä»¥ç»­è·‘")
            else:
                print("  âš ï¸ ç°æœ‰è¾“å‡ºä¸æ˜¯å­—å…¸ç»“æ„ï¼Œå¿½ç•¥ç»­è·‘æ•°æ®")
        except Exception as exc:  # pylint: disable=broad-except
            print(f"  âš ï¸ è¯»å–å·²æœ‰è¾“å‡ºå¤±è´¥ï¼Œå¿½ç•¥ç»­è·‘æ•°æ®: {exc}")

    print("\nğŸš€ å¼€å§‹å¤„ç†è®ºæ–‡...")
    print(f"ğŸ’¾ ç»“æœå°†åŠ¨æ€ä¿å­˜åˆ° {output_path}")

    if args.workers == 1:
        for idx, paper_file in enumerate(paper_files, 1):
            paper_id = paper_file.stem
            if not paper_needs_work(results, paper_id):
                print(f"â­ï¸  è·³è¿‡å·²å®Œæˆ: {paper_id}")
                continue

            selected = select_missing_reviewers(results, paper_id)
            entry = results.get(paper_id)
            paper_id, review_content = process_single_paper(
                (paper_file, reviewer_ai, selected, entry)
            )
            results[paper_id] = review_content
            save_results(results, output_path)
            print(f"  ğŸ’¾ å·²ä¿å­˜ ({idx}/{len(paper_files)})")
    else:
        tasks = []
        for paper_file in paper_files:
            paper_id = paper_file.stem
            if not paper_needs_work(results, paper_id):
                print(f"â­ï¸  è·³è¿‡å·²å®Œæˆ: {paper_id}")
                continue
            selected = select_missing_reviewers(results, paper_id)
            entry = results.get(paper_id)
            tasks.append((paper_file, reviewer_ai, selected, entry))

        with ThreadPoolExecutor(max_workers=args.workers) as executor:
            future_map = {executor.submit(process_single_paper, task): task[0] for task in tasks}
            for idx, future in enumerate(as_completed(future_map), 1):
                paper_id, review_content = future.result()
                results[paper_id] = review_content
                save_results(results, output_path)
                print(f"  ğŸ’¾ å·²ä¿å­˜ ({idx}/{len(tasks)})")

    print("\nâœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜")
    print("\n" + "=" * 80)
    print("å¤„ç†å®Œæˆç»Ÿè®¡")
    print("=" * 80)
    print(f"æ€»è®ºæ–‡æ•°: {len(results)}")

    total_reviews = 0
    successful_reviews = 0
    all_ratings: List[float] = []
    ratings_by_reviewer: Dict[str, List[float]] = {r["id"]: [] for r in REVIEWERS}

    for paper_data in results.values():
        reviews = paper_data.get("reviews", [])
        total_reviews += len(reviews)
        for entry in reviews:
            review = entry.get("review", {})
            reviewer_id = entry.get("reviewer_id")
            rating_val = review.get("rating", {}).get("value", -1)
            if isinstance(rating_val, (int, float)) and rating_val >= 0:
                successful_reviews += 1
                all_ratings.append(float(rating_val))
                if reviewer_id in ratings_by_reviewer:
                    ratings_by_reviewer[reviewer_id].append(float(rating_val))

    failed_reviews = total_reviews - successful_reviews

    print(f"æ€»è¯„å®¡æ•°: {total_reviews}")
    print(f"æˆåŠŸç”Ÿæˆ: {successful_reviews}")
    print(f"å¤±è´¥: {failed_reviews}")

    if all_ratings:
        avg_rating = sum(all_ratings) / len(all_ratings)
        print(f"\næ•´ä½“å¹³å‡è¯„åˆ†: {avg_rating:.2f}")
        print(f"è¯„åˆ†èŒƒå›´: {min(all_ratings)} - {max(all_ratings)}")

        print("\næŒ‰è¯„å®¡å‘˜ç»Ÿè®¡:")
        for reviewer in REVIEWERS:
            rid = reviewer["id"]
            scores = ratings_by_reviewer[rid]
            if scores:
                reviewer_avg = sum(scores) / len(scores)
                print(f"  {reviewer['name']} ({rid}): å¹³å‡ {reviewer_avg:.2f} (æ ·æœ¬æ•°: {len(scores)})")

    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path.resolve()}")


if __name__ == "__main__":
    main()


