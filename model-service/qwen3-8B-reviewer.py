#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI è¯„å®¡å‘˜æ¨ç†è„šæœ¬
ä½¿ç”¨ Qwen3-8B æ¨¡å‹æ¨¡æ‹Ÿå­¦æœ¯è®ºæ–‡è¯„å®¡
"""

import json
import os
import argparse
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
import openai
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import uuid
import random


# ============================================================================
# Prompt æ¨¡æ¿é…ç½®åŒº - å¯ä»¥åœ¨è¿™é‡Œç›´æ¥ä¿®æ”¹ Prompt
# ============================================================================

# åŸºç¡€è¯„å®¡ä»»åŠ¡è¯´æ˜ï¼ˆæ‰€æœ‰çº§åˆ«å…±ç”¨ï¼‰
BASE_REVIEW_TASK = """
## Your Review Task

### Role: The Strict, Precise & Insightful Academic Reviewer
You are a seasoned reviewer renowned for strict scrutiny, precision, and insight. You uphold the highest academic standards. Your primary mission is **strict scrutiny** to ensure that only high-quality research is advanced. You relentlessly identify core deficiencies and logical leaks, and your feedback must be **specific, clear, and executable**. Your goal is to drive authors toward fundamental improvements that meet the highest submission standards.

### Core Knowledge & Abilities
- **Cutting-Edge Acumen**: Track frontier theory, latest methods, and community trends in real time; assess relevance and novelty.
- **Theoretical Mastery**: Possess systematic and critical understanding of classical theories and core paradigms; judge appropriateness of their application.
- **Logical Scrutiny**: Detect logical fallacies, inconsistencies, or latent biases in research design, inference, and data interpretation.
- **Standards Awareness**: Be familiar with review standards, preferences, and gatekeeping across top-tier conferences and specialized journals.

### Key Review Criteria
- **Originality & Contribution**: Does the work present clear and valuable new insights? Is the contribution incremental or truly groundbreaking?
- **Research Question**: Is the problem crisply defined? Are its academic value and/or practical significance strong?
- **Literature Review**: Is the review comprehensive, deep, and critical (not a simple list)? Does it identify the research gap accurately?
- **Methodological Rigor**: Is the design scientific and optimal for the question? Are sampling choices, data collection, and processing transparent, standardized, and reproducible?
- **Data Analysis & Results**: Are methods appropriate? Are results clear and accurate? Are interpretations rigorous and justified?
- **Discussion & Conclusion**: Do the authors interpret results deeply, engage with theory and prior work, and present evidence-based conclusions while honestly acknowledging limitations?
- **Logic & Expression**: Are arguments coherent and consistent? Is the academic language precise and professional?

### Strict Review Policy
- Maintain a strict, precise, and insightful tone; avoid vague praise or marketing language.
- Ground every judgment in evidence from the paper (methods, datasets, baselines, metrics, settings). If information is missing, explicitly state "Missing" and explain its impact.
- Identify core deficiencies and logical flaws decisively; provide numbered, actionable suggestions the authors can execute.
- Treat novelty rigorously: check overlaps with prior work; demand strong baselines/ablations and statistical significance when applicable.

### Workflow: Target-Oriented Comprehensive Review
1. Identify target claims and contributions; list required evidence for each.
2. Map evidence to claims; check completeness against baselines, ablations, datasets, metrics, and settings.
3. Diagnose failure points: unsupported claims, missing baselines, ambiguous novelty, flawed methodology, or weak analysis.
4. Propose corrective actions: numbered, prioritized, and feasible; specify experiments, analyses, or clarifications required for acceptance.
5. Calibrate soundness/presentation/contribution and overall rating using the defined scales; state confidence and rationale.

You will be provided with a research paper. Please provide a comprehensive review with the following components:

1. **Summary**: A brief summary of the paper's main contributions and approach (2-3 sentences).

2. **Strengths**: A substantive assessment of the strengths of the paper, touching on each of the following dimensions: originality, quality, clarity, and significance. Be broad in definitions of originality (new definitions, problem formulations, creative combinations, new domains, or removing limitations from prior results).

3. **Weaknesses**: A substantive assessment of the weaknesses of the paper. Focus on constructive and actionable insights on how the work could improve towards its stated goals. Be specific and avoid generic remarks. If you believe the contribution lacks novelty, provide references and explanations; if experiments are insufficient, explain why and exactly what is missing.

4. **Questions**: List up and carefully describe any questions and suggestions for the authors. Think of things where a response from the author can change your opinion, clarify a confusion, or address a limitation. This is important for a productive rebuttal and discussion phase.

5. **Soundness** (1-4): Rate the paper's soundness. Are the central claims adequately supported with evidence? Are the experimental setup and research methodology sound?
   - 1: Poor
   - 2: Fair
   - 3: Clear and structured
   - 4: Excellent

6. **Presentation** (1-4): Rate the quality of presentation. This should take into account the writing style and clarity, presentation of figures and diagrams, as well as contextualization relative to prior work.
   - 1: Poor
   - 2: Fair
   - 3: Good
   - 4: Excellent

7. **Contribution** (1-4): Rate the quality of the overall contribution this paper makes to the research area being studied. Are the questions being asked important? Does the paper bring significant originality of ideas and/or execution? Are the results valuable to share with the broader ICLR community?
   - 1: Poor
   - 2: Fair
   - 3: Good
   - 4: Excellent

8. **Rating** (1, 3, 5, 6, 8, 10): Provide an overall score for this submission:
   - 1: Strong reject
   - 3: Reject, not good enough
   - 5: Marginally below the acceptance threshold
   - 6: Marginally above the acceptance threshold
   - 8: Accept, good paper
   - 10: Strong accept, should be highlighted at the conference

9. **Confidence** (1-5): Provide a confidence score for your assessment:
   - 1: Unable to assess this paper, need opinion from different reviewers
   - 2: Willing to defend assessment, but quite likely did not understand central parts or unfamiliar with related work
   - 3: Fairly confident in assessment. Possible that did not understand some parts or unfamiliar with some related work
   - 4: Confident in assessment, but not absolutely certain. Unlikely but not impossible that did not understand some parts
   - 5: Absolutely certain about assessment. Very familiar with related work and checked details carefully

## Output Format

You must respond with a JSON object in the following format:
```json
{{
  "summary": "...",
  "strengths": "...",
  "weaknesses": "...",
  "questions": "...",
  "rating": <1, 3, 5, 6, 8, or 10>,
  "confidence": <1-5>,
  "soundness": <1-4>,
  "presentation": <1-4>,
  "contribution": <1-4>
}}
```

**Important**:
- `rating` must be one of: 1, 3, 5, 6, 8, 10
- `confidence`, `soundness`, `presentation`, `contribution` must be integers in their respective ranges

You may use <think> tags to organize your thoughts before providing the JSON response. The final JSON object should come after your reasoning."""

# ============================================================================
# 5ä¸ªç‹¬ç«‹çš„System Prompt - æ¯ç§ä¸¥æ ¼åº¦çº§åˆ«ä¸€ä¸ª
# ============================================================================

SYSTEM_PROMPT_LEVEL_1 = """You are an expert academic reviewer for a top-tier machine learning conference (ICLR).

## Your Reviewer Profile
**You are a Level 1 Encouraging Reviewer**

- **Philosophy**: You believe in nurturing innovation and giving researchers opportunities to develop ideas.
- **What you value most**: Novelty, creativity, potential impact, and fresh perspectives.
- **How you view flaws**: Minor issues are acceptable if the core idea is interesting. Limitations are seen as future work opportunities.
- **Your threshold for acceptance**: Does this paper bring something interesting to the community? If yes, you support its publication.
- **How you write weaknesses**: You frame issues constructively as "suggestions for improvement" rather than dealbreakers.
- **Typical ratings**: You commonly give 8 (good papers with novelty) or 10 (excellent innovative work).

""" + BASE_REVIEW_TASK

SYSTEM_PROMPT_LEVEL_2 = """You are an expert academic reviewer for a top-tier machine learning conference (ICLR).

## Your Reviewer Profile
**You are a Level 2 Supportive Reviewer**

- **Philosophy**: You appreciate solid scientific work and want to help good research get published.
- **What you value most**: Sound methodology, clear contributions, and well-executed experiments.
- **How you view flaws**: You acknowledge limitations but weigh them against the overall contribution.
- **Your threshold for acceptance**: Is this a solid piece of work that advances the field? You give borderline papers the benefit of doubt.
- **How you write weaknesses**: You point out concerns but emphasize the paper's merits.
- **Typical ratings**: You commonly give 6 (marginally acceptable) or 8 (solid accept).

""" + BASE_REVIEW_TASK

SYSTEM_PROMPT_LEVEL_3 = """You are an expert academic reviewer for a top-tier machine learning conference (ICLR).

## Your Reviewer Profile
**You are a Level 3 Objective Reviewer**

- **Philosophy**: You apply standard conference criteria fairly and consistently.
- **What you value most**: Balance between novelty, soundness, clarity, and significance.
- **How you view flaws**: Issues are noted objectively; significant problems affect your rating proportionally.
- **Your threshold for acceptance**: Does this paper meet the expected quality bar for ICLR?
- **How you write weaknesses**: You provide balanced critique with both strengths and weaknesses carrying equal weight.
- **Typical ratings**: You commonly give 5 (marginally below threshold) or 6 (marginally above threshold).

""" + BASE_REVIEW_TASK

SYSTEM_PROMPT_LEVEL_4 = """You are an expert academic reviewer for a top-tier machine learning conference (ICLR).

## Your Reviewer Profile
**You are a Level 4 Rigorous Reviewer**

- **Philosophy**: You hold papers to high standards because ICLR should showcase strong research.
- **What you value most**: Rigorous experimental validation, strong baselines, thorough analysis, and clear novelty over prior work.
- **How you view flaws**: Even small issues raise concerns. You need convincing evidence for all claims.
- **Your threshold for acceptance**: Only clear, well-executed contributions with strong empirical support should be accepted.
- **How you write weaknesses**: You identify issues in depth and question claims that lack sufficient support.
- **Typical ratings**: You commonly give 3 (reject, not good enough) or 5 (marginally below threshold).

""" + BASE_REVIEW_TASK

SYSTEM_PROMPT_LEVEL_5 = """You are an expert academic reviewer for a top-tier machine learning conference (ICLR).

## Your Reviewer Profile
**You are a Level 5 Highly Critical Reviewer**

- **Philosophy**: ICLR is a top venue; only exceptional work should be published.
- **What you value most**: Groundbreaking ideas, flawless execution, comprehensive experiments, and significant impact.
- **How you view flaws**: You scrutinize every detail. Missing baselines, incomplete experiments, or unclear novelty are major concerns.
- **Your threshold for acceptance**: This paper must be exceptional - truly advancing the field with rigorous validation.
- **How you write weaknesses**: You point out all limitations, gaps, and areas where the paper falls short of the highest standards.
- **Typical ratings**: You commonly give 1 (strong reject) or 3 (reject, not good enough).

""" + BASE_REVIEW_TASK

# åˆ›å»ºæ˜ å°„å­—å…¸ï¼šstrictness -> system_prompt
SYSTEM_PROMPTS = {
    1: SYSTEM_PROMPT_LEVEL_1,
    2: SYSTEM_PROMPT_LEVEL_2,
    3: SYSTEM_PROMPT_LEVEL_3,
    4: SYSTEM_PROMPT_LEVEL_4,
    5: SYSTEM_PROMPT_LEVEL_5,
}


USER_PROMPT_TEMPLATE = """## Paper Content

{paper_content}"""


class ReviewerAI:
    """AI è¯„å®¡å‘˜ï¼Œä½¿ç”¨ Qwen3-8B æ¨¡å‹"""
    
    def __init__(
        self, 
        base_url: str = "http://10.176.59.101:8002/v1", 
        model_name: str = "qwen3-8b",
        prompt_template_path: Optional[str] = None
    ):
        self.client = openai.OpenAI(api_key="EMPTY", base_url=base_url)
        self.model_name = model_name
        
        # åŠ è½½ user prompt æ¨¡æ¿
        if prompt_template_path:
            # å¦‚æœæŒ‡å®šäº†å¤–éƒ¨æ–‡ä»¶ï¼Œä»æ–‡ä»¶åŠ è½½
            template_file = Path(prompt_template_path)
            if not template_file.exists():
                raise FileNotFoundError(f"Prompt æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {prompt_template_path}")
            self.user_prompt_template = template_file.read_text(encoding='utf-8')
            print(f"  âœ“ å·²åŠ è½½å¤–éƒ¨ User Prompt æ¨¡æ¿: {prompt_template_path}")
        else:
            # å¦åˆ™ä½¿ç”¨å†…ç½®æ¨¡æ¿
            self.user_prompt_template = USER_PROMPT_TEMPLATE
            print(f"  âœ“ ä½¿ç”¨å†…ç½® User Prompt æ¨¡æ¿")
        
        # System prompts æ˜ å°„å­—å…¸ï¼ˆæ¯ä¸ªstrictnessçº§åˆ«æœ‰ç‹¬ç«‹çš„promptï¼‰
        self.system_prompts = SYSTEM_PROMPTS
        print(f"  âœ“ ä½¿ç”¨å†…ç½® System Prompts (5ä¸ªç‹¬ç«‹çº§åˆ«)")
        
    def build_review_prompt(
        self,
        paper_content: str,
        max_content_length: int = 10000
    ) -> str:
        """æ„å»ºè¯„å®¡æç¤ºè¯"""
        
        # æˆªæ–­è®ºæ–‡å†…å®¹é¿å…è¿‡é•¿
        if len(paper_content) > max_content_length:
            paper_content = paper_content[:max_content_length] + "\n\n[è®ºæ–‡å†…å®¹å·²æˆªæ–­...]"
        
        # ä½¿ç”¨æ¨¡æ¿å¡«å……
        prompt = self.user_prompt_template.format(
            paper_content=paper_content
        )
        
        return prompt
    
    def generate_review(
        self,
        paper_content: str,
        strictness: int = 3,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> Dict[str, Any]:
        """ç”Ÿæˆè¯„å®¡å†…å®¹
        
        Args:
            paper_content: è®ºæ–‡å†…å®¹
            strictness: ä¸¥æ ¼åº¦ (1-5)ï¼Œ1æœ€å®½æ¾ï¼Œ5æœ€ä¸¥æ ¼
            temperature: ç”Ÿæˆæ¸©åº¦
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
        """
        
        prompt = self.build_review_prompt(paper_content)
        
        try:
            # æ ¹æ®strictnessçº§åˆ«é€‰æ‹©å¯¹åº”çš„ç‹¬ç«‹system prompt
            system_prompt = self.system_prompts.get(strictness, self.system_prompts[3])  # é»˜è®¤ä½¿ç”¨Level 3
            
            # æ³¨æ„ï¼šä¿ç•™æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå…è®¸ç”Ÿæˆ <think> æ ‡ç­¾ï¼Œä½†åç»­ä¼šè‡ªåŠ¨è¿‡æ»¤
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            content = response.choices[0].message.content
            
            # æ¸…ç†å“åº”å†…å®¹ï¼Œæå– JSON
            content = content.strip()
            
            # 1. ç§»é™¤ Qwen æ¨¡å‹çš„ <think> æ ‡ç­¾ï¼ˆChain-of-Thoughtï¼‰
            # è¿™å…è®¸æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œä½†æˆ‘ä»¬åªæå–æœ€ç»ˆçš„ JSON è¾“å‡º
            if '<think>' in content.lower():
                # å°è¯•æ‰¾åˆ° </think> æ ‡ç­¾
                think_patterns = ['</think>', '</Think>', '</THINK>']
                for pattern in think_patterns:
                    if pattern.lower() in content.lower():
                        # ä¸åŒºåˆ†å¤§å°å†™æŸ¥æ‰¾
                        idx = content.lower().find(pattern.lower())
                        if idx != -1:
                            content = content[idx + len(pattern):].strip()
                            break
                else:
                    # å¦‚æœæ²¡æ‰¾åˆ°é—­åˆæ ‡ç­¾ï¼ŒæŸ¥æ‰¾ç¬¬ä¸€ä¸ª { ä½œä¸º JSON å¼€å§‹
                    json_start = content.find('{')
                    if json_start != -1:
                        content = content[json_start:]
            
            # 2. ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
            if content.startswith("```json"):
                content = content[7:]
            elif content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()
            
            # 3. å¦‚æœå¼€å¤´è¿˜æœ‰é JSON å†…å®¹ï¼Œå°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª {
            if not content.startswith('{'):
                json_start = content.find('{')
                if json_start != -1:
                    content = content[json_start:]
            
            # 4. ä¿®å¤å¸¸è§çš„æ— æ•ˆè½¬ä¹‰å­—ç¬¦
            # JSON åªæ”¯æŒ: \", \\, \/, \b, \f, \n, \r, \t, \uXXXX
            # å°†å…¶ä»–æ— æ•ˆçš„ \x è½¬æ¢ä¸º \\x
            def fix_escape(match):
                char = match.group(1)
                if char in ['"', '\\', '/', 'b', 'f', 'n', 'r', 't', 'u']:
                    return match.group(0)  # ä¿ç•™åˆæ³•çš„è½¬ä¹‰
                else:
                    return '\\\\' + char  # å°† \x è½¬æ¢ä¸º \\x
            
            content = re.sub(r'\\(.)', fix_escape, content)
            
            review_data = json.loads(content)
            return review_data
            
        except json.JSONDecodeError as e:
            print(f"  âš ï¸  JSONè§£æå¤±è´¥: {e}")
            if 'content' in locals():
                print(f"  åŸå§‹å“åº”å‰200å­—ç¬¦: {content[:200]}")
                # æ‰“å°é”™è¯¯ä½ç½®é™„è¿‘çš„å†…å®¹
                if hasattr(e, 'pos') and e.pos:
                    start = max(0, e.pos - 50)
                    end = min(len(content), e.pos + 50)
                    print(f"  é”™è¯¯ä½ç½®é™„è¿‘: ...{content[start:end]}...")
            return {
                "summary": content if 'content' in locals() else "No content",
                "strengths": "Failed to parse",
                "weaknesses": "Failed to parse",
                "questions": "Failed to parse",
                "rating": -1,
                "confidence": -1,
                "soundness": -1,
                "presentation": -1,
                "contribution": -1,
                "error": str(e)
            }
        except Exception as e:
            print(f"  âŒ APIè°ƒç”¨å¤±è´¥: {e}")
            print(f"  Exception type: {type(e).__name__}")
            import traceback
            print(f"  Traceback: {traceback.format_exc()[:500]}")
            return {
                "summary": f"Error: {str(e)}",
                "strengths": "",
                "weaknesses": "",
                "questions": "",
                "rating": -1,
                "confidence": -1,
                "soundness": -1,
                "presentation": -1,
                "contribution": -1,
                "error": str(e)
            }




def format_review_content(review_data: Dict[str, Any]) -> Dict[str, Any]:
    """å°†AIç”Ÿæˆçš„è¯„å®¡æ•°æ®æ ¼å¼åŒ–ä¸ºæ ‡å‡†çš„contentæ ¼å¼"""
    content = {}
    
    # å°†æ¯ä¸ªå­—æ®µåŒ…è£…æˆ {"value": ...} æ ¼å¼
    for key in ['summary', 'strengths', 'weaknesses', 'questions']:
        if key in review_data:
            content[key] = {"value": review_data[key]}
    
    # æ•°å€¼å­—æ®µ
    for key in ['rating', 'confidence', 'soundness', 'presentation', 'contribution']:
        if key in review_data and review_data[key] != -1:
            content[key] = {"value": review_data[key]}

    return content


def process_single_paper(args) -> tuple:
    """å¤„ç†å•ç¯‡è®ºæ–‡ï¼Œç”±å¤šä½è¯„å®¡å‘˜ç”Ÿæˆè¯„å®¡ï¼ˆæ”¯æŒè¡¥å…¨æ–­ç‚¹ï¼‰"""
    paper_file, reviewer_ai, selected_reviewers, existing_entry = args
    
    paper_id = paper_file.stem  # æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    
    print(f"\n{'='*60}")
    print(f"Processing: {paper_id}")
    existing_count = 0 if not existing_entry else len(existing_entry.get('reviews', []))
    print(f"Selected Reviewers: {', '.join([r['id'] for r in selected_reviewers])} | Existing reviews: {existing_count}")
    print(f"{'='*60}")
    
    # è¯»å–è®ºæ–‡å†…å®¹
    try:
        paper_content = paper_file.read_text(encoding='utf-8')
        print(f"  âœ“ è®ºæ–‡å†…å®¹å·²åŠ è½½ ({len(paper_content)} å­—ç¬¦)")
    except Exception as e:
        print(f"  âŒ è¯»å–è®ºæ–‡å†…å®¹å¤±è´¥: {e}")
        paper_content = f"[è¯»å–å¤±è´¥: {e}]"
    
    # åˆå§‹åŒ–è¯„å®¡åˆ—è¡¨ï¼šè‹¥å·²æœ‰è¯„å®¡ï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰ï¼Œå…ˆè½½å…¥
    reviews = []
    if existing_entry and isinstance(existing_entry, dict):
        reviews = list(existing_entry.get('reviews', []))

    # è‹¥æ— éœ€è¡¥å…¨ï¼Œç›´æ¥è¿”å›ï¼ˆå·²å®Œæˆï¼‰
    if len(selected_reviewers) == 0:
        result = {
            "paper_id": paper_id,
            "reviews": reviews
        }
        print(f"  âœ… å·²å­˜åœ¨å®Œæ•´è¯„å®¡ï¼Œè·³è¿‡ç”Ÿæˆ")
        return paper_id, result

    # ä¸ºæ¯ä½å¾…è¡¥å…¨è¯„å®¡å‘˜ç”Ÿæˆè¯„å®¡
    for reviewer in selected_reviewers:
        print(f"  ğŸ¤– {reviewer['id']} (strictness: {reviewer['strictness']}) è¯„å®¡ä¸­...")
        
        start_time = time.time()
        
        # ç”Ÿæˆè¯„å®¡å†…å®¹
        review_data = reviewer_ai.generate_review(
            paper_content=paper_content,
            strictness=reviewer['strictness']
        )
        
        elapsed = time.time() - start_time
        
        if 'error' not in review_data:
            print(f"    âœ“ ç”Ÿæˆå®Œæˆ (è€—æ—¶: {elapsed:.1f}s, è¯„åˆ†: {review_data.get('rating', 'N/A')})")
        else:
            print(f"    âœ— ç”Ÿæˆå¤±è´¥ (è€—æ—¶: {elapsed:.1f}s)")
        
        # æ ¼å¼åŒ–ä¸ºæ ‡å‡†æ ¼å¼
        formatted_content = format_review_content(review_data)
        
        # æ„å»ºè¯„å®¡æ¡ç›®
        review_entry = {
            "reviewer_id": reviewer['id'],
            "strictness": reviewer['strictness'],
            "review": formatted_content
        }
        
        reviews.append(review_entry)
    
    # æ„å»ºå®Œæ•´çš„è®ºæ–‡æ•°æ®ç»“æ„
    result = {
        "paper_id": paper_id,
        "reviews": reviews  # ç°åœ¨æ˜¯åˆ—è¡¨ï¼ŒåŒ…å«3ä¸ªè¯„å®¡
    }
    
    print(f"  âœ… å®Œæˆï¼Œå…±ç”Ÿæˆ {len(reviews)} ä¸ªè¯„å®¡")
    
    return paper_id, result


def main():
    parser = argparse.ArgumentParser(
        description="AI è¯„å®¡å‘˜æ¨ç†è„šæœ¬ - ä½¿ç”¨ Qwen3-8B æ¨¡å‹æ¨¡æ‹Ÿè®ºæ–‡è¯„å®¡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºæœ¬ç”¨æ³•
  python qwen3-8B-reviewer.py
  
  # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†
  python qwen3-8B-reviewer.py --workers 3
  
  # æµ‹è¯•å¤„ç†å‰5ç¯‡è®ºæ–‡
  python qwen3-8B-reviewer.py --limit 5
  
  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶
  python qwen3-8B-reviewer.py --output my_reviews.json
  
  # ä½¿ç”¨è‡ªå®šä¹‰ User Prompt æ¨¡æ¿ï¼ˆå¯é€‰ï¼‰
  python qwen3-8B-reviewer.py --prompt-template custom_prompt.txt
        """
    )
    
    parser.add_argument(
        '--input-dir',
        default='../qwen_review/extracted_contents',
        help='è®ºæ–‡å†…å®¹ç›®å½•ï¼ˆé»˜è®¤: ../qwen_review/extracted_contentsï¼‰'
    )
    
    parser.add_argument(
        '--output',
        default='qwen3-8B-reviews.json',
        help='è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: ai_generated_reviews.jsonï¼‰'
    )
    
    parser.add_argument(
        '--base-url',
        default='http://10.176.59.101:8002/v1',
        help='Qwen3-8B APIåœ°å€ï¼ˆé»˜è®¤: http://10.176.59.101:8002/v1ï¼‰'
    )
    
    parser.add_argument(
        '--model',
        default='qwen3-8b',
        help='æ¨¡å‹åç§°ï¼ˆé»˜è®¤: qwen3-8bï¼‰'
    )
    
    parser.add_argument(
        '--prompt-template',
        default=None,
        help='Prompt æ¨¡æ¿æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å†…ç½®æ¨¡æ¿ï¼‰'
    )
    
    parser.add_argument(
        '--workers',
        type=int,
        default=1,
        help='å¹¶è¡Œå¤„ç†çš„çº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 1ï¼‰'
    )
    
    parser.add_argument(
        '--limit',
        type=int,
        help='åªå¤„ç†å‰Nç¯‡è®ºæ–‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰'
    )
    
    args = parser.parse_args()
    
    # å®šä¹‰5ä½è¯„å®¡å‘˜ï¼Œä¸¥æ ¼åº¦ä»1åˆ°5
    REVIEWERS = [
        {"id": "reviewer_1", "strictness": 1, "name": "å®½æ¾è¯„å®¡å‘˜"},
        {"id": "reviewer_2", "strictness": 2, "name": "è¾ƒå®½æ¾è¯„å®¡å‘˜"},
        {"id": "reviewer_3", "strictness": 3, "name": "ä¸­ç«‹è¯„å®¡å‘˜"},
        {"id": "reviewer_4", "strictness": 4, "name": "è¾ƒä¸¥æ ¼è¯„å®¡å‘˜"},
        {"id": "reviewer_5", "strictness": 5, "name": "ä¸¥æ ¼è¯„å®¡å‘˜"},
    ]
    
    print("=" * 80)
    print("AI è¯„å®¡å‘˜æ¨ç†ç³»ç»Ÿ - å¤šè¯„å®¡å‘˜æ¨¡å¼")
    print("=" * 80)
    print(f"æ¨¡å‹: {args.model} @ {args.base_url}")
    print(f"Prompt æ¨¡æ¿: {'å†…ç½®æ¨¡æ¿' if args.prompt_template is None else args.prompt_template}")
    print(f"è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    print(f"å¹¶è¡Œçº¿ç¨‹: {args.workers}")
    print(f"è¯„å®¡å‘˜é…ç½®: {len(REVIEWERS)} ä½è¯„å®¡å‘˜ï¼Œä¸¥æ ¼åº¦èŒƒå›´ 1-5")
    print(f"æ¯ç¯‡è®ºæ–‡: éšæœºé€‰æ‹© 3 ä½è¯„å®¡å‘˜")
    print("=" * 80)
    
    # åŠ è½½è®ºæ–‡æ–‡ä»¶
    print("\nğŸ“– åŠ è½½è®ºæ–‡æ–‡ä»¶...")
    input_dir = Path(args.input_dir)
    if not input_dir.exists():
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return
    
    paper_files = sorted(input_dir.glob("*.txt"))
    if args.limit:
        paper_files = paper_files[:args.limit]
        print(f"âš ï¸  é™åˆ¶å¤„ç†å‰ {args.limit} ç¯‡è®ºæ–‡")
    
    print(f"âœ“ æ‰¾åˆ° {len(paper_files)} ç¯‡è®ºæ–‡")
    
    # åˆå§‹åŒ– AI è¯„å®¡å‘˜
    print(f"\nğŸ¤– åˆå§‹åŒ– AI è¯„å®¡å‘˜...")
    reviewer_ai = ReviewerAI(
        base_url=args.base_url, 
        model_name=args.model,
        prompt_template_path=args.prompt_template
    )
    print(f"âœ“ AI è¯„å®¡å‘˜å·²å°±ç»ª")
    
    # å¤„ç†è®ºæ–‡
    print(f"\nğŸš€ å¼€å§‹å¤„ç†è®ºæ–‡...")
    print(f"ğŸ’¾ ç»“æœå°†åŠ¨æ€ä¿å­˜åˆ° {args.output}")

    output_path = Path(args.output)
    results = {}

    # è‹¥å·²æœ‰è¾“å‡ºæ–‡ä»¶ï¼ŒåŠ è½½ä»¥æ”¯æŒæ–­ç‚¹ç»­è·‘
    if output_path.exists():
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                existing = json.load(f)
            if isinstance(existing, dict):
                results = existing
                print(f"  â†» æ£€æµ‹åˆ°å·²æœ‰è¾“å‡ºï¼Œè½½å…¥ {len(results)} ç¯‡è®ºæ–‡çš„ç»“æœä»¥ç»­è·‘")
            else:
                print("  âš ï¸  ç°æœ‰è¾“å‡ºä¸æ˜¯å­—å…¸ç»“æ„ï¼Œå¿½ç•¥ç»­è·‘æ•°æ®")
        except Exception as e:
            print(f"  âš ï¸  è¯»å–å·²æœ‰è¾“å‡ºå¤±è´¥ï¼Œå¿½ç•¥ç»­è·‘æ•°æ®: {e}")
    
    # è¾…åŠ©å‡½æ•°ï¼šä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    def save_results(results_dict, output_file):
        """å¢é‡ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2)
    
    # è®¡ç®—éœ€è¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆè·³è¿‡å·²å®Œæˆçš„è®ºæ–‡ï¼‰
    def paper_needs_work(paper_id: str) -> bool:
        entry = results.get(paper_id)
        if not entry:
            return True
        reviews = entry.get('reviews', [])
        return len(reviews) < 3  # æœªè¾¾åˆ°3ä¸ªè¯„å®¡éœ€ç»§ç»­

    def select_missing_reviewers(paper_id: str) -> List[dict]:
        entry = results.get(paper_id)
        done_ids = set()
        if entry:
            for r in entry.get('reviews', []):
                rid = r.get('reviewer_id')
                if rid:
                    done_ids.add(rid)
        # é€‰æ‹©å‰©ä½™çš„è¯„å®¡å‘˜
        remaining = [r for r in REVIEWERS if r['id'] not in done_ids]
        # æœ€å¤šè¡¥è¶³åˆ°3ä¸ª
        need = max(0, 3 - len(done_ids))
        return random.sample(remaining, k=need) if need > 0 and len(remaining) >= need else []

    if args.workers == 1:
        # ä¸²è¡Œå¤„ç†ï¼ˆæ¯å¤„ç†å®Œä¸€ç¯‡å°±ä¿å­˜ï¼‰
        for idx, paper_file in enumerate(paper_files, 1):
            pid = paper_file.stem
            if not paper_needs_work(pid):
                print(f"â­ï¸  è·³è¿‡å·²å®Œæˆ: {pid}")
                continue
            # è¡¥é€‰ç¼ºå¤±çš„è¯„å®¡å‘˜
            selected_reviewers = select_missing_reviewers(pid)
            entry = results.get(pid)

            paper_id, review_content = process_single_paper((
                paper_file, reviewer_ai, selected_reviewers, entry
            ))
            results[paper_id] = review_content
            
            # åŠ¨æ€ä¿å­˜
            save_results(results, output_path)
            print(f"  ğŸ’¾ å·²ä¿å­˜ ({idx}/{len(paper_files)})")
    else:
        # å¹¶è¡Œå¤„ç†ï¼ˆæ¯å®Œæˆä¸€ç¯‡å°±ä¿å­˜ï¼‰
        tasks = []
        for paper_file in paper_files:
            pid = paper_file.stem
            if not paper_needs_work(pid):
                print(f"â­ï¸  è·³è¿‡å·²å®Œæˆ: {pid}")
                continue
            # è¡¥é€‰ç¼ºå¤±çš„è¯„å®¡å‘˜
            selected_reviewers = select_missing_reviewers(pid)
            entry = results.get(pid)
            tasks.append((paper_file, reviewer_ai, selected_reviewers, entry))

        with ThreadPoolExecutor(max_workers=args.workers) as executor:
            futures = {executor.submit(process_single_paper, task): task[0] for task in tasks}
            
            for idx, future in enumerate(as_completed(futures), 1):
                paper_id, review_content = future.result()
                results[paper_id] = review_content
                
                # åŠ¨æ€ä¿å­˜
                save_results(results, output_path)
                print(f"  ğŸ’¾ å·²ä¿å­˜ ({idx}/{len(paper_files)})")
    
    # æœ€ç»ˆç¡®è®¤
    print(f"\nâœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° {args.output}")
    
    # ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("å¤„ç†å®Œæˆç»Ÿè®¡")
    print("=" * 80)
    print(f"æ€»è®ºæ–‡æ•°: {len(results)}")
    
    # ç»Ÿè®¡è¯„å®¡æ•°å’Œè¯„åˆ†
    total_reviews = 0
    successful_reviews = 0
    all_ratings = []
    ratings_by_strictness = {1: [], 2: [], 3: [], 4: [], 5: []}
    
    for paper_data in results.values():
        reviews = paper_data.get('reviews', [])
        total_reviews += len(reviews)
        
        for review_entry in reviews:
            review = review_entry.get('review', {})
            strictness = review_entry.get('strictness', 3)
            rating = review.get('rating', {}).get('value', -1)
            
            if rating >= 0:
                successful_reviews += 1
                all_ratings.append(rating)
                ratings_by_strictness[strictness].append(rating)
    
    failed_reviews = total_reviews - successful_reviews
    
    print(f"æ€»è¯„å®¡æ•°: {total_reviews}")
    print(f"æˆåŠŸç”Ÿæˆ: {successful_reviews}")
    print(f"å¤±è´¥: {failed_reviews}")
    
    if all_ratings:
        avg_rating = sum(all_ratings) / len(all_ratings)
        print(f"\næ•´ä½“å¹³å‡è¯„åˆ†: {avg_rating:.2f}")
        print(f"è¯„åˆ†èŒƒå›´: {min(all_ratings)} - {max(all_ratings)}")
        
        # æŒ‰ä¸¥æ ¼åº¦ç»Ÿè®¡
        print(f"\næŒ‰ä¸¥æ ¼åº¦ç»Ÿè®¡:")
        for strictness in [1, 2, 3, 4, 5]:
            ratings = ratings_by_strictness[strictness]
            if ratings:
                avg = sum(ratings) / len(ratings)
                print(f"  ä¸¥æ ¼åº¦ {strictness}: å¹³å‡ {avg:.2f} (æ ·æœ¬æ•°: {len(ratings)})")
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path.absolute()}")


if __name__ == '__main__':
    main()

