{
  "02DCEU6vSU": {
    "paper_id": "02DCEU6vSU",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Gen-LRA, a novel Membership Inference Attack (MIA) for generative models that leverages likelihood ratio hypothesis testing to detect privacy leakage caused by overfitting. The method operates under a shadow-box threat model, requiring no assumptions about model architecture or access, and demonstrates superior performance over existing MIAs across diverse datasets and model architectures."
          },
          "strengths": {
            "value": "The paper presents a theoretically grounded approach to membership inference by formalizing privacy auditing as a hypothesis testing problem, which is a significant contribution. The method's computational efficiency and lack of reliance on model access or parameterization are major advantages. The comprehensive experimental evaluation across multiple datasets, architectures, and attack parameters strengthens the claim of broad applicability. The work also highlights critical privacy risks in differentially private models, which is highly relevant to real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed comparison with state-of-the-art MIAs for generative models, such as those based on score matching or adversarial training. The theoretical analysis of why Gen-LRA outperforms existing methods is limited, and the paper does not thoroughly address potential limitations (e.g., performance on non-tabular data or specific model types like GANs). Additionally, the practical implications of the findings for model developers and auditors are not sufficiently discussed."
          },
          "questions": {
            "value": [
              "How does Gen-LRA handle generative models with fundamentally different architectures (e.g., GANs vs. VAEs) or non-tabular data?",
              "What specific limitations of existing MIAs does Gen-LRA address, and how does its hypothesis testing framework overcome them?",
              "Are there scenarios where Gen-LRA might fail, such as when the reference dataset is not representative of the training data?",
              "How does the choice of threshold in Equation (1) affect the attack's performance, and is there a principled way to optimize it?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Gen-LRA, a novel Membership Inference Attack (MIA) for generative models that leverages a likelihood ratio framework to detect privacy leakage caused by overfitting. It operates under a shadow-box threat model without requiring access to the model's architecture or parameters, and demonstrates superior performance across diverse datasets and model configurations."
          },
          "strengths": {
            "value": "Originality is evident in the likelihood ratio-based approach for MIAs, which differs from heuristic-based prior work. The experiments are comprehensive, covering multiple datasets, architectures, and attack parameters. The paper clearly defines the MIA formalism and threat model. The significance is high, as it highlights critical privacy risks from overfitting in generative models, including subgroup-specific leakage."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with recent state-of-the-art MIAs for generative models (e.g., methods from 2023-2024). The hypothesis testing framework requires more technical detail, particularly how null/alternative hypotheses are constructed. The analysis of subgroup leakage is superficial, with no quantitative evidence or examples. The computational efficiency claims are not supported by runtime benchmarks."
          },
          "questions": {
            "value": "1. How is the likelihood ratio computed in practice? What specific statistical tests are used for hypothesis validation? 2. Are there cases where Gen-LRA fails compared to other MIAs, and why? 3. How does the reference dataset's quality and size affect performance? 4. What are the exact computational costs of Gen-LRA compared to existing methods?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces Gen-LRA, a novel Membership Inference Attack (MIA) for generative models that leverages likelihood ratio hypothesis testing to detect overfitting in synthetic data. The approach operates under a shadow-box threat model with minimal assumptions about model architecture or access, claiming superior performance over existing MIAs in evaluating privacy risks."
          },
          "strengths": {
            "value": "Originality is strong in applying likelihood ratio testing to MIAs for generative models, addressing a critical gap in privacy auditing. The formalization of the MIA game and threat model is rigorous, with clear connections to existing work. The experiments span diverse datasets and architectures, demonstrating broad applicability. The significance of the problem (privacy risks in generative models) is high, and the paper emphasizes practical relevance."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art MIAs, such as those based on likelihood estimation or model inversion. The reliance on a reference dataset (assumed to be independently sampled) may not reflect real-world scenarios where such data is unavailable. The hypothesis testing framework's robustness to adversarial examples or noisy data is unaddressed. Experimental results lack statistical significance testing for performance gains, and the paper does not discuss computational efficiency trade-offs."
          },
          "questions": {
            "value": [
              "How was the reference dataset $R$ obtained in practice? Is it assumed to be publicly available or derived from external sources?",
              "What are the limitations of Gen-LRA when applied to models with strong differential privacy guarantees?",
              "How does Gen-LRA handle cases where synthetic data is generated using non-parametric methods (e.g., GANs) that lack explicit likelihood estimates?",
              "Are the reported performance gains statistically significant across all benchmark configurations?",
              "How does Gen-LRA compare to model-agnostic MIAs in terms of computational cost and scalability?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "02kZwCo0C3": {
    "paper_id": "02kZwCo0C3",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes SAIL, a unified framework for online reinforcement learning from human feedback (RLHF) in large language models (LLMs). SAIL addresses challenges like distribution shift and reliance on preference oracles by formulating online alignment as a bilevel optimization problem, reducing it to a computationally efficient single-level method. The approach emphasizes continuous self-improvement through online exploration and iterative feedback refinement, demonstrating significant performance gains over existing methods."
          },
          "strengths": {
            "value": "The paper presents a novel approach to online RLHF by formalizing alignment as a bilevel optimization problem and reducing it to a single-level method, which addresses distribution shift while maintaining computational efficiency. The work is theoretically grounded, with clear motivation for tackling critical challenges in existing methods. The experimental results show substantial improvements in win rate and evaluation rewards, suggesting practical relevance. The paper also introduces a self-improvement mechanism that reduces dependence on preference oracles, a key contribution for real-world deployment."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental comparisons with state-of-the-art online RLHF methods beyond DPO, making it difficult to assess the magnitude of SAIL's improvements. The theoretical analysis of convergence and computational complexity is not sufficiently elaborated. The description of the self-improvement mechanism is vague, with no concrete analysis of how it mitigates preference oracle dependence. Additionally, the experiments mention 'extensive' evaluations but do not specify datasets, baselines, or statistical significance of results."
          },
          "questions": {
            "value": "1. How does SAIL specifically address distribution shift without relying on a preference oracle? 2. What are the exact datasets and baselines used in the experiments, and how do they compare to other online RLHF methods? 3. Can the authors provide a detailed analysis of the computational efficiency claims, including runtime or resource comparisons with DPO? 4. How is the 'self-improving' mechanism implemented, and what guarantees exist for its effectiveness in reducing oracle dependence?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes SAIL, a unified framework for online reinforcement learning from human feedback (RLHF) in large language models (LLMs). SAIL addresses challenges in online RLHF, such as distribution shift and reliance on preference oracles, by formulating alignment as a bilevel optimization problem and reducing it to a computationally efficient single-level method. The approach claims performance improvements over existing methods, including higher win rates and evaluation rewards."
          },
          "strengths": {
            "value": "The paper introduces a novel framework for online RLHF, addressing critical challenges like distribution shift and preference oracle dependency. The theoretical foundation in bilevel optimization and the connection to DPO-style methods show originality. The proposed self-improvement mechanism is promising for reducing reliance on external supervision. The paper also emphasizes computational efficiency, which is crucial for real-world deployment."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete, with key details (e.g., datasets, baselines, ablation studies) missing due to content truncation. The claims of 11.6% win rate improvement and 3.6-point reward gains lack context, such as comparison to specific state-of-the-art methods or statistical significance. The theoretical analysis of the single-level reduction and convergence guarantees is not fully presented. The mechanism for relaxing the preference oracle assumption is not explained in detail."
          },
          "questions": {
            "value": [
              "What specific datasets and baselines were used in the experiments? How do the results compare to existing online RLHF methods like PARL or recent approaches?",
              "How was the 'self-improving mechanism' implemented? What metrics were used to evaluate its effectiveness in reducing preference oracle dependency?",
              "Are the claimed performance gains statistically significant? What hyperparameters or training details were used to achieve these results?",
              "Can the paper provide ablation studies showing the contribution of each component (e.g., the single-level reduction, exploration term, self-improvement mechanism)?",
              "What are the limitations of the bilevel-to-single-level reduction? Are there scenarios where the approximation might fail?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes SAIL, a method for online alignment of large language models (LLMs) by formalizing the problem as a bilevel optimization task. The approach reduces this to a computationally efficient single-level optimization using reward-policy equivalence, enabling continuous self-improvement through online exploration and iterative feedback. SAIL aims to address challenges in online RLHF, including distribution shift, computational intractability, and reliance on preference oracles, with experimental results showing performance improvements over existing methods."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel bilevel optimization framework for online RLHF and proposes a single-level reduction that preserves theoretical guarantees while improving efficiency. The self-improvement mechanism that reduces reliance on preference oracles is also innovative. Quality: The experimental setup includes multiple datasets and metrics (win rate, evaluation rewards), with quantitative improvements reported. Clarity: The problem formulation and contributions are well-structured, and the paper provides a clear motivation for online alignment. Significance: Addressing distribution shift and computational efficiency in online RLHF is critical for real-world deployment of LLMs, making this work relevant to the community."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how SAIL mitigates distribution shift, despite claiming to address this challenge. The self-improvement mechanism is described conceptually but lacks concrete implementation details or theoretical justification. The experiments are incomplete (the figure and some sections are truncated), and the comparison with baselines is limited—e.g., it's unclear how SAIL compares to PARL or other online RLHF methods. The computational overhead claims are not substantiated with rigorous measurements. The paper also does not discuss potential limitations, such as sensitivity to hyperparameters or scalability to larger models."
          },
          "questions": {
            "value": "1. How does SAIL explicitly address the interdependence between model updates and data generation to mitigate distribution shift? 2. What is the exact mechanism of the self-improvement loop, and how does it avoid reliance on preference oracles? 3. Are the experiments conducted on standard benchmarks (e.g., HF datasets) or custom setups, and how do they compare to prior work? 4. What are the computational costs of SAIL compared to DPO or other methods, and how does it scale to larger models? 5. Are there any ablation studies to validate the contribution of specific components (e.g., the gradient term for exploration)?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "04RGjODVj3": {
    "paper_id": "04RGjODVj3",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes HyperEEGNet, an architecture combining HyperNetworks with EEGNet to adaptively generate weights for motor imagery classification using resting-state EEG data. The method aims to reduce calibration time and improve generalization across users and sessions, demonstrating competitive performance on small and large datasets."
          },
          "strengths": {
            "value": "Originality: Introduces HyperNetworks for adaptive weight generation in EEG decoding, leveraging underutilized resting-state data. Quality: Clear methodology with detailed preprocessing and connectivity analysis. Clarity: Well-structured paper with explicit contributions and experimental setup. Significance: Addresses critical BCI challenges (calibration time, cross-user generalization) with potential for edge computing applications."
          },
          "weaknesses": {
            "value": "Limited experimental validation: Key results (e.g., specific accuracy metrics, comparison with state-of-the-art baselines) are missing due to truncated content. Overreliance on small dataset: Claims of generalization capabilities are based on 9 participants, which may not be statistically robust. Lack of ablation studies: No analysis of HyperNetworks' specific contribution or comparison with alternative weight-generation methods. Underdeveloped theoretical justification: Minimal discussion of why HyperNetworks are suitable for this task compared to other meta-learning approaches."
          },
          "questions": {
            "value": "1. What specific performance metrics (accuracy/F1/ROC-AUC) were achieved on the BNCI 2014 dataset? 2. How does HyperEEGNet compare to established transfer learning approaches for BCI? 3. What is the model size and computational efficiency compared to EEGNet? 4. Were there any failure cases in the cross-user scenario that could reveal limitations? 5. How was the resting-state connectivity analysis quantitatively linked to classification performance?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper proposes HyperEEGNet, a novel architecture combining HyperNetworks with EEGNet to generate adaptive weights for motor imagery classification using resting-state EEG data. It demonstrates the model's ability to generalize across users and sessions, leveraging resting-state data to reduce calibration time and improve edge computing feasibility."
          },
          "strengths": {
            "value": "Originality: Introduces HyperNetworks for adaptive weight generation in motor imagery classification, leveraging underutilized resting-state EEG data. Quality: Experiments show competitive performance on small datasets and scalability with larger data. Clarity: Well-structured paper with clear problem motivation and methodology. Significance: Addresses critical BCI challenges like user-specific calibration and real-world deployment, with potential for practical impact."
          },
          "weaknesses": {
            "value": "The integration of HyperNetworks into EEGNet is not thoroughly explained, leaving gaps in understanding how adaptive weights are generated. Experiments on the 33-participant dataset lack detailed analysis of generalization capabilities. Comparisons with state-of-the-art methods (e.g., transfer learning, traditional deep learning) are missing. The role of specific resting-state features in performance improvements is not quantified. Ablation studies or analysis of hyperparameter sensitivity are absent."
          },
          "questions": {
            "value": [
              "How exactly are HyperNetworks integrated with EEGNet? What are the key architectural differences compared to standard EEGNet?",
              "What specific features or patterns from resting-state EEG data contribute most to the model's performance? Are there domain-specific insights?",
              "How does HyperEEGNet compare to baseline methods like transfer learning or other BCI-specific architectures on the same datasets?",
              "What are the exact evaluation metrics (e.g., accuracy, F1-score) and how do they compare to EEGNet on the 9-participant dataset?",
              "How does the model handle variations in resting-state data quality across participants? Are there ablation studies on noisy channel data?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes HyperEEGNet, a novel architecture combining HyperNetworks (HNs) with EEGNet to generate adaptive weights for motor imagery (MI) classification using resting-state EEG data. The method aims to improve cross-user and cross-session generalization by leveraging resting-state data, demonstrating comparable performance to EEGNet on a 9-participant dataset and better scalability with 33 participants. The work also highlights potential benefits for edge computing due to smaller model footprints."
          },
          "strengths": {
            "value": "The paper introduces a creative combination of HyperNetworks with EEGNet, addressing a critical gap in BCI generalization. The focus on underutilized resting-state EEG data is novel and relevant to real-world BCI deployment. The experiments on two datasets (60 and 9 participants) demonstrate scalability and generalization capabilities. The potential impact on edge computing and reduced calibration time is significant. The work also contributes to understanding the utility of resting-state data for downstream tasks."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art models beyond EEGNet, limiting the assessment of novelty. The integration of HyperNetworks into EEGNet is not clearly explained, with insufficient technical details about how HNs adapt weights. The experiments on the smaller BNCI 2014 dataset are minimal, and the results are not thoroughly analyzed. The theoretical justification for using HNs in this context is missing. The computational efficiency claims for edge computing are not empirically validated. The paper is cut off mid-methodology, leaving critical details unresolved."
          },
          "questions": {
            "value": "1. How are HyperNetworks specifically integrated into EEGNet's architecture? What are the key design choices for weight generation? 2. Are there ablation studies to isolate the contribution of HNs versus traditional EEGNet? 3. What is the exact performance metric comparison between HyperEEGNet and EEGNet on the 9-participant dataset? 4. How does the model handle variability in resting-state data across participants? 5. What are the computational costs of HNs compared to baseline models, and how do they scale with larger datasets? 6. Are there any limitations in the BNCI 2014 dataset that could affect the generalizability of results?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "04qx93Viwj": {
    "paper_id": "04qx93Viwj",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper provides a comprehensive analysis of the environmental impact of developing large language models (LLMs), focusing on carbon emissions, water consumption, and energy use across three stages: model development, training, and inference. The authors estimate that model development accounts for ~50% of total emissions, highlight fluctuations in power consumption during training, and emphasize the need for transparency in AI environmental reporting."
          },
          "strengths": {
            "value": "Originality is strong, as the paper systematically addresses the often-overlooked environmental costs of model development, not just training. The methodology is rigorous, incorporating sub-second power measurements and accounting for Scope 1/2/3 emissions and water usage. Clarity is good, with detailed explanations of metrics and their real-world analogies (e.g., '2.1 gasoline tanker trucks'). Significance is high, as it raises critical awareness about the hidden costs of LLM development and sets a benchmark for future transparency."
          },
          "weaknesses": {
            "value": "The scope is limited to the authors' own models, which may not generalize to other systems. The paper lacks comparative analysis with existing environmental studies on AI, making it hard to contextualize their findings. Some claims (e.g., '50% of training impact') rely on assumptions about development workloads that are not fully justified. The methodology section is incomplete, leaving gaps in how specific metrics like embodied carbon were calculated."
          },
          "questions": {
            "value": "1. How were the assumptions about model development workloads validated? Were they based on internal logs or approximations? 2. Could the authors provide more details on how regional carbon intensity factors were applied during training? 3. How do the authors reconcile their findings with previous studies that report lower development impact? 4. What steps were taken to account for uncertainties in water consumption estimates, particularly for hardware manufacturing?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper provides a comprehensive assessment of the environmental impact of developing large language models (LLMs), including hardware manufacturing, model development, and training. The authors estimate carbon emissions and water consumption across multiple model sizes (20M-13B parameters) and highlight that model development accounts for ~50% of the total environmental impact, surpassing previous studies that focused primarily on training. They also analyze power consumption fluctuations during training and emphasize the need for greater transparency in AI environmental reporting."
          },
          "strengths": {
            "value": "The paper's originality lies in its holistic approach, extending beyond training to include hardware manufacturing, model development, and water usage—areas often overlooked in prior work. The methodology is rigorous, incorporating Scope 1, 2, and 3 emissions, sub-second power measurements, and region-specific carbon intensity. The significance is high, as it addresses a critical gap in AI sustainability research and raises awareness about the hidden costs of model development. The clarity of the problem statement and the structured presentation of findings are commendable."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodological specifics, particularly in calculating embodied carbon and water consumption, which are critical for reproducibility. The methodology section is incomplete, leaving key questions unanswered (e.g., how upstream/downstream impacts were quantified). The experimental validation is limited, with no comparison to alternative estimation frameworks or sensitivity analyses. Additionally, the paper's focus on a single model series (OLMo) reduces generalizability, and the absence of benchmarks for their metrics weakens the impact of their claims."
          },
          "questions": {
            "value": "1. How were upstream embodied carbon and water consumption calculated, and what data sources were used for hardware manufacturing? 2. What assumptions were made about energy efficiency and carbon intensity during model development phases? 3. How do the authors address regional variability in water scarcity and energy grid composition? 4. Can the methodology be adapted to other model architectures or training environments? 5. What are the limitations of using sub-second power measurements for estimating long-term environmental impacts?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper provides a comprehensive assessment of the environmental impact of developing large language models (LLMs), including hardware manufacturing, model development, training, and inference. The authors estimate carbon emissions and water consumption across multiple model sizes (20M to 13B parameters) and highlight that model development accounts for ~50% of total environmental impact. They also analyze power consumption fluctuations during training and advocate for greater transparency in AI environmental reporting."
          },
          "strengths": {
            "value": "Originality: The paper pioneers a holistic evaluation of LLM environmental impact, covering underreported aspects like model development and embodied carbon. Quality: The methodology includes granular power consumption measurements and detailed lifecycle analysis. Clarity: The paper is well-structured, with clear explanations of metrics and contextualization of findings. Significance: It addresses a critical gap in AI sustainability research, urging industry-wide transparency and accountability."
          },
          "weaknesses": {
            "value": "The paper lacks comparative analysis with other studies beyond cited works, limiting the ability to contextualize its findings. Water usage estimates rely on assumptions about data center efficiency, which could vary widely. The environmental impact of different regions/data centers is not thoroughly explored. Mitigation strategies or actionable solutions are not discussed, focusing only on impact quantification."
          },
          "questions": {
            "value": "How were embodied carbon and water consumption calculated for hardware manufacturing? What assumptions were made about data center efficiency for water usage estimates? How do these findings compare to other LLMs not mentioned in the paper? Could the methodology be adapted to assess smaller-scale AI systems? What are the limitations of using region-specific carbon intensity data?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "06B23UkNid": {
    "paper_id": "06B23UkNid",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces MV-CLAM, a framework that leverages a multi-querying transformer (MQ-Former) to align 2D and 3D molecular structures with text. The approach aims to address challenges in multi-modal molecular representation learning by enabling simultaneous cross-modal projection, improving molecule-text retrieval, captioning, and zero-shot tasks."
          },
          "strengths": {
            "value": "The paper tackles a relevant and timely problem in molecular language modeling, addressing the limitations of single-modal alignment. The MQ-Former's shared self-attention mechanism offers a novel way to unify 2D/3D molecular embeddings into a single text token, which could enhance interpretability. The motivation is well-justified, and the potential applications in drug discovery and chemical analysis are significant. The paper also highlights computational efficiency gains compared to separate alignment modules."
          },
          "weaknesses": {
            "value": "The paper is truncated, making it impossible to evaluate the full experimental setup, ablation studies, or comparisons with state-of-the-art methods. Key technical details about MQ-Former's architecture and the cross-modal projection mechanism are missing. The claims of 'state-of-the-art performance' lack concrete metrics or baseline comparisons. The zero-shot editing and question-answering results are mentioned but not elaborated, leaving their validity unclear."
          },
          "questions": {
            "value": "1. What specific design choices in MQ-Former enable it to unify 2D and 3D molecular embeddings? 2. How does the paper address the challenge of aligning heterogeneous modalities (e.g., graph-based 2D vs. coordinate-based 3D) within a shared space? 3. What baselines were used for molecule-text retrieval and captioning, and how do they compare to existing methods like Q-Former or MolCA? 4. Can the authors provide additional evidence for the computational efficiency claims? 5. How are zero-shot molecule editing and question-answering evaluated, and what metrics are used?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes MV-CLAM, a framework that integrates 2D and 3D molecular representations into a unified text token using a novel multi-querying transformer (MQ-Former). The method aims to address limitations in cross-modal alignment between molecular structures and text by employing shared self-attention layers and a cross-model projector, demonstrating improvements in molecule-text retrieval, captioning, and zero-shot tasks."
          },
          "strengths": {
            "value": "The paper introduces a novel architecture (MQ-Former) for multi-view molecular alignment, which addresses a gap in existing work that focuses on single-modal representations. The approach is theoretically grounded in reducing computational overhead and improving consistency through shared embeddings. The potential applications in drug discovery and molecular interpretation are significant, and the paper emphasizes practical benefits like zero-shot molecule editing. The problem formulation is timely, given the growing interest in multi-modal molecular modeling."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as comparisons against state-of-the-art multi-modal methods like GIT-Mol or MolLM. Key claims about computational efficiency and performance improvements are not substantiated with quantitative results. The method's ability to handle complex molecular structures (e.g., large molecules) is unaddressed, and there is no ablation study to isolate the contributions of MQ-Former. The truncated content also prevents evaluation of the full methodology and results."
          },
          "questions": {
            "value": "1. What specific baselines were used for comparison in molecule-text retrieval and captioning tasks? 2. How does MQ-Former achieve 'more than half' reduction in training time, and what metrics were used to measure this? 3. Are there ablation studies demonstrating the necessity of the shared self-attention layer or cross-modal projector? 4. How does the framework handle the inherent differences between 2D graph and 3D conformer representations? 5. What are the limitations of the zero-shot molecule editing results, and how were they validated?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces MV-CLAM, a framework that integrates 2D and 3D molecular representations into a unified text embedding space using a novel multi-querying transformer (MQ-Former). The approach aims to address challenges in cross-modal alignment between molecular structures and text, demonstrating improvements in molecule-text retrieval, captioning, and zero-shot tasks."
          },
          "strengths": {
            "value": "Originality is evident in combining 2D and 3D molecular views for cross-modal alignment, which is less explored in prior work. The methodology introduces a shared self-attention mechanism to consolidate multi-view embeddings, which could enhance interpretability. The paper highlights computational efficiency gains (over 50% faster training) compared to single-view approaches. Clarity is strong in explaining the problem and framework design, while the significance lies in advancing molecular-language modeling for biomedical applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the necessity of MQ-Former's components or the benefits of multi-view alignment. Experimental results are not fully presented (the content is truncated), so it is unclear if the claimed state-of-the-art performance is robust across diverse benchmarks. The comparison with existing methods like Q-Former or GIT-Mol is insufficient, and the paper does not address potential limitations of the shared self-attention approach (e.g., information loss during consolidation)."
          },
          "questions": {
            "value": "How does MQ-Former specifically differ from existing cross-modal frameworks like Q-Former? What metrics were used to evaluate zero-shot molecule editing and question-answering tasks? Are there cases where multi-view alignment fails, and how are they handled? The paper mentions computational efficiency gains—what exact metrics (e.g., FLOPs, training time) support this claim? How were hyperparameters tuned for the shared self-attention layer?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "06ZvHHBR0i": {
    "paper_id": "06ZvHHBR0i",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes a multi-agent framework where large language models (LLMs) act as advocates, judges, and juries to evaluate LLM outputs, inspired by legal and decision-theoretic principles. It introduces two architectures (MORE and SAMRE) and a probabilistic model for error reduction, but lacks detailed experimental validation."
          },
          "strengths": {
            "value": "The paper demonstrates strong theoretical originality by integrating legal, psychological, and decision-theoretic concepts into a novel multi-agent evaluation framework. The motivation is well-reasoned, and the architectural ideas (e.g., courtroom-inspired debate, voting-based juries) show creativity. The structure and writing are clear, and the problem of LLM evaluation is significant. However, the lack of concrete experiments limits the assessment of practical impact."
          },
          "weaknesses": {
            "value": "The paper fails to provide specific experimental results or baselines for comparison, making it impossible to validate the claimed advantages of the proposed framework. The probabilistic model for error reduction is described only abstractly without mathematical formulation or empirical support. The architectures (MORE/SAMRE) are illustrated in a figure, but their implementation details, training procedures, and evaluation metrics remain unspecified. The connection between the theoretical motivations (e.g., bounded rationality, persuasion theories) and the technical framework is not clearly articulated."
          },
          "questions": {
            "value": "1. What specific experiments were conducted to evaluate the MORE/SAMRE architectures? What datasets, metrics, and baselines were used? 2. How is the probabilistic model for error reduction defined mathematically, and what evidence supports its validity? 3. How do the authors address potential issues like adversarial manipulation of advocates or bias in jury voting? 4. Are there quantitative results demonstrating improvements over existing LLM evaluation methods?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes a novel framework for evaluating large language models (LLMs) by simulating a courtroom-like multi-agent system where LLMs act as advocates, judges, and juries. The approach integrates theories from decision theory, legal systems, psychology, and voting theory to create a dynamic evaluation process. Two architectures, MORE (multiple advocates per answer) and SAMRE (single advocate with multiple rounds), are introduced to assess LLM outputs through structured debate and aggregation of judgments."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by combining interdisciplinary theories (e.g., legal adversarial processes, bounded rationality) into a novel multi-agent framework for LLM evaluation. The clarity of the problem statement and theoretical motivation is commendable, with a structured organization that connects abstract concepts to practical architectures. The significance is high, as the work addresses a critical gap in LLM evaluation methods, potentially offering a scalable alternative to human or automated metrics. The probabilistic model for error reduction and the focus on bias mitigation further strengthen the contribution."
          },
          "weaknesses": {
            "value": "The paper lacks concrete experimental validation of the proposed architectures. While the framework is theoretically compelling, there is no evidence of how the MORE/SAMRE systems perform compared to baselines (e.g., human evaluations or existing metrics). Key details about implementation (e.g., how LLMs are trained as advocates, how juries aggregate judgments) are absent. Additionally, the paper does not address potential scalability issues or computational costs of the multi-agent system, which could limit practical applicability."
          },
          "questions": {
            "value": [
              "What specific experiments were conducted to validate the MORE and SAMRE architectures? How do they compare to existing evaluation methods in terms of accuracy or efficiency?",
              "How are the LLM advocates trained or prompted to act as 'advocates' versus 'judges'? Are there specific instructions or fine-tuning procedures described?",
              "The paper mentions a probabilistic model for error reduction—what assumptions underlie this model, and how is it integrated into the framework?",
              "Are there limitations to the adversarial setup (e.g., risk of collusion among advocates or overfitting to specific tasks)? How does the framework handle edge cases or ambiguous outputs?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes a novel framework for evaluating large language models (LLMs) by using LLMs themselves as interacting agents in a courtroom-inspired multi-agent system. The approach involves LLMs acting as advocates, judges, and juries to perform dynamic, context-aware evaluations. The authors introduce two architectures (MORE and SAMRE) and outline a probabilistic model for error reduction, but the paper lacks concrete experiments or results to validate their claims."
          },
          "strengths": {
            "value": "The paper demonstrates originality by integrating concepts from decision theory, legal theory, and voting theory into an LLM evaluation framework. The interdisciplinary inspiration is a strong point, and the proposed multi-agent system offers a novel perspective on automated evaluation. The structured presentation and clear organization of sections (e.g., motivation, contributions, related work) show methodological rigor. However, the lack of specific experimental details limits the assessment of quality and significance."
          },
          "weaknesses": {
            "value": "The paper is overly theoretical and lacks concrete experiments, baselines, or quantitative results to substantiate its claims. Key components like the probabilistic model and architectures (MORE/SAMRE) are not elaborated with implementation details or validation. The related work section is incomplete, and the paper fails to address critical limitations (e.g., computational costs, scalability). The absence of comparisons to existing evaluation methods (e.g., human judgments, automated metrics) undermines the significance of the proposed framework."
          },
          "questions": {
            "value": "1. What specific tasks or datasets were used to evaluate the MORE/SAMRE architectures? 2. How does the probabilistic model quantify error reduction, and what assumptions does it rely on? 3. Are there ablation studies to isolate the impact of multi-advocate systems versus single-agent evaluations? 4. How do the authors address potential biases in LLM-generated 'judgments' given their own training data? 5. What are the computational requirements and scalability challenges of the proposed framework?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "09LEjbLcZW": {
    "paper_id": "09LEjbLcZW",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "AutoKaggle提出一个基于多智能体协作的框架，通过阶段式工作流、迭代调试和机器学习工具库自动化处理数据科学竞赛任务。该框架将数据科学流程分解为六个阶段，并通过五个专用智能体（Reader, Planner, Developer, Reviewer, Summarizer）协同完成，实验在Kaggle竞赛中验证了其有效性，取得了0.85的验证提交率和0.82的综合得分。"
          },
          "strengths": {
            "value": "原创性：提出阶段式多智能体协作框架，结合迭代调试与可解释性报告，填补了自动化数据科学工具在复杂任务分解和透明性方面的空白。质量：实验设计针对真实Kaggle竞赛场景，量化指标（提交率、综合得分）直观反映效果。清晰度：框架结构（图1）和阶段划分（六阶段流程）逻辑清晰，技术描述详实。显著性：解决数据科学自动化中的关键问题（任务分解、代码质量、可解释性），对提升领域生产力具有实际价值。"
          },
          "weaknesses": {
            "value": "对比实验不足：未与现有自动化工具（如AutoML框架或LLM代理）进行直接对比，缺乏基线方法的性能基准。工具库透明度低：机器学习工具库的具体实现细节（如专家代码片段来源、自动生成代码的逻辑）未充分披露。可解释性深度有限：报告系统仅描述生成内容，未说明如何量化评估决策过程的可解释性。实验泛化性存疑：仅在8个Kaggle竞赛中验证，未测试不同数据分布或任务复杂度的鲁棒性。"
          },
          "questions": {
            "value": [
              "能否补充与主流自动化工具（如AutoML、Hugging Face AutoTrain）的对比实验？具体基线方法的性能指标如何？",
              "机器学习工具库中专家代码片段的来源和验证机制是什么？自动生成代码如何确保领域知识的准确性？",
              "报告系统中决策过程的可解释性如何量化评估？是否通过用户研究或指标验证其教育价值？",
              "框架在处理非结构化数据（如文本/图像）或超大规模数据集时的扩展性如何？"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces AutoKaggle, a multi-agent framework for automating data science tasks in Kaggle competitions. It proposes a phase-based workflow with specialized agents (Reader, Planner, Developer, Reviewer, Summarizer) and integrates iterative debugging, unit testing, and a machine learning tools library. The framework is evaluated on 8 Kaggle competitions, achieving a validation submission rate of 0.85 and a comprehensive score of 0.82."
          },
          "strengths": {
            "value": "The paper presents a structured, phase-based approach to data science automation, which addresses the complexity of real-world pipelines. The multi-agent collaboration framework is novel in its integration of specialized roles for task decomposition. The inclusion of iterative debugging and unit testing enhances code reliability. The evaluation on real Kaggle competitions demonstrates practical applicability, and the comprehensive reporting system improves transparency and interpretability, aligning with the goal of democratizing data science."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of how agents collaborate, particularly the interactions between the Planner, Developer, and Reviewer. The evaluation metrics (0.85 and 0.82) are not contextualized against baselines or human performance, making it difficult to assess their significance. The machine learning tools library is described but not thoroughly characterized, leaving unclear how it differs from existing toolkits. Additionally, the paper does not address limitations, such as scalability to larger datasets or handling of non-tabular data."
          },
          "questions": {
            "value": "1. How are the agents' responsibilities and interactions formally defined? For example, what criteria does the Planner use to generate strategies, and how does the Reviewer validate the Developer's code? 2. What baselines or human performance metrics were used to compare AutoKaggle's 0.85 and 0.82 scores? 3. How does the machine learning tools library handle domain-specific knowledge, and what evidence supports its effectiveness? 4. Are there ablation studies demonstrating the contribution of individual components (e.g., phase-based workflow vs. multi-agent collaboration) to the overall performance?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces AutoKaggle, a multi-agent framework for automated data science competitions. It proposes a phase-based workflow with five specialized agents (Reader, Planner, Developer, Reviewer, Summarizer) to handle tasks like data cleaning, feature engineering, and model building. The framework emphasizes iterative debugging, unit testing, and comprehensive reporting to ensure code quality and transparency. Evaluation on 8 Kaggle competitions shows a validation submission rate of 0.85 and a comprehensive score of 0.82."
          },
          "strengths": {
            "value": "The paper presents a novel multi-agent system with a structured phase-based workflow, addressing the complexity of data science tasks. The integration of iterative debugging and unit testing improves code reliability, while the comprehensive reporting enhances interpretability. The framework's adaptability and user-centric design are strengths. The use of real-world Kaggle competitions for evaluation adds practical relevance."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparison with existing frameworks or baselines, making it hard to assess the novelty and effectiveness of AutoKaggle. The agents' collaboration mechanisms and specific roles are not clearly explained. The machine learning tools library is mentioned but not elaborated. The evaluation metrics (0.85 and 0.82) are not contextualized against human performance or other automated systems. The paper also omits ablation studies to validate individual components' contributions."
          },
          "questions": {
            "value": "1. What are the specific baselines used for comparison (e.g., human performance, existing automated tools)? 2. How do the agents interact and delegate tasks in practice? 3. Are the evaluation metrics statistically significant, and how do they compare to prior work? 4. What is the computational cost and scalability of AutoKaggle? 5. How does the framework handle edge cases or non-tabular data types?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0EP01yhDlg": {
    "paper_id": "0EP01yhDlg",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes a novel approach to multi-token prediction in transformers by leveraging rank-r canonical tensor decomposition. The method generalizes existing rank-1 approximations to capture token dependencies, framing the problem as a mixture of experts. It claims to improve inference speed with minimal computational overhead, particularly in self-speculative decoding scenarios."
          },
          "strengths": {
            "value": "The paper introduces a theoretically grounded method connecting tensor decomposition to multi-token prediction, addressing a critical inefficiency in autoregressive models. The approach is novel in its fusion of CP decomposition with transformer architectures, and the clarity of mathematical formulations and figures enhances readability. The potential significance lies in improving sampling efficiency for practical NLP applications, with claims of scalability across model sizes."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation (the final section is cut off), making it difficult to assess the empirical claims. The connection to tensor decomposition is not sufficiently differentiated from existing methods like MoE, and the paper does not address potential issues with expert dominance in the mixture. The theoretical analysis of computational overhead and scalability remains underdeveloped."
          },
          "questions": {
            "value": [
              "How does the computational cost of the rank-r decomposition compare to baseline multi-token prediction methods?",
              "What specific baselines were used to demonstrate the 30% speed improvement claimed in the abstract?",
              "How are the mixture weights $w_\\alpha$ regularized to prevent expert dominance during training?",
              "Are there ablation studies showing the impact of different rank values $r$ on performance?",
              "How does the method scale when predicting more than 3 tokens (n > 3) as suggested in Figure 1?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces a novel approach for multi-token prediction in transformers using rank-r canonical probability decomposition, aiming to improve inference speed while maintaining accuracy. By framing the method as a mixture of experts and leveraging tensor decomposition, the authors propose a model that predicts multiple tokens simultaneously with minimal computational overhead."
          },
          "strengths": {
            "value": "Originality is strong, as the connection between multi-token prediction and tensor decomposition (specifically CP decomposition) is a fresh perspective. The method's potential to enhance speculative decoding and reduce inference latency addresses a critical challenge in NLP. The clarity of the mathematical formulation and the structured presentation of the approach are commendable. The significance is high, given the practical importance of efficient language models for real-world applications."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about experiments, ablation studies, and comparisons with baseline methods unresolved. The claim of 'minimal overhead' lacks quantitative validation. The connection to Mixture of Experts (MoE) is mentioned but not elaborated, leaving unclear how existing MoE techniques are leveraged. The auxiliary load-balancing loss is briefly mentioned but not fully explained, raising concerns about potential mode collapse or expert domination."
          },
          "questions": {
            "value": [
              "What specific experimental results (e.g., speedup metrics, token acceptance rates) support the claim of improved inference efficiency?",
              "How does the proposed method compare to existing multi-token prediction approaches (e.g., [Gloeckle et al. 2024]) in terms of accuracy and computational cost?",
              "Can the authors provide ablation studies on the rank-r parameter and its impact on performance across different model sizes?",
              "How is the MoE interpretation integrated into the training process, and what techniques are used to ensure balanced expert utilization?",
              "What is the exact implementation of the auxiliary load-balancing loss, and how does it prevent dominant experts from overshadowing others?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes a novel method for multi-token prediction in transformers by leveraging rank-r canonical tensor decomposition. It generalizes prior rank-1 approaches to capture token dependencies, framing the model as a mixture of experts with learnable weights. The approach aims to improve inference speed while maintaining accuracy, with claims of scalability across model sizes."
          },
          "strengths": {
            "value": "The paper introduces a novel connection between multi-token prediction and tensor decomposition, offering a theoretically grounded approach to model dependencies among future tokens. The method's integration into existing transformer architectures with minimal overhead is a practical advantage. The use of softmax constraints and logsumexp for numerical stability demonstrates careful design. The potential for scalability and compatibility with speculative decoding highlights its significance for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as quantitative speed improvements, comparison with baselines, or ablation studies on hyperparameters like rank r. The claimed 'minimal overhead' is not supported by computational cost analysis. The figure referenced in the text is missing, hindering understanding of the architecture. The connection to Mixture of Experts (MoE) is superficial, with no discussion of how the auxiliary load-balancing loss impacts training dynamics. The paper also fails to address limitations of the tensor decomposition approach, such as potential overfitting or scalability issues for large r."
          },
          "questions": {
            "value": [
              "What specific metrics (e.g., tokens per second, latency reduction) demonstrate the claimed speed improvements?",
              "How does the model's performance vary with different values of r, and what is the trade-off between rank and computational cost?",
              "Are there comparisons with alternative multi-token prediction methods (e.g., grouped attention, recurrent extensions)?",
              "How is the auxiliary load-balancing loss implemented in practice, and what evidence supports its effectiveness?",
              "What are the exact computational costs of the rank-r decomposition compared to the baseline single-token prediction?",
              "How does the model handle varying n (number of predicted tokens), and are there limitations for large n?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0F1rIKppTf": {
    "paper_id": "0F1rIKppTf",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces 'mirror Schrödinger bridges' as a novel framework for conditional resampling, where the goal is to generate in-distribution variations of input samples by solving a Schrödinger bridge problem between a distribution and itself. The method leverages entropy regularization and alternating minimization to produce stochastic processes that map input samples to proximal but distinct outputs, with control over the degree of variation."
          },
          "strengths": {
            "value": "The paper presents a theoretically grounded approach to an underexplored problem—mapping a distribution to itself via Schrödinger bridges. The theoretical contributions include a proof of convergence via alternating minimization and a novel interpretation of conditional resampling through entropy-regularized couplings. The work connects to established areas like optimal transport and diffusion models, while offering algorithmic simplifications. The clarity of the problem formulation and the potential practical utility of controlling sample proximity are strong points."
          },
          "weaknesses": {
            "value": "The empirical evaluation is incomplete due to the paper's truncation, leaving critical questions about the method's performance unanswered. The paper lacks detailed experiments on real-world datasets, quantitative comparisons with baselines, and ablation studies to validate the claimed benefits. Additionally, the relationship between the proposed method and existing self-transport approaches (e.g., flow matching) is not sufficiently clarified. The theoretical analysis assumes continuous-state spaces, which may limit applicability to discrete or high-dimensional settings."
          },
          "questions": {
            "value": "1. What specific datasets and metrics were used to evaluate the method's effectiveness in generating proximal samples? 2. How does the proposed method compare to existing self-transport approaches (e.g., flow matching) in terms of computational efficiency and sample quality? 3. Are there theoretical guarantees for the convergence of the alternating minimization procedure under practical constraints (e.g., finite data)? 4. How does the method handle high-dimensional or discrete data, where the assumptions of continuous-state spaces may not hold?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces mirror Schrödinger bridges as a novel framework for conditional resampling, focusing on mapping a distribution to itself. The key idea is to leverage the Schrödinger bridge problem between a distribution and itself to generate in-distribution variations of input samples. The authors theoretically analyze the method's properties, including its connection to entropy-regularized transport, and demonstrate its potential for controlled sample generation."
          },
          "strengths": {
            "value": "The paper presents a fresh perspective on Schrödinger bridges by focusing on self-mapping, an underexplored area. The theoretical analysis of time symmetry and alternating minimization procedures shows originality in connecting optimal transport with stochastic processes. The potential applications in generating proximal samples with controlled variation highlight the practical significance. The paper also provides a clear motivation for why this problem matters, particularly in scenarios requiring in-distribution perturbations."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete (the paper appears truncated), leaving critical questions about the method's empirical performance unanswered. The paper lacks comparisons with baseline methods for conditional resampling, making it difficult to assess the practical benefits. The theoretical guarantees are somewhat abstract, and it's unclear how the proposed method addresses specific limitations of existing approaches like diffusion models or optimal transport. Additionally, the connection to prior work on self-transport maps is not thoroughly discussed."
          },
          "questions": {
            "value": "1. Can the authors clarify how mirror Schrödinger bridges differ fundamentally from existing methods like diffusion models or score-based models in terms of their mathematical formulation and computational efficiency? 2. What are the specific metrics used to evaluate the quality of generated proximal samples, and how do they compare to baselines? 3. How sensitive is the method to hyperparameters like the noise level σ in the Ornstein-Uhlenbeck process? 4. Are there theoretical guarantees about the convergence of the alternating minimization procedure? 5. How does the proposed method handle high-dimensional data, and what are its computational limitations?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces mirror Schrödinger bridges, a novel framework for conditional resampling by solving the Schrödinger bridge problem between a distribution and itself. The method generates in-distribution variations of input samples through entropy-regularized transport, with theoretical guarantees on convergence via alternating minimization and practical control over sample proximity."
          },
          "strengths": {
            "value": "The paper demonstrates originality by focusing on self-mapping via Schrödinger bridges, an underexplored direction in optimal transport. The theoretical analysis leverages time symmetry to establish convergence properties, and the conceptual framework for controlling in-distribution variations is novel. The paper situates its work within a comprehensive literature review, highlighting connections to entropy-regularized OT, expectation maximization, and stochastic interpolants. The clarity of the problem formulation and the potential practical implications for generative modeling are notable."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation, with no experiments or quantitative results to support claims about the method's effectiveness. The theoretical analysis of the alternating minimization procedure is superficial, omitting critical details on convergence guarantees or computational complexity. The definition of 'proximal samples' is vague, and the paper does not clarify how the method compares to existing techniques like diffusion models or score-based generative models. The truncated content also prevents a full assessment of the practical implementation and ablation studies."
          },
          "questions": {
            "value": "1. How is the alternating minimization procedure implemented practically? What are the key hyperparameters and their impact on performance? 2. What metrics are used to quantify 'proximity' between generated and input samples, and how is this controlled? 3. How does the proposed method compare to existing self-transport approaches (e.g., Albergo et al. 2023) in terms of efficiency and quality? 4. What datasets and benchmarks were used for experiments, and what baselines were compared against? 5. Are there limitations to the path measure formulation that could affect scalability or applicability to high-dimensional data?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0GzqVqCKns": {
    "paper_id": "0GzqVqCKns",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper explores the use of diffusion models to probe the latent hierarchical structure of data. The authors theoretically predict that in hierarchical models, data changes during forward-backward diffusion occur in correlated chunks with a diverging length scale at a phase transition. They validate this prediction on text and image datasets using state-of-the-art diffusion models, demonstrating how latent variable changes manifest in observable data."
          },
          "strengths": {
            "value": "The paper's originality lies in applying diffusion models to analyze data hierarchies, a novel approach. The theoretical analysis using mean-field models and experimental validation on diverse modalities (text/image) are robust. The clarity of the methodology and the significance of understanding data structure for machine learning are strong. The work bridges theoretical insights with practical applications, offering a tool to study latent hierarchies in real-world data."
          },
          "weaknesses": {
            "value": "The paper lacks a clear definition of the 'phase transition' in the synthetic model, making it difficult to assess its relevance. The synthetic model (probabilistic context-free grammars) is not compared to alternative hierarchical frameworks. Experimental validation on real data is limited: for example, the use of masked diffusion language models (MDLM) does not address how masking affects correlation length measurements. The analysis of image data relies on patch embeddings, but the impact of this tokenization method on results is not thoroughly discussed. The dynamical susceptibility metric is mentioned but not rigorously justified or quantified."
          },
          "questions": {
            "value": [
              "How is the phase transition in the synthetic model defined, and how does it relate to the empirical observations in real data?",
              "Why was the probabilistic context-free grammar chosen as the synthetic model? Are there alternative hierarchical models that could have been used for comparison?",
              "What baseline models or controls were used to validate that the observed correlation lengths are specific to hierarchical structures?",
              "How do the results on real data (e.g., WikiText, ImageNet) compare to non-hierarchical baselines, and what statistical significance do the findings have?",
              "What are the limitations of using patch embeddings for image tokenization in this context, and how might alternative approaches affect the results?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper explores the use of diffusion models to probe the latent hierarchical structure of data. The authors theoretically predict that forward-backward diffusion processes reveal correlated changes in data at a critical noise level, which they validate on synthetic hierarchical models and real-world text/image datasets. They measure dynamical susceptibility and observe correlation length divergences, suggesting hierarchical latent variables in generative models."
          },
          "strengths": {
            "value": "Originality: The work bridges diffusion models with hierarchical data analysis, offering a novel framework to study latent structures. Quality: Theoretical analysis is rigorous, and experiments span both synthetic and real-world modalities (text/image). Clarity: The paper is well-structured with clear sections and logical flow. Significance: If validated, the findings could advance understanding of how diffusion models encode hierarchical representations, with implications for AI interpretability."
          },
          "weaknesses": {
            "value": "The synthetic hierarchical model (probabilistic context-free grammar) is simplistic and may not capture real-world data complexity. Experiments on real data lack comprehensive baselines (e.g., non-hierarchical models) and quantitative metrics beyond qualitative agreement. The phase transition point is not rigorously defined or quantified in real-world settings. The dynamical susceptibility metric is introduced without sufficient justification or comparison to alternative measures. The paper does not address potential confounding factors (e.g., model-specific biases in diffusion architectures)."
          },
          "questions": {
            "value": "1. How is the phase transition point defined in real data, and how does it relate to the synthetic model's parameters? 2. What ablation studies were performed to validate the necessity of the hierarchical structure in the synthetic model? 3. How do the results depend on the choice of noise schedule or diffusion architecture? 4. Are there quantitative metrics (e.g., correlation coefficients, statistical significance) to support the 'qualitative agreement' with theory? 5. Could the observed correlation lengths be artifacts of the specific diffusion models used (e.g., MDLMs/DDPMs)?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper explores the use of forward-backward diffusion processes to probe the latent hierarchical structure of data. The authors theoretically predict that in hierarchical models, data changes occur through correlated chunks with a diverging length scale at a phase transition point. They validate this prediction on synthetic data and demonstrate similar phenomena in real text and image datasets using state-of-the-art diffusion models."
          },
          "strengths": {
            "value": "Originality: The work creatively applies diffusion models to study latent hierarchical structures, bridging theoretical models (probabilistic context-free grammars) with practical experiments. Quality: Theoretical analysis is rigorous, and experiments on synthetic and real data (text/image) are well-designed. Clarity: The paper is structured logically, with clear sections and thorough background. Significance: Understanding latent data structure is critical for AI, and the findings could inform better generative models and data analysis."
          },
          "weaknesses": {
            "value": "The synthetic hierarchical model (probabilistic context-free grammars) may not fully capture real-world data complexity, limiting generalizability. The real-data experiments (e.g., WikiText, ImageNet) rely on indirect measurements (token correlations) that may not directly validate the theoretical phase transition. The paper lacks discussion on how to extend this framework to other modalities or handle non-hierarchical data. The dynamical susceptibility analysis is described but not thoroughly explained or visualized."
          },
          "questions": {
            "value": "How do the assumptions of the synthetic hierarchical model (e.g., tree-structured dependencies) affect the interpretation of real-world results? What are the limitations of using masked diffusion language models (MDLMs) for capturing latent variable changes in text? Can the observed correlation length peaks be distinguished from noise or other non-hierarchical effects in real data? How might the phase transition point in the theoretical model relate to practical training dynamics of diffusion models?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "0JjsZC0w8x": {
    "paper_id": "0JjsZC0w8x",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces CORAL, a context-wise order-agnostic language modeling framework that integrates iterative refinement into the LLM architecture. It employs sliding blockwise decoding with order-agnostic token prediction and backward reconstruction, enabling parallel processing and reducing inference latency. Experiments show improvements in reasoning tasks and speedups over autoregressive baselines, though code generation performance drops due to order-agnostic output inconsistencies."
          },
          "strengths": {
            "value": "The paper presents a novel approach to iterative refinement by combining order-agnostic modeling with architectural modifications, addressing efficiency and dependency capture limitations of autoregressive models. The sliding blockwise decoding strategy is theoretically sound and demonstrates practical gains in accuracy and speed on reasoning tasks. The method's integration of generalized RoPE for position awareness is creative. The empirical results are compelling, showing significant improvements on GSM8K and LogiQA, and the paper provides clear explanations of the methodology through figures and equations."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of why order-agnostic outputs degrade code generation performance, which is critical for understanding trade-offs. The experiments focus narrowly on reasoning tasks, leaving the generalizability of CORAL to other domains (e.g., code) unexplored. The theoretical justification for the sliding block decoding strategy is limited, and ablation studies on hyperparameters like context window size or block size are insufficient. Comparisons with alternative order-agnostic methods (e.g., permutation-based models or diffusion-based approaches) are not sufficiently detailed."
          },
          "questions": {
            "value": [
              "How does the sliding block decoding handle variable-length sequences or sequences requiring long-range dependencies beyond the context window?",
              "What is the exact mechanism of the generalized RoPE adaptation, and how does it ensure target-aware representations without architectural changes?",
              "Can the drop in code generation pass rates be mitigated through post-processing or hybrid approaches that combine order-agnostic and autoregressive generation?",
              "How does CORAL compare to permutation-based models or diffusion-based methods in terms of trade-offs between generation quality, speed, and flexibility?",
              "Are there ablation studies demonstrating the impact of different context window sizes (k) and block sizes (b) on performance and efficiency?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces CORAL, a novel framework for order-agnostic language modeling that integrates iterative refinement into the LLM architecture. By using sliding blockwise decoding and context-wise dependency modeling, CORAL aims to improve inference efficiency and performance on reasoning tasks while maintaining computational efficiency."
          },
          "strengths": {
            "value": "The paper presents a creative combination of order-agnostic modeling and iterative refinement, addressing a key limitation of autoregressive models. The empirical results on reasoning tasks demonstrate measurable accuracy gains and speedups, supporting the practical relevance of the approach. The methodology is well-structured, with clear explanations of the sliding block decoding mechanism and generalized RoPE adaptation. The work contributes to the ongoing effort to balance generation quality and efficiency in LLMs."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with state-of-the-art order-agnostic or non-autoregressive (NAR) models, making it difficult to assess the relative novelty and effectiveness of CORAL. The code generation results show a performance drop, but the authors do not analyze the root causes or provide ablation studies to validate the components of their approach. The experimental setup is incomplete (e.g., figures and appendices are referenced but not provided), and the two-stage training strategy is not sufficiently detailed. The paper also fails to address critical limitations of order-agnostic modeling, such as consistency guarantees or handling of variable-length sequences."
          },
          "questions": {
            "value": "1. What specific baselines were used for comparison (e.g., other NAR models, permutation-based approaches)? 2. How does CORAL handle variable-length generation tasks where order-agnostic outputs may introduce inconsistencies? 3. Why does the code generation pass rate decrease, and what mechanisms could mitigate this issue? 4. Are there ablation studies demonstrating the contribution of sliding block decoding vs. other components? 5. How is the generalized RoPE applied in practice, and what are its theoretical guarantees for order-agnostic generation? 6. What are the exact implementation details of the two-stage training strategy?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces CORAL, a context-wise order-agnostic language modeling framework that integrates iterative refinement into the architecture of large language models (LLMs). By enabling multi-token forward prediction and backward reconstruction within sliding context windows, CORAL aims to improve inference efficiency and performance on reasoning tasks while maintaining computational efficiency. The method leverages a generalized Rotary Position Embedding (RoPE) to preserve target-aware representations, and preliminary results show significant accuracy gains on GSM8K and LogiQA, though code generation tasks exhibit a trade-off between speed and quality."
          },
          "strengths": {
            "value": "The paper presents a novel approach to iterative refinement by directly embedding it into the LLM architecture, addressing limitations of autoregressive (AR) models. The method's emphasis on computational efficiency and parallel processing is well-motivated, with clear experimental validation on reasoning tasks. The introduction of sliding blockwise order-agnostic decoding and generalized RoPE demonstrates creative problem-solving. The paper also provides a thorough comparison with existing methods and acknowledges the quality-speed trade-off, showing self-awareness of limitations."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of how the sliding blockwise decoding and order-agnostic modeling are implemented, particularly how dependencies are managed within context windows. The experiments focus on reasoning tasks but do not compare with state-of-the-art non-autoregressive (NAR) or diffusion-based methods, which could provide stronger baselines. The analysis of code generation failures is superficial, and the paper does not address how to mitigate inconsistencies in order-agnostic outputs. Additionally, the theoretical justification for the proposed framework is underdeveloped, with limited discussion of how the order-agnostic approach avoids the pitfalls of prior permutation-based methods."
          },
          "questions": {
            "value": "1. How does the sliding blockwise decoding mechanism handle dependencies that span across multiple blocks? 2. What specific modifications were made to the RoPE to ensure target-aware representations, and how were these validated? 3. Are there ablation studies demonstrating the contribution of each component (e.g., sliding blocks vs. order-agnostic modeling) to the overall performance? 4. How does the paper address the inconsistency issues in code generation, and what potential solutions are proposed? 5. Could the authors provide more details on the two-stage training strategy and how it integrates order-agnostic capabilities into existing AR-LLMs?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0OB3RVmTXE": {
    "paper_id": "0OB3RVmTXE",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper identifies a critical vulnerability in diffusion models called 'concept resurgence,' where fine-tuning on unrelated data can reintroduce concepts previously erased through unlearning. The authors systematically investigate this phenomenon, demonstrating its prevalence under benign conditions and analyzing its causes through experiments with Stable Diffusion v1.4 and the MACE unlearning method."
          },
          "strengths": {
            "value": "The paper addresses a timely and important problem in AI safety, highlighting a critical gap in current unlearning techniques for diffusion models. The experimental design is thorough, with clear demonstrations of concept resurgence across multiple scenarios. The work contributes to the growing literature on model unlearning and raises important ethical considerations. The paper is well-structured, with a logical flow from problem statement to analysis."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient comparison with alternative unlearning approaches beyond MACE, limiting the generalizability of its findings. The experiments focus exclusively on Stable Diffusion v1.4, without testing other diffusion models or architectures. The analysis of 'concept resurgence' mechanisms is superficial, with no ablation studies to isolate contributing factors. The paper does not propose solutions to mitigate this vulnerability, focusing only on characterization."
          },
          "questions": {
            "value": [
              "How generalizable are the findings to other diffusion model architectures beyond Stable Diffusion v1.4?",
              "What specific aspects of the MACE unlearning method make it particularly susceptible to concept resurgence?",
              "Are there any architectural or training modifications that could prevent concept resurgence without compromising unlearning effectiveness?",
              "How does the degree of regularization during unlearning correlate with concept resurgence in different types of concepts (e.g., explicit content vs. copyright-protected material)?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper identifies 'concept resurgence' as a critical vulnerability in text-to-image diffusion models, where fine-tuning on unrelated data can reintroduce concepts previously erased through unlearning. The authors systematically investigate this phenomenon using experiments with Stable Diffusion v1.4 and the MACE unlearning method, demonstrating that even non-adversarial fine-tuning can undermine concept erasure."
          },
          "strengths": {
            "value": "The paper's originality lies in uncovering a novel vulnerability in diffusion model updates, which has significant implications for model safety. The methodology is rigorous, with systematic experiments and analysis of unlearning/fine-tuning interactions. The clarity of the problem statement and figures (e.g., Figure 1) effectively communicates the issue. The significance is high, as it challenges the reliability of incremental model updates and raises concerns about compliance with safety and privacy standards."
          },
          "weaknesses": {
            "value": "The experiments are limited to Stable Diffusion v1.4 and MACE, leaving open questions about generalizability to other models or unlearning techniques. The paper lacks concrete mitigation strategies for concept resurgence, focusing only on analysis. The theoretical explanation for why concept resurgence occurs is superficial, with limited exploration of underlying mechanisms like weight interference or representation overlap."
          },
          "questions": {
            "value": "1. How generalizable is concept resurgence across different diffusion models (e.g., SDXL, LCM) or unlearning methods beyond MACE? 2. What specific conditions (e.g., fine-tuning data size, model architecture) most strongly correlate with resurgence? 3. Are there practical mitigations (e.g., regularization techniques, architectural changes) to prevent resurgence without compromising model performance? 4. How does the choice of 'mapping concept' (e.g., generic vs. specific) affect resurgence rates, as implied by the analysis?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper identifies a critical vulnerability in text-to-image diffusion models called 'concept resurgence,' where fine-tuning on unrelated data can unintentionally reintroduce concepts previously erased through unlearning. The authors systematically investigate the prevalence, causes, and implications of this phenomenon, focusing on the state-of-the-art MACE method for mass concept erasure."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by uncovering a novel vulnerability in diffusion model updates, which has significant implications for safety and alignment. The experimental methodology is rigorous, with systematic analysis across benchmarks and thorough investigation of unlearning/fine-tuning interactions. The writing is clear and well-structured, with appropriate contextualization of related work. The findings address an important problem in AI safety, particularly for deployed models requiring continual updates."
          },
          "weaknesses": {
            "value": "The experiments primarily focus on Stable Diffusion v1.4 and MACE, limiting generalizability to other architectures or unlearning methods. The analysis of 'concept resurgence' causes lacks depth, particularly regarding how specific architectural components (e.g., cross-attention mechanisms) contribute to the phenomenon. The paper does not propose mitigation strategies, leaving the practical impact of the findings ambiguous. The ablation studies could be more comprehensive, with clearer quantification of resurgence magnitude across different unlearning strengths."
          },
          "questions": {
            "value": "1. How generalizable are these findings to other diffusion model architectures (e.g., SDXL, LCM) or unlearning approaches beyond MACE? 2. What specific architectural or training factors most strongly correlate with concept resurgence? 3. Could the observed resurgence be mitigated through modified fine-tuning protocols (e.g., constrained optimization, concept-specific regularization)? 4. How do these results compare when using alternative unlearning baselines (e.g., data deletion vs. parameter adjustment)?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "0QZcoGdmtJ": {
    "paper_id": "0QZcoGdmtJ",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces an efficient and accurate auditing procedure for f-differential privacy (f-DP) that requires only a single run of the privacy mechanism. The approach leverages the f-DP curve for fine-grained privacy accounting, addressing limitations of traditional ε, δ parameters. The authors propose a recursive analysis to bound adversary success in membership inference attacks, improving upon prior work that relied on multiple runs or loose bounds."
          },
          "strengths": {
            "value": "Originality is demonstrated through the use of f-DP curves for tighter privacy estimation, which generalizes beyond traditional ε, δ parameters. The methodology is technically sound, with a novel recursive relation to handle dependency in adversary guesses. The paper's clarity is strong, with detailed technical explanations. The significance is high, as efficient auditing is critical for practical deployment of differentially private systems."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-explanation, limiting assessment of experimental validation and completeness of the proposed algorithm. The claims about superior performance over Steinke et al. (2023) lack concrete evidence due to incomplete results. The recursive analysis's practical implementation details are unclear, and the generalization to broader canary injection scenarios is not fully elaborated."
          },
          "questions": {
            "value": "1. What specific metrics were used to demonstrate the superiority of the proposed method over Steinke et al. (2023)? 2. How are the recursive bounds implemented in practice, and what are their computational costs? 3. Can the authors provide additional details on the generalization to broader canary injection methods? 4. How does the f-DP curve analysis handle mechanisms with complex privacy behaviors not captured by simple Gaussian or DP-SGD examples?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces an efficient and accurate auditing procedure for f-differential privacy (f-DP) that requires only a single run of the target mechanism. The method leverages the f-DP curve for fine-grained privacy analysis, addressing limitations of prior approaches that relied on ε, δ parameters or required multiple runs. The authors propose a recursive bound on adversary success probabilities and demonstrate improved performance on Gaussian mechanisms and DP-SGD."
          },
          "strengths": {
            "value": "The paper's originality lies in its focus on f-DP curves, which offer a more nuanced privacy analysis than traditional ε, δ parameters. The computational efficiency (single run) and theoretical contributions, such as the recursive relation for bounding adversary success, are novel. The clarity of the technical overview and the potential significance of improving privacy auditing in large-scale ML systems are strong. The experiments on real-world mechanisms suggest practical relevance."
          },
          "weaknesses": {
            "value": "The paper is truncated, leaving critical details about the recursive bound derivation, experimental results, and comparisons with other methods incomplete. The analysis of dependencies in adversary guesses lacks depth, and the practical implications of the f-DP curve estimation are not fully explained. The method's generalizability to non-Gaussian mechanisms or complex models remains unclear."
          },
          "questions": {
            "value": [
              "How is the f-DP curve estimated for the target mechanism? What assumptions are made about the mechanism's properties?",
              "What are the limitations of the proposed method in terms of mechanism types or data distributions?",
              "How does the computational complexity of the recursive algorithm compare to existing methods like Steinke et al. (2023)?",
              "Are there scenarios where the f-DP-based auditing could fail, and how are these addressed?",
              "What is the exact relationship between the proposed recursive bound and the trade-off functions used in f-DP?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a novel auditing procedure for f-differential privacy (f-DP) that requires only a single run of a privacy mechanism, leveraging the f-DP curve for tighter empirical privacy estimates. The approach addresses limitations of prior methods by using a recursive analysis of adversary success probabilities and generalizing canary injection techniques."
          },
          "strengths": {
            "value": "Originality: The paper proposes a single-run auditing framework using f-DP curves, which offers a more granular privacy analysis than traditional ε, δ parameters. The recursive bound on adversary success probabilities represents a novel technical contribution. Quality: The method is computationally efficient, avoiding the multiple runs required by prior work. Clarity: The technical overview and problem formulation are well-structured, with clear comparisons to Steinke et al. (2023). Significance: Improving privacy auditing for large-scale ML models is critical, and the approach has potential to enhance practical deployments of differentially private systems."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about the recursive analysis, experimental results, and comparisons to prior work incomplete. The claim that f-DP provides 'tighter' estimates lacks concrete empirical validation. The generalization to broader canary injection scenarios is mentioned but not elaborated. The theoretical analysis of dependency in adversary guesses is described but not fully justified. The paper does not address how the method scales to high-dimensional data or complex models."
          },
          "questions": {
            "value": "1. What are the exact experimental results comparing the proposed method to Steinke et al. (2023) on Gaussian mechanisms and DP-SGD? 2. How is the f-DP curve computed for real-world mechanisms, and what assumptions are made about the mechanism's behavior? 3. What is the computational complexity of the proposed algorithm, and how does it scale to large datasets? 4. How does the method handle dependencies between canary guesses in practice? 5. Are there theoretical guarantees for the recursive bound under specific f-DP conditions?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0R3ha8oNPU": {
    "paper_id": "0R3ha8oNPU",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces SecCodePLT, a unified platform for evaluating security risks in Code GenAI models, focusing on insecure code generation and cyberattack helpfulness. The platform combines expert-verified data with automated generation, employs dynamic evaluation metrics, and demonstrates improved performance over existing benchmarks like CYBERSECEVAL."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in evaluating Code GenAI security risks, combining expert validation with scalable automated methods. The dual focus on insecure coding and cyberattack helpfulness covers key security scenarios. Dynamic evaluation metrics (e.g., test cases) improve precision over static methods. The platform's application to real-world models like Cursor highlights practical relevance. The structured comparison with existing benchmarks (Table 1) provides clear context for improvements."
          },
          "weaknesses": {
            "value": "The methodology for the two-stage data creation pipeline lacks technical detail (e.g., how LLM-based mutators operate, validation criteria). The dynamic evaluation environment and metrics are not described in depth, limiting reproducibility. Experiments are limited to four models, with no analysis of model-specific strengths/weaknesses. The claim of 'first platform to enable end-to-end cyberattack assessment' requires stronger justification, as similar work may exist. The paper's truncated content (e.g., Section 2 ends abruptly) hinders full evaluation of contributions."
          },
          "questions": {
            "value": "1. How exactly are LLM-based mutators used to generate vulnerability data? What safeguards prevent low-quality outputs? 2. What specific dynamic metrics (e.g., test coverage, exploit success rates) are used for evaluating cyberattack helpfulness? 3. How is the 'expert verification' process implemented? Are there quantitative measures of inter-annotator agreement? 4. Why is the Cyber Kill Chain framework prioritized over other attack models? 5. How does SecCodePLT handle false positives/negatives in dynamic testing compared to static methods?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces SecCodePLT, a unified platform for evaluating security risks in code generation AI (Code GenAI). It addresses limitations in existing benchmarks by combining expert-verified and automated data generation, incorporating dynamic testing metrics, and creating a real-world environment for cyberattack assessment. The platform demonstrates superior performance in identifying security risks compared to existing benchmarks and reveals vulnerabilities in state-of-the-art models like Cursor."
          },
          "strengths": {
            "value": "The paper's originality lies in its comprehensive approach to evaluating both insecure coding and cyberattack helpfulness, which are critical security scenarios for Code GenAI. The methodology for combining expert validation with automated data generation (e.g., LLM-based mutators) addresses scalability and quality trade-offs. Dynamic metrics (e.g., test case execution) improve precision over static rules or LLM judgments. The practical application to real-world models (e.g., Cursor) highlights its relevance. The paper also clarifies the limitations of prior work through a detailed comparative analysis (Table 1)."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of the two-stage data creation pipeline, such as how LLM-based mutators are implemented or how seed samples are validated. The dynamic evaluation environment for cyberattacks is not sufficiently explained—e.g., how are attack execution metrics defined? The experiments mention 'extensive experiments' but omit specific results (e.g., quantitative comparisons with CYBERSEVAL). The truncated Table 1 raises concerns about the completeness of benchmark comparisons. Additionally, the paper does not discuss potential biases in expert-verified data or scalability challenges in large-scale deployment."
          },
          "questions": {
            "value": [
              "How exactly are LLM-based mutators used to generate data from seed samples? What criteria define a 'valid' mutation?",
              "What specific dynamic metrics are used to evaluate cyberattack helpfulness? How are attack execution success rates measured in the real environment?",
              "The paper claims SecCodePLT outperforms CYBERSEVAL but does not provide quantitative results (e.g., F1 scores, accuracy). Can the authors share these metrics?",
              "What are the exact security risks identified in Cursor? How do these risks compare to those detected by existing benchmarks?",
              "How is the expert validation process scaled to handle large datasets? Are there plans to open-source this process for reproducibility?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces SecCodePLT, a unified platform for evaluating security risks in Code GenAI models. It addresses limitations of existing benchmarks by combining expert-verified and automated data generation, incorporating dynamic evaluation metrics, and covering both insecure coding and cyberattack helpfulness. The platform demonstrates superior performance over existing benchmarks and identifies security risks in a state-of-the-art code agent."
          },
          "strengths": {
            "value": "The paper presents a novel methodology for creating high-quality, scalable security benchmarks by integrating expert input with automated generation. It addresses critical gaps in existing work by emphasizing dynamic evaluation (e.g., test cases) over static metrics and covering both insecure coding and cyberattack scenarios. The platform's application to real-world models like Cursor reveals previously unknown security risks, showcasing its practical relevance. The structured comparison with prior benchmarks (Table 1) and the focus on hybrid metrics (static + dynamic) highlight methodological rigor."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of the dynamic evaluation framework, such as how test cases are designed or validated. The claims about superior performance over CYBERSECEVAL are not substantiated with comprehensive ablation studies or comparisons against alternative dynamic metrics. The expert verification process is vaguely described, leaving questions about how data quality is ensured at scale. Additionally, the paper does not address potential limitations of SecCodePLT, such as the cost of expert involvement or generalizability to other security scenarios. The application to Cursor is mentioned but lacks quantitative results or specific examples of identified risks."
          },
          "questions": {
            "value": "1. How are the dynamic test cases for insecure coding designed? Are they based on standardized security frameworks (e.g., OWASP)? 2. What specific metrics are used to evaluate 'prompt faithfulness' in SecCodePLT, and how do they differ from CYBERSECEVAL? 3. Can the authors provide more details on the expert validation process, including how vulnerabilities are prioritized or verified? 4. How does the platform handle false positives/negatives in dynamic evaluation, and what mitigation strategies are employed? 5. Are there plans to open-source the dynamic testing environment for reproducibility?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0RHMnPj8no": {
    "paper_id": "0RHMnPj8no",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper addresses differentially private (DP) optimization for nonsmooth nonconvex (NSNC) problems, proposing algorithms that achieve improved sample complexity bounds for finding Goldstein-stationary points. The authors introduce a single-pass DP algorithm with a sample complexity improvement of Ω(√d) over prior work and a multi-pass algorithm that further reduces complexity by leveraging empirical risk minimization (ERM) generalization. They also establish a theoretical result showing that Goldstein-stationary points from ERM generalize to the population loss."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to DP NSNC optimization by focusing on Goldstein-stationary points, which is a fresh perspective compared to traditional gradient-based methods. The theoretical contributions include tight sample complexity bounds that improve on existing results, particularly for high-dimensional settings. The work bridges DP and non-convex optimization, addressing a gap in the literature. The clarity of the problem formulation, theorems, and comparisons with prior work is strong, and the paper provides a structured analysis of both single-pass and multi-pass algorithms."
          },
          "weaknesses": {
            "value": "The paper lacks experimental validation, which limits the ability to assess the practical relevance of the theoretical improvements. The generalization result (Proposition 5.1) relies on assumptions about the relationship between empirical and population losses that are not thoroughly justified. The reliance on zero-order algorithms (with first-order variants deferred to an appendix) may raise questions about practical applicability. Additionally, the paper does not explicitly compare its bounds to other DP non-convex works beyond Zhang et al. (2024), leaving room for further contextualization."
          },
          "questions": {
            "value": "1. How do the proposed algorithms handle specific types of nonsmooth nonconvex functions (e.g., those with discontinuities or non-Lipschitz behavior)? 2. What are the exact assumptions on the Lipschitz constants or other problem parameters, and how do they affect the sample complexity bounds? 3. Can the generalization result (Proposition 5.1) be extended to other notions of stationarity or loss structures? 4. Are there practical challenges in implementing the zero-order algorithms, and how do the first-order variants in Appendix C address these? 5. How does the multi-pass algorithm's polynomial runtime scale with dimensionality and privacy parameters?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper presents improved differentially private (DP) algorithms for nonsmooth nonconvex (NSNC) optimization, achieving better sample complexity bounds for finding Goldstein-stationary points. It introduces a single-pass algorithm with a dimension-dependent improvement over prior work and a multi-pass algorithm leveraging empirical risk minimization (ERM) with generalization guarantees to the population loss."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in DP optimization for NSNC settings, which is underexplored. The theoretical contributions are substantial, with clear improvements in sample complexity bounds, particularly the dimension-independent term in the single-pass algorithm. The multi-pass approach with ERM and generalization analysis is novel and well-justified. The paper is well-structured, with rigorous mathematical analysis and clear comparisons to prior work. Notation and definitions are consistent and well-explained."
          },
          "weaknesses": {
            "value": "The paper lacks experimental validation, which limits the assessment of practical relevance. The generalization result (Proposition 5.1) is critical but requires further clarification on its assumptions and tightness. The focus on zero-order algorithms, while theoretically sound, may not fully exploit gradient information available in some applications. The comparison to Zhang et al. (2024) relies heavily on theoretical bounds without addressing potential practical trade-offs."
          },
          "questions": {
            "value": [
              "How do the authors handle the trade-off between privacy parameters (ε, δ) and sample complexity in practice?",
              "Are there specific assumptions about the Lipschitz constants or smoothness of the component functions f(x; ξ) that are critical to the analysis?",
              "What is the practical impact of the generalization bound in Proposition 5.1, and how does it affect the overall sample complexity for stochastic objectives?",
              "How do the proposed algorithms compare to non-private baselines in terms of convergence rates or computational efficiency?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper addresses differentially private (DP) optimization for non-smooth and non-convex (NSNC) problems, proposing improved algorithms for finding Goldstein-stationary points. The authors introduce a single-pass DP algorithm with a sample complexity bound that is Ω(√d) smaller than prior work, and a multi-pass algorithm that further reduces the complexity by leveraging empirical risk minimization (ERM) and generalization guarantees from ERM to the population loss."
          },
          "strengths": {
            "value": "The paper makes significant theoretical contributions by improving sample complexity bounds for DP NSNC optimization, which is a challenging and underexplored area. The single-pass algorithm's dimension-independent term (1/αβ³) is a novel improvement over prior work. The multi-pass approach introduces a new DP ERM algorithm with sublinear dimension-dependent complexity, addressing a key gap in the literature. The generalization analysis (Proposition 5.1) provides a critical link between empirical and population losses, enhancing the practical relevance of the results. The paper is well-structured, with clear theorems and comparisons to prior work."
          },
          "weaknesses": {
            "value": "The algorithms are zero-order, relying only on function evaluations, which may be less efficient than gradient-based methods. While first-order variants are mentioned in the appendix, their analysis and practical implications are not detailed in the main text. The theoretical results depend on assumptions about the Lipschitzness and structure of the loss functions, which are not explicitly discussed. The generalization bound (Proposition 5.1) introduces an additive d/β² term in the sample complexity, which could be restrictive for certain parameter regimes. The lack of empirical validation or implementation details limits the practical insights."
          },
          "questions": {
            "value": "1. How do the zero-order algorithms compare in terms of computational efficiency to first-order methods, and what are the trade-offs in practice? 2. Are there specific assumptions on the loss functions (e.g., Lipschitz constants, smoothness) that are critical for the theoretical guarantees, and how do these affect real-world applicability? 3. What are the empirical performance trends of the proposed algorithms on synthetic or benchmark NSNC problems, particularly in high-dimensional settings? 4. How sensitive are the sample complexity bounds to the choice of privacy parameters (ε, δ) and stationarity parameters (α, β)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0RUQmLFF1D": {
    "paper_id": "0RUQmLFF1D",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces Concept2Concept, a framework for auditing text-to-image (T2I) models by analyzing concept associations in generated images. It extracts high-level concepts from generated images using visual grounding models, characterizes their conditional distributions, and applies the framework to detect biases in synthetic and real-world datasets. The authors also present an open-source visualization tool for human-in-the-loop auditing."
          },
          "strengths": {
            "value": "The paper addresses a critical problem of auditing T2I models for undesirable associations, which is highly relevant given the societal impact of these systems. The framework's focus on interpretable concept distributions and its application to real-world case studies (e.g., detecting CSAM in datasets) demonstrate practical significance. The integration of visual grounding models for localized concept analysis is a novel approach compared to prior work that focuses on social biases. The open-source tool and interactive visualization also enhance usability for non-technical users."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous comparisons with existing auditing frameworks like TBYB or CUPID, making it unclear how Concept2Concept improves upon prior work. The concept extraction method relies on a single visual grounding model (e.g., BLIP) without ablation studies or analysis of its limitations. The case studies (e.g., CSAM detection) are presented as anecdotes rather than systematic evaluations. The paper does not address how the framework scales to large-scale models or diverse modalities. Additionally, the theoretical foundation for defining 'concepts' is underdeveloped, leaving ambiguity about their scope and granularity."
          },
          "questions": {
            "value": "1. How does the framework handle ambiguous or context-dependent concepts (e.g., 'bank' as a noun vs. a verb)?\n2. What metrics were used to validate the quality of concept extraction, and how do they compare to existing benchmarks?\n3. Were the CSAM findings independently verified, and how generalizable are these results to other datasets?\n4. How does the framework account for variations in visual grounding model performance across different image domains?\n5. What are the computational costs of the framework, and how does it scale to large datasets?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces Concept2Concept, a framework for auditing text-to-image (T2I) models by characterizing the conditional distribution of generated images using interpretable concepts. It enables systematic analysis of prompt-image associations, demonstrates applications in detecting harmful content and dataset biases, and provides an open-source visualization tool for non-technical users."
          },
          "strengths": {
            "value": "Originality: The framework addresses a critical gap in auditing T2I models by focusing on concept-level analysis rather than instance-level bias probing, offering a novel approach to understand conditional distributions. Quality: The methodology combines visual grounding with statistical analysis of concept distributions, validated through concrete case studies. Clarity: The paper is well-structured, with clear explanations of the framework's components and practical applications. Significance: The work has direct implications for model safety, fairness, and alignment, particularly in detecting harmful content and synthetic dataset biases."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed comparison with existing methods like TBYB or CUPID, making it harder to assess the framework's relative advantages. The visual grounding model's reliability and limitations (e.g., handling ambiguous concepts) are not thoroughly discussed. The case studies, while impactful, focus on specific datasets (e.g., StableImageNet), raising questions about generalizability. Additionally, the framework's ability to detect nuanced biases (e.g., cultural or contextual associations) is not explicitly addressed."
          },
          "questions": {
            "value": [
              "How does the framework handle ambiguous or context-dependent concepts (e.g., 'bank' as a riverbank vs. financial institution)?",
              "What metrics are used to quantify 'concept co-occurrences' and 'stability of concepts'? Are these metrics validated against human annotations?",
              "The paper mentions CSAM detection in a human-preferences dataset—can the framework distinguish between intentional and accidental associations?",
              "How does the framework scale to large-scale T2I models or diverse language prompts beyond the tested cases?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces Concept2Concept, a framework for auditing text-to-image (T2I) models by characterizing their conditional distributions through interpretable concepts. The approach involves extracting high-level concepts from generated images and analyzing their distributions to uncover biases, unsafe content (e.g., CSAM), and misalignments. The authors also present an open-source visualization tool for non-technical users to explore these associations."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem: auditing T2I models for safety and fairness. The framework's focus on concept-level analysis offers a novel perspective compared to prior work that primarily targets social biases. The integration of visual grounding models for localized concept extraction and the development of an interactive tool demonstrate practical utility. The case studies (e.g., detecting CSAM in datasets) highlight the framework's real-world relevance and potential impact."
          },
          "weaknesses": {
            "value": "The experimental validation is limited to a few datasets (e.g., StableImageNet, Pick-a-Pic) and lacks systematic comparison with existing auditing methods. The detection of CSAM and misaligned classes relies on anecdotal evidence without quantitative metrics or thorough analysis of false positives/negatives. The framework's scalability and robustness to diverse prompt distributions are not rigorously evaluated. Additionally, the paper does not address potential limitations of visual grounding models (e.g., accuracy, bias) that could affect the reliability of concept extraction."
          },
          "questions": {
            "value": [
              "How does the framework handle ambiguous or context-dependent concepts (e.g., 'bank' as a noun vs. verb)?",
              "What metrics were used to evaluate the accuracy of concept extraction, and how do they correlate with downstream auditing tasks?",
              "How generalizable is the framework to T2I models beyond those tested (e.g., different architectures or training data)?",
              "What steps were taken to mitigate false positives in detecting harmful content like CSAM?",
              "Are there any known cases where the framework failed to detect biases or unsafe associations, and how were these addressed?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0Th6bCZwKt": {
    "paper_id": "0Th6bCZwKt",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces GMM-GDA, a graph data augmentation method leveraging Gaussian Mixture Models (GMMs) to enhance GNN generalization. The authors propose a theoretical framework based on Rademacher complexity to analyze the impact of augmentation on generalization error and validate their approach through influence function analysis. The method aims to improve training data diversity while maintaining computational efficiency."
          },
          "strengths": {
            "value": "The paper's theoretical framework is a key strength, offering rigorous analysis of how data augmentation affects GNN generalization. The use of GMMs for graph representation augmentation is innovative, as GMMs can approximate complex distributions, potentially capturing richer structural patterns than existing methods. The focus on computational efficiency and scalability addresses practical challenges in real-world GNN applications. The paper is well-structured, with clear explanations of GNN fundamentals and prior work, and the contributions are explicitly outlined."
          },
          "weaknesses": {
            "value": "The experimental evaluation is insufficiently detailed, with no concrete results or comparisons to existing augmentation methods like G-Mixup or GEOMIX. The paper truncates mid-sentence, leaving critical information about the proposed method's implementation and theoretical analysis incomplete. The practical challenges of applying GMMs to graph data (e.g., handling heterogeneous features, scalability to large graphs) are not addressed. The theoretical claims about Rademacher complexity and influence functions lack depth, and the paper does not discuss potential limitations of GMMs (e.g., sensitivity to initialization, computational overhead)."
          },
          "questions": {
            "value": "1. How exactly are GMMs applied to graph data? Are node features, graph embeddings, or hidden representations used as input? 2. What specific metrics and baselines were used to demonstrate the superiority of GMM-GDA over existing methods? 3. How does the paper address the computational complexity of training GMMs on large-scale graphs? 4. Can the authors clarify the incomplete theoretical analysis of influence functions and their connection to generalization? 5. What are the limitations of using GMMs for graph augmentation, and how are they mitigated?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces GMM-GDA, a graph data augmentation method leveraging Gaussian Mixture Models (GMMs) to enhance the generalization of Graph Neural Networks (GNNs). The approach is grounded in a theoretical framework analyzing generalization error via Rademacher complexity and influence functions, aiming to improve training data diversity while maintaining computational efficiency."
          },
          "strengths": {
            "value": "The paper's theoretical contributions are robust, with a clear analysis of generalization error using Rademacher complexity and influence functions, offering novel insights into augmentation mechanisms. The GMM-GDA method is well-motivated, combining GMMs' distribution approximation capabilities with GNNs' hidden representations for efficient augmentation. The paper is structured clearly, with thorough background on GNNs and existing augmentation techniques. The significance of addressing GNN generalization, especially for out-of-distribution data, is highly relevant to the field."
          },
          "weaknesses": {
            "value": "The paper lacks experimental validation to support its claims, as the content is cut off before results are presented. Key details about GMM-GDA's implementation, such as how GMMs are trained on hidden representations or the specific augmentation strategies (e.g., sampling methods), are not elaborated. The theoretical framework's practical implications and limitations remain underexplored. Additionally, the paper does not compare GMM-GDA to existing methods like G-Mixup or Subgraph sampling in terms of performance metrics or computational efficiency."
          },
          "questions": {
            "value": "1. What are the specific experimental results demonstrating GMM-GDA's superiority over existing methods? 2. How is the GMM trained on graph hidden representations, and what hyperparameters are used? 3. What ablation studies or analysis of GMM-GDA's sensitivity to distribution assumptions are provided? 4. How does the theoretical framework translate to practical improvements in GNN generalization? 5. Are there limitations to the Rademacher complexity analysis when applied to graph-structured data?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a novel graph data augmentation technique called GMM-GDA, which leverages Gaussian Mixture Models (GMMs) to enhance the generalization of Graph Neural Networks (GNNs). The approach is grounded in a theoretical framework analyzing generalization error via Rademacher complexity and influence functions, with claims of improved efficiency and performance over existing methods."
          },
          "strengths": {
            "value": "The paper presents a theoretically grounded approach to graph augmentation, combining Rademacher complexity analysis with GMM-based sampling. The focus on hidden representations rather than raw graph structures is novel. The method's efficiency claims and potential scalability for large graphs are promising. The theoretical analysis using influence functions provides a principled perspective on augmentation's impact on generalization."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with critical sections (e.g., experiments, related work) cut off, making it impossible to assess empirical validation or comparisons with state-of-the-art methods. The theoretical framework lacks concrete examples or bounds that demonstrate practical utility. The GMM-GDA implementation details, computational complexity analysis, and scalability claims are not sufficiently explained. The paper also fails to address limitations of GMMs in modeling graph data, such as handling heterogeneous structures or high-dimensional representations."
          },
          "questions": {
            "value": [
              "What specific datasets and benchmarks were used to evaluate GMM-GDA, and how does it compare to established augmentation methods like G-Mixup or GEOMIX?",
              "How are the GMM parameters initialized and trained on graph representations, and what guarantees exist for convergence?",
              "What is the exact time complexity of GMM-GDA, and how does it scale with large graph sizes compared to existing methods?",
              "How does the theoretical analysis of influence functions translate to practical improvements in generalization performance?",
              "Are there cases where GMM-GDA might fail (e.g., sparse graphs, high-dimensional features), and how are these addressed?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 2
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0XT3Lg6S2Q": {
    "paper_id": "0XT3Lg6S2Q",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces AdaWarp, a novel neural network module for medical image registration that incorporates a piece-wise smooth (P-S) physical prior. AdaWarp leverages a differentiable bilateral grid to enable edge-preserving, low-frequency deformation field approximation, balancing accuracy and efficiency. The method is evaluated on two multimodal datasets, demonstrating superior performance in accuracy-efficiency and accuracy-smoothness trade-offs compared to existing approaches."
          },
          "strengths": {
            "value": "The paper presents a creative integration of a physical prior (P-S assumption) into a neural network architecture, addressing a gap in learning-based registration. The differentiable bilateral grid is a novel component that combines global smoothness with local discontinuity preservation. The work is well-motivated by both medical imaging challenges and prior literature on bilateral filtering. Experiments on diverse datasets highlight practical relevance, and the clear structure of the paper enhances readability. The focus on efficiency and smoothness trade-offs is significant for real-world medical applications."
          },
          "weaknesses": {
            "value": "The experimental evaluation lacks depth, with limited details about dataset characteristics, hyperparameter choices, and baseline comparisons. The ablation studies are not thoroughly described, making it hard to assess the contribution of individual components. The P-S assumption is based on anecdotal observations from cardiac/abdominal scans, but its generalizability to other modalities or anatomies is unclear. The differentiable bilateral grid's training dynamics and optimization strategy are not fully explained, which could impact reproducibility. The paper also does not address potential limitations in handling extreme deformations or the computational cost of the bilateral grid."
          },
          "questions": {
            "value": "1. What are the specific details of the two datasets used (e.g., modality, size, deformation characteristics)? 2. How were the hyperparameters for AdaWarp and baselines selected, and what ablation studies were performed? 3. Can the guidance map generator explicitly encode anatomical boundaries, and how is this validated? 4. What is the computational cost of AdaWarp compared to existing methods, and how does it scale with input resolution? 5. How does the differentiable bilateral grid handle extreme deformations where the P-S assumption might fail?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces AdaWarp, a neural network module for medical image registration that incorporates a piece-wise smooth (P-S) physical prior through a differentiable bilateral grid. The approach aims to balance registration accuracy, efficiency, and smoothness by leveraging low-resolution feature maps and adaptive filtering, demonstrating improvements over existing methods on two multi-modal datasets."
          },
          "strengths": {
            "value": "Originality: The paper addresses a gap in learning-based registration by systematically integrating a P-S prior into a neural architecture, which is less explored in prior work. Quality: The methodology is well-structured, with clear components (encoder, guidance mapper, bilateral grid) and plausible theoretical grounding. Clarity: The problem statement, contributions, and experimental setup are clearly articulated, with visual comparisons to support claims. Significance: Medical image registration is a critical task, and the proposed efficiency-accuracy trade-off could have practical implications for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art methods, particularly in terms of computational efficiency metrics (e.g., inference speed, GPU memory usage). The experimental evaluation is limited to two datasets, and it is unclear whether they are standard benchmarks or custom splits. The differentiable bilateral grid's implementation details (e.g., grid dimensions, training dynamics) are not sufficiently explained, which may hinder reproducibility. Additionally, the theoretical justification for the P-S assumption's superiority over global smoothness constraints is underdeveloped."
          },
          "questions": {
            "value": [
              "What specific datasets were used for evaluation, and how do they compare to widely adopted benchmarks like BraTS or LiTS?",
              "How does AdaWarp's computational cost (e.g., FLOPs, inference time) compare to methods like VoxelMorph or FourierNet?",
              "Can the authors provide ablation studies to isolate the contribution of the differentiable bilateral grid versus other components?",
              "How is the guidance map generated, and what design choices were made to ensure it captures local intensity differences effectively?",
              "What are the limitations of the P-S assumption in scenarios with extreme deformations or non-smooth structures?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces AdaWarp, a neural network module for medical image registration that leverages the piece-wise smooth (P-S) assumption to balance accuracy and efficiency. The method combines an encoder, guidance map generator, and differentiable bilateral grid to approximate deformation fields using low-resolution feature maps, achieving edge-preserving, computationally efficient registration."
          },
          "strengths": {
            "value": "The paper demonstrates originality by systematically integrating the P-S assumption into a learnable framework, addressing a gap in existing learning-based registration methods. The design of the differentiable bilateral grid is novel, offering a structured approach to edge preservation while reducing computational complexity. The experiments on diverse modalities (MRI/CT) and input constraints (un/semi-supervised) highlight the method's practical significance. The clarity of the problem statement, motivation, and technical description is strong, with sufficient contextualization of related work."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of individual components (e.g., the guidance map vs. bilateral grid). The experiments, while covering two datasets, do not compare against a broad range of state-of-the-art methods (e.g., VoxelMorph, LapIRN, or recent transformer-based approaches). The justification for the P-S assumption is largely theoretical; empirical validation of its necessity is limited. Additionally, the computational efficiency claims are not quantified with explicit metrics (e.g., FLOPs, inference time) compared to baselines."
          },
          "questions": {
            "value": "How does AdaWarp handle extreme deformations where the P-S assumption might break? What is the exact computational overhead of the differentiable bilateral grid compared to standard convolutional operations? Are the guidance maps explicitly trained to capture local intensity differences, or do they emerge implicitly? How does the method scale to 3D medical images with higher resolution? Could the bilateral grid's performance be further improved with adaptive kernel size or dynamic range domain discretization?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0Yfjerm9Zp": {
    "paper_id": "0Yfjerm9Zp",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes a dual-reward probabilistic inference framework to enhance the faithfulness of LLM-generated rationales by incorporating domain-specific context-aware rewards. The method uses a sequential Monte Carlo approach with local and global rewards to improve coherence and contextual alignment, achieving better accuracy and faithfulness without significant computational overhead."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in LLM interpretability by focusing on faithfulness, a poorly defined but important aspect of model reliability. The dual-reward mechanism introduces a novel approach to balance task accuracy and contextual coherence. The empirical results on three tasks show measurable improvements, and the method's computational efficiency is a strong practical advantage. The paper also highlights a clear problem (contextual unfaithfulness) and provides a structured framework for addressing it."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of the proposal distribution and reward mechanisms, as key sections (e.g., 3.2-3.4) are cut off. The comparison with prior work is superficial, and the domain-specific expert model used for baseline comparisons is not clearly characterized. The faithfulness evaluation methodology is not fully explained, and the claims about '10% improvements' lack statistical significance analysis. The paper also fails to address potential limitations of the proposed approach, such as scalability to diverse domains."
          },
          "questions": {
            "value": [
              "Please clarify the exact formulation of the local and global rewards. How are they computed during inference?",
              "What specific domain-specific features does the proposal distribution prioritize, and how is it trained or derived?",
              "How was the expert model in Table 1 developed? Is it a pre-trained model, a fine-tuned version, or a custom architecture?",
              "Are the 33% accuracy gains and 10% faithfulness improvements statistically significant across all datasets?",
              "How does the method handle out-of-domain inputs, given the focus on domain-specific context?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes a probabilistic inference framework for enhancing the faithfulness of large language models (LLMs) in rationale generation. The approach introduces dual rewards (local and global) to guide sequential Monte Carlo search, using domain-specific proposal distributions to improve contextual coherence. Experiments show improvements in accuracy and faithfulness across three reasoning tasks while maintaining computational efficiency."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in LLM interpretability by proposing a novel inference-time method that avoids costly retraining. The dual-reward mechanism offers a clear theoretical framework for balancing task accuracy and faithfulness. The empirical results demonstrate significant improvements (33% accuracy gain, 10% faithfulness improvement) with minimal computational overhead. The comparison between Llama3 and expert models provides concrete evidence of the problem's validity. The work's focus on controllable generation during inference is both original and practically relevant."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing faithfulness-enhancement methods like Chain-of-Thought prompting or reinforcement learning. The domain-specific proposal distribution's construction is not thoroughly explained - how are domain-specific tokens identified? The experimental evaluation focuses on BLEU scores rather than human evaluations of rationale quality. The theoretical analysis of how dual rewards improve faithfulness is superficial. The claim of 'similar computational costs to beam search' requires more concrete evidence (e.g., runtime measurements)."
          },
          "questions": {
            "value": [
              "How exactly are the domain-specific proposal distributions constructed? Do they require additional training or can they be derived from existing model statistics?",
              "What specific metrics were used to quantify 'faithfulness' improvements beyond BLEU scores?",
              "How do the local and global rewards differ in their formulation and optimization objectives?",
              "Are the 33% accuracy gains statistically significant across all seven datasets?",
              "What is the exact relationship between the proposed method and existing constrained decoding approaches?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a dual-reward probabilistic inference framework to enhance the faithfulness of LLM-generated rationales. The approach uses domain-specific proposal distributions to incorporate both local and global rewards, aiming to improve logical coherence and contextual alignment without significantly increasing computational costs. Empirical results show improvements in accuracy and faithfulness across three reasoning tasks."
          },
          "strengths": {
            "value": "The paper's originality lies in its dual-reward mechanism and domain-specific proposal distribution, which address a critical gap in LLM interpretability. The method's computational efficiency (1.3× beam search cost) is a strong practical advantage. The empirical results demonstrate measurable improvements in faithfulness metrics, and the problem of unfaithful rationales is highly relevant. The paper also provides clear comparisons between generalist and domain-specific models, highlighting the importance of context-aware generation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of how local and global rewards are computed or integrated into the sequential Monte Carlo framework. The three evaluation tasks are not clearly defined, making it difficult to assess the generality of the approach. The domain-specific proposal distribution's dependency on specialized data is not discussed, which could limit scalability. Additionally, the comparison with existing methods (e.g., decomposed Chain-of-Thought) is superficial and lacks ablation studies to isolate the impact of the dual-reward mechanism."
          },
          "questions": {
            "value": "1. How are the local and global rewards mathematically formulated and normalized? 2. What specific domain-specific data is required to train the proposal distribution, and how does this affect the method's applicability to new domains? 3. Are the three evaluation tasks representative of diverse reasoning scenarios, or are they biased toward specific types of problems? 4. How does the method handle cases where the domain-specific distribution is not available or poorly defined?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "0YkZe9nwiC": {
    "paper_id": "0YkZe9nwiC",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes SIGnAL, a reinforcement learning (RL)-based framework for generative active learning that combines data generation and selection. It introduces an acquisition function measuring both informativeness and relevance, transformed into a reward signal for optimizing the generative model. The method is validated on text classification tasks, particularly when original data is limited."
          },
          "strengths": {
            "value": "The paper presents a novel approach by integrating RL to address the dynamic and delayed nature of informativeness in generative active learning. The acquisition function that combines informativeness and relevance is a significant contribution. The framework's potential to leverage large generative models across tasks demonstrates broad applicability. The clarity of the problem statement and the logical flow of the methodology are commendable, though limited by the paper's truncation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results and comparisons with existing methods like GAAL or ASAL, which are mentioned in the related work. The acquisition function's implementation details are vague, and the paper does not address how it avoids generating irrelevant out-of-distribution (OOD) data. The validation on text classification is limited, with no ablation studies or analysis of hyperparameter sensitivity. The theoretical justification for the RL-based reward mechanism is underdeveloped."
          },
          "questions": {
            "value": "How is the acquisition function explicitly defined, and what metrics are used to measure informativeness and relevance? What specific strategies does SIGnAL employ to prevent the generation of irrelevant OOD data, given the paper's critique of prior methods? Are there comparisons with baseline methods (e.g., random sampling, pool-based active learning) on standard datasets? How does the framework handle the computational cost of RL optimization during data generation?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes the Self-Informed Generative Active Learning (SIGnAL) framework, which combines reinforcement learning (RL) with active learning to generate and select informative data instances. The framework uses an acquisition function that evaluates both informativeness and relevance, translating this into a reward signal for the generative model. The approach aims to address limitations of traditional pool-based active learning and prior generative methods by leveraging RL to optimize data generation."
          },
          "strengths": {
            "value": "The paper introduces a novel integration of RL into generative active learning, addressing the dynamic and delayed nature of informativeness through reward-based optimization. The acquisition function that balances informativeness and relevance is a significant contribution, offering a structured approach to avoid out-of-distribution (OOD) data. The framework's generality and potential applicability to large language models (LLMs) are promising. The work also contextualizes itself within existing active learning and synthetic data generation literature."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive experimental validation. While the abstract mentions text classification tasks, the full results are cut off, leaving critical details about performance metrics, baselines, and comparisons with existing methods (e.g., GAAL, BADGE) unaddressed. The design of the reward function and acquisition function is not sufficiently explained, particularly how relevance is quantified. The claim that SIGnAL avoids irrelevant OOD data is not empirically supported. Additionally, the practical alignment with LLMs is mentioned but not elaborated on, leaving questions about scalability and implementation."
          },
          "questions": {
            "value": [
              "What specific metrics were used to evaluate the effectiveness of SIGnAL in text classification tasks? How does it compare to baselines like random sampling, pool-based active learning, and prior generative methods?",
              "How is the acquisition function's 'relevance' component defined and computed? Are there ablation studies to validate its necessity?",
              "What are the details of the RL setup (e.g., reward shaping, policy architecture)? How does the framework handle the delayed nature of informativeness during training?",
              "How does SIGnAL address the risk of over-optimization in generative models, which the paper acknowledges as a challenge?",
              "What are the computational costs and scalability limitations of the framework, especially when applied to large language models?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Self-Informed Generative Active Learning (SIGnAL), a reinforcement learning (RL)-based framework for active learning that generates and selects data instances for annotation. It combines an acquisition function measuring both informativeness and relevance, which is transformed into a reward signal for the generative model. The approach aims to address limitations of traditional pool-based and query-synthesizing methods, particularly in data-scarce scenarios."
          },
          "strengths": {
            "value": "The paper presents a novel framework that integrates RL with active learning, addressing dynamic and delayed informativeness through reward-based optimization. The acquisition function combining informativeness and relevance is a promising contribution. The work aligns with emerging trends in leveraging generative models for active learning and provides a generalizable approach applicable to various tasks. The structured presentation and clear problem motivation are notable strengths."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficiently detailed, with no comparison to strong baselines or ablation studies. Key components like the RL policy training mechanism, reward shaping, and handling of over-optimization remain underexplained. The paper lacks analysis of how the acquisition function balances informativeness and relevance. The claim about aligning large language models (LLMs) with the framework is not elaborated, and the text classification experiments are not fully described."
          },
          "questions": {
            "value": "1. How is the RL policy trained, and what specific reward function is used to balance informativeness and relevance? 2. What ablation studies demonstrate the contribution of the acquisition function's components? 3. How does SIGnAL address the risk of generating irrelevant out-of-distribution data, as highlighted in the introduction? 4. Are there comparisons to state-of-the-art query-synthesizing methods like GAAL or BADGE? 5. How generalizable is the framework to tasks beyond text classification?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0a7TRHhhcS": {
    "paper_id": "0a7TRHhhcS",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a novel framework that integrates choice theory and social intelligence to model spatial-temporal counting processes, such as crime occurrences or bike-sharing activity. The approach uses random utility functions and mixture-of-experts models to capture latent human preferences and social dynamics, aiming to improve predictive accuracy and interpretability of event distributions."
          },
          "strengths": {
            "value": "The paper presents a novel integration of choice theory and social intelligence into spatial-temporal modeling, addressing a gap in traditional methods that neglect human decision-making. The methodology leverages mixture-of-experts and random utility functions, offering potential for interpretability. The empirical evaluation on real-world datasets suggests practical relevance, and the framework's adaptability to external interventions is a significant contribution. The structure and clarity of the introduction and related work are strong."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results and comparisons with state-of-the-art models, making it difficult to assess the model's effectiveness. Key components like the 'sparse selection and ranking functions' and 'social intelligence integration' are not sufficiently explained. The connection between the proposed framework and existing choice models (e.g., MNL, MoE) is underdeveloped. The paper is cut off mid-section, leaving critical details about the methodology and evaluation incomplete."
          },
          "questions": {
            "value": [
              "How exactly does the model incorporate social intelligence? What specific mechanisms account for mutual influences and social norms?",
              "What are the implementation details of the mixture-of-experts framework? How are the experts trained and combined?",
              "What metrics were used to evaluate predictive accuracy, and how does the model compare to baseline spatial-temporal models (e.g., LGCP, neural ODEs)?",
              "Are there ablation studies demonstrating the contribution of individual components (e.g., sparse selection, social dynamics)?",
              "How does the model scale to large datasets, and what are its computational limitations?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a novel framework that integrates choice theory and social intelligence to model spatial-temporal counting processes, such as crime or bike-sharing activities. The approach uses random utility functions and a mixture-of-experts architecture to capture latent human preferences, aiming to improve interpretability and predictive accuracy by modeling individual decision-making influenced by social dynamics."
          },
          "strengths": {
            "value": "The paper presents a compelling conceptual integration of choice theory and social intelligence into spatial-temporal modeling, addressing a gap in existing methods that focus solely on statistical dependencies. The use of mixture-of-experts and sparse selection mechanisms shows potential for capturing heterogeneous decision-making patterns. The empirical validation on real-world datasets suggests practical relevance, and the emphasis on interpretability aligns with growing demands for explainable AI in critical domains like crime analysis."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results, such as specific metrics, baselines, or ablation studies, making it difficult to assess the model's performance. The description of key components like the 'soft ranking-based sparse selection' and 'mixture-of-experts' is overly brief, leaving unclear how they are implemented or why they outperform existing methods. The novelty claims are not sufficiently contextualized against prior work, such as deep choice models or social influence-aware spatial-temporal models. Additionally, the truncation of the paper prevents evaluation of critical sections like experiments and comparisons."
          },
          "questions": {
            "value": [
              "What specific baselines were compared against, and how does the model perform quantitatively on standard metrics (e.g., MAE, log-likelihood)?",
              "How are the 'mixture-of-experts' and sparse selection mechanisms trained and optimized? Are there theoretical guarantees for convergence?",
              "Can the authors clarify how their model differs from existing choice-theoretic approaches (e.g., MNL, neural choice models) and why their social intelligence component is uniquely effective?",
              "What ablation studies were conducted to validate the contribution of individual components (e.g., social norms, sparse selection)?",
              "How does the model scale to large datasets, and what are its computational limitations?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes a preference-driven spatial-temporal counting process model that integrates choice theory and social intelligence to analyze human-generated event data (e.g., crime or bike-sharing activities). The approach uses utility functions to capture latent human preferences, combined with mixture-of-experts models and ranking mechanisms, to explain how individual decisions shape event distributions over time and space. Empirical results on real-world datasets demonstrate its predictive accuracy and interpretability."
          },
          "strengths": {
            "value": "The paper introduces a novel framework that bridges spatial-temporal modeling with choice theory and social intelligence, addressing a gap in traditional models that overlook human decision-making factors. The integration of mixture-of-experts and ranking-based sparse selection mechanisms shows creative problem-solving. The theoretical foundation is solid, with references to established concepts like random utility modeling and social norms. The potential significance lies in its applications for policy analysis and behavioral insights, though this remains to be fully validated."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results and comparisons with baseline models, making it difficult to assess the model's empirical effectiveness. The related work section is incomplete (e.g., truncated references to choice models), and the technical description of the mixture-of-experts architecture is vague. The social intelligence component is mentioned but not clearly operationalized. Additionally, the paper does not address limitations of the proposed approach, such as scalability or assumptions about utility functions."
          },
          "questions": {
            "value": "1. How does the model handle high-dimensional spatial-temporal data? What specific metrics were used to evaluate predictive accuracy? 2. Can the authors clarify how social intelligence is quantified and integrated into the utility functions? 3. What baseline models were compared against, and what are the quantitative results? 4. Are there theoretical guarantees for the convergence or stability of the proposed method? 5. How does the mixture-of-experts architecture avoid overfitting, given the complexity of the model?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0bcRCD7YUx": {
    "paper_id": "0bcRCD7YUx",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces VALL-E 2, a zero-shot text-to-speech synthesis (TTS) system that claims to achieve human parity. The key innovations are 'Repetition Aware Sampling' to address decoding instability and 'Grouped Code Modeling' to improve inference efficiency. The authors demonstrate improvements over prior work on LibriSpeech and VCTK datasets, particularly in robustness, naturalness, and speaker similarity."
          },
          "strengths": {
            "value": "The paper presents two novel techniques (Repetition Aware Sampling and Grouped Code Modeling) that address critical limitations of prior TTS systems. The claim of human parity is significant, and the experimental results on standard benchmarks suggest meaningful improvements. The work has potential real-world applications for speech generation for individuals with disabilities. The paper is well-structured and clearly explains the technical contributions."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of the proposed techniques. The claim of human parity is not rigorously justified, as the metrics (e.g., ΔScore) are defined relative to ground truth without clear normalization or comparison to human baselines. The experiments focus on two datasets (LibriSpeech and VCTK), but the generalization to other domains or speaker variations is unexplored. The paper also does not compare with state-of-the-art non-autoregressive TTS systems that may achieve similar performance with different trade-offs."
          },
          "questions": {
            "value": "1. How exactly are the human parity metrics (e.g., ΔScore) calculated? The paper states that the model's scores exceed ground truth, but this is logically problematic since ground truth represents human performance. 2. What specific prior models are outperformed by VALL-E 2, and how do the results compare to non-autoregressive approaches like Soundstorm or NaturalSpeech 2? 3. Are there any limitations to the 'simple utterance-wise' data requirements, and how does this affect the model's ability to generalize to rare speakers or languages?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces VALL-E 2, a zero-shot text-to-speech synthesis system that claims to achieve human parity. It builds on VALL-E by introducing two key innovations: Repetition Aware Sampling (to address token repetition and infinite loops) and Grouped Code Modeling (to improve inference speed and long-sequence modeling). The paper reports strong performance on LibriSpeech and VCTK datasets but lacks detailed experimental validation for its claims."
          },
          "strengths": {
            "value": "Originality: The paper proposes two novel techniques (repetition-aware sampling and grouped code modeling) to address stability and efficiency in neural codec TTS. Quality: The work addresses a critical challenge in zero-shot TTS and leverages large-scale datasets. Clarity: The structure is logical, with clear problem statements and motivations. Significance: Achieving human parity in zero-shot TTS would be a major breakthrough, with potential applications for assistive technologies."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation for its human parity claims. Metrics like robustness, naturalness, and similarity are not quantified with statistical significance or comparison to human baselines. The improvements from the two proposed techniques (repetition-aware sampling and grouped code modeling) are not isolated or ablated. The paper does not address potential limitations of the LibriSpeech/VCTK datasets (e.g., domain-specific biases) or provide qualitative examples of synthesized speech. The computational efficiency gains are not quantified with concrete speed benchmarks."
          },
          "questions": {
            "value": [
              "How were the human parity metrics (robustness, naturalness, similarity) defined and measured? What are the specific thresholds for 'human parity'?",
              "Are the reported improvements on LibriSpeech/VCTK statistically significant? What are the p-values or confidence intervals?",
              "How do the two proposed techniques (repetition-aware sampling and grouped code modeling) contribute individually to the results? Are there ablation studies?",
              "What is the exact definition of 'high-quality speech for complex sentences'? Are there qualitative examples or quantitative metrics to support this claim?",
              "How does the data efficiency of VALL-E 2 compare to prior work (e.g., ELLA-V, Mega-TTS)? What is the impact of using 'simple utterance-wise data' on model performance?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces VALL-E 2, a zero-shot text-to-speech synthesis (TTS) system that achieves human parity for the first time. The key innovations include Repetition Aware Sampling, which improves decoding stability by adapting sampling strategies based on token repetition, and Grouped Code Modeling, which accelerates inference by reducing sequence length through codec code grouping. The system demonstrates superior performance on LibriSpeech and VCTK datasets compared to prior methods."
          },
          "strengths": {
            "value": "Originality: The paper presents two novel techniques (Repetition Aware Sampling and Grouped Code Modeling) to address stability and efficiency in zero-shot TTS. Quality: The experiments on standard benchmarks (LibriSpeech, VCTK) show significant improvements in robustness, naturalness, and speaker similarity. Clarity: The problem statement and technical contributions are well-structured, with clear explanations of the proposed methods. Significance: Achieving human parity in zero-shot TTS has broad implications for applications like assistive technologies for speech-impaired individuals."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of Repetition Aware Sampling and Grouped Code Modeling. The definition of 'human parity' is not rigorously justified, relying on relative metrics (e.g., ΔSMOS) without direct human evaluation. The comparison to prior work (e.g., ELLA-V, RALL-E) is limited, and it is unclear how VALL-E 2 outperforms these methods. Additionally, the efficiency gains from Grouped Code Modeling are not quantified in terms of computational cost or inference speed."
          },
          "questions": {
            "value": "How was 'human parity' defined and validated? Were there human evaluations to confirm that VALL-E 2's metrics (e.g., ΔSMOS > 0) correlate with human perception? What are the exact computational savings from Grouped Code Modeling, and how do they compare to prior non-autoregressive approaches? Are there any trade-offs in speech quality when using Grouped Code Modeling? How does the system handle out-of-domain speakers or languages not tested in the experiments?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0bcUyy2vdY": {
    "paper_id": "0bcUyy2vdY",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces the Multi-play Multi-armed Bandit with Scarce Shareable Arm Capacities (MP-MAB-SAC) problem, focusing on resource allocation scenarios like LLM inference serving. It provides three main contributions: (1) a tight sample complexity lower bound and an algorithm to match it, (2) new instance-independent and dependent regret lower bounds, and (3) a data-efficient exploration algorithm (PC-CapUL) with theoretical guarantees. The work addresses gaps in prior research by modeling capacity constraints more realistically and separating capacity information from reward variance."
          },
          "strengths": {
            "value": "The paper's originality lies in its novel problem formulation, particularly the reward function that isolates capacity information in the mean, enabling deeper theoretical analysis. The theoretical contributions are significant, including closing sample complexity gaps and establishing the first instance-independent regret lower bounds. The clarity of the exposition is strong, with logical organization and clear connections to practical applications. The significance is high due to the relevance of resource-constrained scenarios in edge computing and LLM serving, with potential impacts on data-efficient learning algorithms."
          },
          "weaknesses": {
            "value": "The experimental validation is underdeveloped in the provided content, with no details on the setup, baselines, or quantitative results to support the claims. The comparison with Wang et al. (2022a) lacks depth, particularly in explaining how the new model's assumptions (e.g., scarce capacities) fundamentally differ from prior work. The algorithm PC-CapUL's design and implementation details are brief, leaving questions about its practical efficacy. The theoretical analysis could clarify how the absence of capacity information in aggregate rewards affects exploration-exploitation trade-offs."
          },
          "questions": {
            "value": "1. Can the authors provide details on the experimental setup, baselines, and results to validate PC-CapUL's data efficiency? 2. How does the new reward function's separation of capacity information impact the algorithm's design compared to Wang et al. (2022a)? 3. Are there practical limitations to the theoretical bounds, such as dependency on specific parameter ranges or assumptions? 4. How does the cost constraint (movement cost c) interact with the exploration-exploitation trade-off in the proposed algorithm?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the Multi-play Multi-armed Bandit with Scarce Shareable Arm Capacities (MP-MAB-SAC) problem, focusing on statistical limits and data-efficient learning. It introduces a new reward model, establishes tight sample complexity and regret bounds, and proposes the PC-CapUL algorithm to balance exploration and exploitation. The work aims to improve resource allocation in applications like LLM inference serving and edge computing."
          },
          "strengths": {
            "value": "Originality is evident in the novel reward model that isolates capacity information in the mean, enabling fundamental insights. The paper provides tight minimax lower bounds for sample complexity and regret, resolving gaps in prior work. The PC-CapUL algorithm introduces active inference and prioritized coordination of UCB/LCB, demonstrating data efficiency. The significance is clear, as the problem has direct applications in resource-constrained scenarios. Clarity is maintained through structured problem formulation and rigorous theoretical analysis."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation beyond basic experiments, leaving the practical efficacy of PC-CapUL unverified in real-world settings like LLM inference. The regret lower bounds' independence from arm capacities (m_k) is counterintuitive and requires deeper justification. The assumption of deterministic capacities may limit applicability to scenarios with stochastic resource constraints. Additionally, the computational complexity of PC-CapUL and its scalability to large K or T are not discussed."
          },
          "questions": {
            "value": "1. How does the new reward model (Equation 5) affect the learning dynamics compared to Wang et al. (2022a)'s model? 2. Are there specific challenges in applying this framework to LLM inference serving that require further investigation? 3. What is the computational complexity of PC-CapUL, and how does it scale with K and T? 4. How does the algorithm handle the cost constraint c < min μ_k in practice? 5. Are there empirical comparisons with alternative approaches beyond the ones mentioned?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper addresses the Multi-play Multi-armed Bandits with Scarce Shareable Arm Capacities (MP-MAB-SAC) problem, focusing on sample complexity, regret bounds, and a novel algorithm (PC-CapUL). The authors propose tighter theoretical guarantees compared to prior work, including a minimax lower bound for sample complexity and instance-independent regret bounds. They also introduce an algorithm that balances exploration and exploitation through prioritized coordination of UCB/LCB."
          },
          "strengths": {
            "value": "The paper makes significant theoretical contributions by closing the sample complexity gap in MP-MAB-SAC and establishing the first instance-independent regret lower bound. The problem formulation is well-motivated with real-world applications in LLM inference and edge computing. The clarity of the problem setup and the structure of the contributions are strong. The proposed algorithm (PC-CapUL) introduces novel ideas for data-efficient exploration, though its practical validation is incomplete."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation to support the proposed algorithm (PC-CapUL). While the abstract mentions numerical experiments, the provided content does not include results, making it difficult to assess the algorithm's practical efficacy. The analysis of regret bounds, particularly the instance-independent bound without dependence on arm capacities, requires further justification. Additionally, the paper does not thoroughly compare PC-CapUL to existing methods or discuss computational complexity."
          },
          "questions": {
            "value": "1. Why does the instance-independent regret lower bound not depend on arm capacities $m_k$? Is this a result of the specific reward model or an inherent property of the problem? 2. What are the exact experimental settings and results that validate the data efficiency of PC-CapUL? 3. How does PC-CapUL handle the cost constraint $c < \\min_k \\mu_k$ in practice, and what are its computational requirements? 4. Are there any limitations or assumptions in the reward model (e.g., sub-Gaussian noise) that could affect the generality of the results?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0e2pcSxQJS": {
    "paper_id": "0e2pcSxQJS",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes PN-GAIL, a novel Generative Adversarial Imitation Learning (GAIL) method that leverages non-optimal information from imperfect demonstrations. The approach distinguishes between positive and negative risks in demonstrations and requires only a small subset of labeled confidence scores, theoretically avoiding imitation of non-optimal data while mimicking imperfect demonstrations. The authors validate their method on six control tasks, demonstrating superior performance over baseline methods."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in imitation learning by handling imperfect demonstrations, which is highly relevant to real-world applications. The method introduces a novel framework (PN-GAIL) that combines discriminative risk assessment with confidence-based weighting, offering a fresh perspective compared to existing weighting/ranking approaches. The theoretical analysis provides insights into how PN-GAIL avoids non-optimal data, and the experimental results on multiple control tasks support the claims. The paper also includes a semi-supervised confidence classifier improvement, enhancing practical applicability."
          },
          "weaknesses": {
            "value": "The theoretical analysis lacks rigorous mathematical proofs, particularly in demonstrating how PN-GAIL strictly avoids non-optimal demonstrations. The experimental section is underdeveloped: it does not specify the nature of 'imperfect demonstrations' (e.g., noisy vs. biased), nor does it compare against a comprehensive set of baselines. The claim about requiring 'only a small subset of labeled confidence scores' is not quantified or justified. Additionally, the paper omits ablation studies to isolate the contribution of non-optimal information leveraging."
          },
          "questions": {
            "value": [
              "How does PN-GAIL specifically differentiate between positive and negative risks in demonstrations? What is the mechanism for identifying non-optimal examples?",
              "What metrics were used to evaluate 'superior performance' on control tasks? How do these metrics correlate with real-world applicability?",
              "The paper mentions a 'semi-supervised confidence classifier'—what is its architecture, and how does it improve over existing methods?",
              "How were the 'imperfect demonstrations' generated in the experiments? Are they synthetic or real-world data, and what types of imperfections do they represent?",
              "What is the exact proportion of labeled confidence scores required, and how does this compare to existing methods like 2IWIL or IC-GAIL?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes PN-GAIL, a novel imitation learning method that leverages non-optimal information from imperfect demonstrations within the GAIL framework. It introduces a discriminator that evaluates both positive and negative risks of demonstrations, requires minimal labeled confidence scores, and theoretically demonstrates the ability to avoid non-optimal data while mimicking imperfect ones. The method is evaluated on six control tasks, showing superior performance compared to baseline methods."
          },
          "strengths": {
            "value": "Originality: PN-GAIL introduces a novel framework to address imperfect demonstrations by explicitly modeling positive/negative risks, which differs from prior weighting/ranking-based approaches. Quality: The theoretical analysis provides a clear justification for avoiding non-optimal data, and the semi-supervised confidence classifier shows practical innovation. Clarity: The paper is well-structured, with a logical flow from problem motivation to method design. Significance: Addressing imperfect demonstrations is critical for real-world applications, and the method's efficiency in using minimal labeled data is promising."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with all relevant baselines (e.g., VILD, CAIL, UID), which limits the assessment of PN-GAIL's relative strengths. The theoretical analysis is brief and does not rigorously prove the claimed properties of the optimal discriminator. Experimental validation is insufficient: the six control tasks are not described in detail, and it is unclear whether the baselines used are state-of-the-art or appropriately chosen. The semi-supervised confidence classifier's design and training procedure are under-explained, making it hard to evaluate its contribution. The paper also does not address how PN-GAIL handles demonstrations with varying degrees of imperfection."
          },
          "questions": {
            "value": [
              "Which specific baselines were used in the experiments? Were they selected to represent the full range of existing methods (e.g., weighting-based, ranking-based, and semi-supervised approaches)?",
              "How exactly does the semi-supervised confidence classifier improve upon existing methods? What is the training objective and architecture of this classifier?",
              "The paper claims PN-GAIL avoids non-optimal demonstrations, but how is 'non-optimal' defined? Is this based on ground-truth labels or an implicit assumption?",
              "What is the exact mechanism by which PN-GAIL's theoretical analysis demonstrates deviation from non-optimal data? Are there formal proofs or just intuitive arguments?",
              "How does PN-GAIL perform when the proportion of non-optimal demonstrations is extremely high or low? Are there ablation studies on confidence score quality and quantity?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces PN-GAIL, a novel imitation learning method that leverages non-optimal information from imperfect demonstrations by distinguishing positive and negative risks. It addresses the limitation of existing methods that assume optimal demonstrations and requires only a small subset of labeled confidence scores. Theoretical analysis and experiments on six control tasks demonstrate its effectiveness compared to baselines."
          },
          "strengths": {
            "value": "Originality: PN-GAIL innovatively incorporates non-optimal risk assessment into GAIL, addressing a critical gap in handling imperfect demonstrations. Quality: The method is theoretically grounded with analysis of discriminator behavior and includes a semi-supervised confidence classifier. Clarity: The paper is well-structured, with clear explanations of the problem and approach. Significance: The work advances practical imitation learning in real-world scenarios where demonstrations are often suboptimal."
          },
          "weaknesses": {
            "value": "The experimental validation is limited: While six control tasks are tested, the results lack statistical significance tests, detailed comparison metrics (e.g., reward curves, variance analysis), and ablation studies on the confidence classifier. Theoretical analysis is superficial: The claim that PN-GAIL 'deviates from non-optimal demonstrations' lacks rigorous proof or quantitative justification. The paper does not address scalability: It is unclear how PN-GAIL performs with high-dimensional or continuous action spaces. The confidence score labeling assumption is underexplored: The paper assumes access to a small labeled subset but does not discuss how this is obtained in practice."
          },
          "questions": {
            "value": [
              "How does PN-GAIL handle cases where the labeled confidence scores are noisy or biased? What is the robustness of the semi-supervised classifier under such conditions?",
              "What are the computational costs of the proposed confidence classifier compared to existing methods? Is it feasible for real-time applications?",
              "The paper mentions 'six control tasks' but does not specify the environments (e.g., Mujoco, Box2D). Can the authors provide details about the experimental setup and baselines used?",
              "How does PN-GAIL compare to recent methods like DAgger or CQL in handling distributional shift in imperfect demonstrations?",
              "The theoretical analysis claims that PN-GAIL avoids non-optimal demonstrations. Can the authors provide a formal proof or empirical evidence for this claim?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "0fwJMANq9P": {
    "paper_id": "0fwJMANq9P",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces Hercules, an algorithm for generating heuristics for combinatorial optimization problems (COPs) using large language models (LLMs). The key contributions are the Core Abstraction Prompting (CAP) method, which reduces unspecificity in search directions by abstracting core components from elite heuristics, and the few-shot Performance Prediction Prompting (PPP) method, which predicts heuristic performance based on semantic similarity to reduce resource costs. Experiments demonstrate Hercules' superior performance and resource efficiency compared to existing methods."
          },
          "strengths": {
            "value": "Originality: CAP and PPP represent novel approaches to addressing two critical challenges in LLM-based heuristic generation (unspecific search directions and resource-intensive evaluation). The paper also provides theoretical analysis of CAP's effectiveness. Quality: The methodology is well-structured, with clear problem formulation and empirical validation across multiple COPs and LLMs. Clarity: The paper is well-written, with illustrative examples (e.g., Figure 1) that effectively explain the differences between existing and proposed methods. Significance: The work addresses practical limitations in LLM-based heuristic generation, with potential impact on optimization research and applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed implementation specifics for CAP and PPP, particularly how core components are abstracted without examples in zero-shot settings. The ablation studies mentioned are not described in detail, making it hard to assess the individual contributions of CAP and PPP. The theoretical proof of CAP's effectiveness is brief and requires more rigorous mathematical analysis. The experiments focus on specific COPs (e.g., TSP) but do not demonstrate generalizability to broader classes of problems. The PPP method's reliability and accuracy metrics are not thoroughly evaluated."
          },
          "questions": {
            "value": [
              "How exactly does CAP abstract core components from elite heuristics without examples in a zero-shot setting? What criteria are used to identify 'core components'?",
              "Can the authors provide more details about the 'rank-based selection mechanism' mentioned in Section 3.1? How does it integrate with CAP?",
              "The paper mentions 'semantic equivalence' in Figure 2 but does not define or quantify this concept. How is semantic similarity measured in PPP, and what is its correlation with actual heuristic performance?",
              "What are the limitations of PPP in terms of prediction accuracy? Are there cases where PPP fails to identify semantically equivalent heuristics?",
              "How does the paper address the trade-off between resource savings and potential loss of diversity in the heuristic search space when using PPP?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Hercules, an algorithm for generating heuristics for combinatorial optimization problems (COPs) using large language models (LLMs). It proposes Core Abstraction Prompting (CAP) to generate specific search directions by abstracting core components from elite heuristics, and Few-shot Performance Prediction Prompting (PPP) to predict heuristic performance based on semantic similarity, reducing resource usage. The method demonstrates superior performance and efficiency compared to existing LLM-based approaches."
          },
          "strengths": {
            "value": "The paper addresses two critical challenges in LLM-based heuristic generation: vague search directions and resource-intensive evaluation. CAP introduces a novel prompting strategy to extract actionable knowledge from elite heuristics, while PPP offers a first-of-its-kind approach to predict heuristic performance, significantly reducing computational costs. Theoretical analysis of CAP's effectiveness and extensive experiments across multiple tasks and LLMs strengthen the claims. Ablation studies further validate the components' contributions."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art methods on specific metrics, and the theoretical proof of CAP's effectiveness is not fully elaborated. The PPP method's predictive accuracy mechanisms are not quantitatively validated. The experimental results are cut off, leaving uncertainty about the full scope of evaluations. The paper also does not clarify how semantic similarity is quantified for PPP or address potential edge cases where predictions might fail."
          },
          "questions": {
            "value": [
              "How does CAP compare to existing prompting strategies in terms of specific performance metrics (e.g., heuristic quality, convergence speed)?",
              "What is the exact mechanism of PPP's predictive accuracy enhancements, and how is semantic similarity quantified?",
              "Are there scenarios where PPP's predictions are unreliable, and how are these handled?",
              "How does the rank-based selection mechanism work, and what criteria are used for ranking heuristics?",
              "What are the limitations of CAP in terms of problem domains or heuristic complexity?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Hercules, an algorithm for generating heuristics for combinatorial optimization problems (COPs) using Large Language Models (LLMs). It proposes two key innovations: Core Abstraction Prompting (CAP) to generate specific search directions by abstracting core components from elite heuristics, and few-shot Performance Prediction Prompting (PPP) to predict heuristic performance based on semantic similarity, reducing computational costs. The approach is evaluated across multiple tasks and LLMs, showing improved performance and efficiency compared to existing methods."
          },
          "strengths": {
            "value": "The paper presents novel methods (CAP and PPP) addressing critical challenges in LLM-based heuristic generation. CAP's zero-shot abstraction of core components is theoretically justified, and PPP's resource-saving mechanism is a significant contribution. The empirical results demonstrate superiority over state-of-the-art methods, and ablation studies validate the effectiveness of the proposed techniques. The work bridges the gap between LLM capabilities and task-specific heuristic design through structured prompting strategies."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with baseline methods in terms of quantitative metrics (e.g., runtime, heuristic quality). The theoretical analysis of CAP is brief, and the proof of its effectiveness relies on limited empirical evidence. PPP's predictive accuracy and reliability mechanisms are not thoroughly explained, with no analysis of error rates or failure cases. The experiments focus on specific COPs (e.g., TSP) without generalization to broader problem domains. The code snippets and examples are illustrative but do not provide sufficient insight into the implementation of CAP/PPP."
          },
          "questions": {
            "value": "1. How does CAP ensure the abstraction of meaningful core components without explicit examples? What criteria define 'elite heuristics' in the context of CAP? 2. Can the authors provide quantitative results on PPP's prediction accuracy and its impact on resource reduction compared to full evaluation? 3. Are the experiments reproducible? What specific COP instances and LLMs were used, and how were hyperparameters tuned? 4. How does the rank-based selection mechanism complement CAP, and what is its theoretical basis? 5. What is the scalability of Hercules-P for large-scale COPs with complex heuristics?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0gGPVbRqOE": {
    "paper_id": "0gGPVbRqOE",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes Splitted Wavelet Differential Inclusion (SWDI), a novel method for neural signal processing that addresses the limitations of traditional wavelet shrinkage by simultaneously estimating both 'strong' and 'weak' signals. The approach uses an ℓ2 splitting mechanism within a differential inclusion framework to separate sparse (strong) and dense (weak) signal components, aiming to improve reconstruction accuracy and capture clinically relevant information in Parkinson's disease (PD) signals."
          },
          "strengths": {
            "value": "The paper tackles a relevant and underexplored problem in neural signal processing: the joint estimation of strong and weak signal components. The theoretical analysis of the differential inclusion and ℓ2 splitting mechanism shows potential for improving estimation accuracy. The application to PD signals, particularly the identification of non-burst activity, has clinical significance. The method's dual-parameter structure offers a fresh perspective compared to traditional thresholding approaches."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, with only a brief mention of a numerical experiment and no concrete results or comparisons to state-of-the-art methods. The theoretical claims (e.g., 'provable better estimation') are not substantiated with rigorous proofs or mathematical derivations. The explanation of how the ℓ2 splitting mechanism interacts with the differential inclusion is unclear, and the paper does not address computational complexity or scalability. Additionally, the related work section is incomplete, with references cut off."
          },
          "questions": {
            "value": [
              "What is the exact formulation of the differential inclusion used in SWDI, and how does it differ from existing methods?",
              "How is the ℓ2 splitting mechanism implemented in practice? What are the optimization dynamics for the sparse and dense parameters?",
              "Are there quantitative comparisons between SWDI and existing wavelet shrinkage methods on standard datasets?",
              "How was the 'medication impact study' conducted? What specific metrics were used to demonstrate improved correlation with dopaminergic treatment?",
              "What are the limitations of the method in terms of computational efficiency or applicability to other neural signal types?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces Splitted Wavelet Differential Inclusion (SWDI), a novel method for neural signal processing that aims to simultaneously estimate both strong and weak signals in wavelet-based denoising. The approach uses an ℓ2 splitting mechanism to separate sparse (strong) and dense (weak) parameters, with theoretical claims of improved estimation over traditional wavelet shrinkage methods."
          },
          "strengths": {
            "value": "The paper addresses a relevant and underexplored problem in neural signal processing by distinguishing between strong and weak signals. The theoretical framework is novel, combining differential inclusion with ℓ2 splitting, which could offer new insights into signal reconstruction. The motivation is well-justified with clinical context (e.g., Parkinson’s disease biomarkers), and the method’s potential to capture both interpretable and non-interpretable signal components is promising. The paper also provides a discretization method for non-orthogonal wavelet matrices, which broadens its applicability."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous experimental validation. While it mentions 'improved accuracy' and 'additional findings of tonic activity,' there are no specific metrics, datasets, or comparisons to state-of-the-art methods. The theoretical analysis is superficial—claims of 'provable better estimation' are not supported by formal proofs or lemmas. The related work section is incomplete, and the paper does not clearly address how SWDI differs from existing methods like SureShrink or Bayesian shrinkage. The practical utility of capturing 'weak signals' is not convincingly demonstrated, and the clinical relevance of the findings remains speculative without statistical validation."
          },
          "questions": {
            "value": [
              "What specific datasets and metrics were used to evaluate SWDI? How does it compare to established wavelet shrinkage methods (e.g., SureShrink, Bayesian shrinkage) in terms of RMSE, SNR, or other standard metrics?",
              "Are there formal proofs or lemmas demonstrating the superiority of SWDI’s closed-form solution path over traditional wavelet shrinkage? How is the 'bias-free' estimation for strong signals mathematically justified?",
              "How was the 'non-burst activity responsive to medication' validated? Were statistical tests performed to confirm the correlation between SWDI-recovered signals and medication effects?",
              "What is the computational complexity of SWDI compared to existing methods? How does the discretization method handle non-orthogonal wavelet matrices in practice?",
              "Why is an ℓ2 splitting mechanism necessary instead of other regularization strategies (e.g., ℓ1 or mixed-norm penalties)? How does this choice impact the separation of strong and weak signals?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Splitted Wavelet Differential Inclusion (SWDI), a novel method for neural signal processing that addresses the limitations of traditional wavelet shrinkage by distinguishing between 'strong' and 'weak' signals. The approach uses an ℓ2 splitting mechanism to simultaneously estimate both signal types, with theoretical guarantees of improved estimation. The method is applied to Parkinson’s disease (PD) signal analysis, demonstrating enhanced correlation with dopaminergic medication effects."
          },
          "strengths": {
            "value": "The paper presents a theoretically grounded approach to a relevant problem in neural signal processing, addressing the underexplored 'weak signal' component. The dual-parameter framework (sparse for strong signals, dense for weak signals) offers a fresh perspective. The method's closed-form solution path and early stopping mechanism are promising. The clinical relevance to PD and potential for capturing movement-related information are significant strengths. The paper is well-structured and clearly motivates the problem."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficiently detailed. The paper lacks comparisons with state-of-the-art methods, such as Bayesian shrinkage or SureShrink, and does not provide quantitative metrics (e.g., SNR, MSE) to demonstrate improvement. The theoretical analysis of the differential inclusion's convergence or optimality is underdeveloped. The application to PD relies on qualitative claims (e.g., 'more significantly correlated') without rigorous statistical validation. The ℓ2 splitting mechanism's necessity and robustness are not thoroughly justified."
          },
          "questions": {
            "value": "1. What specific numerical experiments were conducted, and how do they compare to baseline methods? 2. How is the early stopping mechanism implemented, and what criteria are used? 3. Are there ablation studies to validate the ℓ2 splitting mechanism's effectiveness? 4. What is the mathematical formulation of the differential inclusion, and are there proofs of its properties? 5. How does the method handle non-orthogonal wavelet decomposition matrices in practice?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "0koPj0cJV6": {
    "paper_id": "0koPj0cJV6",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a watermarking scheme for black-box language models (LLMs) that enables third-party users to detect LLM-generated text without requiring access to the model's internal probabilities. The method leverages sequence sampling, employs a secret key for watermarking, and claims distortion-free properties, adaptability, and performance advantages over white-box schemes."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in existing watermarking techniques by enabling third-party detection under black-box constraints, which is both novel and practically significant. The formalization using cumulative distribution functions (CDFs) and pseudorandom number generators (PRNGs) provides a mathematically rigorous foundation. The ability to chain multiple keys and the distortion-free property are innovative contributions. The work also acknowledges prior limitations in the field and positions itself as an improvement over existing methods."
          },
          "weaknesses": {
            "value": "The paper lacks concrete experimental results to validate its claims, such as detection accuracy, false positive rates, or comparisons with white-box schemes. The algorithm's practical implementation details (e.g., handling of tokenization, adaptability to varying sequence lengths) are under-specified. Additionally, the paper does not thoroughly address potential vulnerabilities to paraphrasing or semantic attacks, which were highlighted in related work. The theoretical guarantees are promising but require empirical verification."
          },
          "questions": {
            "value": "1. How does the proposed method handle paraphrasing or semantic substitution attacks that could undermine watermark detectability? 2. What are the specific experimental results demonstrating performance gains over white-box watermarking schemes? 3. How is the secret key managed and secured in practice, especially against potential adversarial extraction? 4. Are there any constraints on the choice of the CDF $ F $, and how does this affect the scheme's robustness?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes a watermarking scheme for black-box language models (LLMs) that requires only sequence sampling access, enabling third-party users to watermark outputs without white-box access. The method uses a continuous CDF and pseudorandom number generator (PRNG) to embed watermarks, claims distortion-free properties, and supports chaining with multiple keys. The authors provide theoretical guarantees and demonstrate performance improvements over existing white-box schemes."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to black-box watermarking, addressing a critical gap in existing methods that rely on white-box access. The theoretical foundation using CDFs and PRNGs is creative and well-structured. The clarity of the algorithm description, including pseudocode in the appendix, enhances readability. The significance is high, as it empowers third-party users to verify LLM outputs without requiring API modifications, which is a practical and important contribution. The paper also acknowledges limitations and suggests future work, showing self-awareness."
          },
          "weaknesses": {
            "value": "The paper is truncated, leaving key sections (e.g., recursive watermarking, full experimental details) incomplete. This makes it difficult to assess the thoroughness of the experiments or the robustness of the method against adversarial attacks. The theoretical guarantees are mentioned but not elaborated, and the paper lacks comparisons with all relevant prior work (e.g., cryptographic approaches in the related work section). The claim of 'distortion-free' generation requires stronger justification, as the paper does not explicitly address how text quality is preserved."
          },
          "questions": {
            "value": [
              "How does the method ensure distortion-free generation without modifying the model's training or weights? What metrics were used to evaluate text quality in experiments?",
              "The paper mentions recursive watermarking but does not elaborate. Can the authors clarify how multiple keys are chained and whether this introduces vulnerabilities?",
              "What specific adversarial attacks were tested, and how does the method resist paraphrasing or synonym substitution attacks mentioned in the related work?",
              "The experiments are described as 'comprehensive,' but the results are not shown. Can the authors provide details on the datasets, baseline models, and evaluation metrics used?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes a black-box watermarking scheme for large language models (LLMs) that requires only sequence sampling access rather than white-box access to next-token probabilities. The method is distortion-free, supports key-based chaining/nesting, and provides theoretical performance guarantees. The authors demonstrate its effectiveness through experiments, showing it can outperform existing white-box methods under certain conditions."
          },
          "strengths": {
            "value": "The paper introduces a novel black-box watermarking approach that addresses a critical limitation of existing methods relying on white-box access. The theoretical foundation using cumulative distribution functions (CDFs) and pseudorandom functions is mathematically rigorous. The method's distortion-free property and key flexibility are significant innovations. The paper also provides clear algorithmic details and discusses multiple detection strategies (e.g., p-value aggregation, likelihood ratio tests). The problem of third-party watermarking is highly relevant given the prevalence of API-based LLMs."
          },
          "weaknesses": {
            "value": "The experimental evaluation is incomplete and lacks critical comparisons. The paper does not report detection accuracy or false positive rates, which are essential metrics for watermarking. The ablation studies are minimal, with no analysis of parameter sensitivity (e.g., impact of n-gram size k). The claim of outperforming white-box methods is unsupported without direct comparisons. The algorithm description is abstract, with key implementation details (e.g., handling variable-length sequences, seed selection) missing. The security analysis against adversarial attacks is superficial."
          },
          "questions": {
            "value": "1. How is the detection threshold determined for different n-gram sizes and text lengths? 2. What specific metrics (e.g., F1-score, AUC) were used to evaluate watermark detection performance? 3. How does the method handle variable-length outputs from LLM APIs that don't expose token-level probabilities? 4. Can the authors provide theoretical guarantees for the distortion-free property under different CDF choices? 5. What is the computational complexity of the encoding/decoding process for long sequences?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0mJZplhexS": {
    "paper_id": "0mJZplhexS",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces the Little-Big algorithm, a model-agnostic two-pass approach to speed up image classifiers by using a lightweight 'little' model to handle easy samples and a 'big' model to process difficult ones. The method achieves significant reductions in multiply-accumulate operations (MACs) across diverse model architectures (e.g., EfficientNet, DeiT3, InternImage) without compromising accuracy."
          },
          "strengths": {
            "value": "The paper's strength lies in its practical simplicity and broad applicability. The two-pass framework is theoretically grounded in the observation that scaled models primarily improve performance on 'difficult' samples, a claim supported by empirical results. The experiments demonstrate substantial MACs reductions (76-81%) across multiple model families, showcasing the method's generality. The clarity of the problem formulation and the clean presentation of results are notable. The approach also provides a strong baseline for efficient deployment of large models, addressing a critical challenge in real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks depth in analyzing why confidence thresholds effectively identify difficult samples. It does not compare with established model compression techniques (e.g., pruning, quantization) or discuss the trade-offs between MACs reduction and other efficiency metrics (e.g., latency). The dependency on the 'little' model's quality is underexplored—what happens if the little model is suboptimal? The theoretical justification for the MACs reduction is minimal, relying heavily on empirical validation. Additionally, the paper does not address potential failure cases (e.g., when the little model misclassifies easy samples) or evaluate performance on datasets beyond ImageNet-1K."
          },
          "questions": {
            "value": [
              "How is the 'little' model selected? Is it pre-trained or trained specifically for this task? What training strategy ensures its effectiveness?",
              "Are there ablation studies on the confidence threshold used to filter samples? How sensitive is the method to this threshold?",
              "What is the impact of the Little-Big approach on inference latency, not just MACs? How does it compare to other compression methods in practice?",
              "How does the method perform on datasets with different distributions or domains beyond ImageNet-1K?",
              "What guarantees exist that the 'big' model will not inherit errors from the 'little' model's misclassifications of easy samples?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces the Little-Big algorithm, a model-agnostic two-pass approach for accelerating image classifiers by using a lightweight 'little' model to handle easy samples and a larger 'big' model only for difficult ones. The method achieves significant MACs reductions across diverse architectures (CNNs, transformers, hybrids) without accuracy loss, demonstrating practical utility for large model compression."
          },
          "strengths": {
            "value": "Originality: The two-pass paradigm challenges the single-pass assumption of traditional models, offering a novel approach to efficiency. Quality: Experiments are thorough, with systematic evaluation across multiple model families and scales. Clarity: The methodology and results are presented clearly, with informative figures. Significance: The practical impact on real-world deployment of large models is substantial, addressing a critical bottleneck in vision systems."
          },
          "weaknesses": {
            "value": "The paper lacks theoretical analysis of why confidence-based sampling works, relying solely on empirical evidence. The confidence threshold for passing samples to the big model is not rigorously justified or optimized. Experiments are limited to ImageNet-1K, with no ablation studies on other datasets or data distributions. The 'model-agnostic' claim is weakened by the need for pre-trained little/big pairs, which may not generalize to all architectures."
          },
          "questions": {
            "value": [
              "How does the method perform on datasets with different class distributions or domain shifts compared to ImageNet?",
              "What is the optimal confidence threshold for different model families, and how is it determined?",
              "Can the little model be trained jointly with the big model instead of using pre-trained pairs?",
              "How does the approach handle streaming data or real-time inference where latency is critical?",
              "What are the memory implications of storing two models (little + big) versus a single scaled-up model?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes the Little-Big algorithm, a model-agnostic two-pass approach to accelerate image classifiers by using a lightweight 'little' model to handle easy samples and a 'big' model for difficult ones. The method achieves significant MACs reductions across diverse model architectures without compromising accuracy."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by rethinking the single-pass inference paradigm and proposing a simple yet effective solution. The experimental validation is thorough, showing consistent MACs reductions across multiple model families. The clarity of the problem statement and methodology is excellent, and the significance of reducing computational costs for large models is highly relevant to both research and industry."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how the 'little' model's training process affects performance, particularly whether it's trained on the same data as the 'big' model. The confidence threshold mechanism for selecting difficult samples is not thoroughly evaluated. Additionally, the method's generalizability to tasks beyond image classification and its comparison to existing compression techniques (e.g., pruning, knowledge distillation) are underexplored."
          },
          "questions": {
            "value": [
              "How is the 'little' model trained? Is it trained on the same dataset as the 'big' model, and how does its training objective differ?",
              "What is the impact of the 'little' model's accuracy on the overall performance? Are there cases where low 'little' model accuracy could lead to errors despite confidence thresholds?",
              "How does the Little-Big approach compare to established model compression methods in terms of efficiency and accuracy trade-offs?",
              "Are there ablation studies on the confidence threshold selection and its effect on MACs reduction and accuracy?",
              "Can the method be generalized to other tasks beyond image classification, such as object detection or segmentation?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "0nJt9aVGtl": {
    "paper_id": "0nJt9aVGtl",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces WaveDiffusion, a novel framework that reframes Full Waveform Inversion (FWI) as a joint diffusion process in a shared latent space. By merging seismic data and velocity map autoencoders into a unified latent space with VQ, the method generates paired seismic-velocity samples that approximately satisfy the governing PDE without explicit constraints. The diffusion process learns to score latent representations based on their deviation from the PDE, guiding the model toward physically consistent solutions."
          },
          "strengths": {
            "value": "Originality is demonstrated through the integration of diffusion models with FWI, offering a new geometric interpretation of the inverse problem. The method's ability to generate PDE-compliant solutions without explicit constraints is novel. The paper's clarity is generally good, with structured sections and clear technical explanations. The significance lies in its potential to inspire new computational imaging approaches, though its practical impact remains to be validated."
          },
          "weaknesses": {
            "value": "The paper lacks comparative experiments against existing FWI methods (traditional or ML-based), making it difficult to assess performance improvements. The claim that generated pairs satisfy the PDE is not quantitatively validated—no metrics or ablation studies are provided. The diffusion model's training objective and score function are under-specified, leaving critical implementation details ambiguous. The paper also omits analysis of failure cases or limitations of the shared latent space approach."
          },
          "questions": {
            "value": "How is the shared latent space trained? What is the exact role of VQ in merging the autoencoders? How is the diffusion model's score function defined—does it use PDE residuals, physics-based losses, or another metric? Are there quantitative results showing the degree to which generated pairs satisfy the PDE? How does the method handle noise or incomplete seismic data? What are the computational costs compared to traditional FWI or other ML approaches?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces WaveDiffusion, a novel framework that reframes Full Waveform Inversion (FWI) as a joint diffusion process in a shared latent space. By merging seismic data and velocity map autoencoders into a unified latent space using VQ, the method trains a diffusion model to generate paired seismic-velocity data that inherently satisfy the governing wave equation without explicit constraints. Experiments on the OpenFWI dataset demonstrate high-fidelity, diverse outputs adhering to physical PDEs."
          },
          "strengths": {
            "value": "Originality is strong, as the paper creatively combines diffusion models with FWI in a shared latent space, offering a new geometric interpretation of the problem. The approach bridges physical PDE constraints with generative modeling, which is novel. The clarity of the exposition is good, with logical flow and clear figures. The significance lies in providing a fresh perspective for FWI, potentially inspiring new directions in computational imaging and physics-informed machine learning."
          },
          "weaknesses": {
            "value": "The paper lacks ablation studies to validate the necessity of the shared latent space and VQ-based codebook. Experimental validation of PDE satisfaction is limited to qualitative observations; quantitative metrics (e.g., residual errors in the wave equation) are missing. The comparison with traditional FWI or other ML methods (e.g., InversionNet) is insufficient. The computational efficiency of the proposed method relative to existing approaches is not discussed."
          },
          "questions": {
            "value": "1. How is the diffusion model's scoring of latent points (based on PDE deviation) implemented? What specific loss or metric is used? 2. How were the PDE constraints quantitatively verified? Were residuals or numerical errors computed? 3. Are there comparisons with traditional FWI or other ML-based methods in terms of accuracy or convergence? 4. What are the computational costs of training and inference compared to existing approaches?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces WaveDiffusion, a novel framework that reframes Full Waveform Inversion (FWI) as a joint diffusion process in a shared latent space. The approach combines dual autoencoders with a diffusion model to generate paired seismic and velocity maps that inherently satisfy the governing wave equation without explicit constraints."
          },
          "strengths": {
            "value": "The paper presents a creative and original approach by integrating diffusion models with latent space representations for FWI, offering a new perspective on solving PDE-constrained problems. The methodology is well-structured, with clear technical details on the dual autoencoder architecture and diffusion process. The experiments on the OpenFWI dataset demonstrate the feasibility of generating high-fidelity, physically consistent pairs. The work also highlights an interesting geometric interpretation of FWI through the lens of diffusion scoring."
          },
          "weaknesses": {
            "value": "The paper lacks a comprehensive comparison with existing FWI methods, making it difficult to assess the practical advantages of the proposed approach. The validation of PDE satisfaction is not thoroughly detailed—e.g., specific metrics or quantitative checks are missing. Additionally, the role of the shared latent space and VQ codebook in capturing physical constraints is not sufficiently explained. The ablation studies or analysis of key components (e.g., diffusion steps, latent space dimensions) are absent."
          },
          "questions": {
            "value": [
              "How is the PDE satisfaction of generated pairs quantitatively validated? Are there specific metrics or error measures used?",
              "What is the exact mechanism by which the diffusion model learns to score latent points based on PDE deviation? Is this learned through a separate loss function or inherent in the diffusion process?",
              "How does the proposed method handle noisy or incomplete seismic data compared to traditional FWI approaches?",
              "Are there limitations to the scalability of this approach for complex geological structures or large-scale datasets?",
              "What ablation studies were conducted to evaluate the impact of the shared latent space versus separate latent spaces?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0owyEm6FAk": {
    "paper_id": "0owyEm6FAk",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates the security risks of LoRA adapters in the share-and-play ecosystem, demonstrating how a backdoor-infected LoRA can be trained once and then merged with multiple task-specific LoRAs to retain both malicious and benign capabilities. The authors highlight the scalability of this attack vector and warn about the potential for widespread deployment of malicious LoRAs."
          },
          "strengths": {
            "value": "The paper addresses a timely and important security concern in the rapidly growing LoRA ecosystem, which is underexplored in existing literature. It provides a clear theoretical framework for understanding how backdoors can be injected and propagated through LoRA adapters. The practical demonstration of merging a single backdoor LoRA with multiple task-specific adapters showcases a novel application of existing techniques. The paper's structure is logical, and the problem statement is well-motivated by the popularity of LoRA in open-source communities."
          },
          "weaknesses": {
            "value": "The experimental validation is minimal and lacks depth. The paper does not provide quantitative results on the effectiveness of the backdoor injection or the success rate of merging with different task-specific LoRAs. The example of biased political content is hypothetical and not empirically tested. The paper also omits discussions on potential detection methods or countermeasures, which would strengthen its practical relevance. Additionally, the theoretical analysis of why the merging process works remains underdeveloped."
          },
          "questions": {
            "value": "How was the backdoor dataset constructed, and what specific triggers were used? What tasks were evaluated to demonstrate the effectiveness of the merged LoRAs? Are there limitations to the merging process when dealing with diverse model architectures or training objectives? How does the rank of the LoRA adapter affect the stealthiness and effectiveness of the backdoor? What are the practical challenges in detecting such backdoors in real-world scenarios?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates a novel security risk in the LoRA (Low-Rank Adaptation) ecosystem, where malicious actors can inject backdoors into LoRA adapters that are then distributed across a share-and-play community. The authors demonstrate that a single backdoored LoRA can be merged with multiple task-specific adapters while retaining both benign and adversarial capabilities, enabling large-scale attacks with minimal effort."
          },
          "strengths": {
            "value": "The paper introduces a timely and critical security concern in the LoRA ecosystem, which is increasingly popular for efficient model adaptation. The originality lies in highlighting a previously unexplored attack vector (LoRA-as-an-Attack) and its potential for scalable deployment. The theoretical framework is well-structured, and the practical example of biased political content illustrates the real-world implications. The work also emphasizes the need for security awareness in open-source model sharing, which is a significant contribution to the community."
          },
          "weaknesses": {
            "value": "The paper lacks concrete experimental validation to support its claims. While the methodology is described in general terms (e.g., training a 'feed-forward only LoRA on a tiny backdoor dataset'), specific details on the attack setup, evaluation metrics, or reproducibility are absent. Additionally, the paper does not address mitigation strategies or detection mechanisms for such backdoors. The related work section is brief, and the paper could benefit from a more comprehensive comparison with existing backdoor attack literature in LLMs."
          },
          "questions": {
            "value": "1. How is the backdoor specifically injected into the LoRA adapter? Are there technical details about the training process or trigger design? 2. What metrics were used to evaluate the effectiveness of the backdoor and the preservation of benign capabilities? 3. How does the proposed attack scale across different tasks and models, and what are its limitations? 4. Are there any practical challenges in merging the backdoored LoRA with other adapters, and how are they addressed? 5. What steps can the community take to detect or prevent such attacks?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates the security risks of LoRA adapters in the share-and-play ecosystem, demonstrating how a single backdoor-infected LoRA can be merged with multiple task-specific LoRAs to scale malicious attacks while retaining both benign and adversarial capabilities. The authors highlight the stealthy nature of such attacks and their potential to compromise open-source LLM workflows."
          },
          "strengths": {
            "value": "The paper addresses a novel and timely security risk in the LoRA ecosystem, which is critical given the widespread adoption of parameter-efficient fine-tuning. The problem formulation is clear, and the threat model is well-motivated with practical examples. The authors provide a structured analysis of the attack mechanism, emphasizing its scalability and stealth. The paper's significance lies in its warning to the community about underexplored vulnerabilities in open-source LLM workflows."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical specifics about the backdoor injection process, such as how the trigger is designed, the exact training methodology for the FF-only LoRA, and the evaluation metrics for assessing the attack's effectiveness. The experiments are incomplete, and the paper does not address countermeasures or detection strategies. Additionally, the claim about retaining 'both malicious and benign capabilities' requires more empirical validation to substantiate the balance between these aspects."
          },
          "questions": {
            "value": "1. How is the backdoor trigger designed to ensure stealth while maintaining task performance? 2. What datasets and metrics were used to evaluate the effectiveness of the merged LoRAs? 3. Are there specific examples of the backdoor's behavior in different tasks? 4. How does the paper address the robustness of the base model against such attacks? 5. What are the limitations of the training-free merging approach in real-world scenarios?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "0rS9o1uKqu": {
    "paper_id": "0rS9o1uKqu",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces TLDR, a network inversion method to reconstruct training-like data from convolutional vision classifiers. The approach uses a conditioned generator to invert models by leveraging classifier properties such as confidence, robustness to perturbations, and gradient behavior. Experiments on standard datasets demonstrate the feasibility of reconstructing semantically similar data even with regularization techniques."
          },
          "strengths": {
            "value": "Originality is strong in addressing privacy risks for real-world CNNs with regularization, which prior work has not thoroughly explored. The methodology combines multiple losses (cross-entropy, KL divergence, etc.) and novel conditioning techniques (soft vector and intermediate matrix conditioning) for diversity. Experiments on diverse datasets (MNIST, CIFAR-10) validate the approach. The paper clearly connects theoretical properties (confidence, robustness) to practical inversion strategies, highlighting significant privacy implications."
          },
          "weaknesses": {
            "value": "The paper lacks analysis of how regularization techniques (e.g., dropout, batch normalization) specifically interact with the inversion process. The reconstruction quality metrics are not compared to baseline inversion methods, making it hard to assess improvement. The use of 'prior knowledge about images' is vague—specifics on how this is implemented (e.g., image priors, distribution assumptions) are missing. The theoretical justification for combining confidence, robustness, and gradient signals is superficial."
          },
          "questions": {
            "value": "1. How does the method handle models trained with aggressive regularization (e.g., high dropout rates)? 2. What ablation studies were conducted to isolate the contribution of each signal (confidence, robustness, gradients)? 3. Are the reconstructed samples quantitatively compared to training data (e.g., via FID scores) or only qualitatively? 4. How is 'prior knowledge about images' formalized, and what impact does it have on reconstruction quality?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces TLDR, a network inversion approach to reconstruct training-like data from vision classifiers with convolutional layers. The method uses a single conditioned generator guided by classifier properties such as model confidence, robustness to perturbations, and gradient behavior to produce semantically similar training data. Experiments on standard datasets demonstrate the feasibility of this attack, highlighting privacy risks in model sharing."
          },
          "strengths": {
            "value": "The paper presents a novel approach to network inversion in realistic settings, addressing convolutional networks with regularization techniques like dropout and batch normalization, which are commonly used in practice. The integration of multiple signals (confidence, robustness, gradients) demonstrates a creative combination of ideas. The empirical validation on diverse datasets shows practical relevance. The clarity of the methodology and problem formulation is strong, and the significance of highlighting privacy risks in model sharing is clear."
          },
          "weaknesses": {
            "value": "The paper lacks comparative analysis with existing inversion methods, making it difficult to assess the novelty and effectiveness of TLDR. The theoretical justification for combining the three signals (confidence, robustness, gradients) is underdeveloped. The experiments focus on standard datasets but do not explore edge cases (e.g., highly regularized models or adversarial training). The paper also does not discuss potential defenses against such attacks, limiting its practical impact."
          },
          "questions": {
            "value": "How does TLDR compare to prior work on model inversion (e.g., [specific papers]) in terms of reconstruction quality and computational efficiency? Are there scenarios where the method fails (e.g., with strong regularization or adversarial training)? What are the limitations of using gradient-based signals for reconstruction? How could the privacy risks identified in this work be mitigated in practice?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces TLDR, a network inversion method to reconstruct training-like data from convolutional vision classifiers. It leverages classifier properties such as confidence, robustness to perturbations, and gradient behavior, using a conditioned generator to produce semantically similar data to the training set. The approach is validated on multiple vision datasets, highlighting privacy risks in model sharing."
          },
          "strengths": {
            "value": "The paper addresses a critical privacy concern in machine learning, which is highly significant. The methodology is novel in applying network inversion to complex CNNs with regularization, combining multiple classifier properties (confidence, robustness, gradients) for reconstruction. The experiments cover diverse datasets, and the approach demonstrates practical relevance. The clarity of the problem statement and technical details is strong, though some sections require deeper elaboration."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons with existing inversion methods, making it unclear how TLDR advances prior work. The proposed 'soft vector conditioning' and 'intermediate matrix conditioning' are not sufficiently explained or justified. The evaluation focuses on qualitative results, with limited quantitative metrics (e.g., FID, IS) or ablation studies. The theoretical basis for combining the three signals (confidence, robustness, gradients) is underdeveloped, and the paper does not address limitations in scalability or real-world applicability."
          },
          "questions": {
            "value": "How does TLDR differ from prior model inversion techniques (e.g., those targeting GANs or simpler models)? What are the specific innovations in 'soft vector conditioning' and 'intermediate matrix conditioning'? Are the reconstructed samples quantitatively evaluated, or are the results purely visual? How does the method handle models with stronger regularization (e.g., deeper networks)? What are the computational costs and practical constraints of the approach?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0sJ8TqOLGS": {
    "paper_id": "0sJ8TqOLGS",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces SPARK, a novel evaluation framework based on the Hierarchical Three-Space Theory, to assess large language models' (LLMs) ability to identify inconsistencies in problem framing. The framework creates benchmarks by introducing inconsistencies in well-defined datasets across mathematics, science, and reading comprehension. It employs two metrics—problem-solving capability rate and challenge rate—to evaluate LLMs' critical thinking, highlighting their limitations and exploring mitigation strategies like modified prompting and fine-tuning."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in evaluating LLMs' ability to detect flaws in problem setups, a key aspect of real-world decision-making. The theoretical foundation in the Three-Space Theory provides a structured approach to analyze problem framing, strategy, and implementation. The framework's focus on cross-domain generalization and mitigation strategies demonstrates practical relevance. The paper also contextualizes its contributions within existing literature, showing awareness of prior work on critical thinking in AI."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, making it impossible to assess the full methodology, experimental design, or results. Key claims about LLM limitations and mitigation strategies lack empirical validation. The SPARK framework's novelty is not sufficiently differentiated from existing benchmarks (e.g., MR-BEN, PRM800K), and the theoretical adaptation to LLMs is not thoroughly justified. The hypotheses (e.g., SSI, PSS) are mentioned but not elaborated, leaving unclear how they were tested or what conclusions were drawn."
          },
          "questions": {
            "value": "1. How were the inconsistencies introduced into the datasets, and what criteria define a 'flawed problem setup'? 2. What specific metrics were used to calculate 'problem-solving capability rate' and 'challenge rate,' and how were they validated? 3. How do the proposed mitigation strategies (e.g., prompting, fine-tuning) compare to existing methods for improving LLM critical thinking? 4. What are the exact experimental results demonstrating LLM limitations, and how do they correlate with the hypotheses (e.g., ADA, KBC)? 5. How does SPARK differ from prior benchmarks like MR-BEN or PRM800K in terms of evaluation scope and methodology?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces SPARK, a framework for evaluating large language models' (LLMs) ability to identify inconsistencies in problem framing, grounded in the Hierarchical Three-Space Theory. The work proposes benchmark creation by introducing inconsistencies in existing datasets and introduces two metrics: problem-solving capability rate and challenge rate. Experiments with state-of-the-art LLMs reveal limitations in critical thinking, particularly in recognizing flawed problem setups."
          },
          "strengths": {
            "value": "The paper addresses an important gap in evaluating LLMs' critical thinking skills, especially their ability to detect inconsistencies in problem framing. The theoretical foundation in the Three-Space Theory provides a structured approach, and the proposed metrics (problem-solving capability rate and challenge rate) offer quantifiable measures. The work also explores mitigation strategies like modified prompting and fine-tuning, adding practical value. The interdisciplinary connection to cognitive science theories enhances the relevance of the framework."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation. Key aspects such as the exact implementation of the SPARK framework, the specific datasets used for benchmarking, and the comparison with existing baselines are under-specified. The claims about the limitations of LLMs in critical thinking are not sufficiently supported by empirical evidence. The theoretical discussion of the Three-Space Theory is superficial, and the paper does not thoroughly address how SPARK differs from existing benchmarks (e.g., MR-BEN, PRM800K). The truncated content (e.g., Section 3) raises concerns about the completeness of the methodology."
          },
          "questions": {
            "value": [
              "What are the specific details of the SPARK framework's five hypotheses (SSI, PSS, RMI, ADA, KBC)? How are they operationalized in experiments?",
              "Which datasets were used to create the benchmarks, and how were inconsistencies introduced? Are the modifications reproducible?",
              "How are the metrics 'problem-solving capability rate' and 'challenge rate' defined and calculated? Are they validated against human annotations?",
              "What baselines were compared against the proposed framework? Are the results statistically significant?",
              "How does SPARK address the limitations of existing benchmarks (e.g., MR-BEN, PRM800K)? What novel insights does it provide?",
              "What are the exact configurations of the LLMs tested (e.g., model size, training data)? How do the mitigation strategies (e.g., modified prompting) compare to prior work on LLM robustness?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces SPARK, a framework grounded in the Hierarchical Three-Space Theory, to evaluate large language models' (LLMs) ability to detect inconsistencies in problem framing. The authors propose a methodology for creating benchmarks by introducing inconsistencies into question-answering datasets and define metrics like problem-solving capability rate and challenge rate. Experiments with state-of-the-art LLMs highlight their limitations in critical thinking, alongside explorations of mitigation strategies such as modified prompting and fine-tuning."
          },
          "strengths": {
            "value": "The paper presents a novel and structured approach to evaluating critical thinking in LLMs by leveraging the Hierarchical Three-Space Theory, which offers a fresh perspective on problem-solving analysis. The framework is generalizable across domains (mathematics, science, reading comprehension) and includes well-defined hypotheses (e.g., SSI, PSS, RMI) for systematic experimentation. The motivation is strong, addressing a critical gap in existing LLM evaluations. The theoretical foundation is robust, and the focus on problem framing inconsistencies aligns with real-world applications, making the work significant for advancing AI evaluation methodologies."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving key details about the experimental setup, results, and analysis incomplete. Without full access to the experiments, it is difficult to assess the validity of the proposed metrics (e.g., problem-solving capability rate, challenge rate) or the effectiveness of mitigation strategies. Additionally, the paper lacks concrete examples of how inconsistencies are introduced into datasets or how the Three-Space Theory is operationalized in practice. The connection between the theoretical framework and the practical benchmarks requires further clarification."
          },
          "questions": {
            "value": [
              "How are inconsistencies and misleading cues systematically introduced into the datasets? What criteria define 'inconsistencies' in the context of this work?",
              "What specific datasets were used for benchmark creation, and how do they ensure diversity across domains (math, science, reading comprehension)?",
              "How are the proposed metrics (problem-solving capability rate, challenge rate) validated? Are they compared to existing benchmarks or human baselines?",
              "What are the limitations of the SPARK framework, and how does it address gaps in prior work (e.g., MR-BEN, PRM800K)?",
              "How do the mitigation strategies (e.g., modified prompting, fine-tuning) perform in practice? Are there quantitative results to support their efficacy?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0sary0UZn5": {
    "paper_id": "0sary0UZn5",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates the architectural limitations and redundancy of Transformers by analyzing the rank of attention score matrices. The authors empirically show that attention ranks increase with head dimension $d_h$ but eventually reach an upper bound (low-rank barrier) and saturation (model-reduction effect). They provide theoretical analysis proving a consistent upper bound of approximately $0.63n$ and a critical threshold $d_h = \\Omega(\\log n)$ for rank saturation."
          },
          "strengths": {
            "value": "The paper presents thorough empirical experiments across diverse model configurations and data distributions, demonstrating consistent observations of attention rank behavior. The theoretical analysis offers rigorous mathematical support for the empirical findings, including precise bounds and critical thresholds. The work addresses a significant gap in understanding Transformer efficiency and capacity, with clear contributions to both theoretical and practical aspects of model design."
          },
          "weaknesses": {
            "value": "The theoretical analysis assumes i.i.d. Gaussian initialization and input data, which may not generalize to real-world scenarios with structured data. The experiments focus on synthetic data and specific model architectures, limiting the immediate applicability of findings to complex real-world tasks. The paper lacks discussion on how to leverage these insights for practical model optimization or efficiency improvements."
          },
          "questions": {
            "value": "1. How do the theoretical results hold under non-Gaussian or structured data distributions commonly found in real-world applications? 2. Can the identified rank saturation threshold ($d_h = \\Omega(\\log n)$) be leveraged to design more efficient Transformer variants? 3. Are there specific real-world tasks or datasets where the low-rank barrier significantly impacts performance, and how can this be mitigated? 4. How do the authors reconcile their findings with recent work on sparse attention mechanisms or other rank-reduction techniques?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates the architectural limitations and redundancy of Transformers by analyzing the rank of attention score matrices. It identifies two key properties: the low-rank barrier (attention rank is upper-bounded by ~0.63n regardless of head dimension) and model-reduction effect (rank saturation occurs at d_h = Ω(log n)). The work combines empirical experiments across diverse settings with theoretical analysis to characterize these phenomena."
          },
          "strengths": {
            "value": "Originality: The rank-based analysis of Transformers offers a novel perspective on their architectural properties. Quality: The empirical study is comprehensive, covering multiple model configurations, data distributions, and sequence lengths. Clarity: The paper is well-structured, with clear notation and logical flow. Significance: The findings provide theoretical insights into Transformer dynamics, with potential implications for model efficiency and design."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparison with prior work on attention rank (e.g., Kanai et al. 2018, Lin et al. 2022), making it unclear if the theoretical upper bound of 0.63n is truly novel. Experimental validation is limited to synthetic data and specific model architectures, with no results on standard NLP/CV benchmarks. The practical implications of the low-rank barrier for model compression or optimization are not discussed. Theoretical analysis assumes i.i.d. initialization and input distributions, which may not reflect real-world scenarios."
          },
          "questions": {
            "value": "1. How do the theoretical results differ from prior work on attention rank analysis? 2. Are the observed rank properties robust to non-i.i.d. input distributions or different initialization schemes? 3. What are the concrete applications of these findings for improving Transformer efficiency? 4. Why is the upper bound exactly 0.63n rather than another constant? 5. How do these results generalize to other attention variants (e.g., sparse attention)?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates the architectural limitations and redundancy of Transformers by analyzing the rank of attention score matrices. It identifies two key phenomena: a low-rank barrier (attention rank upper bounded by ~0.63n) and model-reduction effect (rank saturation at d_h = Ω(log n)), supported by extensive experiments and theoretical analysis."
          },
          "strengths": {
            "value": "The paper demonstrates rigorous empirical validation across diverse model configurations and data distributions, showcasing consistent observations of rank saturation. The theoretical analysis provides concrete mathematical bounds, addressing an important gap in understanding Transformer capacity. The work's focus on efficiency and scalability aligns with critical challenges in large-scale model deployment. Clarity is maintained through structured notation and problem formulation."
          },
          "weaknesses": {
            "value": "The theoretical analysis lacks depth in explaining why the 0.63n bound emerges from Transformer architecture, relying heavily on empirical validation. The experiments primarily use synthetic data with i.i.d. assumptions, which may not reflect real-world data complexities. The practical implications of the model-reduction effect (e.g., compression strategies) are not fully explored. The paper also does not compare against recent rank-optimization methods like Sparse Attention or Long-Range Transformers."
          },
          "questions": {
            "value": [
              "How was the 0.63n theoretical bound derived? Is it dependent on specific assumptions about input distributions or model parameters?",
              "Were the experiments conducted on diverse real-world datasets beyond standard NLP benchmarks (e.g., vision, audio)?",
              "How does the model-reduction effect impact practical applications like model pruning or knowledge distillation?",
              "What are the limitations of the i.i.d. initialization and input assumptions in the theoretical analysis?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0uRc3CfJIQ": {
    "paper_id": "0uRc3CfJIQ",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes ORSO, an algorithm that frames reward function selection as an online model selection problem to accelerate reward design in reinforcement learning. It claims to improve data efficiency and reduce computational costs by adaptively allocating training time to shaping reward functions, with empirical results showing superior performance over prior methods like EUREKA."
          },
          "strengths": {
            "value": "Originality: The approach of treating reward selection as an online decision-making problem with regret guarantees is novel, though the connection to multi-armed bandits and model selection is somewhat standard. Quality: The experiments on continuous control tasks with Isaac Gym demonstrate clear improvements in data efficiency and performance. Clarity: The paper is well-structured, with clear definitions and a logical flow. Significance: Addressing reward design efficiency is critical for real-world RL applications, and the claimed 8× speedup is impactful."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with state-of-the-art reward design methods beyond EUREKA. The theoretical analysis of regret bounds is superficial, with no detailed derivation or assumptions. The experiments focus on a limited set of tasks (continuous control) without exploring other domains. The claim of 'expert-level' reward function selection is not sufficiently validated with ablation studies or human comparison metrics. The computational efficiency gains are not quantified in terms of actual wall-clock time reduction."
          },
          "questions": {
            "value": "1. How does ORSO handle reward functions with highly correlated performance metrics? 2. What are the specific assumptions required for the regret guarantees, and how do they hold in non-stationary environments? 3. Can the authors provide ablation studies showing the contribution of online selection vs. static reward evaluation? 4. How does ORSO scale to larger reward function spaces beyond the K=5 or K=10 examples in the experiments? 5. What is the exact definition of 'task reward' versus 'shaping reward' in the theoretical analysis?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper proposes ORSO, an algorithm that frames reward function selection in reinforcement learning as an online model selection problem. By leveraging online decision-making techniques, ORSO automatically identifies high-performing shaping reward functions with provable regret guarantees, demonstrating improved data efficiency and computational speed compared to prior methods like EUREKA."
          },
          "strengths": {
            "value": "The paper introduces a novel approach by applying online learning techniques to reward design, which is a fresh perspective in RL. The theoretical guarantees (regret bounds) and empirical results on continuous control tasks highlight the method's rigor and practicality. The clarity of the problem formulation, methodology, and experiments is strong, with clear comparisons to existing work. The significance of addressing reward design efficiency is high, as it directly impacts the scalability of RL in complex tasks."
          },
          "weaknesses": {
            "value": "The paper assumes access to a finite set of candidate reward functions, but it does not discuss how these candidates are generated or selected, which could limit the method's applicability. The experiments compare ORSO to EUREKA but lack comparisons to other state-of-the-art methods, leaving the relative performance unclear. Additionally, the theoretical analysis may rely on restrictive assumptions (e.g., stationary reward distributions) that are not fully validated in dynamic environments. The paper also does not address the sensitivity of ORSO to hyperparameters or the quality of the initial candidate rewards."
          },
          "questions": {
            "value": [
              "How are the candidate shaping reward functions $\\mathcal{R}^K$ selected? Are they manually designed, automatically generated, or learned from data?",
              "What is the impact of the diversity and representativeness of the candidate set on ORSO's performance? How does the method handle cases where the optimal reward function is not in $\\mathcal{R}^K$?",
              "Are there ablation studies demonstrating the contribution of specific components of ORSO (e.g., the model selection algorithm vs. the policy optimization module)?",
              "How does ORSO scale to larger or more complex reward spaces, and what are the computational constraints of the method?",
              "What are the limitations of the regret guarantees? Do they hold under non-stationary or adversarial reward environments?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes ORSO, an algorithm that frames reward function selection in reinforcement learning as an online model selection problem. By treating shaping reward functions as 'arms' in a multi-armed bandit framework, ORSO dynamically allocates computational resources to identify high-performing rewards with provable regret guarantees. The method claims to improve data efficiency and reduce computational costs compared to prior approaches, demonstrating superior performance on continuous control tasks."
          },
          "strengths": {
            "value": "Originality is evident in reframing reward selection as an online decision-making problem with theoretical guarantees. The paper addresses a critical challenge in RL—efficient reward design—through a novel algorithmic approach. Clarity is strong in the problem formulation and experimental setup, with clear comparisons to baseline methods. The significance lies in the potential to automate reward engineering, reducing reliance on manual intervention and improving practical applicability of RL systems."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete due to the paper being cut off, limiting assessment of scalability and generalizability. The theoretical analysis of non-stationary reward dynamics is underdeveloped, with insufficient discussion of how ORSO adapts to changing task reward distributions. The comparison to prior work lacks depth, particularly in quantifying the 50% performance improvement claimed. The computational efficiency claims require further verification with detailed ablation studies."
          },
          "questions": {
            "value": "1. What specific metrics were used to evaluate 'high-quality' reward functions, and how do they correlate with task reward performance? 2. How does ORSO handle non-stationary environments where the optimal reward function may change during training? 3. Are the regret guarantees tight in practical settings, and what assumptions underlie the theoretical analysis? 4. What is the exact computational overhead of the online model selection component compared to baseline methods?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "0vtftmYQGV": {
    "paper_id": "0vtftmYQGV",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes SNAP-TTA, a sparse test-time adaptation (TTA) framework tailored for edge devices with limited computational resources. It introduces two components: Class and Domain Representative Memory (CnDRM) for efficient sampling of representative data and Inference-only Batch-aware Memory Normalization (IoBMN) to adjust normalization layers during inference. The method reduces adaptation frequency and data usage while maintaining competitive accuracy, even with adaptation rates as low as 0.01."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in deploying TTA on edge devices, where latency and computational constraints are paramount. The proposed framework combines sparse adaptation with novel sampling and normalization techniques, offering a practical solution for real-time applications. The methodology is well-structured, and the integration with five state-of-the-art TTA algorithms demonstrates broad applicability. The motivation for sparse adaptation is compelling, and the paper highlights the trade-offs between accuracy and latency effectively."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results and quantitative comparisons to validate the claims. Key details about CnDRM and IoBMN, such as how representative samples are selected or how normalization is implemented, are not sufficiently explained. The evaluation on edge devices (e.g., Raspberry Pi 4) is mentioned but not elaborated with specific metrics or benchmarks. Additionally, the paper does not address how the adaptation rate is determined for different scenarios or how the framework generalizes across diverse domains."
          },
          "questions": {
            "value": "1. What specific metrics (e.g., accuracy, latency, FLOPs) were used to evaluate SNAP-TTA's performance? 2. How is the adaptation rate (e.g., 0.01–0.5) determined in practice, and what factors influence this choice? 3. Can the authors provide ablation studies to isolate the contributions of CnDRM and IoBMN? 4. How does the framework handle domain shifts that are not captured by the representative samples in CnDRM? 5. What are the exact computational savings achieved by IoBMN compared to traditional normalization methods?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes SNAP-TTA, a sparse test-time adaptation (TTA) framework designed for latency-sensitive edge devices. It introduces two components: Class and Domain Representative Memory (CnDRM) for efficient sampling and Inference-only Batch-aware Memory Normalization (IoBMN) to reduce adaptation overhead. The method claims to maintain accuracy with minimal adaptation rates (e.g., 0.01) while significantly reducing latency compared to existing TTA approaches."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in deploying TTA on edge devices, where latency and computational constraints are paramount. The proposed framework's focus on sparse adaptation aligns with real-world applications like autonomous driving. The methodology introduces novel components (CnDRM and IoBMN) that could potentially reduce computational overhead. The problem formulation is relevant, and the motivation for sparse adaptation is well-justified given the limitations of existing TTA methods on resource-constrained devices."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation. While the abstract mentions results with adaptation rates as low as 0.01, there is no quantitative analysis of accuracy trade-offs or comparisons to baseline methods. The methodology description is incomplete (e.g., the section on CnDRM is cut off), making it difficult to assess technical rigor. The paper also does not explicitly compare SNAP-TTA to existing sparse TTA approaches or discuss limitations of the proposed method. The experimental setup on edge devices (e.g., Raspberry Pi 4) is mentioned but not thoroughly evaluated."
          },
          "questions": {
            "value": "1. How are the representative samples selected in CnDRM? What criteria or metrics are used for class and domain representativeness? 2. What is the exact implementation of IoBMN, and how does it differ from existing normalization techniques? 3. Are there ablation studies demonstrating the effectiveness of CnDRM and IoBMN individually? 4. Which five state-of-the-art TTA algorithms were combined with SNAP-TTA, and how do they perform at reduced adaptation rates? 5. How does the paper address domain shifts with high label noise, as mentioned in the CnDRM motivation? 6. What is the computational complexity of SNAP-TTA compared to existing methods, and how does it scale with different adaptation rates?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes SNAP-TTA, a sparse test-time adaptation (TTA) framework designed for latency-sensitive edge applications. It introduces two components: Class and Domain Representative Memory (CnDRM) for efficient sampling and Inference-only Batch-aware Memory Normalization (IoBMN) for on-the-fly normalization during inference. The method reduces adaptation frequency and data usage while maintaining accuracy, as demonstrated by experiments with five SOTA TTA algorithms."
          },
          "strengths": {
            "value": "The paper addresses a critical real-world problem: the latency-accuracy trade-off in TTA for edge devices. The proposed framework introduces novel components (CnDRM and IoBMN) that directly target computational efficiency. The methodology is well-structured, and the motivation for sparse adaptation aligns with practical deployment constraints. The paper also highlights the importance of balancing adaptation frequency with performance, which is a significant contribution to TTA research."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete, with limited details on datasets, baselines, and quantitative comparisons. Key claims (e.g., 'competitive accuracy at adaptation rate 0.01') lack specific metrics or ablation studies. The paper references an appendix for details but does not provide sufficient information in the main text. The novelty of CnDRM and IoBMN is not thoroughly justified, and the mechanism of IoBMN is unclear compared to existing normalization techniques."
          },
          "questions": {
            "value": "1. What datasets and TTA baselines were used for experiments? How do SNAP-TTA's results compare to non-sparse TTA methods in terms of accuracy and latency? 2. How is the adaptation rate defined (e.g., per batch, per sample)? Are there ablation studies on the impact of different adaptation rates? 3. What is the exact mechanism of IoBMN, and how does it differ from prior work like domain adaptation normalization? 4. Are there limitations to the proposed method (e.g., sensitivity to data distribution shifts)?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "0y3hGn1wOk": {
    "paper_id": "0y3hGn1wOk",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces FIUBENCH, a benchmark for evaluating Vision Language Model (VLM) unlearning under the 'Right to be Forgotten' setting. The authors propose a two-stage evaluation pipeline using a synthetic Fictitious Facial Identity VQA dataset and incorporate privacy attacks (membership inference and adversarial privacy extraction) to assess unlearning effectiveness. They evaluate four baseline methods and highlight limitations in their performance, emphasizing the need for robust privacy evaluations."
          },
          "strengths": {
            "value": "The paper addresses a timely and important problem: privacy concerns in VLMs under regulatory frameworks like the Right to be Forgotten. The introduction of a structured benchmark (FIUBENCH) with a novel two-stage evaluation pipeline and privacy-aware metrics is a significant contribution. The formalization of VLM unlearning as forgetting private image-text pairs, rather than visual attributes, is a clear conceptual advancement. The paper also highlights the importance of robust evaluation through privacy attacks, which is underexplored in prior work."
          },
          "weaknesses": {
            "value": "The synthetic dataset construction (using SFHQ faces with fictitious backgrounds) lacks validation for realism and generalizability. The experimental evaluation is superficial, with no comparison to strong baselines or ablation studies. The paper does not address how the two-stage pipeline mitigates confounding factors in unlearning performance. The claims about 'significant trade-offs' between utility and forget quality are not quantified or supported by detailed analysis. The robustness of privacy attacks (e.g., membership inference) is not thoroughly validated against existing methods."
          },
          "questions": {
            "value": [
              "How was the synthetic Fictitious Facial Identity VQA dataset validated for realism and relevance to real-world privacy scenarios?",
              "What are the specific limitations of the four baseline methods tested, and how do they compare to alternative unlearning approaches (e.g., data removal, model editing)?",
              "How does the two-stage evaluation pipeline ensure that unlearning targets are not inadvertently retained in the model?",
              "What metrics were used to quantify the trade-offs between model utility and forget quality, and how do they align with existing literature?",
              "How do the proposed privacy attacks (membership inference, adversarial extraction) compare to standard benchmarks for evaluating model privacy?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces FIUBENCH, a novel benchmark for evaluating Vision Language Model (VLM) unlearning under the 'Right to be Forgotten' framework. It formalizes VLM unlearning as forgetting private image-text pairs, constructs a synthetic Fictitious Facial Identity VQA dataset, and proposes a two-stage evaluation pipeline with privacy attacks to assess unlearning effectiveness. Experiments on four baseline methods reveal limitations in balancing model utility and privacy preservation."
          },
          "strengths": {
            "value": "Originality is strong through the creation of a specialized benchmark for VLM unlearning, a domain less explored than LLM unlearning. The two-stage evaluation pipeline and integration of privacy attacks (membership inference, adversarial extraction) provide a novel framework for robust assessment. The paper clearly articulates the unique challenges of unlearning in VLMs, distinguishing between visual attributes and linked private information. The practical motivation for privacy-preserving VLMs is compelling, and the technical details of dataset construction are well-explained."
          },
          "weaknesses": {
            "value": "The synthetic dataset's realism and generalizability to real-world privacy scenarios are unclear. The paper lacks detailed ablation studies to isolate the impact of specific evaluation components (e.g., privacy attacks vs. standard metrics). The choice of baseline methods is not thoroughly justified, and the paper does not address how to handle cases where private data is not explicitly paired in the training set. The evaluation metrics focus on binary 'forget' vs. 'retain' but lack quantitative measures of partial forgetting or trade-offs."
          },
          "questions": {
            "value": "1. How were the synthetic facial identities and private backgrounds generated to ensure realism and diversity? 2. What mechanisms prevent the VQA pairs from inadvertently leaking non-private information? 3. How does the two-stage pipeline ensure that unlearning targets are not over-exposed during training? 4. Are the privacy attacks designed to test for residual knowledge or complete erasure? 5. How does the benchmark handle cases where private data exists in unpaired forms (e.g., images without text)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces FIUBENCH, a novel benchmark for evaluating Vision Language Model (VLM) unlearning under the 'Right to be Forgotten' setting. The benchmark includes a synthetic Fictitious Facial Identity VQA dataset and a two-stage evaluation pipeline with privacy attacks to assess unlearning effectiveness. The authors evaluate four baseline methods, highlighting limitations in their ability to balance model utility and privacy."
          },
          "strengths": {
            "value": "The paper addresses a critical yet underexplored area of VLM unlearning, which is essential for privacy-preserving AI. The formalization of VLM unlearning as forgetting private image-text pairs, rather than visual attributes, is a novel conceptual contribution. The two-stage evaluation pipeline and integration of membership inference and adversarial privacy attacks provide robust metrics. The detailed dataset construction process (e.g., synthetic faces with fictitious backgrounds) demonstrates methodological rigor. The work's significance lies in its potential to guide future research on privacy in VLMs."
          },
          "weaknesses": {
            "value": "The synthetic dataset may lack real-world diversity and generalizability, as it relies on SFHQ synthetic faces and GPT-4o-generated VQA pairs, whose quality and relevance are not thoroughly validated. The evaluation of baseline methods is limited to four approaches, with no comparison to state-of-the-art unlearning techniques or ablation studies on the benchmark's components. The paper does not address how FIUBENCH scales to other privacy scenarios beyond facial identities. Additionally, the two-stage pipeline's implementation details (e.g., how unlearning is applied) are unclear due to the truncated content."
          },
          "questions": {
            "value": [
              "How was the quality of GPT-4o-generated VQA pairs validated? Are there concerns about hallucinations or inconsistencies in the synthetic dataset?",
              "What are the limitations of using synthetic faces compared to real-world facial data in terms of evaluating unlearning effectiveness?",
              "Why were only four baseline methods selected? Could the results be influenced by the choice of baselines?",
              "How does FIUBENCH handle cases where private information is not explicitly paired with facial identities (e.g., implicit associations)?",
              "What are the computational costs and practical feasibility of the two-stage evaluation pipeline for real-world deployment?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "10JOlFIPjt": {
    "paper_id": "10JOlFIPjt",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces NEMO, a multimodal contrastive learning framework for classifying cell types and brain regions from electrophysiological data. By jointly embedding activity autocorrelations and extracellular waveforms, NEMO achieves state-of-the-art performance on opto-tagged and public datasets, demonstrating strong potential for scalable, label-efficient neural data analysis."
          },
          "strengths": {
            "value": "The paper presents a novel multimodal contrastive learning approach (NEMO) that addresses key limitations of prior unimodal methods. Its use of unlabeled data for pre-training and fine-tuning for downstream tasks is well-motivated. The experiments demonstrate competitive results on both cell-type and brain-region classification, with particular strength in label-scarce scenarios. The method's ability to combine physiological modalities (EAPs and spiking activity) shows clear originality. The paper also provides a clear technical framework and contextualizes its contributions within existing work."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with recent state-of-the-art methods beyond PhysMAP and VAEs, such as the CEED framework or other multimodal approaches. The evaluation on brain region classification is limited to a single dataset (IBL), raising concerns about generalizability. The ablation studies and analysis of modality contributions are insufficient. The paper also does not address potential biases in the opto-tagged datasets or the impact of preprocessing choices on results. The theoretical justification for the multimodal contrastive objective is minimal."
          },
          "questions": {
            "value": "1. How do the opto-tagged labels in the NP Ultra and Neuropixels 1 datasets compare to ground-truth molecular/ histological labels? 2. What specific augmentations are applied to EAPs and ACG images during pre-training? 3. Why does NEMO outperform CEED, which also uses raw extracellular recordings? 4. Are the results on the IBL dataset representative of other brain region classification tasks? 5. How does NEMO handle cross-subject generalization, given the dataset-specific nature of electrophysiological data?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces NEMO, a multimodal contrastive learning framework for classifying neuron cell-types and brain regions using electrophysiological data. NEMO jointly embeds extracellular waveforms and spiking activity autocorrelations, achieving state-of-the-art performance on opto-tagged and public datasets. The method leverages a CLIP-like objective to align modalities in a shared latent space, enabling downstream classification tasks."
          },
          "strengths": {
            "value": "Originality: NEMO addresses a critical gap in neuroscience by combining contrastive learning with multimodal electrophysiological data, offering a novel approach to cell-type and brain-region classification. Quality: The method is well-validated through rigorous experiments on multiple datasets, demonstrating robustness in label-limited scenarios. Clarity: The paper provides a clear technical description of the framework and its components, with illustrative figures. Significance: Accurate classification of neurons from electrophysiological recordings has broad implications for understanding neural computation and could accelerate large-scale in-vivo studies."
          },
          "weaknesses": {
            "value": "Limited dataset diversity: The experiments focus on a narrow set of opto-tagged and IBL datasets, raising questions about generalizability to other recording platforms or species. Computational efficiency: The paper does not discuss the computational cost of pre-training or inference, which could limit scalability. Methodological depth: The contrastive learning objective is described as CLIP-like, but the paper lacks ablation studies to validate the necessity of key components (e.g., modalities, augmentation strategies)."
          },
          "questions": {
            "value": [
              "How does NEMO handle variations in data distribution across different brain regions or experimental conditions? Are there any specific challenges in cross-region generalization?",
              "What ablation studies were conducted to evaluate the contribution of each modality (EAPs vs. autocorrelations) or the contrastive learning objective?",
              "The paper mentions 'label-limited regimes' but does not quantify how performance degrades with fewer labels. Could the authors provide more details on this analysis?",
              "Are there any cases where NEMO fails, and what factors (e.g., noise, data quality) contribute to these failures?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces NEMO, a multimodal contrastive learning framework for classifying neuronal cell-types and brain regions using electrophysiological data. The method jointly embeds extracellular waveforms and spiking activity autocorrelations, achieving state-of-the-art performance on opto-tagged and brain-wide mapping datasets."
          },
          "strengths": {
            "value": "Originality: NEMO addresses a novel integration of multimodal electrophysiological features (waveforms and autocorrelations) through contrastive learning, which has not been explored in prior work. Quality: The method is theoretically grounded in contrastive learning principles and validated on multiple datasets. Clarity: The paper is well-structured with a clear schematic diagram and detailed experimental setup. Significance: Solving cell-type/region classification from electrophysiological data has broad implications for neuroscience, enabling scalable, non-invasive analysis of neural circuits."
          },
          "weaknesses": {
            "value": "The paper lacks ablation studies to quantify the contribution of individual components (e.g., autocorrelation vs. waveform features). The comparison with PhysMAP and VAEs focuses on accuracy but omits computational efficiency and scalability analysis. The generalizability of NEMO to other species or recording modalities is not discussed. The fine-tuning process for downstream tasks is described briefly without hyperparameter details."
          },
          "questions": {
            "value": [
              "How does NEMO handle variations in data quality across different brain regions or experimental setups?",
              "What is the computational cost of pre-training and fine-tuning NEMO compared to existing methods?",
              "Are there specific neuron types or datasets where NEMO's performance degrades, and why?",
              "How were the hyperparameters for the contrastive learning objective determined, and what ablation studies were conducted?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "13G5KXm98a": {
    "paper_id": "13G5KXm98a",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a novel visualization method for active learning (AL) called confidence decision boundary visualization, which uses Voronoi tessellation and ridge confidence to capture the dynamic behavior of AL query strategies. The approach aims to address limitations in existing visualizations by emphasizing boundary regions and tracking model evolution across iterations."
          },
          "strengths": {
            "value": "The paper introduces a fresh perspective on AL visualization by combining Voronoi tessellation with confidence metrics, which could provide new insights into query strategy behavior. The method's focus on boundary regions aligns with the core challenge of AL. The paper's structure is logical, and the motivation for addressing cumulative model effects is well-articulated. The experimental setup on standard datasets demonstrates practical relevance."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous theoretical justification for why Voronoi tessellation with ridge confidence improves upon existing methods. The experimental evaluation is incomplete (e.g., no direct comparison to state-of-the-art visualization techniques, no quantitative metrics to validate the proposed metrics). Key methodological details (e.g., how ridge confidence is computed, how dimensionality reduction affects results) are under-specified. The claimed 'enhanced information content' in sparse regions is not empirically validated."
          },
          "questions": {
            "value": [
              "How is ridge confidence mathematically defined and computed? What evidence supports its effectiveness in capturing decision boundary uncertainty?",
              "What specific dimensionality reduction method was used for the 2D Voronoi diagrams? How does this choice impact the visualization's fidelity?",
              "How were the experiments designed to isolate the contribution of the proposed visualization method from other factors (e.g., model architecture, hyperparameters)?",
              "What baseline methods were compared against? The paper mentions 'existing visualizations' but provides no concrete benchmarks.",
              "How does the method handle high-dimensional data beyond the 2D projection? Is there a risk of information loss in the dimensionality reduction step?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a confidence decision boundary visualization method for active learning (AL) using Voronoi tessellation and ridge confidence. The approach aims to enhance understanding of AL query strategies by capturing nuanced variations in sampling behavior, uncertainty dynamics, and the impact of newly sampled points on the model. The method is evaluated on MNIST and CIFAR-10 datasets."
          },
          "strengths": {
            "value": "The paper presents a novel visualization technique combining Voronoi tessellation with confidence metrics, addressing a gap in existing AL visualizations. The method's focus on sparse regions and boundary uncertainty is theoretically sound. The experiments provide insights into query strategy behaviors, such as entropy-based methods and diversity sampling, which could inform future AL research. The paper's clear structure and technical depth contribute to its academic value."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with existing visualization methods, making it difficult to assess the novelty and effectiveness of the proposed approach. The ridge confidence metric is not rigorously defined or quantitatively evaluated. The experiments are limited to two datasets (MNIST and CIFAR-10), with insufficient analysis of how the method generalizes to other domains. The theoretical justification for using Voronoi tessellation in high-dimensional spaces is underdeveloped, and the paper does not address potential limitations of dimensionality reduction steps."
          },
          "questions": {
            "value": [
              "How is the ridge confidence metric formally defined? What specific properties of the decision boundary does it capture?",
              "Are there quantitative metrics (e.g., AUC, F1-score) to evaluate the effectiveness of the visualization in distinguishing high/low uncertainty regions?",
              "How does the method handle the curse of dimensionality when projecting high-dimensional data into 2D for Voronoi tessellation?",
              "What ablation studies were conducted to isolate the impact of Voronoi tessellation versus other components of the visualization pipeline?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a confidence decision boundary visualization method for active learning (AL) using Voronoi tessellation and ridge confidence. The approach aims to capture the cumulative effects of model training and sampling strategies, enabling deeper analysis of query strategies through granular boundary partitioning. The method is evaluated on MNIST and CIFAR-10 datasets, revealing insights into uncertainty patterns and sampling behaviors."
          },
          "strengths": {
            "value": "Originality is demonstrated by applying Voronoi tessellation to AL visualization, addressing limitations of prior methods that fail to capture iterative model accumulation. The quality of methodology is strong, with clear mathematical formulations and integration of ridge confidence for boundary analysis. Clarity is good, though some sections (e.g., experimental setup) could be more explicit. Significance is high, as AL visualization remains a critical but underdeveloped area, and the paper provides actionable insights into query strategy dynamics."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with existing AL visualization techniques, making it hard to assess the novelty's impact. Experiments are limited to two datasets, and the scalability of Voronoi-based methods to high-dimensional or non-Euclidean data is unaddressed. The ridge confidence metric's validation is insufficient, with no ablation studies or baseline comparisons. The claim about avoiding 'undefined blank regions' requires further justification, as Voronoi tessellation may still struggle with sparse data."
          },
          "questions": {
            "value": "1. How does the method handle high-dimensional data when dimensionality reduction is applied? What are the trade-offs in preserving boundary information? 2. What specific metrics were used to evaluate ridge confidence, and how do they correlate with AL performance? 3. Are there ablation studies showing the contribution of Voronoi tessellation versus alternative partitioning methods? 4. How does the approach scale to large datasets or non-image data types (e.g., tabular data)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "15lk4nBXYb": {
    "paper_id": "15lk4nBXYb",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces CCM-DiT, a method for camera-pose controllable video generation using DiT frameworks. It embeds sparse camera-pose information into temporal self-attention layers via a Sparse Motion Encoding Module and LoRA fine-tuning, achieving state-of-the-art performance on long video generation tasks."
          },
          "strengths": {
            "value": "The paper presents a novel approach to integrating camera-pose control into DiT-based video generation, addressing a critical gap in controllability for creative applications. The use of LoRA for efficient fine-tuning and sparse motion encoding demonstrates practical innovation. The method's potential to enhance trajectory consistency and object consistency in long videos is significant. The paper also contextualizes its work within recent DiT advancements, showing awareness of the field's trajectory."
          },
          "weaknesses": {
            "value": "The experimental evaluation lacks depth, with insufficient comparison to baseline DiT methods like OpenSora. The paper does not clarify how the sparse motion encoding module converts Plücker coordinates into sparse motion fields. Key implementation details (e.g., VAE architecture, loss function design) are under-specified. The claim of SOTA performance is not substantiated with quantitative metrics or ablation studies. The paper also omits qualitative results showcasing camera-pose control effectiveness."
          },
          "questions": {
            "value": "1. How exactly does the Sparse Motion Encoding Module transform pixel-wise motion fields into sparse representations? 2. What specific metrics were used to quantify trajectory consistency and object consistency? 3. Why was RealEstate10K chosen over other camera-pose datasets like KITTI or EPIC-KITCHENS? 4. Are there qualitative comparisons showing improved camera-pose alignment vs. OpenSora? 5. How is the VAE trained to encode camera-pose sequences, and what is its architecture? 6. What ablation studies were conducted to verify the contribution of each component (sparse encoding, LoRA, etc.)?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes CCM-DiT, a method for camera-pose controllable video generation using DiT frameworks. It introduces sparse motion encoding and LoRA fine-tuning to embed camera-pose sequences into temporal self-attention layers, demonstrating improved performance on long video generation tasks compared to LDM-based methods."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in controllable video generation by focusing on camera-pose sequences, which is essential for creative applications. The use of OpenSora's ST-DiT framework with LoRA for efficient fine-tuning is practical. The RealEstate10K dataset provides strong baseline support. The method's novelty in integrating sparse motion encoding and temporal attention injection is well-motivated, and the claims of SOTA performance for long videos are significant."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the effectiveness of key components like the sparse motion encoding module. The experimental evaluation is limited, with no comparison to recent DiT-based methods beyond OpenSora. The technical details of the VAE training and motion field conversion are under-specified, and the claims of trajectory consistency improvements are not supported by quantitative metrics. The relationship to prior work (e.g., Tora, TrackGe) is not clearly differentiated."
          },
          "questions": {
            "value": [
              "How does the sparse motion encoding module specifically convert Plücker coordinates into sparse motion fields? What is the exact mechanism for handling camera-pose sequences across multiple frames?",
              "What quantitative metrics were used to evaluate trajectory consistency and object consistency? Are these metrics standard in the field?",
              "How does the method handle camera-pose sequences with complex or non-uniform motion patterns? Are there any failure cases or limitations discussed?",
              "What is the computational overhead of the proposed modules compared to baseline DiT models? How does this affect scalability for longer videos?",
              "How does the paper address potential conflicts between camera-pose guidance and text conditioning in the cross-attention layers?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes CCM-DiT, a method for camera-pose controllable video generation using DiT frameworks. It introduces sparse motion encoding and LoRA fine-tuning to embed camera-pose sequences into temporal self-attention layers, demonstrating improvements in long video generation tasks on the RealEstate10K dataset."
          },
          "strengths": {
            "value": "The paper addresses a specific and important problem in video generation: camera-pose controllability, which is underexplored in DiT-based methods. The use of LoRA for efficient fine-tuning and sparse motion encoding shows practical value. The method builds on existing frameworks (OpenSora) and provides a clear technical pipeline. The paper also contextualizes its work within related domains like motion control and latent diffusion models."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with state-of-the-art DiT-based methods beyond OpenSora. Key claims (e.g., SOTA performance) are not supported by quantitative metrics or ablation studies. The novelty is unclear—how does this differ from Tora or other trajectory-based methods? The mathematical formulations (e.g., motion matrices) are underspecified. The experiments on RealEstate10K are not contextualized against alternative datasets or baselines. The paper also omits analysis of failure cases or limitations."
          },
          "questions": {
            "value": [
              "How does CCM-DiT differ from Tora (Zhang et al., 2024b) in terms of camera-pose integration? What specific innovations does it introduce?",
              "What metrics were used to quantify 'trajectory consistency' and 'object consistency'? Are these standard metrics in the literature?",
              "Why was RealEstate10K selected over other datasets like Something-Something or Epic-Kitchens? How does the dataset's annotation quality impact results?",
              "Are there ablation studies showing the contribution of the sparse motion encoding module vs. LoRA fine-tuning?",
              "How does the method handle out-of-distribution camera-pose sequences not seen during training?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "1762Fbr4HK": {
    "paper_id": "1762Fbr4HK",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces dynamic SINDy, a method combining variational inference with SINDy to identify non-autonomous dynamical systems with time-varying coefficients. It uses a variational autoencoder (VAE) to model ODE coefficients as latent variables, enabling uncertainty quantification and handling of noisy, non-stationary data. The approach is validated on synthetic datasets (e.g., Lorenz system) and real neuronal activity data from C. elegans."
          },
          "strengths": {
            "value": "Originality: The integration of variational inference with SINDy for non-stationary systems addresses an underexplored problem. The use of VAEs to model time-varying coefficients as latent variables is novel. Quality: Experiments on synthetic and real data demonstrate practical applicability. Clarity: The paper is well-structured with clear sections and references to prior work. Significance: The focus on non-autonomous systems and real-world applications (e.g., neuroscience) highlights its relevance to interdisciplinary research."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing methods for non-autonomous systems, such as switching linear dynamical systems or Koopman-based approaches. The experimental validation on real data (C. elegans) is limited, with insufficient analysis of how well the method handles chaos or high noise. The dynamic VAE architecture's specifics (e.g., how temporal dependencies are captured) are not elaborated, and the paper is cut off mid-section, missing critical details. The theoretical justification for the VAE-SINDy combination is weak, with no ablation studies to assess component contributions."
          },
          "questions": {
            "value": [
              "How does dynamic SINDy explicitly model time-varying coefficients? Are they treated as continuous latent variables, and how is their temporal evolution enforced?",
              "What ablation studies were conducted to evaluate the impact of the VAE component versus traditional SINDy? How does the method perform without uncertainty quantification?",
              "The paper mentions C. elegans data but does not provide quantitative metrics (e.g., reconstruction error, coefficient accuracy). How does dynamic SINDy compare to baselines on this dataset?",
              "The paper is cut off mid-section. What additional comparisons or experiments were planned for Section 4.6? How does dynamic SINDy handle sudden vs. gradual non-stationarity?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces dynamic SINDy, a method combining variational inference with SINDy to identify sparse ordinary differential equations (ODEs) for non-stationary dynamical systems. It models time-varying coefficients as latent variables, enabling uncertainty quantification and handling noisy, chaotic data. The approach is validated on synthetic datasets (e.g., Lorenz system, nonlinear oscillators) and real neuronal activity data from C. elegans."
          },
          "strengths": {
            "value": "Originality is strong, as dynamic SINDy addresses non-stationary systems—a gap in prior work. The integration of variational inference with SINDy offers a novel framework for uncertainty-aware system identification. The paper demonstrates practical relevance through synthetic and real-world experiments. Clarity is maintained through structured organization and clear problem formulation. Significance is high, as the method advances interpretable modeling for complex, time-dependent systems."
          },
          "weaknesses": {
            "value": "The experiments focus on limited datasets (e.g., C. elegans neuronal activity), and the paper lacks comparisons with state-of-the-art methods for non-stationary systems. The dynamic VAE component is not thoroughly explained, leaving questions about how time-varying coefficients are parameterized. The theoretical analysis of the method’s robustness to noise or model misspecification is insufficient. Additionally, the paper does not discuss computational scalability or limitations in high-dimensional settings."
          },
          "questions": {
            "value": [
              "How does the dynamic VAE architecture specifically encode time-varying coefficients, and what assumptions are made about their temporal structure?",
              "Are there quantitative comparisons with alternative methods for non-stationary system identification (e.g., dynamic mode decomposition or time-varying SINDy variants)?",
              "What are the computational costs of dynamic SINDy compared to standard SINDy or other generative models?",
              "How does the method handle cases where the latent variables are not separable from the system dynamics (e.g., non-separable $ f(\\mathbf{x}, t) $)?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces dynamic SINDy, a method combining variational inference with SINDy to identify non-autonomous dynamical systems with time-varying coefficients. It models these coefficients as latent variables using a deep generative model (VAE), enabling uncertainty quantification and handling noisy, non-stationary data. The approach is validated on synthetic datasets (e.g., Lorenz system) and C. elegans neuronal activity, though the real-world application is limited in scope."
          },
          "strengths": {
            "value": "Originality: The integration of VAEs with SINDy to model time-varying coefficients in non-autonomous systems addresses a gap in existing literature. Quality: The method is theoretically grounded, with clear connections to SINDy and VAEs. Clarity: The paper is well-structured, with dedicated sections for background, methodology, and experiments. Significance: Non-stationary systems are prevalent in fields like neuroscience and climate science, making this work potentially impactful for interpretable modeling of complex dynamics."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive experimental validation. While synthetic data is used, the comparison with baselines (e.g., switching linear dynamical systems, group sparse regression) is incomplete due to truncated content. The real-world C. elegans application is superficial, with no detailed analysis of model performance or uncertainty quantification. The role of the VAE in capturing time-varying coefficients is not thoroughly explained, and ablation studies are missing. The paper also does not address scalability to high-dimensional data or long time series."
          },
          "questions": {
            "value": "1. How are the time-varying coefficients explicitly modeled as latent variables within the VAE framework? 2. What specific VAE architecture (e.g., timeVAE) is used, and how does it capture temporal dependencies? 3. Are there limitations to the types of non-stationarity (e.g., abrupt vs. gradual changes) that dynamic SINDy can handle? 4. How does the method compare to Koopman-based approaches for non-autonomous systems? 5. What metrics are used to evaluate the accuracy of the recovered ODEs, and how do they account for noise and non-stationarity?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "17idjbdHVW": {
    "paper_id": "17idjbdHVW",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes DVRGTFW, a decentralized stochastic Frank-Wolfe algorithm combining variance reduction, gradient tracking, and multi-consensus to address constrained optimization problems. It claims improved incremental first-order oracle (IFO) complexity bounds for both convex and non-convex settings compared to existing methods."
          },
          "strengths": {
            "value": "The paper introduces a novel algorithmic framework that integrates variance reduction and gradient tracking into decentralized Frank-Wolfe (FW) methods, addressing computational and communication efficiency. The convergence analysis is distinct from prior work, and the theoretical bounds (e.g., $\tilde{O}(n + \\sqrt{\frac{n}{m}} L \\varepsilon^{-1})$) are claimed to be tighter than existing results. The empirical validation supports the theoretical claims, and the paper provides a comprehensive comparison table to contextualize its contributions."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental comparisons with recent decentralized FW methods (e.g., [Nguyen et al., 2024] or [Hou et al., 2022]) beyond the table. The theoretical analysis assumes smoothness and convexity, but the handling of non-convex constraints or non-smooth objectives is unclear. The communication complexity analysis in the non-convex case relies on approximations (e.g., 'closed to the lower bound') without rigorous justification. Additionally, the practical implications of the improved IFO complexity (e.g., scalability to large $n$ and $m$) are not thoroughly discussed."
          },
          "questions": {
            "value": "1. How does the algorithm handle non-convex constraints beyond the assumed convex set $\\mathcal{X}$? 2. Are the theoretical bounds tight under weaker assumptions (e.g., restricted strong convexity)? 3. What are the practical trade-offs between variance reduction and gradient tracking in terms of implementation complexity? 4. How does the algorithm scale with heterogeneous data distributions across agents?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes DVRGTFW, a decentralized stochastic Frank-Wolfe algorithm combining variance reduction, gradient tracking, and multi-consensus to address constrained finite-sum optimization. It claims improved IFO complexity bounds for both convex and non-convex settings compared to existing methods, with theoretical convergence guarantees and empirical validation."
          },
          "strengths": {
            "value": "Originality lies in integrating variance reduction and gradient tracking into decentralized FW, addressing computational and communication efficiency. Theoretical analysis introduces novel convergence guarantees, with complexity bounds that reportedly improve upon prior work. The structured presentation includes a comprehensive table comparing results, and the significance of decentralized optimization for large-scale systems is well-motivated."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation (e.g., specific datasets, hyperparameters, or comparisons to baselines beyond the table). Theoretical claims depend on assumptions (e.g., smoothness, convexity) that may not hold in practical scenarios. The connection between the proposed method and existing decentralized FW frameworks is not thoroughly discussed, and the practical implications of the communication complexity bounds remain unclear."
          },
          "questions": {
            "value": "1. How does DVRGTFW handle non-convex constraints in practice? Are there specific assumptions about the objective functions? 2. What is the exact implementation of the 'multi-consensus' step, and how does it interact with gradient tracking? 3. The paper mentions communication complexity close to a lower bound—what is the quantitative gap? 4. Are the IFO complexity improvements robust to heterogeneous data distributions across agents? 5. How does the algorithm scale with the number of agents (m) and dataset size (n) in real-world settings?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes DVRGTFW, a decentralized stochastic Frank-Wolfe algorithm that combines variance reduction, gradient tracking, and multi-consensus to address constrained finite-sum optimization problems. It claims improved IFO complexity bounds for both convex and non-convex settings, along with near-optimal communication complexity in non-convex cases."
          },
          "strengths": {
            "value": "The paper introduces a novel algorithmic framework by integrating variance reduction, gradient tracking, and multi-consensus into decentralized Frank-Wolfe (FW) methods, addressing computational and communication efficiency. The theoretical analysis presents tighter IFO complexity bounds compared to prior work, particularly in large-scale settings. The paper also provides a comprehensive comparison table summarizing existing methods, demonstrating the novelty of their approach. The problem formulation and convergence guarantees are well-motivated, with clear connections to real-world applications like distributed machine learning."
          },
          "weaknesses": {
            "value": "The empirical validation is limited, with no detailed experiments on specific datasets or comparisons against baselines beyond theoretical bounds. The paper assumes smoothness and convexity conditions that may not hold in practical scenarios. The communication complexity analysis for non-convex cases relies on specific assumptions about the network topology (e.g., spectral gap $1 - \\lambda_2(W)$) that are not thoroughly discussed. Additionally, the integration of 'multi-consensus' is not clearly explained in the algorithm design or analysis."
          },
          "questions": {
            "value": [
              "What specific datasets or benchmarks were used to validate the empirical performance of DVRGTFW? How does it compare to existing methods in terms of convergence speed and scalability?",
              "How sensitive are the theoretical guarantees to the assumptions of smoothness and convexity? Are there extensions for non-smooth or non-convex objectives?",
              "The paper mentions 'multi-consensus' but does not elaborate on its implementation. How does this component interact with variance reduction and gradient tracking?",
              "The communication complexity bound for non-convex cases is claimed to be near-optimal. How does this compare to lower bounds in the literature, and what are the practical implications of the $1/\\sqrt{1 - \\lambda_2(W)}$ dependency?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "1AYrzmDK4V": {
    "paper_id": "1AYrzmDK4V",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces the 'smoothing attack,' a novel method to bypass statistical watermarking in language models by leveraging a weaker reference model to smooth out distributional perturbations caused by watermarks. The attack mixes token distributions from the watermarked model and the weaker reference model, adjusting the mixture based on watermark significance to maintain text quality while evading detection."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by proposing a watermark-agnostic attack that does not require knowledge of the watermark's specific token sets or parameters. The methodology is technically sound, with clear explanations of the smoothing mechanism. The experiments are comprehensive, testing against eight watermark strategies across multiple models, and show significant improvements over prior attacks. The paper also addresses a critical practical limitation in existing watermarking techniques, making it highly relevant to the field."
          },
          "weaknesses": {
            "value": "The experiments are limited to specific models (Llama2-7b, OPT-1.3b) and may not generalize to other architectures or watermarking schemes. The paper lacks detailed analysis of the attack's effectiveness under varying watermark significance levels or context-dependent green token assignments. Additionally, the dependency on the reference model's quality is not thoroughly explored, such as the minimum required strength of the reference model for the attack to succeed."
          },
          "questions": {
            "value": [
              "How generalizable is the smoothing attack to watermarking methods not tested in this paper, such as those using non-uniform or context-dependent green token assignments?",
              "What is the exact mechanism for determining the mixture coefficient based on watermark significance? Are there empirical thresholds or theoretical guarantees?",
              "Can the attack be adapted to scenarios where the watermark's green token set is dynamically updated or highly obfuscated?",
              "How does the attack perform when the reference model's distribution is significantly different from the watermarked model's? Are there cases where the attack fails?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces the 'watermark smoothing attack,' a method to bypass statistical watermarking in large language models (LLMs) by leveraging a weaker reference model to smooth out distributional perturbations caused by watermarks. The attack maintains text quality while making the watermark undetectable, demonstrating effectiveness across multiple watermarking strategies."
          },
          "strengths": {
            "value": "The paper addresses a critical practical concern by proposing a realistic attack scenario that does not rely on stronger models, which is a significant departure from prior work. The watermark-agnostic nature of the attack, combined with its ability to work with weaker models, is a notable strength. The experiments are comprehensive, covering eight watermarking strategies and showcasing the attack's efficacy even with limited resources. The problem formulation and methodology are clearly articulated, and the paper highlights a fundamental limitation of existing watermarking techniques."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed analysis of how the mixture coefficient (which balances the reference and watermarked models) is determined, leaving the method's adaptability to different watermark strengths unclear. Additionally, the experiments focus on specific models (Llama2-7b and OPT-1.3b), and the generalizability to other LLM architectures or more complex watermarking schemes (e.g., context-dependent green token assignments) is not thoroughly discussed. The paper also does not address potential countermeasures or limitations of the attack in real-world deployment scenarios."
          },
          "questions": {
            "value": "1. How is the mixture coefficient calculated in practice, and what criteria determine its adjustment based on watermark significance? 2. Are there specific cases where the attack might fail, such as with highly robust or context-aware watermarking methods? 3. How does the attack perform on LLMs with different architectures or training data compared to Llama2-7b and OPT-1.3b? 4. What are the computational costs of the attack, and how does it scale to longer texts or more complex models?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a 'watermark smoothing attack' that demonstrates the vulnerability of statistical watermarking in language models (LLMs) to minor text modifications. The attack leverages a weaker reference model to smooth out distribution perturbations caused by watermarks, enabling text generation with high quality while bypassing detectors. The authors validate their method against eight watermarking strategies, showing significant improvements over existing attacks."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in LLM security by highlighting a fundamental flaw in statistical watermarking. The proposed attack is practical, as it does not require access to a stronger model, making it more realistic than prior work. The method is watermark-agnostic, applicable to various watermarking schemes without prior knowledge of green token assignments. The experimental results are comprehensive, showing the attack's effectiveness in removing watermarks while maintaining text quality."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of the attack's limitations, such as scenarios where it might fail. The experimental setup could be more transparent, including specific metrics for text quality (e.g., BLEU, ROUGE) and the exact configurations of the watermarked models. The comparison with prior attacks (e.g., Piet et al.) is not fully contextualized, and the paper does not discuss potential countermeasures or defenses against this attack. Additionally, the theoretical justification for the smoothing approach is somewhat superficial."
          },
          "questions": {
            "value": [
              "How does the attack perform on watermarks with context-dependent green token assignments (e.g., KGW) versus context-independent ones (e.g., Unigram)?",
              "What are the specific text quality metrics used to evaluate the generated outputs, and how do they compare to the original unwatermarked model?",
              "Could the attack be mitigated by adversarial training or more sophisticated watermarking schemes, and why weren't these explored?",
              "How sensitive is the attack to the choice of reference model (e.g., TinyLlama vs. OPT-125m), and what are the minimal requirements for the reference model's strength?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "1D3TjFidCS": {
    "paper_id": "1D3TjFidCS",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces Logarithmic Linear Unit (LogLU), a novel activation function designed to address limitations of existing functions like ReLU, Leaky ReLU, and ELU. LogLU combines linear and logarithmic components to mitigate dead neuron and vanishing gradient issues while maintaining computational efficiency. The authors demonstrate its ability to solve the XOR problem with a single neuron and claim improved convergence and performance on benchmark datasets."
          },
          "strengths": {
            "value": "The paper presents a creative activation function with rigorous mathematical analysis of differentiability and non-linearity. The focus on solving fundamental problems like dead neurons and vanishing gradients is significant. The XOR problem demonstration highlights LogLU's efficiency in capturing non-linear patterns. The work addresses a critical aspect of neural network design and offers a fresh perspective on activation function design."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete due to content truncation, lacking specific metrics, ablation studies, and comparisons with baseline activation functions on standard benchmarks. The paper does not discuss computational complexity trade-offs or numerical stability for extreme input values. The claim of superior performance on large-scale datasets requires concrete evidence. The theoretical analysis assumes ideal conditions without addressing practical implementation challenges."
          },
          "questions": {
            "value": "1. Can the authors provide detailed experimental results comparing LogLU with established activation functions on standard datasets? 2. How does LogLU handle numerical stability for very negative inputs? 3. What are the computational costs compared to ReLU and ELU? 4. Have the authors tested LogLU on more complex tasks beyond XOR? 5. How does the activation function perform in very deep networks?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "该论文提出了一种新的激活函数LogLU，结合对数元素以解决ReLU等现有激活函数的局限性，如死神经元问题和梯度消失。作者声称LogLU在XOR问题和基准数据集上表现优于现有方法，但实验部分不完整，部分结果被截断。"
          },
          "strengths": {
            "value": "LogLU的数学定义清晰，可微性分析严谨，且在理论层面解决了梯度消失问题。其设计思路具有创新性，尝试通过非线性对数部分增强模型表达能力。论文对XOR问题的解决进行了初步探索，展示了激活函数在简单逻辑任务中的潜力。"
          },
          "weaknesses": {
            "value": "实验部分严重不足，缺乏完整的结果对比（如与Swish、Mish等先进激活函数的定量比较）。未提供足够的消融实验验证LogLU的改进效果。对计算效率的分析仅停留在理论层面，缺乏实际训练时间或资源消耗的测量。XOR实验的实现细节和结果未明确展示，无法验证其声称的‘单神经元解决能力’。"
          },
          "questions": {
            "value": [
              "LogLU在基准数据集（如CIFAR-10/100）上的具体性能指标（如准确率、收敛速度）是什么？与现有激活函数的对比实验是否经过多次随机种子验证？",
              "论文提到LogLU的计算复杂度低于ELU，但未明确说明其具体优化点。是否考虑了对数运算的计算开销？",
              "XOR实验中使用的网络结构是否与传统多层感知机（MLP）一致？单神经元的实现是否依赖特定初始化策略？",
              "LogLU在负输入区域的梯度（1/(1-x)）在x接近1时可能变得不稳定，作者如何避免数值问题？"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces LogLU, a novel activation function designed to address the dead neuron and vanishing gradient problems in deep neural networks. By combining linear and logarithmic components, LogLU aims to improve convergence and performance. The paper theoretically analyzes its differentiability, non-linearity, and gradient properties, and demonstrates its ability to solve the XOR problem with a single neuron."
          },
          "strengths": {
            "value": "The paper presents a clear theoretical analysis of LogLU's mathematical properties, including differentiability and non-linearity. The motivation for addressing dead neurons and vanishing gradients is well-founded, and the XOR experiment highlights LogLU's potential for capturing non-linear patterns. The paper also compares LogLU to existing activation functions, showing its computational efficiency. The structure of the paper is logical, with a focus on addressing specific limitations of prior work."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete and lacks detailed quantitative results. The paper mentions benchmark datasets (e.g., Caltech 101, Imagenette) but does not provide specific metrics (e.g., accuracy, training time) for comparison. The XOR experiment is described theoretically but lacks empirical results or ablation studies to confirm its claims. The paper also does not discuss potential drawbacks of LogLU, such as computational cost of logarithmic operations or performance on large-scale tasks. The comparison with Swish and Mish is superficial without concrete evidence of superiority."
          },
          "questions": {
            "value": [
              "What are the specific quantitative results of LogLU on benchmark datasets like Caltech 101 and Imagenette? How does it compare to ReLU, Leaky ReLU, ELU, Swish, and Mish in terms of accuracy and training speed?",
              "The paper claims LogLU can solve the XOR problem with a single neuron. Are there empirical results or ablation studies to support this? How does this compare to other activation functions?",
              "How does the computational cost of LogLU (e.g., logarithmic operations) compare to other activation functions? Are there any trade-offs in terms of inference time or memory usage?",
              "What is the performance of LogLU on deeper networks or more complex tasks beyond the XOR problem? Are there experiments on standard benchmarks like CIFAR-10 or MNIST?",
              "The paper mentions mitigating exploding gradients but does not provide analysis of how LogLU handles very deep networks. Are there experiments on architectures with 100+ layers?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "1DIdt2YOPw": {
    "paper_id": "1DIdt2YOPw",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates uncertainty-based abstention in large language models (LLMs) to improve reliability, correctness, and safety. It introduces In-Dialoage Uncertainty (InDU), a verbalized measure of uncertainty using hedge words, and combines it with statistical uncertainty metrics. The study demonstrates that abstaining based on these measures enhances performance across three scenarios: correctness, hallucination reduction, and safety, particularly with RLHF-finetuned models."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem in LLM reliability, which is essential for real-world deployment. The introduction of InDU as a novel verbalized uncertainty measure is innovative, and the study's focus on practical applications (correctness, hallucinations, safety) highlights its significance. The paper's structure is logical, and the potential impact of reducing hallucinations and improving safety is substantial. The mention of RLHF integration also aligns with current trends in LLM training."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with key sections (e.g., 'Uncertainty Estimation') cut off, making it impossible to assess the technical details of InDU's computation or statistical uncertainty measures. The experiments lack critical details: how thresholds for abstention are determined, baselines for comparison, and validation of the 50% hallucination reduction claim. The computational overhead claim is unsubstantiated without evidence. Additionally, the paper does not address how InDU interacts with other uncertainty measures or validate its robustness across different model architectures."
          },
          "questions": {
            "value": [
              "How is In-Dialoage Uncertainty (InDU) quantified? Are specific hedge words or NLP techniques used, and how are they validated?",
              "What criteria or datasets were used to determine thresholds for statistical uncertainty and InDU? Were these thresholds optimized on a validation set?",
              "How do the authors compare their approach to existing uncertainty-based abstention methods (e.g., entropy-based methods or explicit prompting)?",
              "What defines 'unsafe' responses in the safety experiments? Are standard benchmarks or datasets used?",
              "How is the claim of 'almost no additional computational overhead' supported? What metrics were used to measure this?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper explores the use of uncertainty-based abstention in large language models (LLMs) to improve reliability, reduce hallucinations, and enhance safety. The authors introduce two uncertainty measures—statistical uncertainty (based on probability distributions) and In-Dialoage Uncertainty (InDU, capturing verbalized uncertainty like 'I don't know')—and demonstrate that abstaining based on these measures significantly improves performance across three key scenarios: correctness, hallucination reduction, and safety. They also investigate the impact of RLHF fine-tuning on these uncertainty metrics."
          },
          "strengths": {
            "value": "The paper's originality lies in applying uncertainty-based abstention to LLMs, particularly in question-answering tasks, which is a novel direction compared to traditional classification settings. The work is methodologically sound, with clear experimental design across multiple real-world scenarios (correctness, hallucinations, safety). The clarity of the problem statement, methodology, and results is strong, and the significance of improving LLM reliability and safety is highly relevant to practical deployment. The integration of both statistical and verbalized uncertainty measures adds depth to the analysis."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing uncertainty estimation methods in LLMs, such as explicit prompting for uncertainty (e.g., Tian et al. 2023) or semantic likelihood approaches (Kuhn et al. 2023). The In-Dialoage Uncertainty (InDU) metric is not rigorously defined or quantified, leaving ambiguity about how it is computed from model outputs. The experiments do not include ablation studies to isolate the contribution of each uncertainty measure or RLHF. Additionally, the paper does not address potential limitations, such as the generalizability of results to other domains or languages, or the trade-offs between abstention rates and model utility."
          },
          "questions": {
            "value": "1. How is In-Dialoage Uncertainty (InDU) quantified? Are there specific linguistic features or heuristics used to detect hedging phrases like 'I don't know'? 2. What baseline methods were compared against for the claimed improvements in correctness and safety? 3. How does RLHF fine-tuning interact with the statistical uncertainty measures—does it alter the distribution of uncertainty scores, and if so, how is this accounted for in the analysis? 4. Are the results robust to variations in dataset composition or model architecture? 5. What is the computational overhead of implementing the uncertainty-based abstention framework, and how does it scale to large models?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates uncertainty-based abstention in large language models (LLMs) to improve reliability, reduce hallucinations, and enhance safety. The authors propose two uncertainty measures—statistical uncertainty (entropy-based) and In-Dialoage Uncertainty (InDU), which captures verbalized uncertainty like 'I don't know.' They demonstrate that abstaining based on these measures, particularly with RLHF-finetuned models, improves correctness, reduces hallucinations by 50%, and increases safety by 70-99%."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in LLM deployment: reliability. It introduces a novel framework combining statistical and verbalized uncertainty measures, offering practical solutions for real-world applications. The experiments are well-structured, showing significant improvements in safety and hallucination reduction. The focus on RLHF-finetuned models aligns with current trends, and the emphasis on natural interaction without explicit uncertainty prompting is a key advantage. The work is timely and relevant to both academia and industry."
          },
          "weaknesses": {
            "value": "The paper is truncated, limiting the ability to fully assess methodology and results. Key details about InDU measurement (e.g., specific metrics, detection mechanisms) are missing. The computational overhead claims lack justification, and the experiments do not compare against existing methods for hallucination or safety mitigation. The generalizability of results across models and datasets is unclear, and the statistical significance of improvements (e.g., 8% correctness boost) is not rigorously validated."
          },
          "questions": {
            "value": [
              "How is In-Dialoage Uncertainty (InDU) quantified? Are specific keywords, linguistic patterns, or models used to detect verbalized uncertainty?",
              "What datasets and LLMs were used for experiments? Are the results model- or dataset-specific?",
              "How do the reported improvements compare to existing approaches (e.g., prompt engineering, post-processing)?",
              "What is the exact computational overhead of the proposed method, and how is it measured?",
              "Are the statistical significance and robustness of results (e.g., 70-99% safety improvement) validated across multiple trials?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "1FY1apsMxc": {
    "paper_id": "1FY1apsMxc",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes PromptGFM, a Graph Foundation Model (GFM) that integrates Graph Neural Networks (GNNs) and Large Language Models (LLMs) through graph vocabulary learning. The approach includes a Graph Understanding Module that mimics GNN workflows in the language space and a Graph Inference Module that establishes a language-based graph vocabulary to enable transferability and scalability. The method aims to address modality incompatibility between graph embeddings and LLMs by replacing out-of-vocabulary (OOV) tokens with compatible representations."
          },
          "strengths": {
            "value": "The paper presents a novel approach to GNN-LLM integration by emphasizing graph vocabulary learning, which addresses a critical gap in cross-modal alignment. The two-module architecture (Graph Understanding and Inference) is well-structured and clearly explained, demonstrating a creative combination of GNN principles and LLM capabilities. The experimental results on node classification and link prediction show strong performance and transferability, suggesting significant practical value. The paper also highlights the importance of expressiveness, transferability, and scalability in graph vocabularies, which are crucial for foundation models."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing methods that also aim to bridge GNNs and LLMs, such as structure verbalizers or hybrid architectures. The claims about OOV token incompatibility are not thoroughly validated with ablation studies or quantitative analysis. The scalability of the graph vocabulary to unseen nodes is mentioned but not empirically demonstrated. Additionally, the paper is cut off mid-section, leaving critical details about the methodology and experiments incomplete, which limits the ability to fully assess the work."
          },
          "questions": {
            "value": "1. How does the proposed graph vocabulary explicitly address the limitations of OOV tokens compared to prior approaches? 2. What specific prompts or mechanisms are used in the Graph Understanding Module to replicate GNN message-passing? 3. Are there ablation studies to validate the contributions of each module? 4. How does PromptGFM handle dynamic or evolving graphs with varying structures? 5. What are the exact metrics used to evaluate transferability across datasets, and how do they compare to state-of-the-art baselines?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes PromptGFM, a Graph Foundation Model (GFM) that integrates Large Language Models (LLMs) and Graph Neural Networks (GNNs) through graph vocabulary learning. The framework includes a Graph Understanding Module, which uses prompts to replicate GNN workflows in the language space, and a Graph Inference Module that establishes a language-based graph vocabulary to resolve modality incompatibility. The method aims to enable seamless GNN-LLM integration and transferable graph-text alignment."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to bridge GNNs and LLMs by explicitly aligning their workflows in the language space, which addresses a critical limitation in existing methods. The focus on graph vocabulary learning for transferability and scalability is promising. The experimental claims of superiority in node classification and link prediction, along with zero-shot transferability, suggest potential significance. The work also provides a clear problem formulation and motivation for GFM development."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as comparisons with state-of-the-art GFM methods or ablation studies to isolate the contributions of each module. The claims about 'universal transferability' and 'scalability' are not sufficiently supported by quantitative evidence. The description of the Graph Understanding Module's prompt design is vague, making it difficult to assess the novelty or feasibility of replicating GNN processes in LLMs. Additionally, the paper does not address potential limitations, such as computational costs or dependency on specific LLM architectures."
          },
          "questions": {
            "value": [
              "How does the Graph Understanding Module explicitly replicate GNN workflows (e.g., neighbor sampling, aggregation, update) in the language space? What specific prompts or architectures are used?",
              "What metrics are used to evaluate the 'expressiveness' and 'transferability' of the proposed graph vocabulary? Are there quantitative results to support these claims?",
              "How does PromptGFM compare to existing GFM approaches (e.g., those in Mao et al., 2024; Xia et al., 2024) in terms of performance and efficiency?",
              "What is the role of multi-instruction fine-tuning in achieving cross-graph and cross-task generalization? Are there ablation studies to validate this component?",
              "How are OOV tokens replaced with compatible representations? Are there examples of these representations or a detailed explanation of the mapping process?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces PromptGFM, a Graph Foundation Model (GFM) that bridges Graph Neural Networks (GNNs) and Large Language Models (LLMs) through graph vocabulary learning. The framework comprises two modules: a Graph Understanding Module that mimics GNN workflows in the language space and a Graph Inference Module that establishes a language-based graph vocabulary to resolve modality incompatibility. The approach aims to enable seamless GNN-LLM integration and transferable graph-text alignment."
          },
          "strengths": {
            "value": "The paper presents a novel approach to GNN-LLM integration by framing LLMs as GNNs through graph vocabulary learning, addressing a critical gap in modality compatibility. The two-module architecture is well-structured, with clear motivations for both components. The emphasis on expressiveness, transferability, and scalability of the graph vocabulary aligns with foundational goals in graph learning. The experimental claims of strong performance and zero-shot transferability suggest significant practical relevance."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of how the graph vocabulary is constructed or how the 'fine-grained replication of GNN flows' is implemented. Experimental results are summarized without specific metrics, baselines, or ablation studies to validate the components. The claim about OOV token incompatibility is not empirically supported, and the paper does not address potential limitations of the language-based vocabulary (e.g., tokenization biases or scalability to very large graphs). The connection to prior work on GNN-LLM integration is superficial, missing critical comparisons to existing methods."
          },
          "questions": {
            "value": [
              "How is the language-based graph vocabulary explicitly defined? What criteria determine the mapping of nodes to tokens?",
              "What specific baselines were used to demonstrate superiority in node classification and link prediction? How do these compare to state-of-the-art GNN-LLM methods?",
              "Are there ablation studies to isolate the contributions of the Graph Understanding Module versus the Graph Inference Module?",
              "How does PromptGFM handle dynamic or evolving graphs, where node identities may change over time?",
              "What is the computational cost of the multi-prompt instruction fine-tuning process compared to traditional GNNs or LLMs?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "1Ffzgglq2I": {
    "paper_id": "1Ffzgglq2I",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes a framework called Binary Reward Labeling (BRL) to bridge the gap between offline preference-based reinforcement learning (PBRL) and reward-based RL. The key idea is to convert preference feedback into scalar rewards via binary labeling, enabling the use of standard offline RL algorithms on preference datasets. The authors theoretically connect their framework to existing PBRL methods and empirically show competitive performance compared to baselines on D4RL benchmarks."
          },
          "strengths": {
            "value": "Originality: The framework addresses a critical gap between PBRL and reward-based RL, offering a generalizable approach. Quality: Experiments on standard benchmarks (D4RL) and theoretical connections to existing methods demonstrate rigor. Clarity: The paper is well-structured, with clear explanations of the BRL methodology. Significance: The ability to repurpose established offline RL algorithms for PBRL could have broad practical implications."
          },
          "weaknesses": {
            "value": "Theoretical analysis is superficial, with limited depth on how BRL interacts with specific offline RL algorithms. Empirical evaluation lacks comprehensive comparison with state-of-the-art PBRL methods (e.g., RLHF, preference-based Q-learning). The binary labeling's optimality is claimed under restrictive conditions (no trajectory overlap), but its generalizability is untested. The paper does not address potential biases in preference data or scalability to high-dimensional environments."
          },
          "questions": {
            "value": "1. How does BRL handle trajectory overlaps, which are common in real-world datasets? 2. What is the exact mechanism for mapping preferences to binary rewards, and how is this validated? 3. Are there ablation studies showing the impact of binary labeling vs. alternative reward labeling strategies? 4. How does the framework scale to complex environments with high-dimensional state spaces?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a framework called Binary Reward Labeling (BRL) to bridge the gap between offline preference-based reinforcement learning (PBRL) and standard reward-based RL. The core idea is to convert preference feedback into scalar rewards via binary labeling, enabling the use of existing reward-based offline RL algorithms. The approach minimizes information loss during the transformation and demonstrates empirical effectiveness on D4RL benchmarks."
          },
          "strengths": {
            "value": "The paper's originality lies in its novel framework for converting preference signals to scalar rewards, which is both general and practical. The theoretical analysis connects BRL to existing PBRL techniques, highlighting its potential to improve efficiency. The empirical results show strong performance compared to recent PBRL baselines, with results comparable to training on true rewards in many cases. The clarity of the problem formulation and the structured presentation of experiments and analysis are notable strengths."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed discussion of BRL's limitations, such as its performance under noisy or sparse preference data. The theoretical analysis is somewhat superficial, with limited exploration of how BRL interacts with specific offline RL algorithms. Additionally, the experiments focus on D4RL benchmarks, but the generalization to other domains or more complex tasks remains unexplored. The comparison to reward modeling is brief, and the paper does not address potential pitfalls of binary labeling in overlapping trajectory scenarios."
          },
          "questions": {
            "value": "1. How does BRL handle noisy or inconsistent preference signals in real-world scenarios? 2. What is the computational overhead of the binary labeling process compared to reward modeling? 3. Are there specific cases where BRL's binary labeling could lead to suboptimal policies, and how can these be mitigated? 4. How does the framework scale to high-dimensional or continuous action spaces? 5. Could the paper elaborate on the theoretical guarantees of BRL when combined with different offline RL algorithms?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a framework called Binary Reward Labeling (BRL) to bridge the gap between preference-based reinforcement learning (PBRL) and standard reward-based offline RL. The key idea is to transform preference feedback into scalar rewards via binary labeling, enabling the use of existing reward-based offline RL algorithms on preference datasets. The approach minimizes information loss during the transformation and demonstrates empirical effectiveness on D4RL benchmarks."
          },
          "strengths": {
            "value": "The paper presents a novel and practical framework (BRL) that addresses a critical gap in offline PBRL by enabling the use of well-established reward-based RL algorithms. The theoretical analysis connects BRL to existing PBRL techniques, and the empirical results show competitive performance compared to state-of-the-art PBRL methods. The clarity of the problem statement, the simplicity of the binary labeling approach, and the practicality of the framework (no modifications to existing algorithms) are significant strengths. The work also highlights the importance of minimizing information loss during feedback transformation, which is a fresh perspective."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with specific PBRL baselines, making it difficult to assess the magnitude of the improvement. The theoretical analysis is brief and does not fully explore the guarantees of the framework. The binary labeling method's effectiveness in cases with overlapping trajectories is not thoroughly explained. Additionally, the empirical validation could include ablation studies, analysis of noisy preference data, or exploration of alternative reward labeling strategies. The paper also does not address potential limitations of the framework, such as scalability to large-scale environments."
          },
          "questions": {
            "value": "1. How is the binary reward labeling implemented in practice? What assumptions are made about the structure of the preference data? 2. How does the framework handle cases where preferences are noisy or sparse? 3. What are the theoretical guarantees of the BRL approach, especially in scenarios with overlapping trajectories? 4. Are there specific PBRL baselines that the framework significantly outperforms, and how does this compare to prior work? 5. How does the binary labeling ensure minimal information loss compared to other reward labeling techniques?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "1H90Gb9rJ9": {
    "paper_id": "1H90Gb9rJ9",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper presents a deterministic algorithm for losslessly optimizing neural networks (NNs) that represent Boolean networks (BNs), focusing on reducing neurons and connections while preserving functional equivalence. The authors propose three methods: a lossless two-layer NN optimization technique, an objective-aware algorithm leveraging NPN classification for shared representations, and an architecture-aware optimization approach. They report significant reductions in network size and speedups in optimization time compared to state-of-the-art methods."
          },
          "strengths": {
            "value": "The paper addresses a practical and relevant problem in NN-based technology mapping, with clear motivation for lossless optimization in safety-critical applications. The experimental results demonstrate substantial improvements in reducing neurons (60%) and connections (70%), which are significant for industrial use cases. The proposed methods, particularly the NPN-based shared representation exploitation, show novelty in leveraging structural properties of subproblems. The paper is well-structured, with a clear problem statement and contributions."
          },
          "weaknesses": {
            "value": "The paper lacks detailed theoretical analysis of the proposed algorithms, such as proofs of functional equivalence preservation or complexity guarantees. The experimental validation is limited to comparisons with a single prior work (Gavier et al., 2023), without benchmarking against other relevant methods. The description of the NPN classification's integration into the optimization process is vague, and the paper does not discuss limitations or edge cases where the approach might fail. Additionally, the paper does not address how the methods scale to larger BNs or real-world industrial examples."
          },
          "questions": {
            "value": "1. How does the deterministic algorithm ensure functional equivalence during optimization? Are there theoretical guarantees for this? 2. What specific constraints or assumptions must the Boolean network satisfy for the proposed methods to work? 3. How is NPN classification applied to identify shared representations, and what metrics are used to determine 'same class' sub-NNs? 4. Are there any cases where the objective-aware algorithm might not provide speedups, and how are these handled? 5. How do the authors validate the practicality of their methods for large-scale industrial BNs?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces methods to optimize neural network (NN) representations of Boolean networks (BNs) while preserving functional equivalence. The authors propose a lossless optimization technique for two-layer NNs, an objective-aware algorithm leveraging NPN classification to accelerate optimization, and an architecture-aware approach for further compression. Experimental results show significant reductions in neurons and connections compared to state-of-the-art methods, along with speedups in optimization time."
          },
          "strengths": {
            "value": "The paper addresses a novel and practical problem: lossless optimization of NNs for Boolean functions, which is critical for applications like circuit simulation. The contributions include a deterministic algorithm for size optimization, an objective-aware method exploiting shared representations, and experimental validation of substantial improvements. The work is theoretically grounded in Boolean function representations and leverages NPN classification, a creative combination of concepts. The significance is clear, with potential impact on neurosymbolic systems and hardware efficiency."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with prior work, such as specific differences between the proposed method and Gavier et al. (2023). The experimental section is incomplete, making it difficult to assess the generality of results (e.g., no mention of varying BN sizes or complexity). The theoretical guarantees for lossless optimization are not fully explained, and the scalability of the objective-aware algorithm to large-scale BNs is unclear. Additionally, the paper does not address potential limitations, such as cases where functional equivalence might be compromised."
          },
          "questions": {
            "value": [
              "How does the deterministic algorithm ensure lossless optimization? What are the theoretical guarantees?",
              "What specific benchmarks or Boolean functions were used in experiments? Are the results generalizable across different BN structures?",
              "How does the NPN classification-based objective-aware algorithm handle sub-NNs with no shared representations?",
              "Are there any constraints on the types of Boolean functions or network architectures where the proposed methods are effective?",
              "What is the computational complexity of the proposed algorithms, and how do they scale to larger BNs?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the problem of optimizing neural network (NN) representations of Boolean networks (BNs) while preserving functional equivalence. The authors propose a deterministic algorithm for lossless optimization of NN size (neurons and connections) and introduce an objective-aware algorithm that accelerates optimization by leveraging shared representations among subproblems. Experimental results demonstrate significant reductions in network size and optimization time compared to state-of-the-art methods."
          },
          "strengths": {
            "value": "The paper makes a clear contribution to a niche but important problem at the intersection of neural networks and Boolean function synthesis. The methodological innovations—particularly the NPN-based objective-aware algorithm—show strong potential for practical applications in circuit simulation and neurosymbolic systems. The experimental results are promising, with substantial reductions in network size and optimization time. The paper is well-structured, with clear motivation, problem formulation, and comparative analysis of existing approaches."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient technical depth in the methodology section, particularly in explaining how the deterministic optimization algorithm ensures functional equivalence. The experimental evaluation is limited in scope, with no details about the benchmark datasets, baselines, or statistical significance of results. The claims about 'up to 70% reduction' are not contextualized with respect to problem complexity or network architecture. The theoretical analysis of NP-hardness and the limitations of prior methods are superficial, leaving gaps in the justification for the proposed approach."
          },
          "questions": {
            "value": "1. How is functional equivalence formally defined and verified during optimization? 2. What specific constraints or properties of Heaviside threshold networks make existing lossless compression techniques inapplicable? 3. Are the claimed speedups dependent on particular hardware/software configurations? 4. How does the objective-aware algorithm handle non-identical subproblems with similar representations? 5. What is the computational complexity of the proposed deterministic optimization algorithm?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "1Iq1qIsc2s": {
    "paper_id": "1Iq1qIsc2s",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates the use of Rotary Position Embeddings (RoPE) as a replacement for Relative Positional Bias (RPB) in vision transformers, particularly in the context of fused attention implementations. The authors analyze the challenges of integrating RPB with fused attention, demonstrate RoPE's superior accuracy and speed, and present an efficient CUDA implementation for RoPE."
          },
          "strengths": {
            "value": "The paper's originality lies in applying RoPE, a technique popular in NLP, to vision transformers and addressing practical challenges with fused attention. The experiments are comprehensive, covering multiple model architectures (ViT, Swin, NAT) and attention patterns. The implementation of a fast RoPE CUDA kernel with Python wrapper adds practical value. The clarity of the problem statement, figures, and comparisons between RPB, APE, and RoPE is strong."
          },
          "weaknesses": {
            "value": "The paper lacks ablation studies on the hyperparameter $k_{rope}$, which determines the fraction of hidden dimensions used for RoPE. The analysis of RoPE's performance on tasks beyond image classification (e.g., object detection) is missing. The theoretical discussion on fused attention's limitations is cursory, and the empirical evidence for speedups requires more detailed quantitative comparisons."
          },
          "questions": {
            "value": "How was the hyperparameter $k_{rope}$ selected, and what are the trade-offs between reduced dimensions and accuracy? Are there specific vision tasks where RoPE's performance degrades compared to RPB? What are the exact speedup numbers when using RoPE with fused attention? How does RoPE generalize to non-image modalities or architectures with different attention patterns?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates the use of Rotary Position Embeddings (RoPE) as a replacement for Relative Positional Bias (RPB) in vision transformers, particularly in the context of fused attention implementations. The authors highlight challenges with RPB in fused attention, demonstrate empirical improvements with RoPE across multiple vision models, and present an efficient CUDA implementation of RoPE."
          },
          "strengths": {
            "value": "Originality: The paper addresses a niche but important problem of integrating positional biases with fused attention, which is underexplored in vision transformers. The focus on RoPE as a viable alternative to RPB in vision models is novel, especially given RoPE's dominance in NLP. Quality: The experiments cover multiple model architectures (ViT, Swin, NAT) and attention patterns, with claims of consistent accuracy improvements. The proposed CUDA implementation for RoPE adds practical value. Clarity: The paper is well-structured, with clear figures (e.g., Figure 1) explaining the challenges of fused attention with RPB. Significance: If validated, the findings could influence the adoption of fused attention in vision models, improving efficiency without sacrificing accuracy."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with other position embedding methods (e.g., APE or learned positional encodings). The empirical results are not tied to standard vision benchmarks (e.g., ImageNet), making it hard to assess real-world relevance. The analysis of the hyperparameter $k_{rope}$ is superficial, with no ablation studies or theoretical justification. The speedup claims are not supported by detailed metrics (e.g., FLOPs, memory usage, or direct comparisons with RPB in the same setup). The paper also does not address potential limitations of RoPE, such as performance on long sequences or non-image modalities."
          },
          "questions": {
            "value": "How does RoPE handle varying input resolutions compared to RPB? The paper mentions interpolation but provides no experiments. What are the trade-offs between RoPE and RPB in terms of memory usage or gradient computation? The paper claims improved speed but does not quantify this against RPB in the same framework. Are there scenarios where RPB might still outperform RoPE, and how does the paper address this? The paper's focus on fused attention implementations (e.g., Flash Attention) does not discuss compatibility with other attention variants (e.g., sparse attention)."
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates the use of Rotary Position Embeddings (RoPE) as a replacement for Relative Positional Bias (RPB) in vision transformers, particularly in the context of fused attention implementations. The authors demonstrate that RoPE avoids the backward pass challenges of RPB, achieves competitive accuracy with reduced computational overhead, and provides a fast CUDA implementation for RoPE."
          },
          "strengths": {
            "value": "The paper introduces a novel application of RoPE in vision transformers, addressing a practical limitation of fused attention. The empirical validation across multiple model architectures (ViT, Swin, NAT) and attention patterns (self, windowed, neighborhood) showcases broad applicability. The efficient CUDA implementation of RoPE and analysis of partial hidden dimension usage for performance trade-offs are technically valuable. The paper also clarifies the theoretical advantages of RoPE over RPB in terms of compatibility with fused kernels."
          },
          "weaknesses": {
            "value": "The experimental scope is limited to three model families, and the analysis of RoPE's performance across different vision tasks (e.g., object detection, segmentation) is missing. The ablation studies on the hyperparameter $k_{rope}$ are insufficient, and the paper does not compare RoPE with other position embedding methods like APE or learned positional encodings. The claims about RoPE's superiority over RPB could benefit from a more detailed theoretical analysis of how RoPE's static nature impacts model generalization."
          },
          "questions": {
            "value": "1. How does RoPE perform on downstream tasks beyond image classification (e.g., object detection, segmentation)? 2. What are the specific trade-offs between using full vs. partial hidden dimensions for RoPE in terms of accuracy and speed? 3. How does RoPE compare to learned positional encodings in terms of adaptability to task-specific spatial patterns? 4. Are there scenarios where RPB might still be preferable despite its fusion challenges?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "1Iu2Yte5N6": {
    "paper_id": "1Iu2Yte5N6",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates the order sensitivity of in-context learning (ICL) in decoder-only LLMs by analyzing prompt embeddings. It identifies a 'clustering property' where prompts with the same first and last demonstrations form clusters in the embedding space, attributed to positional encoding and causal attention masks. The authors propose Cluster-based Search, a method that reduces the time complexity of demonstration selection from factorial to quadratic while maintaining performance."
          },
          "strengths": {
            "value": "The paper introduces a novel clustering property in prompt embeddings, linking ICL order sensitivity to LLM mechanisms like positional encoding and attention masks. Theoretical analysis and empirical validations (UMAP, K-Means, derivative norms) are thorough. The proposed Cluster-based Search offers significant efficiency gains (92-100% time reduction) with minimal performance loss, addressing a critical bottleneck in self-adaptive ICL. The work bridges gaps between ICL dynamics and LLM architecture, offering actionable insights for optimization."
          },
          "weaknesses": {
            "value": "The experimental validation of the clustering property lacks depth, particularly in analyzing how clustering varies across different LLM architectures or tasks. The theoretical analysis assumes ideal conditions, but practical implications of causal attention masks and positional encoding require more concrete evidence. The paper does not compare Cluster-based Search directly with existing order-robust ICL methods, leaving the relative advantages unclear. Additionally, the ablation studies on hyperparameters (e.g., cluster granularity) are underdeveloped."
          },
          "questions": {
            "value": [
              "How does the clustering property generalize across diverse LLM architectures (e.g., GPT vs. Llama) and tasks (classification vs. reasoning)?",
              "What is the exact mechanism by which positional encoding and causal attention masks influence the clustering phenomenon? Can this be validated with ablation studies?",
              "Are there scenarios where the clustering property breaks down, and how does Cluster-based Search handle such cases?",
              "How does the performance of Cluster-based Search scale with larger demonstration sets or more complex tasks?",
              "Why was the experimental section truncated, and are there additional results that could strengthen the claims?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates the order sensitivity of in-context learning (ICL) in decoder-only LLMs by analyzing the prompt embedding space. The authors identify a 'clustering property' where prompts with the same first and last demonstrations form clusters, attributed to causal attention masks and positional encoding. They propose Cluster-based Search, a method to accelerate demonstration selection and ordering, reducing time complexity from factorial to quadratic while maintaining performance."
          },
          "strengths": {
            "value": "The paper's originality lies in linking ICL order sensitivity to the structural properties of LLMs (causal attention, positional encoding) through the clustering property. Theoretical analysis and empirical validations (UMAP, K-Means, gradient-based token importance) are comprehensive. The practical contribution of Cluster-based Search addresses a critical efficiency bottleneck in ICL. The clarity of the problem statement, methodology, and results is strong, with clear connections to prior work."
          },
          "weaknesses": {
            "value": "The theoretical analysis of causal attention's role is limited to ideal conditions, with insufficient empirical validation on real-world LLMs. The experiments focus on specific datasets (e.g., SST-2, AddSub) and models (e.g., GPT-2, Llama), raising questions about generalizability. The claim that clustering is driven by 'first-demonstration clustering' lacks deeper analysis of why first demonstrations dominate. The comparison to exhaustive search is limited to time metrics, with unclear trade-offs in accuracy across tasks."
          },
          "questions": {
            "value": [
              "How do the authors ensure that the clustering property is not an artifact of the embedding space's dimensionality or specific dataset characteristics?",
              "What ablation studies were conducted to isolate the contributions of causal attention masks vs. positional encoding to the clustering phenomenon?",
              "The paper mentions symbolic tasks (e.g., symbolic sentiment classification) but does not elaborate on how clustering behaves in these non-semantic settings.",
              "How does Cluster-based Search perform on large-scale LLMs (e.g., GPT-3) compared to the open-source models tested? Are there domain-specific limitations?",
              "The theoretical proof of causal attention's role assumes ideal conditions. How do these assumptions align with the behavior of real-world LLMs?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates the order sensitivity of in-context learning (ICL) in decoder-only LLMs by analyzing prompt embedding spaces. The authors identify a 'clustering property' where prompts with the same first and last demonstrations form clusters in embedding space, driven by causal attention masks and positional encoding. They propose Cluster-based Search, a method that accelerates demonstration selection/ordering by leveraging this property, reducing time complexity from factorial to quadratic while maintaining performance."
          },
          "strengths": {
            "value": "The paper addresses a critical yet underexplored issue in ICLR: order sensitivity of ICL. The clustering property is a novel and theoretically grounded insight, linking ICL behavior to LLM architecture (causal attention, positional encoding). The work combines rigorous empirical validation (UMAP, K-Means, derivative analysis) with practical contributions (Cluster-based Search). The method's significant speedup (92-100% reduction) while preserving accuracy demonstrates strong practical relevance. Experiments span diverse tasks and LLMs, enhancing generalizability."
          },
          "weaknesses": {
            "value": "The theoretical analysis relies on idealized assumptions about attention mechanisms, which may not fully capture real-world LLM dynamics. The empirical validation of clustering mechanisms (e.g., causal attention vs. positional encoding) lacks ablation studies across different model architectures. The Cluster-based Search implementation details are sparse, making reproducibility challenging. The paper does not address how clustering property interacts with task complexity or model scale. The claim about 'self-adaptive ICL settings' is underdeveloped without specific use case examples."
          },
          "questions": {
            "value": "1. How was the causal attention mask implemented in the experiments? Were there variations in attention patterns across different LLMs? 2. What is the exact mechanism linking positional encoding to last-demonstration clustering? 3. How does Cluster-based Search handle cases where the clustering property breaks down (e.g., complex tasks)? 4. Were there any tasks where the proposed method underperformed compared to exhaustive search? 5. How sensitive is the clustering property to prompt length or demonstration diversity?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "1Njl73JKjB": {
    "paper_id": "1Njl73JKjB",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a principled framework to evaluate sparse autoencoders (SAEs) by comparing them to supervised feature dictionaries derived from task-relevant concepts. It applies this to the indirect object identification (IOI) task using GPT-2 Small, demonstrating that SAEs can capture interpretable features and that certain variants match supervised methods in disentanglement and control. The work also highlights qualitative phenomena in SAE training, such as feature splitting and magnitude preferences."
          },
          "strengths": {
            "value": "The paper's main strength lies in its novel, principled evaluation framework that addresses a critical gap in SAE research by grounding assessments in task-specific supervised dictionaries. The IOI task case study is well-executed and relevant, providing concrete evidence of SAEs' utility. The methodological clarity and structured approach to disentanglement, control, and causal faithfulness are commendable. The significance of improving interpretability and controllability in LLMs is high, and the work opens avenues for more rigorous SAE evaluations."
          },
          "weaknesses": {
            "value": "The paper lacks detailed implementation specifics for constructing supervised feature dictionaries, such as the exact process for computing $v_{\text{IO}=john}$. The experiments are narrowly focused on the IOI task, with limited generalizability discussed. The computational cost of creating supervised dictionaries is not addressed, which could hinder practical adoption. The qualitative phenomena (e.g., feature splitting) are mentioned but not thoroughly analyzed. Additionally, the comparison to supervised features assumes the latter are optimal, which may not always hold."
          },
          "questions": {
            "value": [
              "How are the supervised feature dictionaries constructed in practice? Are there specific steps or assumptions in computing $v_{\text{IO}=john}$?",
              "What are the limitations of applying this framework to tasks with ambiguous or poorly defined attributes?",
              "How does the computational overhead of creating supervised dictionaries scale with more complex tasks or larger models?",
              "Are the supervised dictionaries truly representative of the model's internal computations, or could they introduce biases?",
              "What ablation studies were performed to validate the necessity of each component in the evaluation framework?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces a principled framework for evaluating sparse autoencoders (SAEs) by comparing them to supervised feature dictionaries derived from task-relevant concepts. The approach focuses on disentanglement, controllability, and causal faithfulness for tasks like indirect object identification (IOI), demonstrating that SAEs can capture interpretable features competitive with supervised methods."
          },
          "strengths": {
            "value": "The paper's originality lies in its novel evaluation framework, which directly compares SAEs to supervised dictionaries instead of relying on indirect metrics. The methodology is rigorously structured with specific tests (approximation, sparse controllability, causal faithfulness) and includes a detailed case study on IOI. The work addresses a critical gap in SAE evaluation and provides qualitative insights into SAE training dynamics. The clarity of the problem formulation and experimental design is strong, with clear connections to mechanistic interpretability."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive experimental validation beyond the IOI task, with limited quantitative results comparing SAEs to supervised dictionaries. The construction of supervised feature dictionaries is not sufficiently detailed, raising questions about their objectivity and generalizability. The analysis of qualitative phenomena (e.g., feature splitting, magnitude preferences) is superficial, relying on toy models without rigorous empirical support. The framework's scalability to complex tasks is unproven, and the paper does not address potential biases in attribute selection for supervised dictionaries."
          },
          "questions": {
            "value": "1. How are task-relevant attributes for supervised dictionaries determined? Are they manually curated, and how does this affect evaluation objectivity? 2. What specific metrics quantify SAEs' competitiveness with supervised features in disentanglement and control? 3. How do the authors address potential biases in selecting attributes for supervised dictionaries? 4. Can the framework be generalized to tasks beyond IOI, and what modifications would be required? 5. What evidence supports the claim that SAEs 'split' features or prioritize magnitude, and how are these phenomena quantified?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes a principled framework for evaluating sparse autoencoders (SAEs) by comparing them to supervised feature dictionaries derived from task-relevant concepts. The approach is applied to the indirect object identification (IOI) task using GPT-2 Small, demonstrating that SAEs can capture interpretable features competitive with supervised methods. The work also identifies qualitative phenomena in SAE training, such as feature splitting and magnitude preferences."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in evaluating SAEs by grounding assessments in task-specific supervised benchmarks, offering a novel and structured methodology. The application to the IOI task is concrete and well-motivated, with clear experimental validation. The framework's generality and focus on causal interpretability align with important goals in mechanistic interpretability. The discussion of feature magnitude and splitting phenomena adds theoretical depth, and the paper effectively contextualizes SAE performance relative to supervised baselines."
          },
          "weaknesses": {
            "value": "The evaluation is limited to the IOI task, with insufficient exploration of scalability to other tasks or domains. The paper does not thoroughly analyze potential biases or limitations in the supervised feature dictionaries used as benchmarks. The qualitative phenomena (e.g., feature splitting) are mentioned but lack quantitative analysis or theoretical justification. Additionally, the comparison between SAE variants (e.g., Gated vs. Top-K) is cursory, with limited discussion of hyperparameter sensitivity."
          },
          "questions": {
            "value": "How robust are the supervised feature dictionaries to variations in task definition or dataset composition? Are there scenarios where SAEs outperform supervised dictionaries, and what distinguishes these cases? What is the impact of different SAE training objectives (e.g., sparsity vs. reconstruction) on the observed phenomena? How might the framework generalize to tasks with more complex or ambiguous concept definitions?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "1NprT9Kz0d": {
    "paper_id": "1NprT9Kz0d",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "TexTailor introduces a method for generating consistent 3D textures from text by addressing texture shift across viewpoints through resampling, a performance preservation loss, and adaptive viewpoint refinement. It leverages diffusion models with modifications to improve texture coherence and generalization."
          },
          "strengths": {
            "value": "Originality lies in applying resampling to 3D texture synthesis and proposing a performance preservation loss to mitigate catastrophic forgetting. The method's adaptive viewpoint refinement addresses geometric complexity, which is a significant contribution. Experiments demonstrate superiority over state-of-the-art methods on Objaverse and ShapeNet car datasets. The paper provides a clear problem formulation and structured approach to texture consistency."
          },
          "weaknesses": {
            "value": "The resampling scheme's integration into DDIM is not thoroughly explained, and the performance preservation loss lacks detailed formulation. The adaptive viewpoint refinement's effectiveness is not empirically validated across diverse geometries. The experiments focus on limited datasets, and ablation studies are missing. The paper also does not address how the method scales to highly complex meshes or non-photorealistic textures."
          },
          "questions": {
            "value": "How is the resampling scheme specifically implemented within the DDIM process? What is the exact mathematical formulation of the performance preservation loss? How is the adaptive viewpoint refinement quantitatively evaluated? Are there limitations in handling meshes with extreme geometric complexity? How does TexTailor compare to non-diffusion-based methods in terms of efficiency?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "TexTailor introduces a novel method for generating consistent 3D textures from text by addressing issues in viewpoint-dependent texture degradation. It employs a resampling scheme within a non-Markovian diffusion process (DDIM), a performance preservation loss to mitigate catastrophic forgetting, and adaptive viewpoint refinement based on mesh geometry. Experiments on Objaverse and ShapeNet car datasets show improved results over existing methods."
          },
          "strengths": {
            "value": "Originality is evident in extending 2D resampling techniques to 3D texture synthesis and combining them with DDIM. The adaptive viewpoint refinement addresses a critical limitation of fixed camera positions. The performance preservation loss is a creative solution to the problem of limited training data. The paper's significance lies in its potential to improve 3D content generation for VR/AR applications, with clear experimental validation on standard datasets."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of each component (resampling, performance loss, viewpoint refinement). The comparison with state-of-the-art methods is limited to a subset of datasets, and it's unclear if TexTailor outperforms more recent approaches. The performance preservation loss is mentioned but not thoroughly explained or justified. The adaptive viewpoint refinement's implementation details and effectiveness on complex geometries require further clarification."
          },
          "questions": {
            "value": [
              "How does the resampling scheme specifically integrate into the DDIM process, and what are the technical details of this integration?",
              "What is the exact formulation of the performance preservation loss, and how does it prevent catastrophic forgetting during fine-tuning?",
              "Can the adaptive viewpoint refinement be quantitatively evaluated in terms of its impact on texture consistency across different mesh complexities?",
              "Are there ablation studies demonstrating the individual contributions of resampling, the performance loss, and viewpoint refinement?",
              "How does TexTailor compare to recent text-to-3D methods that do not rely on predefined viewpoints (e.g., implicit neural representations)?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "TexTailor introduces a novel method for generating consistent 3D object textures from text by addressing texture shift across viewpoints and suboptimal camera positioning. The approach combines resampling within a diffusion process, a performance preservation loss to mitigate overfitting, and adaptive viewpoint refinement based on mesh geometry."
          },
          "strengths": {
            "value": "Originality is demonstrated by extending 2D image inpainting resampling to 3D texture synthesis within a non-Markovian diffusion framework. The method tackles specific limitations of existing approaches (texture drift, viewpoint dependency) with a clear problem-solution structure. Experimental validation on standard datasets and comparison against state-of-the-art methods highlight its practical significance. The paper's clarity in explaining technical components and its focus on real-world applications (VR/AR) add to its value."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of the performance preservation loss formulation and its mathematical justification. The adaptive viewpoint refinement mechanism is described conceptually but lacks implementation specifics. Experimental results rely on LPIPS/FID metrics without ablation studies to isolate the impact of individual components. The subset of Objaverse used is not clearly defined, raising concerns about generalizability. The paper also does not address computational efficiency or scalability."
          },
          "questions": {
            "value": "1. Can the authors provide the exact formulation of the performance preservation loss and its derivation? 2. How is the adaptive viewpoint refinement implemented computationally, and what metrics guide the dynamic adjustments? 3. What ablation studies demonstrate the contribution of resampling vs. the performance preservation loss? 4. How was the subset of Objaverse selected, and what statistical properties does it share with the full dataset? 5. Are there limitations to TexTailor's applicability for highly complex geometries or non-rectangular meshes?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "1OyE9IK0kx": {
    "paper_id": "1OyE9IK0kx",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates the challenges of achieving faithful Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs). The authors explore three techniques—activation editing, fine-tuning, and in-context learning (ICL)—to improve faithfulness but find limited success, suggesting inherent difficulties in eliciting trustworthy reasoning from LLMs."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem: ensuring trustworthiness in LLMs through faithful CoT reasoning, which is essential for high-stakes applications. The exploration of multiple techniques (activation editing, fine-tuning, ICL) demonstrates comprehensive methodology. The problem statement is well-motivated, and the paper contributes to a growing literature on LLM transparency. The clarity of the problem definition and structure is strong."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental analysis, including specific benchmarks, datasets, and baseline comparisons. The results are described in general terms (e.g., 'limited success') without quantitative evidence or ablation studies. The activation editing strategy is vaguely described, with no explanation of how the 'faithfulness vector' was identified. The failure to generalize across tasks is noted, but the paper does not analyze why these methods failed or how the metrics from Lanham et al. (2023) were applied. The absence of a clear path forward or actionable insights limits the paper's impact."
          },
          "questions": {
            "value": [
              "What specific benchmarks and datasets were used to evaluate the techniques? How were the metrics from Lanham et al. (2023) operationalized?",
              "Were there strong baselines (e.g., standard CoT without interventions) for comparison? How do the results compare to state-of-the-art methods for CoT faithfulness?",
              "How was the 'faithfulness vector' in activation editing identified? What criteria were used to determine its effectiveness?",
              "Why did fine-tuning and ICL show only marginal improvements? Were there any specific tasks or data characteristics where these methods worked better?",
              "What is the relationship between the faithfulness metric and the actual internal reasoning of the LLM? How was this validated?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates the challenges of achieving faithful Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) by evaluating three strategies: activation editing, fine-tuning, and in-context learning. The authors find limited success in improving faithfulness, highlighting the inherent difficulty of aligning CoT reasoning with the model's actual behavior."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem—faithfulness in LLM reasoning—which is essential for high-stakes applications. The methodology is rigorous, leveraging established metrics from Lanham et al. (2023) and testing multiple approaches. The empirical analysis is thorough, and the discussion on the limitations of current techniques provides valuable insights for future research. The paper also clearly contextualizes its work within the broader landscape of LLM transparency."
          },
          "weaknesses": {
            "value": "The paper lacks novel contributions, as the strategies tested (activation editing, fine-tuning, ICL) are variations of existing methods rather than original approaches. The experiments are limited in scope, with insufficient details on benchmark selection, dataset diversity, and the specific implementation of activation editing. The conclusion that current methods are insufficient is valid, but the paper does not propose new frameworks or deeper analysis of why these techniques fail, which limits its impact."
          },
          "questions": {
            "value": "1. How exactly were the faithfulness metrics from Lanham et al. (2023) applied in the experiments? Were they used as evaluation criteria or for training? 2. What specific benchmarks and datasets were tested, and how representative are they of real-world scenarios? 3. The activation editing strategy is described as 'probing LLMs to identify a vector/direction corresponding to faithfulness'—could the authors elaborate on how this vector was derived and validated? 4. How do the 'controlled scenarios' with marginal improvements compare to the broader benchmarks in terms of task complexity and domain diversity?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates the challenges of generating faithful Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) by evaluating three strategies—activation editing, fine-tuning, and in-context learning. The authors find that these approaches offer limited success in improving faithfulness, highlighting the inherent difficulty of aligning CoT reasoning with the model's actual behavior. They also analyze the barriers to faithfulness and suggest directions for future research."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem: ensuring trustworthiness in LLMs through faithful CoT reasoning, which is essential for high-stakes applications. The empirical analysis is thorough, covering multiple benchmark tasks and strategies. The creative combination of activation editing, fine-tuning, and in-context learning as approaches to improve faithfulness demonstrates originality. The work's significance is underscored by its focus on transparency and reliability in LLMs, which are key concerns for real-world deployment."
          },
          "weaknesses": {
            "value": "The paper's findings of limited success for the tested strategies may not fully explore the potential of these methods. The experiments lack depth in analyzing why the strategies fail to generalize, such as whether specific task characteristics or model architectures play a role. Additionally, the paper does not compare its approaches to alternative methods for improving faithfulness, which could provide a more comprehensive context. The truncated content also raises concerns about the completeness of the analysis."
          },
          "questions": {
            "value": "1. How were the metrics for faithfulness defined, and could alternative metrics yield different insights? 2. Were the tested strategies evaluated across diverse model architectures or only specific ones? 3. What specific aspects of CoT reasoning (e.g., logical consistency, alignment with internal computations) were most challenging to improve? 4. Could the limited generalization be attributed to the choice of benchmark tasks, and how might the results vary with other datasets?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "1R5BcYS8EC": {
    "paper_id": "1R5BcYS8EC",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces SysCaps, a framework that uses natural language descriptions (system captions) to interface with surrogate models for complex energy systems (CES). The approach combines text embeddings from fine-tuned language models with time-series data to improve surrogate accuracy and generalization, supported by experiments on real-world building and wind farm simulators."
          },
          "strengths": {
            "value": "The paper addresses a significant gap in making surrogate models more accessible via natural language, which is both novel and practical. The multimodal architecture is lightweight and well-motivated, and the use of LLMs to generate synthetic captions from metadata is a creative solution to data scarcity. The experiments on real-world systems demonstrate improved accuracy and generalization compared to traditional methods, and the open-source release of code/data is a valuable contribution. The paper also highlights potential applications like language-driven design exploration, which expands the scope of surrogate modeling."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of why the proposed method outperforms baselines, such as ablation studies on the multimodal architecture or the impact of different LLM-generated captions. The generalization claims (e.g., robustness to synonyms) are not thoroughly validated with quantitative experiments. The evaluation strategy for caption quality (attribute classifier) is underdeveloped, and the paper does not compare against state-of-the-art multimodal or surrogate modeling approaches. Additionally, the paper is cut off, leaving critical sections (e.g., full related work, detailed experiments) incomplete, which limits the ability to assess the work fully."
          },
          "questions": {
            "value": "1. How do the authors ensure the quality and relevance of LLM-generated SysCaps, especially in edge cases? 2. What ablation studies were conducted to validate the necessity of the multimodal architecture versus standalone text or time-series models? 3. How does the proposed method compare to existing surrogate modeling techniques (e.g., physics-informed neural networks) on the same datasets? 4. Are there limitations to the generalization capabilities when applying SysCaps to entirely new domains beyond buildings and wind farms?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces SysCaps, a framework that uses natural language descriptions ('system captions') to interface with surrogate models for complex energy systems (CES). The approach combines text embeddings from fine-tuned language models with time-series data for regression tasks, demonstrating improved accuracy and generalization over traditional methods. The work also proposes a method to generate synthetic captions using LLMs and evaluates their effectiveness on real-world building and wind farm simulators."
          },
          "strengths": {
            "value": "Originality is evident in applying language interfaces to surrogate modeling for CES, particularly through the SysCap concept. The lightweight multimodal architecture addresses a gap in combining text and time-series data for regression. The paper provides rigorous experiments on real-world simulators, showing superior accuracy and novel generalization capabilities. The open-source release of code and data enhances reproducibility and future research. The discussion on language-driven design exploration adds practical value."
          },
          "weaknesses": {
            "value": "The paper lacks comparison with state-of-the-art surrogate models beyond one-hot baselines, making it hard to assess the full impact of SysCaps. The LLM-based caption generation process is not detailed enough—key specifics like the exact LLM architecture, training data, or evaluation metrics for caption quality are missing. The generalization experiments (e.g., synonym substitution) lack quantitative results. The claim about regularization through prompt augmentation is not empirically validated."
          },
          "questions": {
            "value": [
              "How exactly are the SysCaps generated using LLMs? What specific LLM and training procedure are used?",
              "What are the exact metrics used to evaluate caption quality beyond the multiclass attribute classifier?",
              "Are there quantitative results for the generalization experiments (e.g., accuracy drop when replacing attribute names with synonyms)?",
              "How does the proposed method compare to other multimodal approaches for time-series regression?",
              "What ablation studies were conducted to validate the contribution of text embeddings vs. traditional encodings?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces SysCaps, a framework for enhancing simulation surrogates of complex energy systems (CES) by integrating natural language descriptions (SysCaps) with multimodal regression models. The approach uses large language models (LLMs) to generate synthetic system captions from simulation metadata, enabling text-based interfaces for improved accessibility and generalization. Experiments on building and wind farm simulators demonstrate superior accuracy and robustness compared to traditional methods."
          },
          "strengths": {
            "value": "The paper presents a novel application of language interfaces for surrogate modeling, addressing a critical gap in making complex systems accessible to non-experts. The lightweight multimodal architecture combining text embeddings and time-series encoding is well-motivated and technically sound. The use of LLMs for synthetic caption generation is innovative, and the open-sourcing of code/data is a valuable contribution. The experiments show promising results in accuracy and generalization, particularly in handling semantically related inputs."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous baseline comparisons (e.g., state-of-the-art multimodal models, domain-specific baselines). The evaluation of SysCap quality relies on a simplistic attribute classifier, which may not capture semantic fidelity. The experiments are limited to two specific domains (buildings/wind farms), raising questions about generalizability. The claim about 'robustness to synonym replacement' lacks quantitative validation. The paper also fails to address potential limitations of LLM-generated captions (e.g., hallucinations, domain-specific inaccuracies)."
          },
          "questions": {
            "value": [
              "How were the LLM-generated SysCaps validated for factual accuracy in the context of energy systems? Were human evaluators involved?",
              "What ablation studies were conducted to assess the contribution of text embeddings vs. traditional encoding methods?",
              "How does the proposed method scale to larger, more complex CES with higher-dimensional metadata?",
              "What are the computational costs of the LLM-based caption generation pipeline compared to traditional feature engineering?",
              "How were the 'held-out systems' selected? Are they representative of real-world distribution shifts?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "1S7kpbfgq9": {
    "paper_id": "1S7kpbfgq9",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces Normalized Space Alignment (NSA), a novel metric for analyzing and aligning neural network representations. NSA compares pairwise distances between point clouds from the same source, handling differing dimensionalities. It combines local and global components to measure structural discrepancies and can serve as a differentiable loss function. The authors claim NSA is computationally efficient and applicable across various deep learning tasks, including robustness analysis and dimensionality reduction."
          },
          "strengths": {
            "value": "Originality: NSA introduces a new approach to representation analysis by combining local and global structural comparison, addressing limitations of existing metrics like CKA and RTD. Quality: The paper provides a clear theoretical framework for NSA, with claims of computational efficiency and differentiability. Clarity: The structure is logically organized, with sections on motivation, related work, methodology, and applications. Significance: The potential applications in robustness analysis and model training could impact diverse areas of deep learning."
          },
          "weaknesses": {
            "value": "The paper lacks detailed mathematical formulations for NSA, making it difficult to assess the theoretical foundations. Experimental validation is insufficient, with no baseline comparisons against established methods like CKA or RTD. The claims about computational efficiency are not supported by complexity analysis or runtime benchmarks. The section on adversarial robustness analysis is underdeveloped, with no quantitative results. The paper does not address potential limitations, such as sensitivity to hyperparameters or scalability to very large datasets."
          },
          "questions": {
            "value": "1. What is the exact mathematical definition of NSA, and how are LNSA and GNSA formally derived? 2. How does NSA handle point clouds with significantly different dimensionalities? 3. What baseline methods were used for comparison in the experiments, and what were the quantitative results? 4. Can the authors provide complexity analysis to support the claim of quadratic computational complexity? 5. How does NSA perform on datasets with varying levels of noise or sparsity?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces Normalized Space Alignment (NSA), a novel metric for analyzing and aligning neural network representations. NSA combines local and global structural analysis by measuring discrepancies in pairwise distances and local intrinsic dimensionality (LID) between two point clouds. It is presented as a differentiable loss function with computational efficiency, applicable to tasks like dimensionality reduction, robustness analysis, and adversarial attack evaluation."
          },
          "strengths": {
            "value": "The paper demonstrates strong methodological rigor by proposing a metric that addresses multiple challenges in representation learning, including computational efficiency and cross-dimensional comparison. The integration of Local Intrinsic Dimensionality (LID) for local analysis and pairwise distance metrics for global alignment is a novel combination. The experimental validation across diverse tasks (e.g., autoencoders, GNN robustness) highlights NSA's versatility. The paper also provides theoretical justification for NSA's properties as a similarity metric and loss function, with claims of quadratic complexity that outperform prior methods like RTD."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of LNSA vs. GNSA. While NSA is claimed to be computationally efficient, the analysis of its scalability on large datasets remains underexplored. The comparison to existing methods (e.g., CKA, RTD) is cursory, with no quantitative benchmarks on standard datasets. Additionally, the theoretical guarantees for NSA's robustness to adversarial perturbations are not rigorously established, despite the claims of utility in GNN analysis. The experimental results for downstream tasks are described in general terms without concrete metrics or statistical significance."
          },
          "questions": {
            "value": "1. How does NSA's performance compare quantitatively to CKA or RTD on standard benchmarks like ImageNet or CIFAR-10? 2. What specific ablation studies were conducted to validate the necessity of combining LID with pairwise distances? 3. Are there theoretical bounds on NSA's sensitivity to adversarial perturbations, or is the correlation with misclassification rates purely empirical? 4. How is the one-to-one mapping between points enforced in cases where the source and target representations have different cardinalities? 5. What are the practical limitations of NSA's quadratic complexity in real-world applications with large-scale data?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces Normalized Space Alignment (NSA), a novel metric for analyzing and aligning neural network representations. NSA combines local and global components (LNSA and GNSA) to measure structural discrepancies between point clouds, offering a computationally efficient alternative to existing methods. It is presented as a differentiable loss function with applications in representation analysis, robustness evaluation, and adversarial attack detection."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by proposing a novel metric that addresses limitations of existing methods like CKA and RTD, particularly in computational efficiency and applicability across dimensionalities. The clarity of the methodology is commendable, with a structured breakdown of LNSA and GNSA. The significance lies in its versatility for diverse tasks (e.g., adversarial robustness analysis) and potential to improve model interpretability and performance. The focus on computational efficiency (quadratic complexity) is a notable strength, addressing scalability challenges in prior work."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as ablation studies or comparisons with baseline methods on standard benchmarks. The claims about NSA's superiority over existing methods (e.g., RTD) are not substantiated with quantitative results. The explanation of how NSA handles E(n) symmetry groups and the theoretical guarantees for its robustness are underdeveloped. Additionally, the truncated content prevents full assessment of the experimental section, leaving critical details unresolved."
          },
          "questions": {
            "value": "1. What specific datasets and baselines were used to validate NSA's performance in downstream tasks? 2. How does NSA's computational efficiency compare to CKA/RTD in practice, and what are the exact complexity bounds? 3. Can the authors provide a formal proof for the mini-batch representativeness claim? 4. How is the correlation between NSA and misclassification rates in adversarial attacks quantified? 5. What are the limitations of NSA in handling non-E(n) symmetric data or high-dimensional spaces?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "1S8ndwxMts": {
    "paper_id": "1S8ndwxMts",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper systematically analyzes evaluation metrics for protein generative models, focusing on quality, diversity, and distributional similarity. It defines desirable properties for metrics (robustness, interpretability, computational efficiency) and evaluates their behavior under synthetic and real-world conditions, offering practical recommendations for researchers."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in protein generative modeling by proposing a structured framework for evaluating model quality. It rigorously defines 'good' protein properties (structural stability, self-consistency) and explores metric design principles. The systematic approach to analyzing metric robustness and computational efficiency is thorough. The focus on both synthetic and real-world data enhances the practical relevance of the findings."
          },
          "weaknesses": {
            "value": "The paper is truncated, preventing full evaluation of experimental results and metric comparisons. Key details about the synthetic datasets, state-of-the-art models tested, and specific metric limitations are missing. The analysis of metric trade-offs (e.g., computational efficiency vs. sensitivity) lacks depth. The absence of direct comparisons with existing evaluation frameworks limits the paper's novelty and impact."
          },
          "questions": {
            "value": "What specific metrics were analyzed in detail, and how do they compare in terms of performance? How were the synthetic datasets designed to control for variables like sequence length or structural complexity? Which real-world protein generative models were used for validation, and how representative are they of current state-of-the-art methods? Are there cases where the proposed metrics fail, and what are the underlying reasons? How do the authors' recommendations address the identified challenges in practical scenarios?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper systematically analyzes evaluation metrics for protein generative models, focusing on quality, diversity, and distributional similarity. It identifies challenges like sample size dependencies and computational efficiency, offering practical recommendations for robust evaluation."
          },
          "strengths": {
            "value": "The paper provides a comprehensive framework for evaluating protein generative models, addressing critical dimensions such as structural stability and self-consistency. It highlights the importance of robustness, interpretability, and computational efficiency in metrics. The systematic approach to analyzing metric behavior under perturbations and real-world conditions is well-structured and theoretically sound."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with the third section on metrics cut off, leaving key details about the analyzed metrics and their results missing. It lacks concrete experimental validation with real-world models, relying heavily on synthetic datasets. The recommendations are general and do not provide actionable implementation guidance. The analysis of computational trade-offs and sample size dependencies is superficial without empirical evidence."
          },
          "questions": {
            "value": "What specific metrics were analyzed in Section 3, and how were they evaluated? How were the synthetic datasets generated, and what controlled properties did they include? What are the computational constraints of the recommended metrics, and how do they scale with sample size? Are there case studies or applications of the proposed recommendations in existing protein design workflows?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper presents a systematic analysis of evaluation metrics for protein generative models, focusing on quality, diversity, and distributional similarity. The authors investigate the behavior of existing metrics under various conditions, identify challenges such as sample size dependencies and computational trade-offs, and provide practical recommendations for robust evaluation."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in protein generative modeling by systematically evaluating metrics, which is highly novel given the lack of standardized benchmarks in this domain. The methodology is thorough, with clear distinctions between quality, diversity, and distributional similarity metrics. The work is well-structured, and the motivation for robust evaluation is compelling. The practical recommendations for researchers demonstrate significant potential impact on the field."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with the metrics section truncated mid-sentence, limiting the ability to assess the full scope of metrics evaluated. The experimental analysis lacks details on synthetic dataset properties, real-world models tested, and quantitative results. The recommendations are mentioned but not elaborated, leaving their practical applicability unclear. The paper also does not address how its findings compare to existing literature on metric robustness in other domains."
          },
          "questions": {
            "value": "1. Could the authors clarify which specific metrics were evaluated in the study? 2. What are the characteristics of the synthetic datasets and real-world models used for experimentation? 3. How were the practical recommendations derived from the analysis? 4. Are there any limitations to the study's generalizability to other protein design tasks or domains? 5. How do the identified challenges (e.g., sample size dependencies) compare to similar issues in text/image generation metrics?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "1VwWi6zbxs": {
    "paper_id": "1VwWi6zbxs",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces τJp, a novel metric derived from the product of task vectors and the Jacobian of a pre-trained model, to address interference in task arithmetic. The authors propose regularization to minimize τJp, reducing the need for coefficient tuning and improving task performance. They validate its effectiveness in scenarios with unknown future tasks and with publicly available fine-tuned models."
          },
          "strengths": {
            "value": "Originality is evident in the τJp metric and its theoretical connection to weight disentanglement, leveraging NTK and linearization insights. The paper addresses practical challenges in task arithmetic, such as interference and coefficient tuning, with clear experimental motivation. The significance lies in enabling scalable, real-world applications of model-editing techniques. Clarity is strong in defining key concepts like task vectors and weight disentanglement, though some sections may lack depth."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing metrics (e.g., weight disentanglement error) and fails to provide ablation studies to isolate τJp's impact. Experimental validation is limited to specific datasets/tasks, with no analysis of scalability across diverse architectures or domains. The theoretical claims about causal relationships require stronger empirical support, and the regularization's computational overhead is underexplored. Additionally, the paper does not address potential limitations of τJp in non-linear or high-dimensional settings."
          },
          "questions": {
            "value": "1. How does τJp compare to existing metrics like weight disentanglement error in terms of correlation with interference? 2. Are the regularization results consistent across different model architectures (e.g., CNNs vs. transformers)? 3. What are the computational costs of calculating τJp, and how does it scale with model size? 4. How does the method handle tasks with highly non-linear relationships or overlapping feature spaces? 5. Are the experiments on public fine-tuned models reproducible, and what specific models were used?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces τJp, a novel metric derived from the product of task vectors and the Jacobian of a pre-trained model, to quantify interference in task arithmetic. The authors propose regularization to minimize τJp, reducing task interference and eliminating the need for coefficient tuning. They validate its effectiveness in scenarios with unknown future tasks and demonstrate practical benefits using publicly available fine-tuned models."
          },
          "strengths": {
            "value": "The paper presents a novel metric (τJp) with a clear theoretical connection to weight disentanglement, addressing a critical gap in task arithmetic. The regularization approach is promising, as it simplifies model editing by reducing reliance on manual coefficient adjustments. The practical applications in incremental learning and real-world scenarios highlight the significance of the work. The paper also contextualizes its contributions within existing literature, showing awareness of prior challenges in reproducibility and scalability."
          },
          "weaknesses": {
            "value": "The experimental validation of τJp's correlation with interference and accuracy is limited. The paper lacks detailed ablation studies or comparisons with baseline methods to quantify the improvement achieved by τJp regularization. Additionally, the theoretical justification for τJp's causal relationship with weight disentanglement is not fully elaborated, leaving some claims unverified. The practical scenarios (e.g., unknown future tasks) are mentioned but require more concrete evidence of scalability."
          },
          "questions": {
            "value": [
              "How was the inverse correlation between τJp and normalized accuracy established? Were statistical tests or visualizations provided?",
              "What baseline methods were compared against to demonstrate the effectiveness of τJp regularization?",
              "Are there any limitations to the τJp metric in different model architectures or task types?",
              "How does the proposed regularization handle cases where task vectors are highly non-linear or overlapping?",
              "What specific real-world applications were tested with publicly available fine-tuned models, and what metrics were used to evaluate performance?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces τJp, a novel metric derived from the product of task vectors and the Jacobian of a pre-trained model, to address interference in task arithmetic. The authors propose regularization to minimize τJp, reducing the need for coefficient tuning and improving task performance. They validate the approach in scenarios with unknown future tasks and demonstrate practical benefits using pre-trained models."
          },
          "strengths": {
            "value": "Originality: The τJp metric offers a fresh perspective on weight disentanglement, linking it to Jacobian properties and task interference. Quality: The paper presents a theoretically grounded approach with practical experiments. Clarity: The problem statement and contributions are well-articulated, though some sections are truncated. Significance: Task arithmetic is a critical area, and the proposed method addresses key challenges in reproducibility and scalability."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing metrics like weight disentanglement error. The theoretical analysis of τJp's causal relationship is not fully developed, and the experiments focus on specific scenarios without broader validation. The practical benefits of τJp regularization on pre-trained models are not thoroughly demonstrated with ablation studies or case studies."
          },
          "questions": {
            "value": [
              "How does τJp compare to existing metrics for measuring task interference? Are there quantitative benchmarks against prior work?",
              "What ablation studies were conducted to validate the effectiveness of τJp regularization in different task configurations?",
              "The paper mentions 'publicly available fine-tuned models' but does not specify which ones or how they were integrated. Could the authors provide more details on this application?",
              "How does the Jacobian-based analysis of τJp generalize across different model architectures and tasks?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "1X1R7P6yzt": {
    "paper_id": "1X1R7P6yzt",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes DGPPo, a framework that simultaneously learns a discrete graph control barrier function (DGCBF) and a distributed safe policy for multi-agent systems (MAS) under unknown discrete-time dynamics, changing neighborhoods, and input constraints. It addresses limitations of existing CBF-based methods by combining reinforcement learning (RL) with DGCBF to ensure safety without requiring a pre-existing nominal policy or known dynamics."
          },
          "strengths": {
            "value": "The paper introduces a novel framework (DGPPo) that integrates discrete-time CBFs with RL for multi-agent safe control, addressing critical gaps in existing methods. The approach is theoretically grounded in handling input constraints and dynamic uncertainty, which are common challenges in real-world MAS. The empirical validation across multiple simulation environments demonstrates practical effectiveness, achieving high task performance and safety rates with a consistent hyperparameter set. The work also extends prior GCBF research to discrete-time settings, which is a meaningful contribution to the field."
          },
          "weaknesses": {
            "value": "The paper lacks detailed theoretical analysis of the convergence properties of DGPPo or the stability guarantees of the learned DGCBF. The experimental section is incomplete (e.g., the text is cut off mid-sentence), making it difficult to assess the thoroughness of the evaluation. Key implementation details, such as how the DGCBF is trained alongside the policy, are not elaborated. Additionally, the comparison with baselines is not sufficiently detailed—e.g., it is unclear which specific methods were used as baselines or how the hyperparameter robustness was quantitatively validated."
          },
          "questions": {
            "value": "1. How is the DGCBF trained in conjunction with the policy? What objective function ensures compatibility between the CBF and the RL policy? 2. The paper claims hyperparameter robustness but does not provide quantitative metrics (e.g., variance in performance across environments). How were the hyperparameters tuned? 3. What are the specific limitations of the DGCBF in handling input constraints, and how does the framework address them? 4. The text is cut off in Section 3.1—can the authors clarify the problem formulation and constraints? 5. How does DGPPo handle the trade-off between safety and task performance in dynamic environments?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes DGPPo, a framework that combines discrete graph CBF (DGCBF) learning with proximal policy optimization to enable safe multi-agent control under unknown discrete-time dynamics, input constraints, and partial observability. It claims to achieve high task performance and safety without requiring a pre-existing nominal policy, validated through experiments across multiple simulation environments."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in multi-agent safe control by integrating control barrier functions (CBFs) with reinforcement learning (RL) in a novel way. The framework's ability to handle unknown dynamics, input constraints, and changing neighborhoods without relying on a pre-defined nominal policy is a significant contribution. The problem formulation is well-motivated, and the approach is theoretically grounded in existing CBF and RL literature. The empirical claims of hyperparameter robustness and strong safety-performance trade-offs are promising, though incomplete validation remains a concern."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical technical details of DGCBF and DGPPo unexplained. The experimental validation is incomplete, making it impossible to assess the rigor of the results or the claimed hyperparameter insensitivity. The theoretical analysis of safety guarantees for the learned policies is missing, which is essential for safety-critical applications. Additionally, the related work section may not fully address recent advancements in decentralized CBFs or RL with safety constraints, potentially underrepresenting the field's state-of-the-art."
          },
          "questions": {
            "value": "1. How is the DGCBF learned, and what are the specific mechanisms for handling changing neighborhoods and input constraints? 2. What are the theoretical guarantees for the safety properties of the policies generated by DGPPo? 3. How does the framework explicitly address partial observability beyond the sensing radius? 4. What are the limitations of the current approach, and how do the authors plan to extend it to handle more complex dynamics or larger-scale systems? 5. Are the experimental results reproducible, and what specific metrics were used to quantify safety rates and task performance?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper proposes DGPPo, a framework that combines reinforcement learning (RL) with discrete graph control barrier functions (GCBFs) to address safe optimal control in multi-agent systems (MAS) under unknown dynamics, input constraints, and changing neighborhoods. It introduces methods for learning discrete CBFs (DCBFs) and extends them to DGCBFs for dynamic environments, demonstrating empirical effectiveness across multiple simulation platforms."
          },
          "strengths": {
            "value": "The paper tackles a critical challenge in MAS: ensuring safety under uncertain dynamics and constraints, which is underexplored in existing literature. The integration of CBFs with RL for discrete-time systems is novel, particularly for handling input constraints and dynamic neighborhoods. The framework's hyperparameter insensitivity and empirical validation across diverse environments highlight its practical relevance. The problem formulation and contributions are clearly articulated, and the related work section is comprehensive."
          },
          "weaknesses": {
            "value": "The paper is truncated, limiting the ability to assess the completeness of experiments, theoretical analysis, and comparisons with state-of-the-art methods. The DGCBF learning process and safety guarantees are not sufficiently detailed. The empirical results lack specifics on environment diversity, baseline comparisons, and quantitative metrics beyond safety rates and task performance. The handling of input constraints and dynamic neighborhoods requires deeper technical justification."
          },
          "questions": {
            "value": "1. Could the authors provide more details on the simulation environments and how they simulate unknown dynamics? 2. How is the DGCBF trained to handle changing neighborhoods, and what guarantees exist for its stability? 3. What specific input constraints are addressed, and how are they incorporated into the CBF formulation? 4. How does DGPPo compare to recent safe RL methods (e.g., CMDP-based approaches) in terms of safety and performance? 5. Are there theoretical bounds on the safety guarantees of the proposed framework?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "1d8Egv45of": {
    "paper_id": "1d8Egv45of",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a self-explaining multi-view deep learning architecture that generates task-relevant, human-interpretable masks for cardiovascular signal analysis. The method learns semantic views of input signals (e.g., ECG/PPG) by attributing samples to semantic states, with experimental validation on three clinical tasks showing improved explainability over post-hoc methods like LIME and SHAP."
          },
          "strengths": {
            "value": "The paper addresses a critical need for interpretability in healthcare AI, proposing a novel architecture that integrates semantic masking during training rather than relying on post-hoc explanations. The approach demonstrates practical applicability in cardiovascular tasks and provides qualitative/quantitative comparisons to existing methods. The work also highlights alignment with domain knowledge, which is significant for clinical adoption."
          },
          "weaknesses": {
            "value": "The experimental validation is limited in scope: the 2-view architecture is tested on only three tasks, with unclear details on dataset characteristics, baseline comparisons (e.g., no comparison to recent SOTA methods like Transformer-based models), and statistical significance of results. The novelty of 'complementary masks' lacks theoretical justification, and the paper does not address potential limitations (e.g., generalizability to other signal types or edge cases). The ablation studies and analysis of mask quality metrics (e.g., faithfulness, stability) are absent."
          },
          "questions": {
            "value": "1. How does the method handle variations in signal quality (e.g., noise, artifacts) in real-world clinical settings? 2. What specific metrics were used to quantitatively evaluate mask quality, and how do they correlate with clinical relevance? 3. Are the semantic states explicitly defined by domain experts, or learned automatically? 4. How does the 2-view architecture scale to more complex tasks or higher-dimensional signals? 5. What ablation studies were conducted to validate the contribution of individual components (e.g., mask network vs. embedding network)?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes a self-explaining multi-view deep learning architecture that generates interpretable semantic masks for cardiovascular signal analysis. The method learns complementary semantic views of input signals (e.g., ECG/PPG) during training, producing task-relevant explanations that outperform post-hoc methods like LIME and SHAP in both qualitative and quantitative evaluations. The 2-view architecture demonstrates comparable performance to state-of-the-art models across three clinical tasks."
          },
          "strengths": {
            "value": "The paper introduces a novel self-explaining framework that integrates interpretability directly into the learning process, addressing a critical gap in healthcare AI. The focus on cardiovascular signals with clinical relevance highlights practical significance. The architecture's ability to generate task-specific semantic views offers fresh insights into model decision-making. The experimental validation against established XAI methods and demonstration of alignment with domain knowledge showcase strong applicability. The work's emphasis on reducing reliance on expert prior knowledge is particularly innovative."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental specifics, such as exact datasets, baseline comparisons, and quantitative metrics for task performance. The definition of 'semantic states' and the mechanism for generating masks remain under-specified. The architecture's technical details (e.g., how semantic views are fused, training objectives) are not thoroughly explained. The related work section is incomplete, limiting the ability to assess novelty. The clinical validation of explanations (e.g., how they align with expert knowledge) is not elaborated."
          },
          "questions": {
            "value": [
              "What are the three specific cardiovascular tasks and datasets used for evaluation? Please provide details on data preprocessing and clinical relevance.",
              "How are semantic states defined and learned? Are they supervised, unsupervised, or semi-supervised? What is the role of domain knowledge in this process?",
              "What quantitative metrics were used to evaluate task performance (e.g., accuracy, AUC) and explanation quality (e.g., faithfulness, sparsity)?",
              "Are there ablation studies demonstrating the necessity of the multi-view architecture? How does the 2-view model compare to single-view variants?",
              "How are the generated masks validated against expert annotations? Please provide examples of aligned interpretations."
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces a self-explaining multi-view deep learning architecture that generates human-interpretable semantic masks for cardiovascular signal analysis. The method learns complementary semantic views of input signals (ECG/PPG) to stratify clinical information, with claims of superior explainability compared to post-hoc methods like LIME and SHAP, and competitive performance on regression/classification tasks."
          },
          "strengths": {
            "value": "The paper addresses an important problem in healthcare AI by proposing a novel architecture for intrinsic explainability. The focus on cardiovascular signals is clinically relevant, and the integration of semantic masks with task-specific learning shows promise. The methodology's emphasis on human-interpretable representations aligns with XAI goals. The paper also attempts to validate alignment with domain knowledge, which adds practical value."
          },
          "weaknesses": {
            "value": "The paper lacks critical details about the architecture's novelty, such as how semantic states are defined or how the mask network is trained. Experiments are insufficiently described—no ablation studies, limited baseline comparisons (e.g., no self-explaining models like Layer-wise Relevance Propagation), and unclear metrics for evaluating explainability quality. The claims about 'complementary masks' and 'task-level performance' are not rigorously substantiated. The truncated content raises concerns about completeness."
          },
          "questions": {
            "value": "1. How are semantic states defined? Are they learned or predefined? 2. What specific metrics were used to quantitatively evaluate explanation correctness? 3. Why were only LIME/SHAP compared, and not other self-explaining models? 4. How is the alignment with domain knowledge validated? 5. What are the computational costs and scalability of the proposed architecture?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "1dDxMPJy4i": {
    "paper_id": "1dDxMPJy4i",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces NEDAG-GP, a nonparametric method for continuous DAG learning that integrates realistic expert knowledge (required edges, initial graphs, topological orderings) and uses Gaussian Processes (GPs) to accurately model edge strengths in weighted adjacency matrices. It claims improvements in structure accuracy and interpretability compared to existing methods like NOTEARS-MLP."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in DAG learning by systematically integrating diverse expert knowledge into continuous frameworks, which is underexplored in prior work. The use of GPs for nonparametric modeling is a novel approach that reduces reliance on expert-specified parameters. The method's theoretical claims (e.g., accurate edge strength representation) and empirical validation on synthetic and real-world datasets (e.g., Gene Regulatory Networks) demonstrate practical relevance. The clarity of the problem statement and the structured presentation of contributions are strong points."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical exposition of how GPs are integrated into the continuous DAG learning framework, making it difficult to assess the novelty and feasibility of the approach. The theoretical analysis (e.g., proofs in Section 4.1) is referenced but not included, undermining the rigor of the claims. The experiments are briefly described, and it is unclear whether NEDAG-GP is compared to recent state-of-the-art methods or if the real-world datasets are sufficiently challenging. The term 'realistic knowledge incorporation' is vague and requires more concrete examples or validation."
          },
          "questions": {
            "value": "1. How does the GP formulation in NEDAG-GP explicitly model edge strengths, and what distinguishes it from existing nonlinear methods like NOTEARS-MLP? 2. Are there specific challenges in applying GPs to continuous DAG learning, and how does the paper address them? 3. What are the computational limitations of the GP-based approach, and how does it scale to large graphs? 4. How are the different forms of expert knowledge (REQ-EDG, INI-GRA, TOP-ORD) implemented and validated in the experiments?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces NEDAG-GP, a nonparametric method for continuous DAG learning that integrates realistic expert knowledge (e.g., required edges, initial graphs, topological orderings) and uses Gaussian Processes (GPs) to produce accurate edge strength estimates in weighted adjacency matrices. The approach aims to improve interpretability and structure learning accuracy compared to existing methods."
          },
          "strengths": {
            "value": "Originality: The integration of multiple realistic forms of expert knowledge into continuous DAG learning is novel, as is the use of GPs for nonparametric weight formulation. Quality: The paper addresses key limitations of prior work, such as inaccurate edge weights in nonlinear models and reliance on parametric assumptions. Clarity: The motivation and problem formulation are well-structured, with clear connections to prior work. Significance: If successful, the method could advance causal discovery in real-world applications requiring domain expertise and interpretability."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation (e.g., baselines, metrics, real-world datasets) due to truncated content, making it difficult to assess claims of superiority. The theoretical analysis of edge strength accuracy is referenced but not elaborated. The integration of expert knowledge (REQ-EDG, INI-GRA, TOP-ORD) is described conceptually but not demonstrated with concrete examples. The paper does not address potential limitations of GP-based approaches in high-dimensional or complex causal scenarios."
          },
          "questions": {
            "value": "1. What specific real-world datasets were used to evaluate NEDAG-GP, and how do they reflect realistic expert knowledge? 2. How were the different forms of expert knowledge (REQ-EDG, INI-GRA, TOP-ORD) operationalized in the experiments? 3. Which baselines were compared against, and what metrics were used to quantify improvements in structure accuracy and edge strength estimation? 4. How does the GP-based weight formulation handle scalability to large graphs? 5. Are there cases where the GP approach might fail compared to parametric methods, and how are these addressed?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces NEDAG-GP, a nonparametric method for directed acyclic graph (DAG) learning that integrates expert knowledge and accurately represents edge strengths using Gaussian Processes (GPs). It addresses limitations in existing continuous structure learning methods by incorporating realistic domain knowledge (e.g., required edges, initial graphs, topological orderings) and improving transparency through accurate weight matrices. The method aims to enhance both structure accuracy and interpretability compared to parametric approaches."
          },
          "strengths": {
            "value": "The paper addresses critical gaps in DAG learning by emphasizing interpretability, expert knowledge integration, and nonparametric modeling. The use of GPs to avoid parametric assumptions is novel and aligns with the goal of reducing expert burden. The motivation for accurate edge strength representation is well-justified, and the paper highlights important applications like gene regulatory networks. The structure of the paper is logically organized, and the problem formulation is clear. The authors also provide theoretical insights into the limitations of existing methods, which strengthens the context for their contributions."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient empirical validation. While the abstract claims superior performance on synthetic and real-world datasets, the provided content does not include results, comparisons, or analysis to substantiate these claims. The theoretical proofs (e.g., in the appendix) are referenced but not elaborated, leaving the rigor of the claims unverified. The integration of expert knowledge (e.g., required edges, topological orderings) is mentioned but not detailed in terms of implementation or evaluation. Additionally, the paper does not clarify how NEDAG-GP addresses computational challenges inherent to GP-based methods, such as scalability to large graphs."
          },
          "questions": {
            "value": "1. How does NEDAG-GP compare to existing nonparametric methods in terms of structure accuracy and edge strength estimation? What baselines were used for evaluation? 2. What specific real-world datasets were tested, and how do they validate the practical utility of the method? 3. Are the theoretical results (e.g., proofs in the appendix) sufficient to guarantee the correctness of the edge strength formulation? 4. How does the method handle computational complexity, particularly with large-scale DAGs where GP inference may be prohibitive? 5. How is the incorporation of expert knowledge (e.g., topological orderings) implemented in practice, and what metrics were used to assess its impact on performance?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "1durmugh3I": {
    "paper_id": "1durmugh3I",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes a knowledge distillation approach to create faster, specialized machine learning force fields (MLFFs) by aligning the energy Hessians of smaller 'student' models with those of larger 'teacher' foundation models. The method aims to preserve physical constraints like energy conservation while significantly improving inference speed, demonstrated across multiple MLFF architectures and chemical subsets."
          },
          "strengths": {
            "value": "The work addresses a critical gap in MLFF efficiency and physical consistency, which is highly relevant to computational chemistry. The use of energy Hessians as a distillation target is novel and well-motivated, as Hessians encode curvature information crucial for molecular dynamics stability. The architecture-agnostic design allows broad applicability, and the experiments span multiple foundation models and tasks, showcasing practical benefits. The paper also highlights a paradigm shift toward releasing specialized 'engines' alongside foundation models, which could influence future MLFF development."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough comparison with alternative distillation methods (e.g., feature-based or output-based KD), making it hard to assess the relative efficacy of Hessian distillation. The theoretical justification for why Hessian alignment improves performance and energy conservation is underdeveloped. Additionally, the implementation details of Hessian precomputation and sampling techniques are not sufficiently described, which limits reproducibility. The experimental results focus on speed and accuracy but do not analyze the impact of distillation on generalization to out-of-distribution chemical systems."
          },
          "questions": {
            "value": "1. How does Hessian distillation compare to feature-based KD in terms of performance and computational overhead? 2. What are the limitations of this approach for chemical systems with highly non-convex potential energy surfaces? 3. Can the method be extended to preserve other physical constraints (e.g., momentum conservation) beyond energy conservation? 4. How sensitive is the performance to the choice of Hessian sampling strategies or subsets used for distillation?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes a knowledge distillation approach to create fast, specialized machine learning force fields (MLFFs) by aligning the energy Hessians of a general-purpose foundation model (teacher) with those of a smaller, specialized model (student). The method aims to improve inference speed while preserving physical constraints like energy conservation, demonstrated through experiments on multiple MLFF foundation models and chemical subsets."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in MLFFs: balancing model size, efficiency, and physical consistency. The use of energy Hessians for distillation is novel and aligns with physical constraints, offering a unique approach compared to existing methods. The architecture-agnostic nature of the method enhances its versatility. Strong experimental validation across multiple models, datasets, and tasks demonstrates significant speedups (up to 20×) and improved performance over undistilled models, with results that are broadly applicable to downstream applications like MD simulations."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of the computational cost of precomputing Hessians for the teacher model, which could be a bottleneck for large-scale applications. The generalizability of the specialized models to chemical spaces beyond the tested subsets is not thoroughly discussed. Additionally, the paper does not compare the proposed Hessian-based distillation with alternative distillation strategies (e.g., feature alignment) in depth, leaving questions about its relative effectiveness. The experimental results emphasize speed and accuracy but do not provide statistical significance tests or ablation studies to isolate the impact of Hessian distillation."
          },
          "questions": {
            "value": "1. How does the computational cost of Hessian precomputation for the teacher model scale with the size of the training data? 2. Are there limitations to the types of chemical systems or datasets where this method excels, and how are these addressed? 3. What is the exact formulation of the distillation loss ($\\mathcal{L}_{KD}$), and how is it optimized alongside the conventional energy-force loss ($\\mathcal{L}_{EF}$)? 4. How does the method ensure energy conservation during MD simulations when distilling from a teacher with direct force parameterization? 5. Are there cases where distillation degrades performance, and how are such scenarios mitigated?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces a knowledge distillation method to create faster, specialized machine learning force fields (MLFFs) by aligning the energy Hessians of a 'teacher' foundation model (FM) with those of a 'student' model. This approach aims to preserve physical constraints like energy conservation while significantly improving inference speed, demonstrated across multiple MLFF FMs and chemical subsets."
          },
          "strengths": {
            "value": "The paper presents a novel and practical approach to MLFF specialization via Hessian-based distillation, which is architecture-agnostic and leverages precomputed Hessians for efficiency. The experiments are comprehensive, covering diverse models, datasets, and downstream tasks, showing up to 20× speed improvements without sacrificing accuracy. The focus on physical constraints (e.g., energy conservation) for molecular dynamics simulations addresses a critical gap in the field. The work also highlights a promising paradigm for releasing 'simulation engines' alongside foundation models."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons with alternative distillation methods (e.g., feature-based or output-based KD) that could contextualize the Hessian-based approach. The theoretical justification for Hessian alignment as a proxy for physical consistency is underdeveloped. The subset selection for distillation is not thoroughly analyzed, and the impact of this choice on generalization is unclear. Implementation details (e.g., hyperparameters, Hessian computation methods) are sparse, potentially hindering reproducibility. The paper also does not address potential limitations in scalability to larger systems or more complex chemical environments."
          },
          "questions": {
            "value": [
              "How does the Hessian alignment method ensure robustness to noisy or imperfect teacher model Hessians?",
              "What specific criteria were used to select the chemical subsets for distillation, and how do these choices affect the student model's performance on unseen systems?",
              "Are there any cases where the student model's performance degrades compared to the teacher, and what factors might contribute to this?",
              "How does the method handle systems where energy conservation is not strictly enforced by the teacher model?",
              "What is the computational cost of precomputing Hessians for large datasets, and how does this scale with system size?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "1eQT9OzfNQ": {
    "paper_id": "1eQT9OzfNQ",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces Activation Beacon, a module for transformer-based LLMs that compresses long contexts by directly compressing activations (keys and values) instead of using soft prompts. It employs a chunk-based approach with beacon tokens, enabling progressive compression, flexible compression ratios, and efficient computation. The method achieves 2x inference speedup and 8x memory reduction while maintaining performance on long-context tasks."
          },
          "strengths": {
            "value": "Originality: The paper presents a novel approach to context compression by focusing on activation-level compression rather than soft prompts, which addresses a key bottleneck in existing methods. Quality: The method's progressive workflow and dynamic compression ratio during training show potential for efficiency and flexibility. Clarity: The abstract and introduction are well-structured, and the figure provides a clear overview of the approach. Significance: If validated, the method could significantly advance LLM efficiency for long-context tasks."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about the experiments, baselines, and technical implementation incomplete. The claims about maintaining performance on 128K-length tasks lack concrete evidence, as no specific results or comparisons are provided. The mechanism for accumulating and reusing activations is not sufficiently explained, and the impact of compression on model quality (e.g., accuracy, coherence) is not quantified. The paper also does not address potential limitations, such as how beacon tokens handle highly dynamic or multi-modal contexts."
          },
          "questions": {
            "value": [
              "How are beacon tokens integrated into the existing transformer architecture? What is the exact mechanism for accumulating and reusing activations across chunks?",
              "What ablation studies were conducted to validate the contribution of each technical design (e.g., progressive compression, dynamic compression ratio)?",
              "How does the method handle contexts longer than the maximum training length (e.g., 128K tokens)? Are there specific benchmarks or datasets used for evaluation?",
              "What are the trade-offs between compression ratio and model performance? Are there experiments on tasks requiring fine-grained reasoning (e.g., question-answering, code generation)?",
              "How does Activation Beacon compare to existing methods like sparse attention or token deletion in terms of efficiency and effectiveness?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Activation Beacon, a plug-in module for transformer-based LLMs that compresses long contexts by directly encoding activations (keys/values) into beacon tokens. The method partitions input into chunks, compresses fine-grained units with interleaved beacon tokens, and reuses activations across chunks to reduce computational and memory costs while maintaining performance on long-context tasks."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in LLM efficiency with a novel approach that directly compresses activations rather than relying on soft prompts. The progressive compression workflow enables fine-grained handling of long contexts and supports flexible compression ratios. The experiments demonstrate significant efficiency gains (2x speedup, 8x memory reduction) on tasks with contexts far exceeding training lengths, while preserving short-context performance. The method's design principles (e.g., activation reuse, dynamic compression ratio sampling) show clear technical innovation."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with state-of-the-art compression methods like \\cite{chevalier2023context, ge2024context} in terms of information retention or task performance. The mechanism for activation accumulation and reuse is not sufficiently explained, particularly how it avoids information loss during chunked processing. The ablation studies on compression ratios and the impact of beacon token design are missing. The experimental setup for long-context tasks (e.g., Needle-in-a-Haystack) lacks quantitative metrics on task-specific accuracy."
          },
          "questions": {
            "value": "1. How does Activation Beacon's activation-based compression compare to soft token methods in terms of preserving task-specific information? 2. What is the exact mechanism for accumulating and reusing activations across chunks, and how is gradient flow preserved during training? 3. Are there limitations to the compression ratio range (e.g., minimum/maximum beacon tokens) that could affect performance? 4. How does the model handle contexts longer than the maximum training length (e.g., 128K) without additional fine-tuning?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces Activation Beacon, a module for transformer-based LLMs that compresses long contexts by directly manipulating activations (keys and values) rather than using soft prompts. It proposes a chunk-based compression workflow, dynamic compression ratio sampling during training, and leverages activation reuse for efficiency. The method achieves 2x inference speedup and 8x memory reduction while maintaining performance on long-context tasks."
          },
          "strengths": {
            "value": "Originality: The direct activation compression approach differs from existing soft-token-based methods. The progressive chunk-wise workflow with beacon token activation reuse is novel. Quality: The paper presents systematic experiments on challenging long-context tasks (e.g., 128K tokens). Clarity: The core idea is well-explained with a figure. Significance: Addressing efficiency challenges in LLMs is highly relevant."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with state-of-the-art compression methods (e.g., \\cite{ge2024context} or \\cite{jiang2023contextb}) that also use soft tokens. The ablation studies are missing to validate individual components (e.g., activation reuse vs. chunking). The theoretical justification for why activation compression preserves information is underdeveloped. The memory reduction claims need quantitative analysis of KV cache usage. The generalization to other architectures (not just Llama-2/Qwen-2) is unexplored."
          },
          "questions": {
            "value": [
              "How does Activation Beacon compare to soft-token methods in terms of information retention? Are there quantitative metrics (e.g., semantic similarity) to support the 'comparable performance' claim?",
              "What is the exact mechanism for 'accumulating and reusing activations' during chunk processing? How is gradient flow maintained across chunks?",
              "The paper mentions '1B plain corpus and 30K fine-tuning samples' but doesn't specify the training setup (e.g., optimizer, learning rate). How does this scale to larger models?",
              "How does the method handle dynamic compression ratios during inference? Is there a way to adjust the ratio without retraining?",
              "The figure shows beacon tokens interleaved with chunks, but the paper doesn't explain how the model learns to prioritize information during compression."
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "1fC4ytCAgb": {
    "paper_id": "1fC4ytCAgb",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a self-conditioned diffusion (SCD) model for consistent human image and video synthesis, framing the task as a spatially-conditioned inpainting problem. It uses a unified denoising network to maintain appearance consistency by spatially conditioning the reference image, introduces a causal feature interaction framework to preserve fine-grained details, and decomposes the generation process into two stages for efficiency."
          },
          "strengths": {
            "value": "The paper presents a novel approach by eliminating the need for separate networks to extract reference features, instead leveraging a single denoising network. The causal feature interaction mechanism and two-stage decomposition are creative solutions to address domain gaps and efficiency. The method's ability to generalize to unseen identities without per-instance fine-tuning is a significant practical advantage. The work builds on established diffusion model techniques and provides a clear problem formulation."
          },
          "weaknesses": {
            "value": "The experimental validation is limited, with no quantitative metrics or comparisons to state-of-the-art methods. The paper lacks ablation studies to isolate the impact of the causal interaction framework or two-stage decomposition. The claims about 'competitive performance' are unsubstantiated without specific benchmarks. The method's effectiveness in preserving fine-grained details is not thoroughly demonstrated, and the implementation details of the spatial conditioning remain unclear."
          },
          "questions": {
            "value": "1. How is the causal feature interaction framework implemented technically? What architectural modifications were made to the denoising U-Net? 2. What specific metrics (e.g., FID, LPIPS, pose consistency) were used to evaluate appearance consistency? 3. Are there ablation studies showing the contribution of each component (spatial conditioning, causal interaction, two-stage process)? 4. How does the two-stage decomposition affect computational efficiency compared to existing methods? 5. What datasets were used for training and evaluation, and how do they compare to standard benchmarks for human image/video synthesis?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a self-conditioned diffusion model (SCD) for consistent human image and video synthesis, addressing appearance consistency challenges by framing the task as a spatially-conditioned inpainting problem. The method uses a unified denoising network to avoid domain gaps between reference and target features, incorporates a causal feature interaction framework, and decomposes the process into two stages for efficiency."
          },
          "strengths": {
            "value": "The paper presents a novel approach by leveraging a single denoising network for both reference and target features, reducing domain gaps. The causal feature interaction framework is a creative solution to preserve fine-grained appearance details. The decomposition into two stages enhances flexibility and efficiency. The method shows competitive performance without per-instance fine-tuning, demonstrating strong generalization. The work is well-positioned within the diffusion model literature and addresses a critical challenge in human-centric synthesis."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the causal interaction framework's effectiveness. The experimental section is incomplete, with limited quantitative results and missing comparisons to baseline methods. The spatial conditioning implementation details (e.g., how spatial concatenation is applied) are unclear. The method's scalability to complex video sequences or diverse identities is not thoroughly discussed. The claim about 'strong generalization' requires more empirical evidence."
          },
          "questions": {
            "value": "1. How is spatial conditioning implemented technically (e.g., through attention mechanisms or feature concatenation)? 2. What ablation studies were performed to validate the causal interaction framework? 3. Are there quantitative metrics comparing SCD to Reference-Net-based methods? 4. How does the two-stage process affect computational efficiency versus single-stage approaches? 5. What specific challenges remain in handling extreme pose variations or occlusions?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes a self-conditioned diffusion (SCD) model for consistent human image and video synthesis, addressing appearance consistency by framing the task as spatially-conditioned inpainting. It introduces a causal feature interaction framework within a unified denoising network, decomposing the process into reference feature extraction and target generation stages to mitigate domain gaps between reference and target features."
          },
          "strengths": {
            "value": "Originality: The spatial conditioning strategy and causal feature interaction framework are novel approaches to address domain gaps in human-centric synthesis. Quality: The method is well-structured, with clear motivation and technical details. Clarity: The paper is well-written, with logical organization and illustrative figures. Significance: The problem of appearance consistency is critical for applications like visual try-on, and the method's efficiency (single network, no per-instance fine-tuning) adds practical value."
          },
          "weaknesses": {
            "value": "The experimental validation is limited, with no quantitative comparisons to state-of-the-art methods like AnimateAnyone or MagicAnimate. The causal interaction framework's implementation details (e.g., how self-queries are restricted) are vague. The decomposition into two stages is mentioned but not thoroughly justified in terms of computational benefits or flexibility improvements."
          },
          "questions": {
            "value": [
              "Can the authors provide quantitative metrics (e.g., FID, LPIPS) comparing their method to existing approaches for human image/video synthesis?",
              "What specific architectural modifications enable the causal feature interaction framework to prevent domain gaps? Are there ablation studies demonstrating its effectiveness?",
              "How does the two-stage decomposition improve efficiency compared to end-to-end training? Are there runtime or resource usage comparisons?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "1jcnvghayD": {
    "paper_id": "1jcnvghayD",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes a novel approach for Bayesian optimization (BO) by combining variational Bayesian last layer (VBLL) neural networks with Gaussian processes (GPs). The method leverages the strengths of VBLLs (scalability, non-Euclidean correlation handling) and GPs (uncertainty quantification) through a connection between Bayesian conditioning and optimization, enabling efficient online training. The authors claim competitive performance on complex tasks and benchmark problems."
          },
          "strengths": {
            "value": "The paper introduces a creative fusion of VBLLs and GP principles, addressing key limitations of both models. The theoretical connection between Bayesian conditioning and optimization is novel, and the method's efficiency for online BO is promising. The work also highlights practical benefits of VBLLs over traditional BNNs and GPs, particularly in handling non-Euclidean data. The paper's structure and technical depth demonstrate rigorous methodological development."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete, with no detailed results on high-dimensional or non-stationary tasks explicitly mentioned. The paper lacks comparisons to state-of-the-art BO methods (e.g., deep kernel learning, neural processes) and does not provide ablation studies to isolate the contribution of the online training scheme. The theoretical analysis of the VBLL-GP connection is superficial, and the paper does not address potential limitations of the variational approximation. The claimed 'significant outperformance' lacks statistical rigor or error bars."
          },
          "questions": {
            "value": "1. Which specific baselines were compared against (e.g., GP, BNNs, deep kernel learning)? 2. How does the method handle high-dimensional input spaces where GP kernels fail? 3. What is the computational cost of the online training algorithm compared to standard BO approaches? 4. Are the hyperparameters of the VBLL model robust to different problem settings? 5. How does the variational approximation affect uncertainty quantification compared to exact GP conditioning?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes a novel approach for Bayesian optimization (BO) surrogate modeling by combining Gaussian Processes (GPs) and Bayesian Neural Networks (BNNs) through Variational Bayesian Last Layer (VBLL) models. The method leverages the uncertainty quantification of GPs and the scalability of BNNs, with a focus on efficient online training via a connection to GP conditioning. The authors demonstrate competitive performance on tasks with complex correlations and established benchmarks."
          },
          "strengths": {
            "value": "The paper introduces a creative combination of GP and BNN strengths, offering a novel framework for BO surrogate modeling. The theoretical connection between VBLLs and GP conditioning is original and provides a fresh perspective. The experiments show promising results on diverse tasks, including discrete inputs and multi-objective problems. The paper is well-structured, with clear explanations of the methodology and meaningful comparisons to baselines. The contribution addresses a critical challenge in BO: balancing uncertainty quantification with scalability."
          },
          "weaknesses": {
            "value": "The experimental evaluation lacks depth in some areas, such as ablation studies to isolate the impact of specific components (e.g., the variational posterior vs. GP conditioning). The paper does not thoroughly compare against state-of-the-art BNNs or recent GP alternatives, which limits the strength of the claims. The theoretical analysis of the VBLL-GP connection is brief, and the paper could elaborate on how the method handles non-Euclidean or high-dimensional data. Additionally, the scalability to very large datasets is not explicitly addressed."
          },
          "questions": {
            "value": [
              "How does the VBLL model specifically handle non-Euclidean correlation structures compared to other BNNs? Are there quantitative metrics to support this claim?",
              "What are the limitations of the current approach in terms of computational efficiency or memory usage, and how do they scale to high-dimensional problems?",
              "The paper mentions a connection to GP conditioning—can the authors elaborate on the theoretical guarantees or assumptions underlying this relationship?",
              "Are there specific scenarios where the VBLL approach underperforms compared to GPs or other BNNs, and what are the root causes?",
              "How sensitive is the method to hyperparameter choices (e.g., the variational posterior parameters), and what strategies are recommended for tuning them?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Variational Bayesian Last Layer (VBLL) models as a surrogate for Bayesian Optimization (BO), combining the uncertainty quantification of Gaussian Processes (GPs) with the scalability of Bayesian Neural Networks (BNNs). The approach leverages variational inference to enable efficient online training by connecting model conditioning with optimization, demonstrating competitive performance on complex tasks and established benchmarks."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in BO by proposing a novel surrogate model that bridges GPs and BNNs. The connection between Bayesian conditioning and optimization is theoretically sound and practically impactful. The experiments show strong empirical performance on diverse tasks, including multi-objective and discrete input problems. The clarity of the problem statement, methodology, and contributions is excellent, with well-structured sections and logical flow."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with state-of-the-art BNN variants and GP kernels, which limits the assessment of novelty. The variational approximation's impact on uncertainty calibration and predictive accuracy is not thoroughly analyzed. Computational efficiency claims are not supported by detailed runtime or scalability experiments. The theoretical analysis of VBLL's properties (e.g., convergence, robustness) is superficial, and the paper does not address potential limitations in high-dimensional settings."
          },
          "questions": {
            "value": "1. How does the VBLL model handle the trade-off between computational cost and model complexity in high-dimensional BO scenarios? 2. Are there specific problem types where VBLL underperforms compared to GPs or other BNNs, and what are the underlying reasons? 3. How sensitive is the model to hyperparameter choices, and what guidelines are provided for tuning? 4. What is the theoretical justification for the variational lower bound's effectiveness in capturing uncertainty compared to exact Bayesian inference?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "1lB5ErmIY0": {
    "paper_id": "1lB5ErmIY0",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces a taxonomy of disagreement sources in human-labeled preference datasets, identifies that most disagreements stem from individual predilections rather than errors, and proposes methods to model rewards as distributions to better handle diverging preferences in LLM training and evaluation."
          },
          "strengths": {
            "value": "Originality is strong through the development of a novel taxonomy of 10 disagreement categories across four high-level classes, addressing a critical gap in understanding human annotation biases. The quality of experiments is solid, with clear comparisons between standard reward models and the proposed distributional approach. Clarity is good, with detailed tables and examples. The significance is high, as the work directly impacts LLM alignment and evaluation practices, particularly for pluralistic systems."
          },
          "weaknesses": {
            "value": "The taxonomy's novelty is unclear—existing literature on annotation bias (e.g., [Sorensen et al. 2024]) may overlap. The paper lacks thorough baseline comparisons (e.g., against non-majority aggregation methods or alternative reward modeling approaches). The proposed distributional reward model's improvement (0.16 AUROC) is promising but not contextualized against state-of-the-art methods. The analysis of LLM-as-Judge evaluations is superficial, with limited discussion on how their method scales to larger benchmarks. The paper also does not address how to handle cases where disagreements are due to annotation errors versus genuine preferences."
          },
          "questions": {
            "value": "1. How was the taxonomy validated? Are the 10 categories distinct and comprehensive compared to existing frameworks? 2. What are the specific limitations of the proposed distributional reward model? 3. How does the method differentiate between disagreements caused by response style vs. annotation errors? 4. Are the results generalizable to other datasets or tasks beyond the ones tested? 5. What are the computational costs of modeling rewards as distributions compared to standard methods?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates diverging preferences in human-labeled datasets for large language models (LLMs), proposing a taxonomy of disagreement sources and challenging existing reward modeling approaches. The authors analyze two datasets, identify that 30% of examples involve diverging preferences driven by individual predilections rather than errors, and introduce methods to model reward distributions instead of single values to better capture user diversity."
          },
          "strengths": {
            "value": "The paper's originality lies in its systematic taxonomy of disagreement sources (10 categories across 4 classes) and its critical analysis of how existing reward models fail to account for diverging preferences. The experiments are well-structured, demonstrating concrete limitations of standard methods (e.g., Bradley-Terry models) and proposing novel distributional reward modeling. The significance is high, as it addresses a critical gap in pluralistic alignment of LLMs. The clarity of the taxonomy and experimental setup is strong, with detailed examples and metrics (e.g., 0.16 AUROC improvement)."
          },
          "weaknesses": {
            "value": "The paper relies on existing datasets without collecting new ones, which may limit generalizability. The taxonomy's comprehensiveness is not thoroughly validated against alternative categorizations. The proposed methods for reward modeling are incremental rather than transformative, and the experiments lack comparisons to alternative approaches (e.g., uncertainty-aware models). The analysis of LLM-as-Judge methods focuses on specific cases (e.g., refusal behaviors) but does not address broader evaluation scenarios. The paper also lacks ablation studies to isolate the impact of individual components of the proposed methods."
          },
          "questions": {
            "value": "1. How were the MultiPref-Disagreements and HelpSteer2-Disagreements datasets selected? Are they representative of real-world preference diversity? 2. Could the taxonomy be extended to include cultural or demographic factors influencing preferences? 3. How do the proposed distributional reward models handle extreme cases of disagreement (e.g., 50-50 splits)? 4. What are the computational costs of modeling rewards as distributions compared to standard methods? 5. How do the authors address potential biases in the original datasets that might influence their taxonomy?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates diverging preferences in human-labeled datasets for LLM training, proposing a taxonomy of disagreement sources and methods to address their impact on reward modeling and evaluation. The authors analyze two datasets, identify 10 disagreement categories, and demonstrate that most disagreements stem from individual preferences rather than errors. They introduce distributional reward models and techniques to mitigate biased LLM-as-Judge evaluations."
          },
          "strengths": {
            "value": "The paper provides a novel, well-structured taxonomy of disagreement sources (10 categories across 4 classes) that advances understanding of human annotation challenges. The analysis of real-world datasets (MultiPref-Disagreements, HelpSteer2-Disagreements) is thorough, with actionable findings about the prevalence of diverging preferences (30% of examples). The proposed methods (distributional reward modeling, divergence-aware evaluation) show measurable improvements (0.16 AUROC gain) and address critical gaps in pluralistic alignment. The work is methodologically sound and directly relevant to LLM training and evaluation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed justification for the 10-category taxonomy (e.g., why these specific categories over others?). The analysis of 'errors' (14-24% frequency) is superficial, with no discussion of how these differ from other categories. Implementation details for distributional reward modeling are sparse, and the paper doesn't address computational feasibility. The impact of their methods on downstream tasks (e.g., dialogue systems) is not empirically validated. Additionally, the claim that 'most disagreements are individual predilections' requires stronger statistical evidence."
          },
          "questions": {
            "value": [
              "How were the 10 categories in the taxonomy determined? Were they derived from the datasets or based on prior literature?",
              "What specific features distinguish 'errors' (e.g., hallucinations) from other disagreement types like 'task underspecification'?",
              "How does the distributional reward modeling method handle computational complexity compared to standard approaches?",
              "Are the proposed divergence-aware evaluation techniques generalizable across different domains or tasks?",
              "What metrics were used to quantify the '75% individual predilection' claim, and how were edge cases handled?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "1oIXRWK2WO": {
    "paper_id": "1oIXRWK2WO",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a learning-to-optimize framework for mixed-integer non-linear programming (MINLP) by proposing differentiable correction layers to handle integer variables and soft penalty terms for constraints. The approach enables gradient-based training of neural networks to generate feasible integer solutions, demonstrating superior performance on large-scale MINLP instances compared to traditional solvers."
          },
          "strengths": {
            "value": "The paper addresses a novel and important problem domain (MINLPs) in learning-to-optimize, which has been underexplored. The methodological contributions include two differentiable rounding layers and a self-supervised training paradigm, which are technically sound. The experiments show consistent performance across diverse problem classes, and the paper effectively contextualizes its work within related literature. The clarity of the writing and organization of sections is strong."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with state-of-the-art MINLP solvers (e.g., Bonmin, Baron) on standard benchmarks, which weakens the empirical validation. The analysis of the differentiable correction layers' robustness to non-convex constraints is limited. The self-supervised training's dependence on hyperparameters (e.g., penalty weights) is not thoroughly discussed. Additionally, the paper does not address scalability to very large problem sizes beyond the reported experiments."
          },
          "questions": {
            "value": "1. How does the method compare to specialized MINLP solvers like Bonmin or Baron on standard test cases? 2. What is the theoretical guarantee (if any) for the feasibility of solutions generated by the correction layers? 3. How sensitive is the performance to the choice of penalty parameters, and what tuning methodology is recommended? 4. Are there specific problem characteristics (e.g., convexity, sparsity) where the approach fails, and why? 5. Can the framework handle combinatorial constraints beyond simple integer rounding?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a learning-to-optimize framework for mixed-integer non-linear programming (MINLP) by proposing differentiable correction layers for integer rounding and a soft penalty for constraint violation. The approach aims to generate high-quality solutions quickly, particularly for large-scale problems where traditional solvers struggle. The method uses self-supervised learning without labeled data, extending end-to-end optimization to handle discrete variables and non-linear constraints."
          },
          "strengths": {
            "value": "Originality: The paper pioneers the application of learning-to-optimize to MINLP, addressing the challenge of non-differentiable integer variables through novel differentiable correction layers. Quality: The combination of soft penalties and rounding layers is theoretically grounded, with a clear motivation for handling mixed-integer and non-linear constraints. Clarity: The paper is well-structured, with a logical flow from problem formulation to experiments. Significance: Solving MINLPs efficiently has broad real-world applications, and the method’s scalability to large problems could address critical gaps in optimization literature."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficiently detailed. The paper mentions three problem classes but does not specify standard benchmarks or provide comparative results against state-of-the-art MINLP solvers (e.g., Baron, SCIP). The self-supervised training strategy lacks ablation studies or analysis of its robustness to hyperparameters. Additionally, the paper does not address how the correction layers handle non-convex constraints or mixed-integer variables with complex dependencies. The claim of 'extremely fast' solution generation is not quantified against traditional methods."
          },
          "questions": {
            "value": [
              "What specific problem instances or benchmarks were used in the experiments? Are they standard MINLP benchmarks (e.g., MINLPLib)?",
              "How does the method handle non-convex constraints? Are there cases where the soft penalty fails to ensure feasibility?",
              "What is the computational overhead of the differentiable correction layers compared to baseline methods?",
              "How does the self-supervised approach compare to supervised learning in terms of solution quality and training efficiency?",
              "Are the correction layers generalizable to other types of integer variables (e.g., binary vs. general integers)?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a learning-to-optimize framework for solving mixed-integer non-linear programs (MINLPs) by proposing two differentiable correction layers for rounding neural network outputs into integer solutions. The approach combines these layers with a soft penalty for constraint violations, enabling gradient-based optimization for problems with integer variables and non-linear constraints. The method is self-supervised, avoiding the need for labeled data, and demonstrates fast solution generation for large-scale MINLPs."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in learning-to-optimize by extending it to MINLPs, a domain with limited prior work. The differentiable correction layers are a novel contribution, enabling gradient flow through integer decisions, which is a major challenge. The self-supervised training paradigm reduces reliance on labeled data, enhancing scalability. The experimental evaluation on diverse problem classes shows promise, particularly for large-scale instances where traditional solvers struggle. The writing is clear, and the paper contextualizes its contributions well within related work."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies on the effectiveness of the differentiable correction layers across varying problem types (e.g., convex vs. non-convex). The experimental comparisons are limited, with no direct benchmarks against state-of-the-art MINLP solvers or prior learning-based methods for mixed-integer problems. The claims about 'high-quality solutions' are not quantitatively supported with metrics like optimality gaps or runtime comparisons. Additionally, the theoretical analysis of the correction layers' convergence properties or robustness to non-convexity is insufficient."
          },
          "questions": {
            "value": [
              "How do the differentiable correction layers perform on non-convex MINLPs compared to convex ones? Are there specific problem characteristics where the method fails?",
              "What are the exact benchmarks used for evaluation, and how do they compare to traditional solvers like BARON or SCIP in terms of solution quality and runtime?",
              "Are there ablation studies demonstrating the necessity of the two correction layers? How sensitive is the method to hyperparameters like the penalty weight?",
              "How does the self-supervised training strategy handle cases where the initial neural network predictions are far from feasible regions?",
              "What is the computational overhead of the correction layers compared to standard gradient-based optimization, and how does this scale with problem size?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "1qGkuxI9UX": {
    "paper_id": "1qGkuxI9UX",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces DITTO, a method for aligning language models (LLMs) using a small number of user demonstrations. DITTO constructs preference data by comparing LLM outputs to demonstrations and intermediate checkpoints, then uses preference optimization (e.g., DPO) to refine the model. The approach is evaluated on author-specific writing tasks and a user study, showing improvements over few-shot prompting, SFT, and self-play methods."
          },
          "strengths": {
            "value": "Originality is strong through the direct use of demonstrations for preference learning, avoiding large datasets. The method's efficiency with <10 demonstrations is novel. Theoretical connections to online imitation learning add depth. Experiments include both controlled benchmarks and a user study, demonstrating practical applicability. Clear motivation for addressing the mismatch between generic LLM outputs and user-specific needs."
          },
          "weaknesses": {
            "value": "The user study has a very small sample size (N=16), limiting statistical validity. Theoretical analysis in Section 3 is brief and lacks concrete proof. Comparisons to related work (e.g., GPO, self-play) are superficial. The paper does not address scalability with more demonstrations or robustness to noisy/ambiguous inputs. The claim about extrapolating beyond demonstrator performance is not empirically validated. Missing details about hyperparameter tuning and baseline implementations."
          },
          "questions": {
            "value": [
              "How were participants selected for the user study, and what criteria ensured the validity of their demonstrations?",
              "What statistical tests were used to confirm the significance of win-rate improvements (e.g., 19% points)?",
              "How does DITTO handle conflicting or inconsistent demonstrations from users?",
              "What is the theoretical basis for the extrapolation claim in Section 3, and how was it validated?",
              "How does the method scale when given more than 10 demonstrations, and what are the diminishing returns?",
              "Were baselines (e.g., SFT, few-shot prompting) implemented with optimal hyperparameters, or were they evaluated with default settings?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces DITTO, a method for aligning language models (LLMs) to specific user or task settings using a small number of demonstrations (<10) as feedback. DITTO generates preference comparison data by treating user demonstrations as preferred over model outputs and intermediate checkpoints, then trains the model via preference optimization (e.g., DPO). The approach is evaluated on author-specific writing tasks and a user study, showing significant improvements over few-shot prompting, SFT, and self-play methods."
          },
          "strengths": {
            "value": "Originality: DITTO's use of direct demonstrations for alignment, rather than prompts or pairwise preferences, is novel and addresses a critical gap in efficient LLM customization. The connection to online imitation learning provides theoretical grounding. Quality: Experiments include diverse benchmarks (news, emails, blogs) and a user study (N=16), with strong empirical results (e.g., 19% average win-rate improvement). Clarity: The method is well-explained, with clear figures and a logical flow from problem statement to evaluation. Significance: The work enables practical, low-data alignment for personalized applications, which is highly relevant for real-world LLM deployment."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of DITTO's limitations, such as performance with noisy or ambiguous demonstrations. The user study's small sample size (N=16) may limit generalizability. While DITTO outperforms baselines, it is unclear how it scales with larger demonstration sets or more complex tasks. The comparison to SFT/RLHF is primarily quantitative; qualitative case studies could strengthen claims about style/task alignment. The theoretical analysis of extrapolation beyond demonstrator performance is brief and lacks empirical validation."
          },
          "questions": {
            "value": "1. How does DITTO handle tasks requiring nuanced trade-offs (e.g., balancing formality and creativity in emails) when demonstrations are limited? 2. What is the impact of demonstration quality on DITTO's performance, and how robust is the method to noisy or inconsistent user inputs? 3. Are there specific task domains where DITTO's approach might underperform compared to traditional SFT or RLHF, and why? 4. Can the theoretical claim about extrapolation beyond demonstrator performance be validated with additional experiments?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces DITTO, a method for aligning language models (LLMs) to specific user preferences using a small number of demonstrations (<10). By treating user demonstrations as preferred over model outputs and intermediate checkpoints, DITTO generates pairwise preference data for optimization (e.g., DPO). The approach is evaluated on author-specific writing tasks and a user study, showing significant improvements over few-shot prompting, SFT, and self-play methods."
          },
          "strengths": {
            "value": "Originality: DITTO's use of direct demonstrations for alignment, rather than prompts or pairwise preferences, represents a novel approach. The connection to online imitation learning provides theoretical grounding. Quality: Experiments include diverse tasks (emails, articles) and a user study (N=16), with clear performance metrics (e.g., 19% avg. win-rate improvement). Clarity: The method is well-explained with figures and pseudocode. Significance: Reducing the data requirements for LLM alignment addresses a critical practical challenge."
          },
          "weaknesses": {
            "value": "The paper lacks ablation studies to isolate the impact of key components (e.g., intermodel comparisons vs. demonstration quality). The user study sample size (N=16) is small, and results may not generalize. Theoretical analysis of extrapolation beyond demonstrator performance is limited to a single section without empirical validation. Comparisons to GPO (Zhao et al. 2023) are superficial, despite GPO's similar focus on few-shot alignment."
          },
          "questions": {
            "value": "1. How does DITTO handle noisy or inconsistent demonstrations? 2. What ablation results are available for the intermodel comparison component? 3. Could the 19% win-rate improvement be attributed to specific aspects of the demonstration collection process? 4. How does DITTO compare to GPO in terms of pre-defined preference groups? 5. Are there cases where DITTO fails, and what are the failure modes?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "1qP3lsatCR": {
    "paper_id": "1qP3lsatCR",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes NetMoE, a method to accelerate MoE training by dynamically rearranging training samples based on expert routing locality to reduce All-to-All communication costs. It formulates the sample placement problem as an integer programming task and claims a 1.67× speedup on 32 GPUs."
          },
          "strengths": {
            "value": "The paper addresses an important problem in distributed MoE training and introduces a novel perspective by leveraging data locality. The motivation is well-justified, and the approach shows potential for improving communication efficiency. The experimental results demonstrate significant speedup, suggesting practical value. The paper also provides clear diagrams to illustrate the communication patterns."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparison with existing methods that might already address similar issues (e.g., dynamic routing optimizations or communication-aware scheduling). The integer programming formulation is not explained thoroughly, and it's unclear how the polynomial-time solution avoids NP-hard complexity. The experimental evaluation is limited (e.g., no ablation studies, no analysis of scaling behavior beyond 32 GPUs). The theoretical guarantees for the proposed algorithm are missing, and the claim of 'optimal placement' requires stronger justification."
          },
          "questions": {
            "value": "1. How does NetMoE compare to existing communication-optimized MoE frameworks like Switch Transformers or DeepSpeed? 2. What specific assumptions enable the polynomial-time solution for the integer programming problem? 3. Are there any cases where dynamic sample placement could increase communication costs? 4. How does the overhead of sample reordering affect overall training time? 5. What is the exact definition of 'data locality' in expert routing, and how is it measured?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces NetMoE, a method to accelerate Mixture of Experts (MoE) training by dynamically adjusting sample placement to reduce All-to-All communication costs. The approach leverages data locality in expert routing and network locality across devices, formulating the problem as an integer programming task to find optimal sample placements. Experiments on 32 GPUs demonstrate a 1.67× speedup compared to existing frameworks."
          },
          "strengths": {
            "value": "The paper presents a novel approach by combining data locality (expert routing patterns) and network locality (high/low-speed communication channels) to address a critical bottleneck in MoE training. The technical contribution of modeling dynamic sample placement as a combinatorial optimization problem is theoretically sound. The experiments show significant efficiency gains, and the paper includes illustrative figures to clarify the motivation and mechanism. The problem statement is well-motivated, and the connection to prior work on expert parallelism and communication optimization is clear."
          },
          "weaknesses": {
            "value": "The paper lacks detailed exposition of how the integer programming problem is solved in polynomial time, which is critical for assessing the feasibility of the approach. The scalability of NetMoE to larger GPU counts or heterogeneous network topologies is not discussed. The experiments focus on a single hardware configuration (32 GPUs) and do not compare against alternative communication optimization strategies (e.g., adaptive routing or hybrid parallelism). Additionally, the overhead of the dynamic sample placement algorithm itself (e.g., computation, memory) is not quantified, which could offset the communication savings in practice."
          },
          "questions": {
            "value": [
              "How is the integer programming formulation translated into a polynomial-time algorithm? What approximations or constraints are applied to ensure tractability?",
              "What is the computational overhead of the dynamic sample placement algorithm, and how does it scale with the number of GPUs or model size?",
              "How does NetMoE handle varying expert routing patterns across different layers or tasks? Is the method generic or task-specific?",
              "Are there any trade-offs between communication efficiency and model accuracy when adjusting sample placements?",
              "How does NetMoE compare to other MoE training frameworks that optimize communication (e.g., using pipelining or hybrid parallelism)?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces NetMoE, a method to accelerate MoE training by dynamically adjusting sample placement based on token locality in expert routing and network topology. The approach models the problem as an integer programming task to minimize All-to-All communication costs, achieving a 1.67× speedup on 32 GPUs compared to existing frameworks."
          },
          "strengths": {
            "value": "The paper addresses a critical bottleneck in MoE training—All-to-All communication—by combining data locality (token-expert preferences) with network locality (intra-node vs. inter-node speeds), a novel angle not explored in prior work. The technical contribution of formulating dynamic sample placement as a combinatorial optimization problem is significant. The experiments demonstrate measurable efficiency gains, and the motivation is well-aligned with real-world distributed training challenges."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with the abstract and introduction cut off mid-sentence, limiting the ability to assess the full methodology. The integer programming formulation and polynomial-time algorithm details are missing, making it unclear how scalability and computational overhead are managed. The experiments lack comparison with baseline methods beyond the claimed 1.67× speedup, and the impact of sample placement on model accuracy is unaddressed. The theoretical analysis of the optimization problem's complexity is also absent."
          },
          "questions": {
            "value": "1. What is the exact formulation of the integer programming problem, and how is it guaranteed to be solvable in polynomial time? 2. How does NetMoE handle dynamic changes in routing patterns across layers/iterations? 3. What is the computational overhead of the sample placement adjustment, and how does it compare to the communication savings? 4. Are there scenarios where the locality assumption fails, and how does NetMoE adapt? 5. How does the method scale to larger GPU counts beyond 32? 6. What is the impact of sample reordering on model training stability or convergence?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "1qq1QJKM5q": {
    "paper_id": "1qq1QJKM5q",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces COMET, a deep learning method that induces a modular, sparse architecture with conditionally overlapping experts. COMET replaces trainable gating functions with fixed, biologically inspired random projections, enabling input similarity-dependent expert overlap. This approach aims to improve learning efficiency, generalization, and scalability without increasing parameters or requiring task IDs."
          },
          "strengths": {
            "value": "The paper addresses critical limitations of existing sparse methods, such as representation collapse and rigid expert specialization, through biologically inspired fixed routing. The integration of neuroscience concepts (e.g., lateral inhibition) with deep learning is innovative. The exponential number of experts and input-dependent overlap offer scalability for complex tasks. The experiments demonstrate applicability across diverse architectures (e.g., vision transformers, GPTs) and tasks (classification, regression). The clear problem formulation and connection to biological systems strengthen the theoretical foundation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of key components (e.g., fixed routing vs. input similarity). Comparisons to state-of-the-art sparse methods (e.g., MoE, sparse training) are superficial, with no rigorous quantitative benchmarks. The claim of 'exponential experts' is vague without formal analysis of parameterization. The experiments focus on single-task generalization but do not thoroughly validate the proposed benefits for multi-task settings (e.g., transfer learning). The fixed routing mechanism's robustness to hyperparameters (e.g., k-winner-take-all) is not explored."
          },
          "questions": {
            "value": "1. How does COMET's fixed routing compare to other non-trainable methods (e.g., fixed sparsity patterns)? 2. What is the exact mechanism enabling input similarity-based overlap, and how is it validated? 3. Are there scenarios where fixed routing fails compared to adaptive gating? 4. How is the k-winner-take-all implemented, and what is its computational overhead? 5. What ablation studies confirm the necessity of biologically inspired components?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces COMET, a deep learning method that induces a modular, sparse architecture using fixed, biologically inspired random projections instead of trainable gating functions. COMET enables overlapping experts based on input similarity, aiming to improve learning efficiency and generalization without increasing trainable parameters or requiring task IDs."
          },
          "strengths": {
            "value": "Originality is strong, as COMET's biologically inspired fixed routing mechanism and input-dependent expert overlap address key limitations of existing sparse methods. The paper demonstrates applicability across diverse tasks (image classification, language modeling, regression) and architectures, suggesting broad utility. Clarity is generally good, with clear motivation and technical exposition. Significance is high, as sparsity remains a critical challenge in scaling neural networks while maintaining efficiency and robustness."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of why fixed routing outperforms trainable gates, particularly in terms of theoretical guarantees or empirical ablation studies. The claim of 'exponential number of experts' is not rigorously justified or quantified. Experimental comparisons with state-of-the-art sparse methods (e.g., MoE variants) are limited, and the paper does not address potential scalability issues with the proposed approach. Additionally, the discussion of transfer learning and continual learning remains speculative without concrete results."
          },
          "questions": {
            "value": "1. How does COMET's fixed routing mechanism specifically mitigate representation collapse compared to trainable gates? 2. What is the exact computational complexity of the k-winner-take-all operation, and how does it scale with large models? 3. Are there scenarios where input similarity-based expert overlap could lead to suboptimal performance? 4. How does the paper reconcile the claim of 'no need for task IDs' with the potential reliance on input structure for effective routing?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces COMET, a deep learning method that induces a modular, sparse architecture using biologically inspired fixed routing. Instead of trainable gating functions, COMET employs fixed random projections and k-winner-take-all mechanisms to create conditionally overlapping experts, aiming to improve learning efficiency and generalization without increasing parameters or requiring task IDs."
          },
          "strengths": {
            "value": "COMET's key strengths lie in its originality, drawing inspiration from neuroscience to address limitations in existing sparse neural networks. The method's design avoids trainable gates, which could mitigate issues like representation collapse and forgetting. The paper's broad applicability across tasks (image classification, language modeling, regression) and architectures (vision transformers, GPTs) demonstrates its generalizability. The integration of concepts from sparsity, modularity, and continual learning shows a thoughtful synthesis of prior work."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation due to truncation, making it impossible to assess COMET's actual performance improvements. The claim of 'exponential number of experts' is vague without technical details on how this is achieved. The biological analogy (e.g., lateral inhibition) is promising but not rigorously linked to the method's mechanics. The absence of ablation studies or comparisons to state-of-the-art sparse methods weakens the contribution's impact. Additionally, the paper does not address potential limitations, such as computational overhead from fixed projections or scalability to very large models."
          },
          "questions": {
            "value": "1. How is the fixed random projection implemented, and what is its relationship to input similarity? 2. What is the exact mechanism for k-winner-take-all, and how does it avoid overfitting? 3. Are there ablation studies showing the contribution of fixed routing vs. traditional gating? 4. How does COMET handle tasks requiring explicit task IDs, which the paper claims to avoid? 5. What are the specific performance metrics (e.g., accuracy, FLOPs) demonstrating improved generalization? 6. How does the 'exponential number of experts' scale with model size, and what are the practical implications?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "204sPiwBbB": {
    "paper_id": "204sPiwBbB",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a novel finetuning method called Training with Annotations (TWA) that leverages span-level error annotations from the MQM dataset to improve machine translation models. TWA distinguishes between error spans (which are penalized) and non-error spans (which are treated as positive signals), aiming to enhance model quality through fine-grained error feedback."
          },
          "strengths": {
            "value": "The paper presents a clear and innovative approach by utilizing span-level annotations, which are more granular than traditional sequence-level labels. The method is well-motivated, with a logical structure that connects the problem of error localization to model training. The experiments on two translation tasks demonstrate practical improvements over baselines like DPO and supervised finetuning. The paper also contextualizes its work effectively within related fields, such as RLHF and automated metrics."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with significant sections (e.g., the full method description, detailed experiments, and results) missing. This makes it difficult to assess the technical depth and rigor of TWA. The claims about superiority over baselines lack statistical significance testing or detailed quantitative analysis. Additionally, the paper does not address potential limitations, such as the dependency on high-quality span annotations or the generalizability of TWA to other tasks beyond MT."
          },
          "questions": {
            "value": [
              "How does TWA handle different types of errors (e.g., fluency vs. accuracy) within span annotations? Are there ablation studies to validate the effectiveness of specific components of TWA?",
              "What is the computational cost of TWA compared to baselines, and how does it scale with larger datasets?",
              "Are the span-level annotations in MQM inherently noisy, and how does TWA mitigate this? Does the method rely on the quality of the annotations for its performance?",
              "How does TWA differ from existing methods like FUDGE or FG-RLHF in terms of training objectives and implementation? Are there theoretical guarantees for its convergence or stability?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces Training with Annotations (TWA), a fine-tuning method for machine translation models that leverages span-level error annotations from the MQM dataset. TWA differentiates between error spans (using a weighted unlikelihood loss) and non-error spans (using cross-entropy loss), while considering sequence trajectories to identify positive signals. Experiments show improvements over sequence-level baselines like DPO and supervised finetuning."
          },
          "strengths": {
            "value": "Originality: TWA is the first work to directly use span-level MQM annotations for training MT models, addressing a gap in prior methods that relied on sequence-level labels. Quality: The approach is conceptually simple yet effective, with clear design choices for handling error/non-error spans. Clarity: The method is well-explained with a figure, and the paper follows a logical structure. Significance: Span-level annotations offer more granular feedback than sequence-level labels, which could generalize to other NLP tasks requiring fine-grained supervision."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental analysis. It does not report specific metrics (e.g., BLEU scores) or ablation studies to validate TWA's components. The comparison with baselines is superficial, omitting critical details like hyperparameter settings or training durations. The MQM dataset's preprocessing steps (e.g., handling severity scores, error categories) are not described, which could impact reproducibility. Additionally, the paper does not address how TWA scales to larger datasets or different domains."
          },
          "questions": {
            "value": [
              "How were the MQM span-level annotations preprocessed? Were severity scores or error categories incorporated into the loss function?",
              "What ablation studies were conducted to isolate the impact of the weighted unlikelihood loss versus cross-entropy on non-error spans?",
              "How does TWA handle overlapping or conflicting annotations within a single translation?",
              "What is the computational overhead of TWA compared to standard fine-tuning? Were there any trade-offs in training time or resource usage?",
              "Could TWA be adapted to other tasks beyond machine translation that involve fine-grained annotations?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces Training with Annotations (TWA), a finetuning method for machine translation models that leverages span-level error annotations from the MQM dataset. TWA differentiates between error spans (which are penalized) and non-error spans (which are treated as positive signals), aiming to improve model quality by incorporating fine-grained error information."
          },
          "strengths": {
            "value": "The paper presents a novel approach to leveraging fine-grained span-level annotations, which is underexplored in machine translation. The method's simplicity and clear motivation for using span-level information over sequence-level labels are strengths. The experimental setup on standard MT tasks (English-German, Chinese-English) and comparison against relevant baselines (e.g., DPO, supervised finetuning) demonstrate practical relevance. The clarity of the method description, supported by a figure, enhances readability. The significance lies in addressing a gap in utilizing detailed error annotations for model improvement."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about the TWA algorithm's implementation (e.g., how the weighted span-level unlikelihood loss is computed) and experimental results incomplete. The comparison with existing methods like FG-RLHF and TNT lacks depth, such as how TWA addresses their limitations. The paper does not discuss ablation studies or the impact of different error severity categories. Additionally, the integration of MQM scores into the loss function is not explicitly explained, which is crucial for reproducibility."
          },
          "questions": {
            "value": [
              "How exactly is the weighted span-level unlikelihood loss calculated? What hyperparameters or mechanisms determine the penalty for error spans versus non-error spans?",
              "Are there ablation studies to validate the contribution of specific components of TWA (e.g., the trajectory-aware positive signal selection)?",
              "How does TWA handle varying error severities (e.g., major vs. minor errors) in the MQM annotations? Is there a mechanism to prioritize certain error types?",
              "What are the exact metrics used to evaluate TWA's performance (e.g., BLEU, TER)? How do the results compare to state-of-the-art MT models?",
              "Could TWA be combined with existing methods like MBR decoding or QE finetuning, and what synergies might arise from such combinations?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "246rHKUnnf": {
    "paper_id": "246rHKUnnf",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces EXPLOREToM, a framework for generating adversarial theory of mind (ToM) data using A* search over a domain-specific language. This approach creates complex, diverse stories to challenge large language models (LLMs), revealing their limitations in social reasoning. The authors demonstrate that state-of-the-art models like Llama-3.1-70B and GPT-4o struggle with their data, while fine-tuning on EXPLOREToM improves performance on the ToMi benchmark by 27 points."
          },
          "strengths": {
            "value": "The paper's originality lies in its adversarial data generation approach, combining A* search with a domain-specific language to create challenging ToM scenarios. The quality of experiments is strong, with clear evaluation of model performance on both generated and existing benchmarks. The clarity of the framework's design and its practical implications for improving LLMs are well-articulated. The significance is high, as ToM reasoning is critical for social intelligence, and the work addresses a key gap in robust evaluation of LLM capabilities."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of specific components (e.g., A* search vs. domain-specific language). The evaluation focuses on a narrow set of models (Llama-3.1-70B, GPT-4o), leaving open questions about generalizability. The domain-specific language's flexibility and potential for broader application are not thoroughly discussed. Additionally, the mechanism for generating ground-truth answers in the mental-state tracker requires more technical detail."
          },
          "questions": {
            "value": "How does the domain-specific language ensure generalizability across different ToM scenarios? What is the exact role of A* search in navigating narrative spaces, and how does it differ from other search-based methods? How are the automatically generated questions validated for accuracy and coverage of ToM aspects? Are there specific story structures (e.g., knowledge asymmetry) that consistently challenge models, and how are these identified?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces EXPLOREToM, a framework for generating adversarial theory of mind (ToM) data using A* search over a domain-specific language. The approach creates complex, diverse stories to stress-test LLMs, demonstrating that state-of-the-art models like GPT-4o and Llama-3.1-70B struggle with the generated data. The method also enables fine-tuning improvements on benchmarks and reveals insights into LLMs' limitations in state tracking and reasoning."
          },
          "strengths": {
            "value": "Originality lies in the adversarial data generation approach using A* search and domain-specific languages, addressing gaps in existing ToM benchmarks. The quality of experiments is strong, with empirical results showing significant model performance drops and fine-tuning gains. Clarity is maintained through structured sections, examples, and figures. The significance is high, as ToM is critical for social intelligence in AI, and the paper directly tackles limitations in evaluation methods."
          },
          "weaknesses": {
            "value": "The paper lacks ablation studies to isolate the impact of key components (e.g., A* search vs. domain-specific language). The claim that EXPLOREToM is a 'conceptual superset' of prior work is not rigorously justified. The evaluation on ToMi benchmark focuses on accuracy improvements but does not analyze why specific aspects of the data (e.g., knowledge asymmetry) drive these gains. Computational costs of the A* search are not discussed, which could limit practicality."
          },
          "questions": {
            "value": "How does the domain-specific language differ from prior approaches in generating ToM scenarios? What specific features of the A* search contribute most to adversarial effectiveness? Are there limitations in the types of stories generated (e.g., cultural or contextual biases)? How are the generated stories validated for plausibility and diversity beyond qualitative examples?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces EXPLOREToM, a framework for generating adversarial theory of mind (ToM) data using A* search over a domain-specific language. It aims to address limitations in existing benchmarks by creating complex, diverse stories that stress-test LLMs' social reasoning capabilities. The approach generates synthetic narratives with controlled mental state tracking, demonstrating significant challenges for state-of-the-art models and showing improvements when fine-tuning on their data."
          },
          "strengths": {
            "value": "Originality is strong, as the adversarial generation approach with A* search over domain-specific languages is novel for ToM evaluation. The methodology combines domain-specific language design with LLM-based story generation, offering a structured way to control narrative complexity. Experiments show meaningful improvements (27-point gain on ToMi) and reveal insights into model failures (e.g., state tracking issues). The paper is well-structured with clear examples and addresses a critical gap in robust ToM evaluation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparison with alternative adversarial generation methods or prior benchmarks. The domain-specific language and A* search mechanics are under-specified, making reproducibility challenging. While the 27-point improvement on ToMi is impressive, the paper does not compare against other fine-tuning baselines or evaluate on additional ToM benchmarks. The analysis of model shortcomings (e.g., state tracking) is cursory, with no ablation studies or quantitative breakdowns of failure modes. The paper also omits limitations of EXPLOREToM itself."
          },
          "questions": {
            "value": "1. How is the domain-specific language defined, and what are its key features that enable complex story generation? 2. What specific aspects of EXPLOREToM-generated data make it more challenging than prior benchmarks (e.g., knowledge asymmetry, state tracking complexity)? 3. Are there ablation studies showing the contribution of A* search vs. other components? 4. How does the framework ensure diversity in generated stories, and what metrics are used to quantify this? 5. What is the exact mechanism for tracking mental states during story generation, and how is ground-truth derived?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "25j2ZEgwTj": {
    "paper_id": "25j2ZEgwTj",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper analyzes the dynamics of gradient descent (GD) for training two-layer ReLU neural networks to learn multiple teacher neurons from Gaussian inputs. The authors propose a three-phase framework (alignment, tangential growth, local convergence) to show global convergence at O(T^{-3}) and reveal an implicit bias toward balanced l2-norm solutions. They extend prior work on single-neuron and exact-parameterization settings by addressing interactions between multiple teachers and students with m, k = O(1)."
          },
          "strengths": {
            "value": "The paper introduces a novel three-phase analysis of GD dynamics for multi-neuron settings, which generalizes prior single-neuron studies. The technical contributions include refined alignment analysis and a new dynamical system approach for tangential components. The global convergence rate of O(T^{-3}) and the implicit bias toward balanced l2-norm solutions are significant theoretical advances. The work addresses a critical gap in understanding feature balancing and implicit bias in neural networks."
          },
          "weaknesses": {
            "value": "The theoretical analysis relies on restrictive assumptions (e.g., orthogonal teacher neurons, same norm, small initialization), which limit practical applicability. The paper lacks empirical validation to support its claims. The convergence rate's optimality is not discussed, and the handling of non-orthogonal teacher neurons is not addressed. The connection between the balanced l2-norm solution and generalization performance remains unclear. The comparison to existing methods is limited, and the practical implications of the findings are underexplored."
          },
          "questions": {
            "value": "1. Are there experiments demonstrating the proposed convergence rates and balanced l2-norm behavior? 2. How sensitive are the results to the assumption of orthogonal teacher neurons? 3. Can the analysis be extended to non-orthogonal or non-identical teacher neurons? 4. What is the exact relationship between the balanced l2-norm and generalization? 5. How do the theoretical sample/time complexities compare to practical training scenarios?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper analyzes the learning dynamics of gradient descent (GD) in two-layer ReLU neural networks, focusing on how GD balances student neurons to achieve an implicit bias toward a minimum 'balanced' l2-norm solution. The authors propose a three-phase analysis (alignment after weak recovery, tangential growth, and local convergence) and prove global convergence at a rate of O(T^{-3}) for the excess risk. They extend prior work on exact-parameterization and single-neuron settings by addressing interactions between multiple teacher and student neurons through refined alignment analysis and a new dynamic system approach."
          },
          "strengths": {
            "value": "The paper introduces a novel three-phase dynamical analysis framework for understanding GD training in two-layer ReLU networks, which is both creative and structurally insightful. The technical contributions include refined alignment analysis for phase 1 and a new dynamical system analysis for tangential components in phase 2, addressing a key challenge in multi-teacher scenarios. The work extends prior results on exact-parameterization and single-neuron settings, offering broader applicability. The clarity of the problem formulation and theoretical exposition is strong, with a clear motivation for studying implicit bias and training dynamics. The significance of understanding GD's convergence and implicit bias in neural networks is high, making this a valuable contribution to the field."
          },
          "weaknesses": {
            "value": "The analysis relies on restrictive assumptions, such as orthogonal teacher neurons of equal length, which may limit the practical relevance of the results. The convergence rate of O(T^{-3}) is theoretically interesting but the sample/time complexity is noted as non-optimal, raising questions about scalability. The paper lacks empirical validation to corroborate the theoretical claims, which is a critical gap given the complexity of the analysis. Additionally, the informal theorem statement omits precise conditions and constants, making it difficult to assess the exact scope of the results. The handling of non-orthogonal teacher neurons or imbalanced initialization scenarios is not discussed, which could affect the generality of the findings."
          },
          "questions": {
            "value": "1. How do the assumptions about orthogonal teacher neurons and equal lengths impact the practical applicability of the results? Can the analysis be extended to non-orthogonal or heterogeneous teacher configurations? 2. What are the precise constants and dependencies in the convergence rate O(T^{-3}), and how do they scale with parameters like m, k, and d? 3. Are there empirical experiments or simulations provided to validate the theoretical claims, particularly the 'balanced' l2-norm implicit bias? 4. How sensitive is the analysis to the choice of initialization (e.g., σ = o(poly(d^{-1/2}))) and step-size (η = o(1))? 5. Could the dynamical system approach in phase 2 be generalized to deeper networks or other activation functions?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper analyzes the dynamics of gradient descent (GD) for training two-layer ReLU neural networks in a regression setting, focusing on how GD recovers teacher neurons and balances features. The authors propose a three-phase analysis (alignment after weak recovery, tangential growth, local convergence) and prove global convergence at O(T^{-3}) rate, along with an implicit bias toward a 'balanced' l2-norm solution. The work extends prior studies to the case where the number of student and teacher neurons (m, k) are constants."
          },
          "strengths": {
            "value": "The paper's originality lies in extending prior work on exact-parameterization and single-neuron settings to the general case of m, k = O(1), which is a significant theoretical contribution. The three-phase dynamical analysis provides a structured framework for understanding GD dynamics, and the proof techniques (e.g., dynamical system analysis for tangential components) are novel. The clarity of the problem formulation, theoretical results, and logical flow are strong. The significance is high, as the findings advance understanding of implicit bias and training dynamics in neural networks, with potential implications for optimizing architectures."
          },
          "weaknesses": {
            "value": "The analysis relies on strong assumptions, such as orthogonal teacher neurons and Gaussian inputs, which may limit practical applicability. The weak recovery condition, while weaker than strong recovery, still imposes constraints on the initialization and training setup. The paper lacks experimental validation to corroborate the theoretical results, making it difficult to assess practical relevance. Additionally, the sample/time complexity is not optimal, and the focus on constant m, k may not address larger-scale scenarios. The dynamical system analysis in Phase 2 requires further justification for its generality."
          },
          "questions": {
            "value": [
              "How do the assumptions of orthogonal teacher neurons and Gaussian inputs affect the generalizability of the results to real-world data?",
              "Are there experimental results demonstrating the claimed convergence rates and balanced l2-norm solutions in practice?",
              "What are the limitations of the weak recovery condition in non-ideal scenarios, such as noisy or non-Gaussian data?",
              "How does the proposed dynamical system analysis in Phase 2 compare to existing methods in terms of scalability and robustness?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "26oSbRRpEY": {
    "paper_id": "26oSbRRpEY",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper proposes StreamingT2V, an autoregressive text-to-video generation method designed to produce long, consistent videos up to 2 minutes. It introduces two key modules: a short-term memory block (CAM) for chunk transitions and a long-term memory block (APM) to preserve scene consistency. A randomized blending approach enhances video quality across chunks. The method addresses limitations of existing approaches, such as video stagnation and inconsistent transitions."
          },
          "strengths": {
            "value": "The paper presents a novel approach to long video generation by combining short- and long-term memory mechanisms, addressing critical gaps in autoregressive text-to-video models. The CAM and APM modules offer creative solutions to temporal consistency and object preservation, which are significant challenges in the field. The work is well-motivated, with clear problem statements and a structured methodology. The experiments suggest improvements over existing methods, and the potential for scalability (e.g., extending to longer videos) is highlighted. The paper also emphasizes the independence of the method from the base text-to-video model, which is a valuable contribution."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as quantitative metrics (e.g., FID, MS-SSIM, or motion quality scores) to substantiate claims of 'high motion amount' and 'consistency.' The comparison with baselines is limited, and it is unclear how StreamingT2V outperforms existing methods like T2V0, ART-V, or Gen-L. The description of the randomized blending approach is vague, and the paper does not address potential limitations, such as computational costs or scalability to extremely long videos. Additionally, the implementation details of CAM and APM are under-specified, making it difficult to assess their technical novelty."
          },
          "questions": {
            "value": [
              "How are the CAM and APM modules specifically implemented? Are they integrated into the diffusion model architecture, and if so, how do they interact with existing components?",
              "What quantitative metrics were used to evaluate motion quality, consistency, and video stagnation? How do these metrics compare to those in prior work?",
              "Are there ablation studies demonstrating the individual contributions of CAM, APM, and the randomized blending approach?",
              "The paper claims that StreamingT2V is independent of the base text-to-video model. How is this achieved, and what are the implications for training or inference?",
              "How does the randomized blending approach handle overlapping chunks? What is the impact on computational efficiency and video quality?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces StreamingT2V, an autoregressive text-to-video generation method designed to produce long videos (up to 2 minutes) with seamless transitions. It proposes three key components: a Conditional Attention Module (CAM) for short-term consistency, an Appearance Preservation Module (APM) for long-term scene preservation, and a randomized blending approach for video enhancement. The method aims to address issues like video stagnation and inconsistent transitions in existing approaches."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in text-to-video generation: extending short-video models to long durations while maintaining temporal consistency. The proposed CAM and APM modules introduce novel mechanisms for leveraging short-term and long-term dependencies, which could advance autoregressive video generation. The clarity of the problem statement, methodology, and contributions is strong, with a logical flow from motivation to solution. The significance is high, as long video generation has practical applications in storytelling and advertising."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results and quantitative comparisons. Key metrics (e.g., FID, LPIPS, motion quality scores) are not provided, making it difficult to assess the claimed improvements over baselines. The description of the 'randomized blending approach' is vague, and its implementation details are missing. The paper also does not discuss computational efficiency or scalability for very long videos. Additionally, the related work section is incomplete, and the evaluation of the APM's effectiveness against existing memory mechanisms is unclear."
          },
          "questions": {
            "value": "1. What specific metrics were used to quantify 'high motion amount' and 'consistency'? 2. How were the baselines selected, and what are their performance numbers for comparison? 3. Can the authors provide ablation studies demonstrating the individual contributions of CAM, APM, and blending? 4. How does the model handle complex or evolving text instructions over long sequences? 5. What are the computational costs and inference time for generating 2-minute videos?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces StreamingT2V, an autoregressive text-to-video generation method designed to produce long, consistent videos by incorporating short-term (CAM) and long-term (APM) memory modules, along with a randomized blending approach for chunk-wise enhancement. The approach aims to address issues like temporal inconsistency, video stagnation, and object appearance drift in existing methods."
          },
          "strengths": {
            "value": "The paper presents a novel architecture with CAM and APM modules that address specific challenges in long video generation, such as maintaining scene consistency and object preservation. The randomized blending approach for chunk enhancement is a creative solution to ensure seamless transitions. The work is well-motivated by the limitations of existing methods, and the three-fold contributions are clearly articulated. The paper also highlights the potential for scalability with improved base models."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, as the content is cut off mid-experiment section. Critical details about the implementation of CAM/APM (e.g., architectural design, training procedures) are missing, making it hard to assess their effectiveness. The claims of superiority over competitors are not substantiated with quantitative results or comparisons. The randomized blending method is described but not explained in depth, leaving questions about its technical feasibility and impact. Additionally, the related work section is incomplete, which weakens the contextualization of the contribution."
          },
          "questions": {
            "value": [
              "How are the CAM and APM modules implemented? What specific mechanisms ensure scene consistency and appearance preservation?",
              "What metrics were used to quantify 'high motion amount' and 'temporal consistency'? Are there ablation studies demonstrating the individual contributions of CAM/APM?",
              "How does the randomized blending approach work in practice? What is the role of overlapping chunks and how is the blending seamless?",
              "Are there qualitative results (e.g., visual comparisons) to support the claim of 'artifact-free transitions'? How do the results compare to baselines like T2V0, MTVG, or Gen-L?",
              "The paper states that StreamingT2V is 'not limited by the Text2Video model used'—how is this demonstrated? What baseline models were tested?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "27SSnLl85x": {
    "paper_id": "27SSnLl85x",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces Rectified Linear Networks (ReLNs) as an equivalence to ReLU networks through Gated Deep Linear Networks (GDLNs), aiming to derive training dynamics and analyze inductive biases toward structured mixed-selective latent representations in contextual tasks. The authors claim that ReLU networks exhibit a bias for node reuse and structured representations, which strengthens with more contexts and hidden layers."
          },
          "strengths": {
            "value": "The paper presents a novel theoretical framework by establishing an equivalence between ReLU networks and GDLNs, which is significant for understanding feature learning in finite-width networks. The focus on structured mixed selectivity and node reuse addresses a critical gap in deep learning theory. The methodological approach to derive training dynamics and analyze inductive biases is rigorous, and the connection to multi-task learning contexts is relevant. The paper also highlights the importance of nonlinearity in real-world tasks, which is a key contribution."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive experimental validation, as the content is cut off, leaving critical details about the XoR task and contextual experiments unverified. The theoretical claims about uniqueness of equivalence and the role of node reuse require deeper analysis. The paper does not compare against baseline models or existing theories (e.g., NTK or linear networks), weakening the novelty argument. Additionally, the practical implications of 'structured mixed selectivity' are not clearly articulated, and the paper fails to address how these representations improve downstream tasks."
          },
          "questions": {
            "value": "1. How exactly is the ReLN constructed to mimic ReLU networks, and what are the limitations of this equivalence? 2. What specific metrics are used to quantify 'structured mixed selectivity,' and how are they validated empirically? 3. How do the results on the XoR task generalize to more complex, real-world datasets? 4. What ablation studies were conducted to isolate the effects of additional contexts and hidden layers on node reuse? 5. How does the proposed theory differ from existing work on inductive biases in neural networks?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces Rectified Linear Networks (ReLNs), a subset of Gated Deep Linear Networks (GDLNs), to analyze feature learning dynamics in finite ReLU networks. It demonstrates that ReLU networks exhibit an inductive bias toward structured mixed-selective latent representations, which are reused across contexts, and shows how this bias amplifies with more contexts and hidden layers."
          },
          "strengths": {
            "value": "Originality is strong through the novel equivalence between ReLU networks and GDLNs, offering a new lens to study feature learning. The methodology is rigorous, combining theoretical analysis with controlled experiments on tasks like XoR and contextual learning. Clarity is maintained through structured sections and illustrative figures. The significance lies in addressing a critical gap in understanding feature learning in practical ReLU networks, which are widely used in deep learning."
          },
          "weaknesses": {
            "value": "The experimental validation is limited, with only brief mentions of an adapted XoR task and a contextual setting. Key details—such as quantitative results, comparison with baselines, and ablation studies on network depth/contexts—are missing. The theoretical equivalence between ReLU and GDLN requires deeper justification, particularly how ReLNs capture nonlinearity. The paper also lacks discussion on potential limitations of the proposed framework in real-world scenarios."
          },
          "questions": {
            "value": "1. How is the ReLN constructed to exactly replicate ReLU dynamics? What guarantees ensure this equivalence? 2. What specific metrics quantify the 'structured mixed selectivity' in experiments? 3. Are the findings robust to different data distributions or task complexities beyond XoR and the described contextual setup? 4. How does the node-reuse bias interact with other regularization techniques (e.g., dropout, weight decay)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces Rectified Linear Networks (ReLNs) as a theoretical framework derived from Gated Deep Linear Networks (GDLNs) to analyze feature learning dynamics in finite ReLU networks. The authors claim that ReLU networks exhibit an inductive bias toward structured mixed-selective latent representations, which become more pronounced with additional contexts and hidden layers."
          },
          "strengths": {
            "value": "The paper presents a novel theoretical connection between ReLU networks and GDLNs, offering a fresh perspective on feature learning dynamics. The clarity of the problem formulation and the structured organization of the paper are strong. The significance of addressing the gap in finite ReLU network theory is high, and the potential implications for understanding structured representations in deep learning are impactful."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation of its theoretical claims, as the experiments section appears truncated. The equivalence between ReLU and GDLN networks is not thoroughly justified with concrete examples or numerical results. The analysis of 'structured mixed selectivity' remains abstract without quantitative measures or comparisons to baseline models. The claims about inductive bias are speculative without supporting data."
          },
          "questions": {
            "value": [
              "What specific empirical results support the equivalence between ReLU networks and ReLNs? Are there experiments on tasks beyond the adapted XoR task?",
              "How is 'structured mixed selectivity' quantified? Are there metrics to compare the representations learned by ReLNs vs. standard ReLU networks?",
              "The paper mentions contextual tasks but does not describe the experimental setup or results. Can the authors provide details on how the contextual control task was implemented?",
              "How do the theoretical dynamics of ReLNs translate to practical training behavior in real-world datasets?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "293V3bJbmE": {
    "paper_id": "293V3bJbmE",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces HEL MET, a comprehensive benchmark for evaluating long-context language models (LCLMs). The authors highlight limitations in existing benchmarks, such as insufficient context lengths, unreliable metrics, and incompatibility with base models, and address these by creating a diverse set of application-centric tasks with controllable context lengths up to 128K tokens, model-based evaluation, and few-shot prompting. They demonstrate that synthetic tasks like NIAH do not reliably predict downstream performance and reveal disparities between open-source and closed-source models."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel, application-centric benchmark (HEL MET) with seven categories addressing gaps in existing evaluations. It creatively combines controllable context lengths, model-based metrics, and few-shot prompting. Quality: The experiments involve 59 LCLMs, and the analysis of trends across tasks is thorough. Clarity: The abstract, tables, and figures are well-structured, and the motivation for HEL MET is clearly articulated. Significance: The findings about synthetic task limitations and open-source model gaps have practical implications for model development and evaluation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology on how the seven categories were selected or validated. Implementation details for controllable context lengths and model-based evaluation are sparse, making reproducibility challenging. The experimental setup for comparing HEL MET with existing benchmarks (e.g., RULER, ∞BENCH) is not fully explained, and the selection of 59 models is not justified. Additionally, the paper references prior work but may overlook key LCLM evaluation studies."
          },
          "questions": {
            "value": "1. How were the seven application-centric categories for HEL MET determined? Were they validated for representativeness? 2. What specific technical challenges arose in implementing controllable context lengths up to 128K tokens, and how were they resolved? 3. Can the authors elaborate on the model-based evaluation method for QA and summarization, and how it outperforms n-gram metrics? 4. How were the 59 LCLMs selected for the study, and are there potential biases in this sample? 5. Are there comparisons with other recent benchmarks (e.g., LongBench, L-Eval) that could further validate HEL MET's advantages?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces HEL MET, a comprehensive benchmark for evaluating long-context language models (LCLMs). The authors address limitations in existing benchmarks by incorporating diverse application-centric tasks, controllable context lengths up to 128K tokens, model-based evaluation metrics, and few-shot prompting. They demonstrate that HEL MET provides more reliable rankings of LCLMs compared to prior benchmarks like NIAH, RULER, and ∞BENCH."
          },
          "strengths": {
            "value": "The paper's originality lies in its holistic approach to benchmarking LCLMs, combining diverse tasks, extended context lengths, and robust evaluation methods. The methodology is well-structured, with clear improvements over prior work (e.g., reference-based metrics for QA/summarization). The significance is high, as effective evaluation of LCLMs is critical for real-world applications. The paper's clarity is aided by tables, figures, and detailed problem formulations."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of individual HEL MET features (e.g., controllable lengths vs. model-based metrics). The experimental analysis of 59 LCLMs is limited in scope—specifically, it is unclear whether all models were evaluated across all seven categories. Additionally, the claims about HEL MET's superiority over existing benchmarks rely on limited comparisons (e.g., Figure 1 only includes six models). The section on 'model-based evaluation' is underdeveloped, with insufficient explanation of how reference-based metrics were implemented."
          },
          "questions": {
            "value": [
              "How were the controllable context lengths (up to 128K tokens) implemented? Were there technical challenges in extending existing datasets to these lengths?",
              "What specific reference-based metrics were used for QA and summarization, and how do they differ from n-gram overlap metrics?",
              "The paper mentions 59 LCLMs but does not clarify whether all models were tested across all seven HEL MET categories. How were the results aggregated across tasks?",
              "Are there any limitations to the seven categories included in HEL MET? For example, do they cover edge cases or niche applications of LCLMs?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces HEL MET, a comprehensive benchmark for evaluating long-context language models (LCLMs) across seven application-centric categories. It addresses limitations of existing benchmarks by incorporating controllable context lengths (up to 128K tokens), model-based evaluation metrics, and few-shot prompting. The authors demonstrate that synthetic tasks like NIAH are unreliable, open-source models lag behind closed ones in complex tasks, and their benchmark provides more consistent rankings of LCLMs."
          },
          "strengths": {
            "value": "Originality is strong, as HEL MET addresses critical gaps in existing benchmarks by focusing on diverse, real-world applications and controllable context lengths. The quality of experiments is high, with evaluations on 59 LCLMs and detailed comparisons. Clarity is adequate, though some sections could be more explicit. Significance is clear, as effective evaluation of LCLMs is crucial for their practical deployment."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology on how controllable lengths and model-based evaluation were implemented. Claims about the reliability of HEL MET are not fully supported by ablation studies or comparisons with baseline metrics. The analysis of open-source vs. closed-model performance is superficial, with no statistical analysis of the observed gaps. The paper also does not address how HEL MET handles model-specific biases or edge cases."
          },
          "questions": {
            "value": [
              "How exactly was the model-based evaluation implemented for QA and summarization tasks? What specific metrics were used instead of ROUGE?",
              "Are the controllable context lengths up to 128K tokens validated across all tasks, or only in specific cases?",
              "What criteria were used to select the 59 LCLMs for evaluation? Are they representative of the broader LCLM landscape?",
              "How were the 'distinct trends' in HEL MET categories quantified? Are there statistical tests to confirm low correlations between categories?",
              "What is the exact definition of 'full-context reasoning' in the analysis of open-source models? How was this measured?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "2AWZTv6kgV": {
    "paper_id": "2AWZTv6kgV",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces Projected Neural Differential Equations (PNDEs), a method for enforcing hard constraints in neural differential equations (NDEs) by projecting the learned vector field onto the tangent space of a constraint manifold. The approach ensures that solutions remain on the manifold, satisfying constraints, and is evaluated on chaotic systems and power grid models, demonstrating improved performance and stability."
          },
          "strengths": {
            "value": "The paper presents a novel approach to enforcing hard constraints in NDEs, addressing a critical limitation of existing methods that rely on soft constraints. The method's theoretical foundation in differential geometry is solid, and the potential applications to safety-critical systems like power grids are significant. The clarity of the exposition, including the schematic in Figure 1, enhances readability. The focus on numerical stability and generalizability aligns with important challenges in scientific machine learning."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental comparisons with existing methods, such as physics-informed neural networks or other constrained NDE approaches. The implementation of the projection operation is not explicitly described, raising questions about computational feasibility. The theoretical analysis of convergence or error bounds is missing, and the specific constraints tested (e.g., conservation laws vs. holonomic constraints) are not clearly detailed. The claim of requiring 'fewer hyperparameters' is not substantiated with empirical evidence."
          },
          "questions": {
            "value": [
              "How is the projection onto the tangent space computed in practice, especially for non-trivial constraint manifolds?",
              "What are the specific constraints tested in the experiments (e.g., conservation laws, holonomic constraints), and how do they relate to the power grid models?",
              "Are there limitations to the types of constraints that PNDEs can handle, and how does this compare to existing methods?",
              "How does the computational cost of PNDEs compare to standard NDEs or physics-informed methods, particularly for high-dimensional systems?",
              "What ablation studies were conducted to isolate the impact of the projection mechanism on performance?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Projected Neural Differential Equations (PNDEs), a method for enforcing hard constraints in neural differential equations (NDEs) by projecting their vector fields onto the tangent spaces of constraint manifolds. The approach aims to improve the generalizability, numerical stability, and accuracy of NDEs in modeling constrained dynamical systems, particularly in complex domains like chaotic systems and power grids."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in NDEs by proposing a mathematically rigorous method for enforcing hard constraints, which is more reliable than existing 'soft constraint' approaches. The theoretical foundation in differential geometry is solid, and the method's ability to handle arbitrary algebraic constraints without coordinate-specific assumptions is a significant advantage. The motivation for constrained modeling in safety-critical applications (e.g., power grids) is compelling, and the experimental claims (e.g., fewer hyperparameters, better performance) suggest practical relevance. The paper also clearly connects to prior work on physics-informed models and highlights the limitations of soft constraints."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-experiment, leaving critical details about the experimental setup, baselines, and quantitative results missing. Without this information, it is impossible to evaluate the empirical claims (e.g., 'outperform existing methods') or assess the method's scalability to larger systems. The dependency on precise knowledge of the constraint manifold's structure is not thoroughly discussed, raising questions about applicability to real-world scenarios where constraints may be unknown or complex. Additionally, the computational cost of projection operations and their impact on training efficiency are not addressed."
          },
          "questions": {
            "value": "1. What specific constraints were tested in the experiments (e.g., conservation laws, holonomic constraints)? How were these constraints defined for the power grid models? 2. How is the projection operation computed for non-trivial manifolds (e.g., nonlinear constraints)? Are there numerical challenges in this process? 3. How does PNDE handle dynamic constraints that evolve over time? 4. What baselines were compared against (e.g., physics-informed neural networks, UDEs)? 5. Are there ablation studies showing the impact of hyperparameters or projection accuracy on performance?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Projected Neural Differential Equations (PNDEs), a method to enforce hard constraints in neural differential equations (NDEs) by projecting their vector fields onto the tangent space of a constraint manifold. This approach aims to improve numerical stability, generalizability, and constraint satisfaction in dynamical systems modeling."
          },
          "strengths": {
            "value": "The paper presents a theoretically grounded method leveraging differential geometry to enforce constraints, addressing a critical gap in existing NDEs that rely on soft constraints. The novelty lies in the projection-based approach, which could offer more rigorous constraint satisfaction than prior work. The motivation for handling complex systems like chaotic dynamics and power grids is timely and relevant. The paper also highlights practical benefits such as reduced hyperparameter tuning, which is valuable for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, with the experiments section truncated and no specific results provided. Key questions remain about the choice of constraint manifolds, computational costs of projection, and comparisons to alternative methods (e.g., Lagrange multipliers, penalty methods). The claims about 'fewer hyperparameters' and 'superior performance' are not substantiated with ablation studies or baseline comparisons. The method's applicability to non-smooth constraints or high-dimensional systems is unclear. Additionally, the paper does not address how the constraint manifold is defined in practice, which is critical for real-world deployment."
          },
          "questions": {
            "value": "1. What specific chaotic systems and power grid models were used in the experiments? How were the constraint manifolds defined for these examples? 2. Which baseline methods were compared against PNDEs, and what metrics were used? 3. How does the projection method handle non-differentiable or non-smooth constraints? 4. What is the computational overhead of the projection step compared to standard NDEs? 5. Are there cases where the projection could fail, and how is this mitigated?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "2Chkk5Ye2s": {
    "paper_id": "2Chkk5Ye2s",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces the Mixture-UCB algorithm, a multi-armed bandit approach for selecting optimal mixtures of generative models. The authors demonstrate that such mixtures can outperform individual models on benchmark datasets using evaluation metrics like FID and KID, and they provide a theoretical regret bound for their method."
          },
          "strengths": {
            "value": "The paper addresses a relevant and underexplored problem of combining generative models for better performance. The Mixture-UCB algorithm is novel in its application of bandit methods to mixture selection, and the theoretical analysis includes a regret bound. The experiments show promising results on image and text models, and the work connects to existing literature on UCB and successive halving. The clarity of the problem formulation and the practical motivation are strong."
          },
          "weaknesses": {
            "value": "The experimental validation is limited in scope, focusing on a single dataset (FFHQ) and a small number of models. The paper does not compare against alternative mixture strategies or baseline methods beyond individual model selection. The theoretical analysis lacks detailed assumptions and specifics about the regret bound. The paper also does not address practical challenges such as normalization of mixture weights or computational complexity. The connection between the quadratic optimization formulation and the bandit framework is not sufficiently explained."
          },
          "questions": {
            "value": "How does Mixture-UCB handle non-convex optimization landscapes in real-world scenarios? What are the constraints on the number of models or the dimensionality of the mixture weights? Are there ablation studies to validate the necessity of the bandit framework over greedy approaches? How does the algorithm scale to larger numbers of models or different evaluation metrics? The paper mentions RKE and Vendi for diversity, but their integration into the framework is unclear."
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes the Mixture-UCB algorithm, a multi-armed bandit approach to find optimal mixtures of generative models that outperform individual models in terms of evaluation scores like FID and KID. The method formulates the problem as a quadratic optimization task and provides theoretical guarantees, supported by empirical results on image and text generation tasks."
          },
          "strengths": {
            "value": "The paper introduces a novel framework for combining generative models via a bandit algorithm, addressing a critical gap in model selection. The theoretical analysis is rigorous, with a provable regret bound for the Mixture-UCB algorithm. The experiments demonstrate practical improvements on benchmark datasets, and the paper clearly contextualizes its contributions relative to prior work. The clarity of the problem formulation and the visualizations (e.g., Figure 1) enhance readability."
          },
          "weaknesses": {
            "value": "The paper assumes evaluation scores are quadratic functions of the mixture weights, which may not hold for all metrics. The experiments focus on image models, leaving the generality of the approach untested on text or other modalities. The theoretical analysis is limited to convex quadratic functions, and the paper does not thoroughly compare against existing MAB methods for mixture selection. Additionally, the paper lacks ablation studies on hyperparameters or sensitivity analysis for the algorithm's performance."
          },
          "questions": {
            "value": "1. How does the Mixture-UCB algorithm handle non-convex or non-quadratic evaluation scores, such as those that are not kernel-based? 2. Are there scenarios where individual models are still preferable, and how does the algorithm detect such cases? 3. What is the computational overhead of the Mixture-UCB algorithm compared to existing MAB methods? 4. How does the algorithm ensure the mixture weights are normalized, and what constraints are imposed on the weight space? 5. Could the paper provide more details on the implementation of the OGD (Online Gradient Descent) component in the Mixture-UCB framework?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes the Mixture-UCB algorithm, a multi-armed bandit approach to find optimal mixtures of generative models that outperform individual models in terms of evaluation scores like FID and KID. The method formulates the mixture selection as a quadratic optimization problem and provides theoretical guarantees, with experiments showing improvements on image and text generation tasks."
          },
          "strengths": {
            "value": "The paper introduces a novel application of bandit algorithms to model mixture selection, addressing a gap in existing work that focuses on single-model selection. The theoretical analysis of the Mixture-UCB algorithm and its regret bound is rigorous. The experiments demonstrate practical improvements on benchmark datasets, and the problem formulation is well-motivated by the limitations of current model selection strategies. The clarity of the abstract and introduction is strong, and the paper contextualizes its contributions within prior work on MAB and generative models."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough discussion of the convexity assumptions for FID/KID scores, which are critical for the validity of the mixture optimization approach. The experiments are limited to specific datasets (e.g., FFHQ) and models (e.g., Kandinsky 3, Stable Diffusion 3), raising questions about generalizability. The theoretical analysis assumes quadratic evaluation functions, but the paper does not clarify how this restricts applicability to other metrics. Additionally, the comparison with existing MAB methods (e.g., Hu et al. 2024) is superficial, and the paper does not address potential limitations of the bandit framework in high-dimensional mixture spaces."
          },
          "questions": {
            "value": "1. How are the convexity assumptions for FID/KID scores validated in practice? Are there cases where these scores fail to be convex, undermining the mixture optimization approach? 2. What are the computational and sample complexity trade-offs of Mixture-UCB compared to existing methods like successive halving or standard UCB? 3. How does the algorithm handle non-convex or non-quadratic evaluation functions, which are common in real-world scenarios? 4. Are the reported improvements in FID/KID scores statistically significant across multiple trials? 5. How are the mixture weights normalized, and what constraints are imposed to ensure valid probability distributions?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "2ET561DyPe": {
    "paper_id": "2ET561DyPe",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces Few-Class Arena (FCA), a benchmark for evaluating vision models in few-class scenarios (2-10 classes). The authors propose a scalable data loading approach, a similarity-based difficulty measure called SimSS, and conduct experiments on ten models across multiple datasets. The work aims to bridge the gap between many-class benchmarks and real-world applications with limited classes."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in vision model evaluation by focusing on few-class scenarios, which are underrepresented in existing benchmarks. The proposed SimSS similarity measure shows strong correlation with model performance (Pearson ≥0.88), demonstrating practical utility. The scalable data loading approach avoids redundant data duplication, enhancing efficiency. The experiments cover diverse architectures and datasets, providing insights into model scaling laws in few-class regimes."
          },
          "weaknesses": {
            "value": "The paper lacks comparison with existing few-class benchmarks or methods, making it hard to assess novelty. The SimSS methodology is not thoroughly explained—key details about CLIP/DINoV2 integration, feature extraction pipelines, and validation metrics are missing. Experiments focus on ResNet family and a limited set of models, without evaluating state-of-the-art few-shot learning approaches. The claimed 'first benchmark' assertion is questionable given prior work on few-shot learning. The paper also fails to address how SimSS generalizes across different tasks or datasets beyond image classification."
          },
          "questions": {
            "value": [
              "How does SimSS differ from existing dataset difficulty metrics? What ablation studies validate its components?",
              "Why were only 10 models evaluated? How do results generalize to other architectures like Vision Transformers or specialized few-shot learners?",
              "What is the exact relationship between SimSS and model performance? Are there cases where high similarity correlates with poor performance?",
              "How does FCA handle class imbalance or distribution shifts in few-class subsets?",
              "What are the computational costs of generating SimSS scores? Is it practical for real-world deployment?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces Few-Class Arena (FCA), a benchmark for evaluating vision models in few-class scenarios (2-10 classes), which is underexplored compared to many-class benchmarks. It proposes a scalable data loading approach, a similarity-based difficulty metric (SimSS), and extensive experiments comparing models like ResNet and Transformers across ten datasets. Key findings include sub-models outperforming full models in the few-class regime and the violation of scaling laws for sub-models."
          },
          "strengths": {
            "value": "Originality: The few-class regime is a novel focus, addressing a critical gap in vision model evaluation. Quality: The experiments are comprehensive, with systematic analysis of model performance across subsets of ImageNet and other datasets. Clarity: The paper is well-structured, with clear explanations of FCA's design and contributions. Significance: The benchmark and SimSS metric provide practical tools for efficient model selection in real-world applications, where few-class scenarios are common."
          },
          "weaknesses": {
            "value": "The paper lacks depth in explaining why sub-models outperform full models in the few-class regime, which could be due to data distribution shifts or architectural biases. The SimSS metric's implementation details (e.g., how CLIP/DINoV2 features are aggregated) are not fully clarified, limiting reproducibility. The experiments focus primarily on ImageNet subsets, with limited exploration of generalization to other domains or tasks. The theoretical justification for the inverse difficulty measurement is superficial, and the paper does not compare SimSS to existing metrics for dataset difficulty."
          },
          "questions": {
            "value": "1. How does SimSS account for different types of class similarity (e.g., semantic vs. visual)? Are there ablation studies on its components? 2. What are the limitations of the scalable data loading approach in handling imbalanced or hierarchical class structures? 3. How does FCA ensure fairness when comparing models with varying architectural complexities? 4. Are the reported Pearson coefficients for SimSS robust across different subsets or models?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Few-Class Arena (FCA), a benchmark for evaluating vision models in few-class scenarios (2-10 classes). The authors propose a novel similarity measure (SimSS) based on CLIP and DINoV2, demonstrate that sub-models outperform full models in few-class settings, and highlight the violation of scaling laws for sub-models. The work addresses a critical gap between many-class benchmarks and real-world applications."
          },
          "strengths": {
            "value": "Originality: The focus on few-class scenarios is novel, addressing a gap in existing many-class benchmarks. The SimSS similarity measure and the observation of scaling law violations in sub-models are creative contributions. Quality: The experiments involve diverse models and datasets, with clear visualizations (e.g., Figure 1) highlighting key findings. Clarity: The paper is well-structured, with a clear problem statement, methodology, and results. Significance: The benchmark and insights have practical value for real-world applications with limited classes, and the tool is designed for extensibility."
          },
          "weaknesses": {
            "value": "The paper is cut off, leaving critical details about the experimental setup (e.g., full list of models/datasets, SimSS computation) and validation missing. The claim that sub-models outperform full models lacks statistical rigor (e.g., p-values, ablation studies). The reliance on CLIP/DINoV2 for similarity measurement is not thoroughly justified. The paper does not compare FCA to existing few-class benchmarks or discuss potential limitations of the proposed approach."
          },
          "questions": {
            "value": "1. What specific models and datasets were used in the experiments (ten models mentioned but not listed)? 2. How is SimSS calculated, and what validation metrics were used to ensure its effectiveness? 3. Are there existing few-class benchmarks that FCA improves upon, and how does it differ from them? 4. What is the computational cost of generating sub-models, and how does it compare to the benefits? 5. How generalizable is SimSS to other tasks beyond image classification?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "2HN97iDvHz": {
    "paper_id": "2HN97iDvHz",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes an LLM-based predictive scheduling system for data centers, aiming to improve operational efficiency and sustainability by predicting resource requirements (execution time, energy consumption) from source code and optimizing GPU allocation. The approach claims to reduce energy consumption by 32% and waiting time by 30% through real-time scheduling."
          },
          "strengths": {
            "value": "Originality: The integration of LLMs for end-to-end predictive modeling in data center scheduling is novel, particularly for generalizing across diverse task types. Quality: The paper presents a practical framework with claimed real-world impact, addressing sustainability in AI infrastructure. Clarity: The structure is logical, with clear sections on methodology and results. Significance: The potential to reduce environmental impact of data centers aligns with critical global challenges."
          },
          "weaknesses": {
            "value": "The paper lacks critical details about the LLM architecture, training data, and evaluation metrics. The experimental results are not rigorously validated (e.g., no comparison to state-of-the-art scheduling algorithms, no ablation studies). The generalization claims are speculative without evidence of performance on unseen tasks. The theoretical foundation for the scheduling algorithm is underdeveloped, and the paper does not address computational overhead of LLM inference in real-time systems."
          },
          "questions": {
            "value": [
              "Which specific LLM architecture was used, and how was it fine-tuned for this task? What training data was employed?",
              "How were the predictive models evaluated in the absence of baseline comparisons (e.g., traditional heuristic methods or other ML approaches)?",
              "What is the computational cost of LLM inference in the proposed pipeline, and how does it scale to large-scale data centers?",
              "How was the 32% energy reduction measured? Was it validated on real hardware or simulated? What were the control conditions?",
              "What limitations exist in the LLM's ability to generalize to tasks outside the training distribution?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes an LLM-based predictive scheduling system for data centers, aiming to reduce energy consumption and queuing delays by predicting task metrics (execution time, energy usage) from source code and optimizing GPU allocation. It claims a 32% energy reduction and 30% delay decrease through collaboration with a data center."
          },
          "strengths": {
            "value": "Originality lies in leveraging LLMs for end-to-end predictive modeling of diverse metrics (e.g., energy, water, carbon) in data centers, which is novel compared to task-specific heuristic approaches. The practical deployment aspect is strong, with sub-second inference and minimal data requirements. The paper addresses a critical sustainability challenge in AI infrastructure, and the real-world results provide preliminary validation. The methodological contributions include a unified framework for handling diverse task types, which contrasts with previous task-specific models."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficient. The paper lacks details on the baseline methods, the data center's infrastructure, and the LLM's training data. The claimed 32%/30% improvements are not contextualized (e.g., compared to state-of-the-art scheduling algorithms). The generalization to metrics like carbon emissions is speculative without evidence. The paper also does not address potential limitations of LLMs (e.g., overfitting, data requirements) or evaluate the framework's scalability to larger systems."
          },
          "questions": {
            "value": "1. How was the LLM trained? What data sources were used for predicting metrics like energy consumption? 2. What specific baseline scheduling algorithms were compared against? 3. How were the metrics (e.g., energy, waiting time) defined and measured in the real-world experiment? 4. Are there ablation studies demonstrating the LLM's contribution to the results? 5. How does the framework handle tasks outside the training distribution, such as novel architectures or hybrid workloads?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces an LLM-based predictive scheduling system for data centers, aiming to improve operational efficiency and sustainability by predicting metrics like execution time, energy consumption, and potentially carbon emissions. It combines an LLM for source-code-driven predictions with a real-time scheduling algorithm, claiming a 32% energy reduction and 30% lower waiting times through collaboration with a data center."
          },
          "strengths": {
            "value": "The paper presents a novel application of LLMs for predictive resource allocation in data centers, addressing a critical sustainability challenge. Its methodological contributions include a versatile predictive model compatible with diverse tasks and a unified framework for automated scheduling. The practical deployment claims (e.g., sub-second inference, minimal data requirements) are promising. The problem formulation is clear, and the real-world results (energy/waiting time reduction) demonstrate significant practical relevance. The paper also highlights the potential for extending predictions to additional sustainability metrics."
          },
          "weaknesses": {
            "value": "The paper lacks critical details about the LLM training process, evaluation metrics, and baseline comparisons. The experimental results are summarized without sufficient technical depth (e.g., no ablation studies, no analysis of prediction accuracy for specific tasks). The generalization capabilities of the LLM to unseen tasks (e.g., composite workloads) are not rigorously validated. The claims about carbon emissions and water usage predictions are speculative without supporting data. The paper is also cut off mid-section, limiting the ability to assess the full methodology."
          },
          "questions": {
            "value": "1. How was the LLM trained for this specific task? What datasets were used, and how were they annotated? 2. What is the baseline for the 32% energy reduction claim? How do the results compare to existing scheduling algorithms? 3. How does the model handle GPU heterogeneity and varying task complexities? 4. What are the limitations of the LLM's predictive accuracy for edge cases (e.g., rare task types)? 5. How were the sustainability metrics (e.g., carbon emissions) quantified in the real-world deployment?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "2LHzKdb8Ao": {
    "paper_id": "2LHzKdb8Ao",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper addresses symmetry mismatch in robotic learning caused by non-top-down camera viewpoints by proposing simple image preprocessing techniques. For RGBD images, depth data is used to reproject side-view images into top-down views, while RGB images are transformed using homographic perspective. These steps improve performance of O(2)-equivariant policies across various tasks and learning algorithms."
          },
          "strengths": {
            "value": "The paper presents a practical and effective solution to a well-identified problem in robotic learning. The approach is simple, generalizable to RGB/RGBD images, and demonstrates consistent empirical improvements across multiple settings. The motivation is strong, with clear connections to prior work on equivariance. The paper also highlights the importance of input symmetry alignment, which is a critical but often overlooked aspect in equivariant learning."
          },
          "weaknesses": {
            "value": "The preprocessing techniques (reprojection and homographic transforms) are standard computer vision methods, not novel contributions. The paper lacks a detailed analysis of why these methods specifically address the symmetry mismatch. The experiments are limited to simulated environments, and the paper does not compare against alternative approaches for handling symmetry mismatches (e.g., more complex equivariant architectures or learned symmetry corrections). The theoretical justification for the effectiveness of the preprocessing steps is also underdeveloped."
          },
          "questions": {
            "value": "1. How do the authors quantify the reduction in symmetry mismatch after applying their preprocessing steps? 2. Are there scenarios where the homographic transform for RGB images might fail to approximate a top-down view effectively? 3. What is the computational overhead of the preprocessing steps in real-time robotic systems? 4. How do the results generalize to more complex or dynamic environments beyond the simulated tasks tested?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper addresses the issue of symmetry mismatch in robotic learning caused by side-view camera perspectives. It proposes simple image preprocessing steps (reprojection for RGBD and homographic transforms for RGB) to align input data with O(2) symmetry, improving performance of equivariant policies in simulated robotic tasks."
          },
          "strengths": {
            "value": "The paper demonstrates originality by creatively applying well-known computer vision techniques to a specific robotics problem. The approach is practical and effective, with empirical evidence of consistent performance improvements across diverse settings. The problem statement is clearly articulated, and the methodology is straightforward. The significance lies in addressing a real-world challenge in robotic learning with minimal computational overhead."
          },
          "weaknesses": {
            "value": "The paper lacks comparative analysis with alternative approaches for handling symmetry mismatches, such as approximate equivariant networks or learned symmetry representations. The experiments are limited to simulated environments without real-world validation. The ablation studies on the proposed preprocessing steps are insufficient, making it unclear which component contributes most to performance gains. The theoretical justification for why these transformations improve equivariance is underdeveloped, and the paper does not address potential limitations (e.g., occlusions, noisy depth data)."
          },
          "questions": {
            "value": [
              "How do the proposed preprocessing steps compare to existing methods for handling symmetry mismatches in the literature?",
              "What is the impact of the preprocessing steps on different types of equivariant networks (e.g., SO(2) vs. SE(2))?",
              "Are there scenarios where these transformations might degrade performance (e.g., complex occlusions or non-uniform lighting)?",
              "How does the computational cost of the preprocessing steps affect real-time robotic applications?",
              "What are the limitations of the homographic transform for RGB images in cases where the scene lacks planar surfaces?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of symmetry mismatch in robotic learning caused by non-top-down camera viewpoints. The authors propose simple image preprocessing techniques (depth-based reprojection for RGBD and homographic transforms for RGB) to align input data with the $O(2)$ symmetry of robotic tasks, demonstrating improved performance across multiple learning paradigms and view angles."
          },
          "strengths": {
            "value": "The paper identifies a practical and important problem in robotic learning with clear relevance to real-world deployment scenarios. The proposed solution is simple, generalizable to RGB/RGBD inputs, and requires no additional training data or privileged information. Empirical results show consistent performance improvements across diverse settings, and the method is computationally efficient. The connection to equivariant learning theory is well-articulated, and the paper builds on a solid foundation of prior work."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous analysis of why the preprocessing steps improve performance (e.g., whether they address specific symmetry violations or noise in sideview data). The experiments are limited to simulated environments, and real-world validation is absent. The novelty is somewhat limited as the preprocessing techniques are standard computer vision methods. The paper does not compare against alternative approaches for addressing symmetry mismatches (e.g., more complex equivariant architectures or data augmentation strategies). Additionally, the impact of camera calibration errors on the proposed method is not discussed."
          },
          "questions": {
            "value": "1. How does the proposed method handle variations in camera calibration or sensor noise in real-world settings? 2. Are the preprocessing steps sensitive to specific camera configurations (e.g., focal length, field of view)? 3. Could the performance gains be attributed to simpler image transformations (e.g., cropping or resizing) rather than the proposed reprojecting techniques? 4. How does the method perform when the task symmetry differs from $O(2)$ (e.g., for 3D manipulation tasks)? 5. What ablation studies have been conducted to isolate the contribution of each preprocessing step?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "2MqyCIxLSi": {
    "paper_id": "2MqyCIxLSi",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Generalized Combinatorial Complex Networks (GCCNs), a framework for systematically transforming graph neural networks (GNNs) into topological deep learning (TDL) models. The authors propose TopoTune, a software tool for implementing GCCNs, and demonstrate their effectiveness through experiments showing superior performance over existing CCNNs with reduced complexity."
          },
          "strengths": {
            "value": "The paper's originality lies in its systematic approach to generalizing GNNs to TDL, addressing a critical gap in standardization and accessibility. The methodology introduces augmented Hasse graphs and GCCNs, which formally generalize CCNNs and maintain permutation equivariance. The clarity of the exposition is strong, with well-structured sections and illustrative figures. The significance is high, as the work advances TDL by enabling broader applicability and easier experimentation, aligning with open problems in the field."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with TDL methods beyond CCNNs, such as hypergraph or simplicial complex models, which limits the scope of its claims. The theoretical analysis of GCCNs' expressiveness and equivariance is brief, with minimal proof details. Experimental validation is limited to two topological domains (simplicial and cell complexes), and the paper does not address how GCCNs perform on more complex or real-world datasets. Additionally, the paper does not thoroughly discuss the computational efficiency or scalability of TopoTune."
          },
          "questions": {
            "value": [
              "How does the systematic generalization method handle topological domains beyond combinatorial complexes, such as hypergraphs or simplicial complexes?",
              "What are the specific limitations of TopoTune in terms of supported topological structures or model architectures?",
              "The paper claims GCCNs outperform CCNNs but does not clarify how the experiments account for varying dataset complexities or hyperparameter tuning.",
              "How are the theoretical guarantees (e.g., permutation equivariance) formally proven, and what assumptions underlie these claims?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces TopoTune, a framework for Generalized Combinatorial Complex Neural Networks (GCCNs), which systematically generalize graph neural networks (GNNs) to higher-order topological domains. The authors propose GCCNs as a novel class of models that formalize and extend Combinatorial Complex Neural Networks (CCNNs), while providing a lightweight software library (TopoTune) for their implementation. Experiments suggest GCCNs outperform CCNNs on benchmark datasets with reduced complexity."
          },
          "strengths": {
            "value": "Originality: The paper presents a novel method for cross-domain topological generalization, addressing a critical gap in TDL standardization. The GCCN architecture introduces a formally generalized framework that unifies multiple topological domains. Quality: The proposed framework is accompanied by a software library (TopoTune) and experiments on diverse datasets. Clarity: The paper is well-structured, with clear motivation and logical organization. Significance: The work tackles foundational issues in TDL, such as accessibility, standardization, and unified theoretical frameworks."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with alternative TDL methods beyond CCNNs, leaving the relative advantages of GCCNs unclear. The theoretical claims (e.g., expressivity, permutation equivariance) are not rigorously proven or contextualized against existing literature. The experimental section is underdeveloped, with no ablation studies, scalability analyses, or analysis of how GCCNs handle different topological structures (e.g., simplicial vs. cell complexes). The description of TopoTune's implementation is minimal, raising questions about its practical utility and integration with existing tools."
          },
          "questions": {
            "value": [
              "What specific baselines were used in the experiments, and how do GCCNs compare to non-CCNN TDL methods (e.g., hypergraph or simplicial GNNs)?",
              "How does the expansion mechanism handle combinatorial complexes with heterogeneous cell ranks or irregular structures?",
              "Are there theoretical guarantees for the expressivity of GCCNs, or is this claim based solely on empirical results?",
              "What are the computational costs and scalability limits of TopoTune compared to existing TDL frameworks?",
              "How do the authors address potential information loss during the graph expansion process, given the paper's earlier criticism of graph-based TDL approaches?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Generalized Combinatorial Complex Networks (GCCNs), a framework for systematically transforming graph neural networks (GNNs) into topological deep learning (TDL) models. The authors propose TopoTune, a software library for implementing GCCNs, and demonstrate their effectiveness on benchmark datasets, showing improved performance over existing CCNNs with reduced model complexity."
          },
          "strengths": {
            "value": "The paper presents a novel framework (GCCNs) that generalizes CCNNs while maintaining expressiveness and permutation equivariance, addressing a key gap in TDL standardization. The methodology is theoretically grounded, with clear connections to combinatorial complexes and Hasse graphs. The introduction of TopoTune as a lightweight, integrated tool enhances accessibility for practitioners. The experimental section demonstrates consistent improvements over CCNNs across multiple datasets, highlighting the practical significance of the work."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with other TDL approaches beyond CCNNs, such as hypergraph or simplicial CNNs, which limits the evaluation of GCCNs' unique advantages. The theoretical analysis of GCCNs' expressiveness and equivariance is brief, with minimal discussion of potential limitations. The experimental setup does not thoroughly explore the impact of hyperparameters or the scalability of TopoTune. Additionally, the paper does not address how GCCNs handle dynamic or evolving topological structures."
          },
          "questions": {
            "value": "1. How does the GCCN framework handle topological domains beyond simplicial and cell complexes (e.g., hypergraphs)? 2. Are there specific scenarios where GCCNs underperform compared to CCNNs, and what are the underlying reasons? 3. What is the computational overhead of the Hasse graph expansion mechanism, and how does it scale with complex size? 4. How does TopoTune compare to existing TDL libraries (e.g., TopoX, TopoBenchmark) in terms of flexibility and feature coverage?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "2NqrA1wYi6": {
    "paper_id": "2NqrA1wYi6",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper addresses the lack of a unified framework for defining and evaluating memory in reinforcement learning (RL) agents. The authors propose formal definitions of memory types (e.g., long-term vs. short-term, declarative vs. procedural) inspired by cognitive science, classify memory-related tasks into two categories (Memory Decision-Making and Meta-RL), and introduce a standardized experimental methodology. They also demonstrate the consequences of violating their proposed framework through experiments."
          },
          "strengths": {
            "value": "The paper tackles a critical issue in RL: the ambiguity of 'memory' definitions, which hinders meaningful comparisons between agents. The formalization of memory types and task classification provides a structured framework for future research. The emphasis on standardizing evaluation methods addresses a significant gap in the field. The work is well-motivated by existing literature and highlights the practical implications of inconsistent definitions. The theoretical foundations from cognitive science add interdisciplinary value."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation of the proposed methodology. While it claims to demonstrate the importance of following the framework, specific experiments, baselines, and quantitative results are missing due to the truncated content. The definitions of memory types (e.g., declarative vs. procedural) are not thoroughly justified or compared to existing RL literature. The distinction between Memory DM and Meta-RL needs clearer justification. The paper also does not address how the proposed framework integrates with existing memory mechanisms (e.g., LSTM, attention)."
          },
          "questions": {
            "value": "1. How are the formal definitions of memory types (e.g., long-term vs. short-term) operationally implemented in the proposed methodology? 2. What specific experiments were conducted to demonstrate the consequences of violating the framework, and what metrics were used? 3. How does the proposed classification of tasks (Memory DM vs. Meta-RL) differ from existing taxonomies in RL literature? 4. Are there any limitations or edge cases where the proposed methodology may not apply? 5. How does the paper address the trade-offs between different memory mechanisms (e.g., computational cost vs. performance)?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper aims to address the ambiguity in defining and evaluating memory in reinforcement learning (RL) agents by introducing formal definitions of memory types (e.g., long-term vs. short-term, declarative vs. procedural) inspired by cognitive science. It proposes a classification of memory-related tasks, a standardized experimental methodology for testing memory capabilities, and demonstrates the consequences of deviating from this methodology through experiments."
          },
          "strengths": {
            "value": "The paper’s originality lies in its systematic attempt to formalize memory concepts from cognitive science into RL, which is a novel and valuable contribution. The methodology for classifying tasks (Memory DM vs. Meta-RL) and evaluating memory capabilities is well-structured and addresses a critical gap in the field. The clarity of the paper is strong, with logical organization and clear problem statements. Its significance is high, as it tackles a foundational issue in RL evaluation that impacts reproducibility and comparison of memory-enhanced agents."
          },
          "weaknesses": {
            "value": "The paper lacks detailed empirical validation of the proposed methodology. While experiments are mentioned, the results and specific benchmarks used are not described in the provided content, making it hard to assess the practical effectiveness of the approach. Additionally, the formal definitions of memory types could benefit from more concrete examples or alignment with existing RL frameworks. The paper also does not thoroughly discuss how the proposed classification interacts with existing memory mechanisms (e.g., attention, recurrent networks)."
          },
          "questions": {
            "value": "1. How does the proposed methodology handle the interplay between different memory types (e.g., declarative and procedural) in complex tasks? 2. What specific benchmarks or datasets were used to validate the experimental methodology, and how do they align with the proposed classification? 3. How does the paper address the potential limitations of cognitive science-inspired definitions in the context of RL agents with non-human-like memory structures? 4. Are there cases where the proposed classification might overlap or conflict with existing RL paradigms, and how are these resolved?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper aims to formalize the concept of memory in reinforcement learning (RL) agents by introducing precise definitions of memory types (e.g., long-term vs. short-term, declarative vs. procedural) inspired by cognitive science. It proposes a classification of memory-related tasks, a standardized experimental methodology for evaluating memory capabilities, and demonstrates the consequences of violating this methodology through experiments."
          },
          "strengths": {
            "value": "The paper's originality lies in its structured attempt to formalize memory concepts in RL, drawing from cognitive science to address ambiguity in existing definitions. The methodology for categorizing tasks (Memory DM vs. Meta-RL) and evaluating memory capabilities shows promise. The significance of addressing the lack of unified validation methods in RL is clear, and the paper's clear organization and alignment with prior work in cognitive psychology are notable strengths."
          },
          "weaknesses": {
            "value": "The paper lacks concrete experimental results to validate the proposed methodology. While it mentions experiments, specific details about the RL agents tested, benchmarks, or quantitative outcomes are absent. The definitions of memory types are not sufficiently distinct from existing literature, and the paper does not address how its framework improves upon prior work. Additionally, the claim that violating the methodology leads to 'extremely incorrect' judgments is not empirically supported."
          },
          "questions": {
            "value": [
              "What specific RL agents and tasks were used in the experiments, and what metrics were employed to evaluate memory capabilities?",
              "How does the proposed methodology differ from existing approaches (e.g., those in Oh et al. 2016 or Fortunato et al. 2020) in terms of rigor and novelty?",
              "Can the authors provide examples of how their framework resolves ambiguities in memory definitions that prior work failed to address?",
              "What are the limitations of the proposed methodology, and how might it fail in complex or real-world scenarios?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "2PRpcmJecX": {
    "paper_id": "2PRpcmJecX",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper provides the first finite-time global convergence analysis for policy gradient methods in average reward Markov decision processes (MDPs). The authors eliminate a smoothness assumption present in prior work, derive explicit smoothness conditions for average reward MDPs, and establish sublinear convergence rates. They also extend their analysis to discounted reward MDPs, improving existing performance bounds."
          },
          "strengths": {
            "value": "Originality: The paper addresses a critical gap in the theoretical understanding of policy gradient convergence for average reward MDPs, which are less studied than discounted reward settings. The elimination of an unverified smoothness assumption is a significant contribution. Quality: The theoretical analysis is rigorous, with precise convergence rates and bounds. Clarity: The paper is well-structured, with clear explanations of technical challenges and contributions. Significance: The results advance the theoretical foundations of reinforcement learning, particularly for applications where average rewards are more natural (e.g., robotics, resource allocation)."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive empirical validation beyond a brief simulation section. The smoothness proof relies on a projection technique, but the generality of this approach across all MDPs is not thoroughly discussed. The finite-time bounds for average reward MDPs are compared to discounted settings, but the practical implications of these bounds (e.g., constant factors, dependency on MDP complexity) are not fully analyzed. Additionally, the paper does not address how their results scale to large-scale or continuous-state MDPs, which limits their applicability to real-world scenarios."
          },
          "questions": {
            "value": [
              "How does the projection technique ensure uniqueness of the value function in average reward MDPs, and what are the limitations of this approach?",
              "What specific classes of MDPs are covered by the smoothness proof, and are there cases where this analysis might not hold?",
              "The paper claims to improve bounds for discounted reward MDPs—can the authors clarify the exact improvements and compare them to existing results in terms of constants and dependencies?",
              "Are there practical challenges in implementing the proposed method for large-scale or continuous-state MDPs, and how does the paper address these?",
              "How do the authors reconcile the sublinear convergence rate $O(\\log T)$ with the $O(1/T)$ rate claimed in the abstract? Are there discrepancies in the analysis?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper presents the first finite-time global convergence analysis of policy gradient methods for average reward Markov Decision Processes (MDPs). The authors address the challenge of eliminating the smoothness assumption required in prior work, derive explicit smoothness properties for average cost functions, and establish sublinear convergence bounds. They also extend their analysis to discounted reward MDPs, improving existing performance guarantees."
          },
          "strengths": {
            "value": "The paper's primary strength lies in its theoretical contributions, particularly the elimination of the smoothness assumption for average reward MDPs, which is a critical gap in the literature. The derivation of explicit smoothness properties for average cost functions provides new insights into the behavior of policy gradients in this setting. The sublinear convergence rate of O(log(T)) is a significant advancement, and the comparison with existing discounted MDP bounds highlights the novelty of their approach. The paper also demonstrates practical relevance through simulations, though details are limited due to truncation."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-contribution, leaving the extension to discounted reward MDPs and its implications incomplete. The experimental validation is briefly mentioned but lacks detailed description, making it hard to assess the empirical rigor. The proof techniques, particularly the projection method to ensure value function uniqueness, are not elaborated sufficiently, raising questions about their generality. Additionally, the paper does not clearly articulate how the new MDP complexity parameters in their bounds compare to existing ones."
          },
          "questions": {
            "value": [
              "What is the exact projection technique used to ensure uniqueness of the value function, and how does it address the non-uniqueness issue in average reward MDPs?",
              "How do the authors' performance bounds for average reward MDPs compare quantitatively to existing bounds in terms of constants and dependency on MDP parameters?",
              "What specific improvements does the analysis bring to discounted reward MDPs, and how do the new bounds differ from state-of-the-art results?",
              "Can the projection technique be generalized to non-tabular or continuous state spaces, and what are the limitations of the current approach?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper presents the first finite-time global convergence analysis of policy gradient methods for average reward Markov Decision Processes (MDPs). The authors address the challenge of analyzing policy gradient convergence in average reward settings, where the absence of a discount factor complicates theoretical guarantees. They eliminate a previously required smoothness assumption, derive an explicit smooth average cost expression, and establish sublinear convergence bounds. The work also extends to discounted reward MDPs, offering improved performance guarantees."
          },
          "strengths": {
            "value": "The paper introduces a novel analysis technique to prove smoothness of the average reward objective without relying on prior assumptions, addressing a critical gap in the literature. The elimination of the smoothness assumption strengthens the theoretical foundation of policy gradient methods for average reward MDPs. The explicit average cost expression and sublinear convergence bounds (O(log T)) are significant contributions, offering new insights into the behavior of policy gradients in this setting. The paper also bridges average and discounted reward MDPs, demonstrating broader applicability. The structure and writing are clear, with well-defined contributions."
          },
          "weaknesses": {
            "value": "The experimental validation is limited to a brief mention of simulations, with no detailed results or comparisons to baseline methods. The paper does not fully address how the projection technique ensures uniqueness of the value function or provide ablation studies to assess the impact of key components. The extension to discounted reward MDPs is mentioned but lacks concrete examples or theoretical justification. The smoothness proof may require further clarification, particularly in handling the non-uniqueness of value functions in average reward settings."
          },
          "questions": {
            "value": "1. How are the simulations conducted? What baselines are compared, and what metrics are used to validate the convergence bounds? 2. Can the authors elaborate on the projection technique used to ensure value function uniqueness and its theoretical guarantees? 3. What specific improvements does the analysis bring to discounted reward MDPs, and how do these compare to existing results? 4. Are the assumptions in the smoothness proof (e.g., ergodicity, tabular settings) clearly stated and justified? 5. How sensitive are the convergence bounds to the complexity parameters of the MDP?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "2QkWSUMQh5": {
    "paper_id": "2QkWSUMQh5",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a novel approach to analyzing the robustness of truss decomposition by modeling edge dependencies through a dependency graph. The authors propose three edge-based measures (Edge Robustness, Edge Strength, EdgeRank) derived from graph metrics on this dependency graph. They demonstrate the utility of these measures in improving GNN-based edge classification tasks, achieving a 3.08% F1 score improvement while maintaining low computational overhead."
          },
          "strengths": {
            "value": "The paper addresses a novel problem of edge-level truss robustness, which is underexplored despite truss decomposition's widespread use. The dependency graph framework provides a structured way to quantify how edge removals propagate through the graph. The proposed measures (Edge Robustness, Edge Strength, EdgeRank) are theoretically grounded and show practical value in improving edge classification. The integration with GNNs demonstrates real-world applicability, and the efficiency claims (3.74× faster than baseline) suggest scalability. The clarity of the problem statement and figures (e.g., Figure 1) enhances readability."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-experiment, leaving critical details about the baseline comparison, implementation of the algorithm, and evaluation metrics incomplete. The theoretical findings enabling the speedup are not elaborated, making it hard to assess their novelty or validity. The edge classification experiments lack details on handling class imbalance, which is crucial for multi-class tasks. The three proposed measures are based on standard graph metrics (in-degree, out-degree, PageRank) without clear justification for why they capture truss robustness uniquely. The paper does not discuss limitations of the dependency graph approach or potential edge cases where it might fail."
          },
          "questions": {
            "value": "1. What is the exact baseline used to claim a 3.74× speedup, and how is the dependency graph computed in practice? 2. How were class imbalance issues addressed in the edge classification experiments? 3. Are the three proposed measures sufficient to capture all aspects of truss robustness, or are there other factors not considered? 4. What are the theoretical findings that enable the efficient computation of the dependency graph, and how do they differ from prior work? 5. How do the measures perform on graphs with varying densities or structures (e.g., sparse vs. dense)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a novel approach to analyze the robustness of truss decomposition by constructing a dependency graph among edges. The authors propose three edge-based measures—Edge Robustness, Edge Strength, and EdgeRank—to quantify the impact of edge removal on neighboring edges. These measures are integrated into a GNN for edge classification, achieving improved performance on multi-class datasets with minimal computational overhead."
          },
          "strengths": {
            "value": "Originality: The paper presents the first study to characterize edge-based robustness in truss decomposition, addressing a critical gap in understanding truss sensitivity. Quality: The theoretical findings and efficient algorithm for dependency graph computation are well-validated, with a 3.74× speedup over baselines. Clarity: Concepts like dependency graphs and edge measures are explained with illustrative examples (e.g., Figure 1). Significance: The integration of robustness measures into GNNs demonstrates practical utility for edge classification, particularly for rare classes, with measurable performance gains."
          },
          "weaknesses": {
            "value": "The paper lacks a comprehensive comparison with existing robustness metrics for truss decomposition, making it harder to assess the novelty of the proposed measures. The experiments focus narrowly on edge classification, leaving open questions about applicability to other tasks (e.g., community detection). The theoretical analysis of the dependency graph's efficiency is brief, and the paper does not address scalability to very large graphs. Additionally, the ablation study of the three measures' individual contributions is underdeveloped."
          },
          "questions": {
            "value": [
              "How do the proposed edge-based measures compare to existing robustness metrics (e.g., core decomposition robustness) in terms of capturing structural nuances?",
              "What are the theoretical guarantees for the efficiency of the proposed algorithm, and how does it scale to graphs with millions of edges?",
              "Are the three measures (Edge Robustness, Edge Strength, EdgeRank) orthogonal, or do they capture overlapping aspects of robustness?",
              "How sensitive are the GNN improvements to the hyperparameters of the proposed measures, and are there scenarios where they might degrade performance?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates the robustness of truss decomposition in graphs, proposing a dependency graph to model edge interactions and introducing three edge-based measures (Edge Robustness, Edge Strength, EdgeRank) to capture structural properties. The authors demonstrate that these measures improve GNN-based edge classification tasks, particularly for rare classes, with minimal computational overhead."
          },
          "strengths": {
            "value": "The paper presents original work on edge-based robustness analysis for truss decomposition, a novel problem in graph mining. The methodology is rigorous, combining theoretical insights (e.g., efficient dependency graph computation) with practical algorithms. The experiments are comprehensive, showing measurable improvements in edge classification tasks. The clarity of the problem statement, figures (e.g., Figure 1), and technical explanations is strong. The significance lies in bridging truss decomposition and GNNs, offering new tools for edge-driven graph analysis."
          },
          "weaknesses": {
            "value": "The study focuses on undirected, unweighted graphs, limiting applicability to real-world scenarios with directed/weighted edges. The scalability of the dependency graph computation to very large graphs is not thoroughly discussed. The edge classification experiments are limited to multi-class datasets, with no analysis of other downstream tasks. The theoretical findings, while useful, lack depth in explaining why the dependency graph captures structural nuances. The paper also does not address how the three measures interact with different GNN architectures."
          },
          "questions": {
            "value": [
              "How would the dependency graph approach scale to dynamic or streaming graphs where edges are added/removed incrementally?",
              "What is the impact of the proposed measures on GNNs with different architectures (e.g., GCN vs. GAT) or training objectives?",
              "Are there cases where the three measures might conflict or fail to capture meaningful structural properties?",
              "Could the framework be extended to handle directed graphs or weighted edges, which are common in real-world applications?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "2TasVD7FXp": {
    "paper_id": "2TasVD7FXp",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces InvestESG, a multi-agent reinforcement learning (MARL) benchmark designed to study the impact of ESG disclosure policies on corporate climate investments. The framework models a social dilemma where profit-driven corporations balance short-term costs and long-term climate risks, while ESG-conscious investors influence corporate behavior through investment decisions. The authors release open-source implementations in PyTorch and JAX and demonstrate that sufficient ESG-informed investor capital can drive corporate mitigation, though greenwashing undermines this effect."
          },
          "strengths": {
            "value": "The paper presents a novel MARL benchmark for a critical real-world problem (climate policy design), addressing a gap in existing economic models. The problem formulation as an intertemporal social dilemma is original and relevant. The open-source release and scalability via PyTorch/JAX are practical contributions. The experiments align with empirical findings, suggesting the benchmark's validity. The use of Schelling diagrams to visualize agent behavior adds clarity."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous baseline comparisons against existing MARL methods or simpler models, making it hard to assess the novelty of their approach. The model's simplifications (e.g., static investor preferences, no dynamic policy feedback) may limit its real-world applicability. Experimental results are not statistically rigorous (e.g., no error bars, limited parameter sweeps). The claim that results align with 'empirical research' is vague without specific references. The social dilemma setup ignores geopolitical or regulatory complexities."
          },
          "questions": {
            "value": "1. How do the authors validate the benchmark's alignment with real-world ESG dynamics beyond qualitative comparisons? 2. What specific MARL algorithms were used, and why were they chosen over alternatives? 3. How sensitive are the results to hyperparameters (e.g., investor capital thresholds, greenwashing costs)? 4. Are there plans to extend the benchmark to include multi-jurisdictional policies or dynamic climate risk scenarios?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces InvestESG, a multi-agent reinforcement learning (MARL) benchmark to study climate investment as a social dilemma. It models interactions between profit-driven corporations and ESG-conscious investors, analyzing how disclosure mandates and information sharing influence corporate mitigation efforts. The authors release open-source implementations and demonstrate that ESG-informed investors can drive corporate cooperation, reducing climate risks."
          },
          "strengths": {
            "value": "The paper presents a novel and timely benchmark for studying climate investment dynamics, addressing a critical socio-economic challenge. The MARL framework enables simulation of complex, long-term interactions between multiple agents, offering a scalable tool for policy analysis. The experiments align with empirical findings and highlight the potential of MARL in informing real-world policy design. The open-source release in PyTorch and JAX enhances reproducibility and community engagement."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous comparison with existing models, making it unclear how InvestESG advances prior work. Experimental results are superficial, with insufficient baselines (e.g., no comparison to non-ESG investor strategies or alternative MARL algorithms). The 'first-principles' model is not clearly differentiated from traditional economic approaches, and the alignment with empirical data is not quantitatively validated. The social dilemma formulation oversimplifies corporate and investor behaviors, neglecting real-world complexities like regulatory variability or heterogeneous agent strategies."
          },
          "questions": {
            "value": "1. How does InvestESG differ from existing MARL or economic models of climate investment? 2. What specific baselines were tested, and why were they chosen? 3. How was the model validated against empirical data—what metrics were used? 4. Are there limitations in the MARL framework's ability to capture real-world dynamics (e.g., scalability, multi-objective trade-offs)? 5. How might the benchmark be extended to incorporate additional factors like geopolitical differences or policy feedback loops?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces InvestESG, a multi-agent reinforcement learning (MARL) benchmark designed to study the impact of ESG disclosure policies on corporate climate investments. The framework models a social dilemma where companies balance short-term profits with long-term climate mitigation, while ESG-conscious investors influence corporate behavior. The authors demonstrate that a critical mass of ESG-focused investors can drive corporate cooperation, reducing climate risks, and highlight MARL's potential for policy analysis."
          },
          "strengths": {
            "value": "The paper presents a novel MARL benchmark (InvestESG) addressing a timely and socially relevant problem—climate investment as a social dilemma. Its originality lies in applying MARL to simulate complex interactions between corporations and investors under ESG policies, which is underexplored in existing literature. The clarity of the problem formulation and experimental setup is strong, with clear connections to real-world policy debates. The significance is high, as the work bridges machine learning and socio-economic policy analysis, offering a scalable tool for testing alternative regulations."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results and comparisons with baseline methods, making it difficult to assess the effectiveness of the proposed MARL framework. Key implementation details (e.g., specific algorithms, hyperparameters) are omitted, limiting reproducibility. The model's simplifications (e.g., assuming homogeneous investor preferences) may not capture real-world complexity, and the paper does not address how these assumptions affect policy recommendations. Additionally, the theoretical analysis of the social dilemma is superficial, with limited discussion of equilibrium dynamics."
          },
          "questions": {
            "value": "1. What specific MARL algorithms were used in the experiments, and how do they compare to existing methods in terms of performance? 2. How were ESG scores calculated, and what mechanisms allow companies to engage in greenwashing? 3. Are there ablation studies demonstrating the impact of investor capital thresholds on corporate behavior? 4. How does the model account for heterogeneity in corporate strategies or investor preferences? 5. What limitations exist in the current framework that could be addressed in future work?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "2VhFZPYqjE": {
    "paper_id": "2VhFZPYqjE",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces CHASE, a framework for generating synthetic evaluation problems using LLMs without human input. The approach constructs challenging problems through a bottom-up method and decomposes generation into verifiable sub-tasks. The authors evaluate CHASE on three domains (QA, code completion, math reasoning) and demonstrate that state-of-the-art LLMs achieve 40-60% accuracy, with significant performance drops on long-context tasks."
          },
          "strengths": {
            "value": "Originality is evident in the novel framework for synthetic evaluation data, addressing a critical gap in LLM evaluation. The quality of experiments is strong, with diverse domains and comparisons to baselines. Clarity is maintained through structured explanations and figures. The significance lies in tackling scalability and realism in benchmarking, which is crucial as LLMs outpace traditional benchmarks."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of why CHASE outperforms baselines like Evol-Instruct. The correctness verification process for sub-tasks is underexplained, and the realism of synthetic data is not rigorously validated. The experiments focus on accuracy metrics but omit ablation studies or error analysis. The generalizability of the framework to other tasks remains unexplored."
          },
          "questions": {
            "value": [
              "How is the correctness of sub-tasks verified during generation? Are there specific validation mechanisms or heuristics?",
              "What metrics beyond accuracy were used to evaluate the quality of synthetic problems (e.g., diversity, complexity)?",
              "How does CHASE handle tasks outside the three domains mentioned? Are there theoretical limitations to its applicability?",
              "What evidence supports the claim that synthetic data reflects real-world scenarios, especially for long-context tasks?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces CHASE, a framework for generating synthetic evaluation benchmarks using Large Language Models (LLMs) without human input. The approach constructs challenging problems through a bottom-up process and decomposes generation into verifiable sub-tasks. The framework is applied to three domains: document-based QA, code completion, and math reasoning, demonstrating that even state-of-the-art LLMs achieve 40-60% accuracy on these benchmarks."
          },
          "strengths": {
            "value": "The paper presents a novel framework (CHASE) for generating scalable, high-quality synthetic evaluation data, addressing a critical gap in LLM evaluation. The bottom-up problem construction and decomposition into verifiable sub-tasks are creative solutions to ensure correctness and difficulty. The experiments across diverse domains (QA, code, math) showcase practical relevance, and the results highlight significant performance gaps between LLMs. The comparison with baselines like Evol-Instruct underscores the framework's effectiveness in generating harder problems."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how the bottom-up approach systematically increases difficulty. The evaluation focuses on accuracy metrics but does not include error analysis or qualitative examples of generated problems. The synthetic data's real-world applicability is not thoroughly discussed, and the paper does not address potential biases or limitations in the generated benchmarks. Additionally, the comparison with existing synthetic data methods is limited to a single baseline."
          },
          "questions": {
            "value": "1. How does the framework ensure that generated problems are both challenging and realistic? 2. What specific criteria are used to decompose tasks into verifiable sub-tasks? 3. Are there cases where the generated data might inadvertently favor certain LLMs? 4. How does CHASE handle domain-specific edge cases (e.g., ambiguous math problems or edge cases in code)? 5. Could the framework be adapted to other tasks beyond the three domains presented?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces CHASE, a framework for generating synthetic challenges to evaluate Large Language Models (LLMs) without human input. The approach constructs problems bottom-up by hiding solution components in context and decomposes generation into verifiable sub-tasks. It creates three benchmarks (CHASE-QA, CHASE-CODE, CHASE-MATH) across diverse domains, showing that state-of-the-art LLMs achieve 40-60% accuracy, with significant performance drops on long-context tasks."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in LLM evaluation with a novel framework (CHASE) that combines bottom-up problem construction and decomposable verification. The experimental scope across three domains and demonstration of LLM performance gaps are significant. The clarity of the problem statement and methodology is strong, and the work highlights practical limitations of current models on long-context tasks."
          },
          "weaknesses": {
            "value": "The paper lacks detailed implementation specifics for CHASE's bottom-up generation and sub-task verification. Baselines are insufficient (only Evol-Instruct is compared), and the benchmarks' construction methodology is under-described. The evaluation metrics (e.g., accuracy calculation) and human validation of generated data are unclear. The performance drop with context size is intriguing but requires deeper analysis of contributing factors."
          },
          "questions": {
            "value": "1. How exactly is the bottom-up problem construction implemented? What are the specific steps for hiding solution components in context? 2. How are sub-tasks decomposed and independently verified? Are there examples of this process? 3. What is the exact methodology for generating long-context problems (e.g., 50k tokens)? 4. How is accuracy measured across benchmarks, and are human evaluations used to validate synthetic data quality? 5. What are the limitations of CHASE in terms of domain adaptability or scalability?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "2VmB01D9Ef": {
    "paper_id": "2VmB01D9Ef",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces AutoHijacker, an automated black-box indirect prompt injection attack framework for LLM agents. It addresses the challenge of sparse feedback in indirect prompt injection tasks by employing a batch-based optimization framework and a trainable memory mechanism, enabling effective attack generation without continuous querying. The method achieves state-of-the-art performance on public benchmarks and demonstrates practical effectiveness against a commercial LLM agent."
          },
          "strengths": {
            "value": "Originality: The paper fills a critical gap by proposing the first automated black-box indirect prompt injection method, addressing limitations of prior handcrafted or white-box approaches. Quality: The method's batch-based optimization and memory mechanism are technically sound, with experiments showing superior performance over 11 baselines. Clarity: The paper is well-structured, with clear explanations of the attack methodology and evaluation setup. Significance: The work highlights critical security vulnerabilities in real-world LLM agents, providing actionable insights for improving robustness."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of the batch optimization and memory components. The evaluation on the commercial LLM agent is superficial, with insufficient details about the platform's architecture or defense mechanisms. The comparison with baselines focuses on success rates but does not analyze attack efficiency or generalizability across different LLM architectures. The theoretical analysis of why batch optimization mitigates sparse feedback is limited."
          },
          "questions": {
            "value": "1. How does the batch-based optimization framework specifically address the sparsity of feedback compared to sequential methods? 2. What are the exact details of the commercial LLM agent evaluation (e.g., model size, defense mechanisms, query limits)? 3. Are there scenarios where AutoHijacker's performance degrades significantly, and how does the memory mechanism handle adversarial defenses? 4. How does the method scale to larger LLMs or more complex agent architectures?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces AutoHijacker, an automated indirect prompt injection attack targeting black-box LLM agents. It addresses the limitations of existing methods by using a batch-based optimization framework and a trainable memory to handle sparse feedback, achieving high attack success rates without requiring external knowledge or continuous querying."
          },
          "strengths": {
            "value": "The paper tackles a critical real-world problem of black-box prompt injection attacks, which is underexplored in prior work. The proposed method's use of batch optimization and memory to handle sparse feedback demonstrates originality in adapting LLM-as-optimizers for indirect attacks. The experimental results on public benchmarks and a commercial platform highlight practical significance. The work also addresses a gap in automated robustness evaluation under realistic constraints."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with specific baselines (e.g., which 11 methods are used? How are they evaluated?). The commercial platform attack's methodology and defense mechanisms are not sufficiently described, raising questions about reproducibility. The theoretical justification for the batch-based optimization framework and memory mechanism is underdeveloped. The paper also omits ablation studies to validate the contribution of key components."
          },
          "questions": {
            "value": [
              "What specific baselines were compared against, and how were they selected? Are they representative of state-of-the-art methods?",
              "How was the commercial LLM agent platform tested? What defenses were in place, and how were they bypassed?",
              "Can the authors clarify the theoretical basis for the batch-based optimization framework and its effectiveness in sparse feedback scenarios?",
              "What ablation studies were conducted to isolate the impact of the memory mechanism and batch optimization?",
              "How does AutoHijacker avoid relying on external knowledge (e.g., user instructions) while still generating effective attacks?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces AutoHijacker, an automated black-box indirect prompt injection attack framework for LLM agents. It addresses the limitations of existing methods by using batch-based optimization and a trainable memory to handle sparse feedback, achieving state-of-the-art results on benchmarks and a commercial LLM agent."
          },
          "strengths": {
            "value": "Originality is strong in targeting black-box indirect prompt injection, a less-explored scenario. The method's batch optimization and memory mechanisms show creative problem-solving. Experiments on public benchmarks and a commercial platform demonstrate practical effectiveness. The paper's structure and clarity are generally good, with clear motivation and technical sections."
          },
          "weaknesses": {
            "value": "Key technical details about the batch-based optimization and memory mechanism are under-specified, making reproducibility challenging. The paper lacks comparison with existing black-box prompt injection methods (e.g., \\cite{liu2024a, pasquini2024}) and does not clarify how the commercial platform's defenses were bypassed. The ethical implications of deploying such attacks are not discussed, which is critical for security-focused work."
          },
          "questions": {
            "value": "1. How exactly does the batch-based optimization framework handle sparse feedback? What specific loss function or reward mechanism is used? 2. Which 8 defenses were tested against, and how do they differ from prior work? 3. Can the authors provide more details about the commercial LLM agent's architecture and defenses? 4. How does AutoHijacker avoid requiring user-specific information, which the paper claims is not needed?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "2aL6gcFX7q": {
    "paper_id": "2aL6gcFX7q",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper investigates vulnerabilities in Retrieval-Augmented Generation (RAG) systems to targeted data poisoning attacks. It provides insights into why these attacks are effective, introduces a novel defense mechanism called DRS (Directional Relative Shifts) to detect poisoned data, and proposes a regularization-based attack algorithm to generate stealthier poisoning data. The work includes empirical evaluations across multiple RAG application scenarios."
          },
          "strengths": {
            "value": "Originality: The paper offers novel insights into the geometric properties of data poisoning attacks, particularly the role of low-variance directions in attack success. The DRS metric and regularization-based attack design are creative contributions. Quality: The experiments span diverse RAG setups (e.g., medical, QA), and the paper claims significant improvements over existing defenses. Clarity: The problem statement and high-level framework are well-structured, though some technical details are incomplete. Significance: Addressing RAG security is critical for safety-critical applications, and the paper highlights a pressing gap in current defenses."
          },
          "weaknesses": {
            "value": "The paper lacks detailed definitions of key concepts like DRS, making it hard to evaluate their theoretical foundation. The regularization approach for attacks is described only superficially, with no ablation studies or analysis of its trade-offs. Experimental results are summarized but lack granular metrics (e.g., exact AUC scores, comparison with state-of-the-art baselines). The paper does not address scalability of DRS or its performance on large-scale datasets. Additionally, the connection between the geometric insights and the proposed methods is not rigorously justified."
          },
          "questions": {
            "value": "1. How is DRS mathematically defined, and what are its theoretical guarantees for detecting poisoned data? 2. What specific baselines were compared against in the experiments, and why do the authors claim superiority over existing defenses? 3. How does the regularization term in the attack algorithm balance stealthiness and attack effectiveness? 4. Are there any limitations to DRS in real-world scenarios (e.g., noisy data, dynamic corpora)? 5. What are the computational costs of implementing DRS, and how does it scale with corpus size?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates the vulnerabilities of Retrieval-Augmented Generation (RAG) systems to targeted data poisoning attacks. The authors provide insights into why these attacks are effective, particularly along directions of low variance in clean data distributions. They propose a novel defense mechanism, DRS (Directional Relative Shifts), and a regularization-based attack algorithm to generate stealthier poisoned data. The work includes experiments across multiple RAG application scenarios, though the paper is truncated, limiting the depth of evaluation."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem: the safety of RAG systems against adversarial attacks. The insights into attack effectiveness, particularly the connection between low-variance data directions and successful poisoning, are novel and theoretically grounded. The proposed DRS defense introduces a new metric for detecting poisoned data, and the regularization-based attack algorithm demonstrates a creative approach to evading defenses. The work's significance is reinforced by RAG's growing use in safety-critical domains like healthcare. The paper's structure and clarity are strong in the abstract and introduction, with a clear problem statement and contributions."
          },
          "weaknesses": {
            "value": "The paper is truncated, leaving key details about the DRS formulation, experimental setup, and results incomplete. For example, the description of the DRS metric is vague, and the attack algorithm's regularization term is not elaborated. Without full experimental results, it is unclear how the proposed methods compare to existing defenses in terms of efficacy or practicality. The analysis of attack effectiveness relies on theoretical claims without empirical validation in the provided text. Additionally, the paper lacks a thorough discussion of limitations, such as the scalability of DRS or its performance under different data distributions."
          },
          "questions": {
            "value": "1. What is the exact definition and calculation of DRS? How does it relate to the eigenvalues of clean data distributions? 2. How does the regularization term in the attack algorithm penalize DRS values? What hyperparameters are used? 3. Are there specific scenarios where DRS fails to detect poisoned data? 4. What datasets and metrics were used in the experiments, and how do they compare to prior work? 5. How does the proposed defense handle non-targeted attacks, which the paper briefly mentions but does not analyze in depth?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates the vulnerabilities of Retrieval-Augmented Generation (RAG) systems to data poisoning attacks, providing insights into why such attacks are effective and proposing novel defense and attack algorithms. The authors analyze that successful attacks occur along directions of low variance in clean data distributions, introduce a defense called DRS (Directional Relative Shifts) to detect poisoned data, and develop a regularization-based attack algorithm to generate stealthier poison. They validate their methods across multiple RAG application scenarios."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem in RAG safety, which is increasingly relevant as these systems are deployed in high-stakes domains. The insights into attack directions along low-variance data distributions are novel and theoretically grounded. The proposed DRS defense introduces a creative metric for detecting poisoning, while the regularization-based attack algorithm demonstrates a thoughtful approach to evading defenses. The experiments span diverse RAG applications, showcasing the practical relevance of the work. The paper is well-structured and clearly articulates its contributions."
          },
          "weaknesses": {
            "value": "The theoretical analysis of why low-variance directions are more susceptible to attacks is somewhat superficial and lacks rigorous mathematical justification. The DRS metric's design and computation are not sufficiently detailed, leaving questions about its robustness. The attack algorithm's regularization term is described conceptually but lacks concrete implementation details. The experimental results are partially cut off, making it difficult to assess the full scope of the claims. Additionally, the paper does not compare against a broad range of existing defenses, which limits the evaluation's comprehensiveness."
          },
          "questions": {
            "value": "1. How is the DRS metric formally defined, and what specific properties of the data distribution does it exploit? 2. What are the hyperparameters and implementation details of the regularization term in the attack algorithm? 3. Are there any ablation studies demonstrating the effectiveness of DRS in isolation versus combined with other defenses? 4. How do the experiments account for varying levels of noise or data diversity in real-world RAG scenarios? 5. What are the limitations of the proposed methods in terms of computational efficiency or scalability?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "2bIQBDSfRk": {
    "paper_id": "2bIQBDSfRk",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces DenseAttention, a novel attention mechanism designed to reduce the computational complexity of Transformers while maintaining exact N×N interactions. It eliminates components like Softmax, LayerNorms, and projection matrices, claiming O(N) time and space complexity through matrix multiplication associativity. The approach also includes MaxNormActivation and Cosine Relative Positional Embeddings, with experiments showing competitive performance on the LRA benchmark and 16K token BERT pre-training."
          },
          "strengths": {
            "value": "The paper presents a creative simplification of the Transformer architecture, targeting efficiency improvements. The focus on reducing memory-bound operations and leveraging matrix multiplication properties is novel. The empirical results on long sequences and the LRA benchmark suggest practical relevance. The introduction of MaxNormActivation and Cosine Relative Positional Embeddings shows effort to address stability and positional encoding challenges."
          },
          "weaknesses": {
            "value": "The paper's core claim of O(N) complexity lacks rigorous mathematical justification. The formula for DenseAttention in the appendix contradicts the abstract by retaining softmax, undermining credibility. The experimental validation is insufficient: no comparison with state-of-the-art long-context models (e.g., Sparse Attention, State-Space Models), no ablation studies on removed components, and no analysis of numerical stability without Softmax. The 16K token BERT pre-training details are minimal, and the theoretical basis for complexity reduction is unclear."
          },
          "questions": {
            "value": "1. How does DenseAttention achieve O(N) complexity when standard matrix multiplication is O(N³)? What approximation or optimization enables this? 2. The appendix formula retains softmax, conflicting with the abstract's claim of eliminating it. Please clarify this discrepancy. 3. What is the exact computational complexity (time/space) of DenseAttention, and how does it compare to prior work? 4. How does MaxNormActivation ensure numerical stability without Softmax? 5. Why does the paper not compare with recent long-context models like Sparse Attention or State-Space Models on the LRA benchmark?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces DenseAttention, a simplified Transformer architecture that claims to achieve linear time and space complexity by eliminating memory-bound components like Softmax, LayerNorms, and residual connections. It leverages matrix multiplication associativity to maintain exact N×N interactions, with MaxNormActivation for stability and Cosine Relative Positional Embeddings. The approach is evaluated on LRA and BERT-large, showing competitive performance."
          },
          "strengths": {
            "value": "Originality: The removal of key components (Softmax, LayerNorms, residual connections) and reliance on matrix multiplication associativity is a novel approach to reducing complexity. Quality: The paper provides empirical validation on BERT-large and LRA, demonstrating practical efficiency gains. Clarity: The method is well-explained with mathematical formulations and clear problem statements. Significance: Addressing Transformer's quadratic complexity is a critical challenge, and the proposed solution has potential for large-scale applications."
          },
          "weaknesses": {
            "value": "Theoretical justification for removing components like LayerNorms and residual connections is insufficient. The paper lacks ablation studies to validate the necessity of these removals. The claim of 'exact N×N interactions' is vague without formal analysis. Numerical stability of MaxNormActivation is not thoroughly discussed. Experimental comparisons with state-of-the-art methods (e.g., FlashAttention, State-Space Models) are limited in scope and detail."
          },
          "questions": {
            "value": "1. How does MaxNormActivation compare to LayerNorm in terms of performance and stability? 2. What ablation studies were conducted to validate the removal of components like residual connections? 3. Can the paper provide a formal proof of the O(N) complexity claim? 4. How does DenseAttention handle long-range dependencies compared to methods like Sparse Attention [4] or State-Space Models [10]? 5. What are the specific metrics used to evaluate BERT-large on LRA, and how do they compare to baselines?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes DenseAttention, a simplified Transformer architecture that claims to achieve O(N) time and space complexity by eliminating memory-bound components like Softmax, LayerNorms, and projection matrices. It introduces MaxNormActivation for numerical stability and Cosine Relative Positional Embeddings, demonstrating competitive performance on long sequences and state-of-the-art results on the LRA benchmark."
          },
          "strengths": {
            "value": "The paper presents a novel approach to reducing Transformer complexity by leveraging matrix multiplication properties, which is theoretically interesting. The practical validation on long sequences (16K tokens) and competitive results on LRA highlight its potential. The removal of redundant components like LayerNorms and softmax could improve hardware efficiency. The work addresses a critical bottleneck in Transformers, offering a fresh perspective on architectural simplification."
          },
          "weaknesses": {
            "value": "The paper's complexity claims are ambiguous: the mathematical formulation still includes QK^T (O(N^2) operations), contradicting the O(N) assertion. The removal of softmax and LayerNorms without sufficient analysis of their impact on performance is a concern. The experiments lack detailed comparisons (e.g., against FlashAttention on small sequences) and specific metrics on the LRA benchmark. The MaxNormActivation mechanism's effectiveness is not thoroughly justified or evaluated."
          },
          "questions": {
            "value": "1. How does the paper reconcile the O(N) complexity claim with the O(N^2) QK^T operation in the mathematical formulation? 2. What ablation studies or experiments were conducted to validate the removal of LayerNorms and softmax? 3. Are the LRA benchmark results statistically significant, and how do they compare to state-of-the-art methods like State-Space Models? 4. What is the exact implementation of Cosine Relative Positional Embeddings, and how does it differ from RoPE?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "2bWf4M5tRo": {
    "paper_id": "2bWf4M5tRo",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes a novel approach to enhance hallucination detection in large language models (LLMs) by introducing noise injection into intermediate hidden layer activations, complementing traditional sampling-based uncertainty estimation. The authors empirically demonstrate that perturbing hidden representations improves detection accuracy compared to prior methods relying solely on next-token sampling."
          },
          "strengths": {
            "value": "The paper presents a creative and novel method for hallucination detection by leveraging intermediate layer noise injection, which addresses a critical limitation of existing sampling-based approaches. The empirical analysis is comprehensive, covering multiple datasets (e.g., GSM8K), models (Llama2-7B/13B, Mistral), and uncertainty metrics. The work is well-motivated, with clear explanations of how hidden representations differ from token-level uncertainty. The figures effectively illustrate the complementary nature of noise injection and sampling."
          },
          "weaknesses": {
            "value": "The paper lacks theoretical justification for why intermediate layer perturbation improves hallucination detection. While experiments show improvements, the mechanism behind this remains unclear. The implementation details of noise injection (e.g., noise magnitude, layer selection) are not thoroughly discussed. Additionally, the paper does not address potential trade-offs, such as computational overhead or impact on model performance. The comparison to state-of-the-art methods is limited, and the paper does not explore ablation studies on different noise injection strategies."
          },
          "questions": {
            "value": "1. How does the noise injection method perform on different types of hallucinations (e.g., factual vs. logical)? 2. What is the optimal strategy for selecting layers to inject noise, and how does this vary across models? 3. Are there scenarios where noise injection might degrade detection performance? 4. How does the computational cost of noise injection compare to existing methods? 5. Could the observed improvements be attributed to changes in token probability distributions rather than uncertainty estimation?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes a method to enhance hallucination detection in large language models (LLMs) by injecting noise into intermediate layers of the model, complementing traditional sampling-based uncertainty estimation. The authors argue that perturbing hidden unit activations provides a distinct source of randomness, leading to improved detection accuracy compared to prior methods that rely solely on prediction layer sampling."
          },
          "strengths": {
            "value": "The paper presents a novel approach to hallucination detection by introducing noise injection into intermediate layers, which offers a complementary perspective to existing sampling-based methods. The empirical analysis demonstrates that this technique improves detection performance on benchmark datasets. The work also provides a clear theoretical motivation for why intermediate layer perturbation may capture different aspects of model uncertainty compared to traditional sampling. The paper is well-structured and includes detailed experimental results."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient baseline comparisons with state-of-the-art hallucination detection methods, making it difficult to assess the relative effectiveness of the proposed approach. The experimental validation is limited to a single dataset (GSM8K) and a few model architectures, which restricts the generalizability of the findings. The noise injection mechanism is not thoroughly explained, including details on the type of noise, its magnitude, and how it interacts with different layers. Additionally, the paper does not address potential limitations, such as computational overhead or sensitivity to hyperparameters."
          },
          "questions": {
            "value": "1. How does the noise injection method compare to other uncertainty estimation techniques (e.g., Bayesian neural networks or ensemble methods) in terms of performance and efficiency? 2. What specific types of noise (e.g., Gaussian, dropout-like) were used, and how were their parameters determined? 3. Are the results reproducible across different tasks and datasets beyond GSM8K? 4. How does the proposed method handle varying lengths of input contexts or complex reasoning tasks? 5. What is the computational cost of injecting noise into intermediate layers, and how does it scale with model size?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a novel approach to enhance hallucination detection in large language models (LLMs) by injecting noise into intermediate hidden unit activations, complementing traditional sampling-based uncertainty estimation. The authors empirically demonstrate that this method improves detection accuracy compared to existing techniques."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel method of perturbing intermediate representations for hallucination detection, which is distinct from prior work focusing on sampling-based uncertainty. Quality: The experiments are well-structured, with clear visualizations (e.g., Figure 2) showing the impact of noise injection. Clarity: The problem formulation and methodology are clearly explained, though some sections are truncated. Significance: Addressing hallucination detection is critical for safe LLM deployment, and the paper provides a practical improvement over existing methods."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with state-of-the-art hallucination detection methods, making it difficult to assess the relative effectiveness of the proposed approach. The experimental validation is limited to the GSM8K dataset and a few LLMs (Llama2-7B/13B, Mistral), without broader generalization analysis. The theoretical justification for why intermediate layer perturbation improves detection is underdeveloped, relying heavily on empirical results. The truncation of the paper raises concerns about missing critical details in the methodology and analysis."
          },
          "questions": {
            "value": "1. How does the proposed noise injection method compare to recent state-of-the-art approaches like [specific references] in hallucination detection? 2. Are the results reproducible with different noise injection parameters (e.g., noise magnitude, layer positions)? 3. What is the computational overhead of the proposed method compared to baseline sampling approaches? 4. Could the observed improvements be attributed to changes in token probability distributions rather than hallucination detection per se?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "2e4ECh0ikn": {
    "paper_id": "2e4ECh0ikn",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a novel evaluation protocol to assess audio foundation models (FMs) on their turn-taking capabilities in conversations. The authors propose using a supervised model trained on human-human interactions to judge turn-taking timing, conduct a user study comparing dialogue systems, and curate benchmarks to evaluate audio FMs on understanding and predicting turn-taking events."
          },
          "strengths": {
            "value": "Originality is strong, as the paper addresses a critical but underexplored aspect of conversational AI—turn-taking dynamics. The quality of the experiments is notable, with a user study and curated benchmarks. Clarity is maintained through structured sections and clear problem formulation. The significance lies in its potential to advance interactive AI systems by highlighting gaps in current models."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving key details about the user study, judge model training, and benchmark curation incomplete. The evaluation metrics lack depth, such as how the judge model's performance is validated. The comparison with prior work on turn-taking prediction is superficial, and the limitations of the proposed protocol (e.g., reliance on human-human data) are not thoroughly discussed."
          },
          "questions": {
            "value": "How is the judge model trained, and what datasets were used? Are there ablation studies to validate the effectiveness of the proposed metrics? How do the results compare to baseline methods like VAD-based systems? What are the specific challenges in applying the protocol to diverse conversational scenarios? Are the curated benchmarks representative of real-world interactions?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces a novel evaluation protocol to assess audio foundation models (FMs) on their turn-taking capabilities in conversational interactions. It proposes a supervised judge model trained on human-human conversations to evaluate timing of turn-taking events, presents insights from a user study on existing dialogue systems, and curates benchmarks to evaluate FMs on understanding and predicting turn-taking dynamics."
          },
          "strengths": {
            "value": "The paper addresses a critical but underexplored aspect of conversational AI: turn-taking dynamics. The proposed evaluation protocol is innovative, moving beyond traditional metrics to focus on temporal alignment of turn transitions. The user study provides actionable insights into existing systems' limitations, such as aggressive interruptions and lack of backchannels. The combination of automated metrics and human-like judging demonstrates a comprehensive approach to assessing conversational fluency. The motivation is strong, as turn-taking is essential for natural human-machine interaction."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology on the judge model's training (e.g., architecture, features, data splits). The user study details are sparse (e.g., participant count, task design). The curated benchmarks are mentioned but not described, and FMs' performance is not quantified. Comparisons to prior work are cursory, and the significance of contributions is not fully justified. The evaluation protocol's limitations (e.g., reliance on a single judge model) are not discussed."
          },
          "questions": {
            "value": [
              "What features does the judge model use to predict turn-taking events, and how is it trained (e.g., supervised with what labels)?",
              "How are the timing metrics defined (e.g., precision of turn transition detection)?",
              "What are the specific details of the user study (e.g., number of participants, dialogue scenarios)?",
              "Which audio FMs were evaluated, and what are their performance metrics on the curated benchmarks?",
              "How does the proposed protocol address edge cases (e.g., overlapping speech, ambiguous cues) compared to prior work?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a novel evaluation protocol to assess audio foundation models (FMs) on their turn-taking capabilities in conversational interactions. The authors propose a supervised judge model trained on human-human conversations to evaluate the timing of turn-taking events, conduct a user study comparing existing dialogue systems, and curate benchmarks to assess audio FMs' ability to understand and predict turn-taking dynamics."
          },
          "strengths": {
            "value": "Originality: The paper addresses a critical but underexplored aspect of conversational AI—turn-taking timing—by proposing a novel evaluation protocol that goes beyond corpus-level statistics. Quality: The approach combines a supervised judge model with user studies and benchmarking, showing systematic analysis of existing systems. Clarity: The problem statement and contributions are well-articulated, with clear motivation for the importance of turn-taking in human-machine interactions. Significance: The work highlights critical limitations in current dialogue systems and provides a foundation for improving conversational AI, which is highly relevant for real-world applications."
          },
          "weaknesses": {
            "value": "The paper is truncated, limiting the ability to assess the depth of experiments and analysis. The user study details (e.g., sample size, system configurations) are not fully described, making it hard to evaluate the robustness of insights. The judge model's training data and performance metrics are not thoroughly explained, raising questions about its reliability. Additionally, the comparison with existing benchmarks (e.g., SD-Eval) lacks specificity, and the paper does not discuss potential biases in the curated test benchmarks."
          },
          "questions": {
            "value": "1. How was the judge model trained, and what datasets were used to ensure its generalizability? 2. What specific metrics were used to quantify 'room for improvement' in audio FMs, and how do they compare to existing benchmarks? 3. Are the user study results statistically significant, and how were confounding variables controlled? 4. How do the authors address potential biases in the human-human conversation datasets used for benchmarking? 5. What are the limitations of the proposed metrics in capturing complex turn-taking dynamics?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "2fojNANZSv": {
    "paper_id": "2fojNANZSv",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes MIXTUREPFN, a method for improving the scalability and effectiveness of In-Context Learning (ICL) on tabular data. The approach combines clustering-based routing of test samples to specialized ICL experts (MICP) and bootstrapping-based finetuning (CAPFN) to address distribution shift and efficiency bottlenecks. The method claims to outperform 19 baselines across 36 tabular datasets."
          },
          "strengths": {
            "value": "The paper identifies a critical scalability challenge for ICL on tabular data and proposes a novel architecture combining clustering and expert routing. The experimental scope is extensive, covering 36 diverse datasets. The method's theoretical motivation for addressing distribution shift and efficiency through clustering is well-structured. The paper's clarity is generally good, with clear problem formulation and technical definitions."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparison with relevant prior work (e.g., clustering-based ICL methods or other tabular learning approaches). Experimental validation is incomplete: no ablation studies, hyperparameter details, or statistical significance tests for the claimed improvements. The efficiency claims (O(1) memory, O(log N) time) are not empirically validated. The bootstrapping policy and clustering methodology are not sufficiently explained. The paper also doesn't address potential limitations like computational overhead of clustering or sensitivity to cluster count."
          },
          "questions": {
            "value": [
              "Which 19 baselines were compared? Please provide specific names and references to enable meaningful comparison.",
              "How were clusters formed? What clustering algorithm was used, and how was the number of clusters determined?",
              "What is the exact bootstrapping policy? How does it differ from standard finetuning?",
              "Are the claimed efficiency gains (O(1) memory, O(log N) time) empirically verified? If so, please provide measurements.",
              "How does MIXTUREPFN handle datasets with mixed feature types (numerical, categorical, ordinal)?",
              "What is the computational cost of the clustering and routing steps? How does this compare to baseline methods?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes MIXTUREPFN, a method for improving In-Context Learning (ICL) on tabular data by combining clustering-based expert routing with bootstrapping to address scalability and distribution shift issues. It claims to outperform 19 baselines across 36 datasets."
          },
          "strengths": {
            "value": "The paper addresses critical challenges in ICL for tabular data, including scalability and distribution shift, which are well-motivated. The proposed approach of using a 'mixture of in-context prompters' with clustering and expert routing is novel and theoretically grounded in Bayesian inference. The experimental evaluation is comprehensive, covering diverse datasets and demonstrating strong empirical performance. The method's efficiency improvements (constant-size contexts) and theoretical connections to prior work (e.g., PFNs) add depth."
          },
          "weaknesses": {
            "value": "The paper lacks detailed explanations of key components, such as how clusters are formed, the clustering algorithm used, and the exact implementation of the bootstrapping policy. The comparison to baselines is not sufficiently contextualized—e.g., it is unclear if the baselines were evaluated under the same conditions or if the 19 baselines include state-of-the-art methods. The paper also does not address potential limitations, such as the sensitivity of clustering to hyperparameters or the computational cost of the proposed approach."
          },
          "questions": {
            "value": "1. How are the clusters determined? What clustering algorithm is used, and how does it handle mixed data types (numerical, categorical, ordinal)? 2. What is the exact implementation of the bootstrapping policy for finetuning experts? 3. Are there ablation studies showing the contribution of each component (e.g., clustering vs. routing vs. bootstrapping)? 4. How does MIXTUREPFN perform on smaller datasets, where ICL is already effective? 5. What are the computational trade-offs of the proposed method compared to existing ICL approaches?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes MIXTUREPFN, a method that combines sparse mixture of experts with in-context learning (ICL) for tabular data. It addresses scalability and distribution shift issues by clustering data, finetuning experts per cluster, and using bootstrapping. The approach claims to achieve state-of-the-art performance on 36 tabular datasets."
          },
          "strengths": {
            "value": "The paper introduces a novel framework (MIXTUREPFN) that effectively combines clustering, expert models, and ICL for tabular data. The theoretical analysis of efficiency (O(1) memory, O(log N) time) is compelling. The extensive experiments across 36 datasets with 19 baselines demonstrate strong empirical performance. The paper also addresses critical challenges in ICL for tabular data, such as distribution shift and scalability, which are well-motivated."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of key components (e.g., clustering vs. bootstrapping). The theoretical efficiency claims (O(1) memory, O(log N) time) are not validated with empirical runtime measurements. The clustering strategy is under-described—how are clusters determined, and how does this scale? The bootstrapping policy is mentioned but not elaborated. Additionally, the paper does not compare with recent state-of-the-art tabular methods beyond the 19 baselines cited."
          },
          "questions": {
            "value": "1. How are clusters determined? Is the clustering method unsupervised, and how does it handle varying data distributions? 2. What is the exact bootstrapping policy used to align pretraining and inference priors? 3. Are there cases where the cluster routing fails, and how is this mitigated? 4. How does MIXTUREPFN handle high-dimensional or sparse tabular data? 5. What are the computational costs of clustering and expert finetuning, and how do they compare to the claimed efficiency gains?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "2gTEW29qsM": {
    "paper_id": "2gTEW29qsM",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces GIT-STORM, a novel world model that replaces the traditional MLP prior in STORM with a MaskGIT prior to improve sequence modeling capabilities. The authors demonstrate performance gains in RL tasks on the Atari 100k benchmark and apply their model to continuous action environments using a state mixer function, addressing a gap in prior work."
          },
          "strengths": {
            "value": "The paper presents a novel architecture (GIT-STORM) that integrates MaskGIT priors into world models, showing improvements in sequence modeling. The experiments on Atari 100k and DMC benchmarks are comprehensive, with ablation studies highlighting the impact of discrete representations. The application to continuous control tasks via a state mixer is a significant contribution. The paper also provides clear comparisons against existing methods like DreamerV3 and IRIS, demonstrating the effectiveness of the proposed approach."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of the state mixer's design and its effectiveness in continuous action environments. The comparison with other transformer-based models (e.g., TECO) is limited, and key implementation details (e.g., hyperparameters, training procedures) are missing. The claims about the MaskGIT prior's superiority over MLP priors are not sufficiently justified with ablation studies on the prior's components. Additionally, the paper does not address potential limitations of the state mixer, such as scalability or generalization to more complex tasks."
          },
          "questions": {
            "value": "1. How is the state mixer function specifically designed to integrate latent states with continuous actions? What are its architectural components? 2. Are there ablation studies comparing the MaskGIT prior against other masked generative models (e.g., BERT, GPT) in the context of world models? 3. Why was the state mixer necessary for continuous control tasks, and how does it address the limitations of categorical latent states? 4. What metrics (beyond FVD) were used to evaluate the model's performance on the DMC benchmark? 5. Are there comparisons with other transformer-based world models (e.g., TECO, STORM) on the DMC tasks?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces GIT-STORM, a world model that replaces the traditional MLP prior in STORM with a Masked Generative Prior (e.g., MaskGIT) to improve sequence modeling. It demonstrates performance gains on Atari 100k RL tasks and applies transformer-based world models to continuous action environments using a state mixer function, addressing a gap in prior work."
          },
          "strengths": {
            "value": "The paper presents a novel architecture (GIT-STORM) that integrates recent advances in masked generative modeling (MaskGIT) into world models, showing measurable improvements on standard benchmarks. The ablation studies and comparisons with existing methods (e.g., DreamerV3, IRIS) provide evidence of the effectiveness of the MaskGIT prior. The work bridges a critical gap by applying transformer-based world models to continuous control tasks, which is a significant contribution to the field. The clarity of the problem statement, methodology, and experimental results is strong, with well-structured tables and figures."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of why the MaskGIT prior outperforms the MLP prior, such as ablation studies on specific components of the prior. The state mixer function for continuous actions is not thoroughly explained, leaving questions about its design and limitations. The experiments on continuous control tasks (e.g., DeepMind Control Suite) are described but lack depth, such as quantitative comparisons with baselines or analysis of failure cases. Additionally, the paper does not discuss computational efficiency or scalability of the proposed method."
          },
          "questions": {
            "value": "1. How does the MaskGIT prior specifically address the limitations of MLP priors in sequence modeling? Are there ablation studies on key components of the prior? 2. What are the limitations of the state mixer function in handling continuous actions, and how does it compare to alternative approaches? 3. Are there quantitative results comparing GIT-STORM to other transformer-based world models on continuous control tasks? 4. How does the model handle long-term dependencies in sequences, and what are the implications for real-world deployment?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces GIT-STORM, a novel world model that replaces the traditional MLP prior in STORM with a MaskGIT prior to improve sequence modeling for reinforcement learning (RL) and video prediction. The authors demonstrate performance gains on the Atari 100k benchmark and extend transformer-based world models to continuous control tasks using a State Mixer function. Key contributions include a comparison of prior architectures and evaluations on DMLab, SSv2, and DeepMind Control Suite."
          },
          "strengths": {
            "value": "The paper addresses a meaningful gap in applying transformer-based world models to continuous control tasks, which is a critical area for real-world RL deployment. The use of MaskGIT priors shows clear empirical improvements in sequence modeling (e.g., FVD metrics) compared to MLP-based baselines. The work builds on prior research (e.g., STORM, DreamerV3) while introducing novel components like the State Mixer. Experiments are well-structured, with ablation studies and comparisons to state-of-the-art methods, showcasing the practical relevance of the approach."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of why MaskGIT priors outperform MLPs, such as ablation studies on hyperparameters or theoretical justification for their effectiveness. The State Mixer function is mentioned but not thoroughly explained, leaving unclear how it integrates latent states with continuous actions. Evaluations on the DeepMind Control Suite are brief, with no specific metrics or comparisons to prior work. The paper also omits discussion of computational efficiency or scalability, which are important for real-world applications."
          },
          "questions": {
            "value": "1. How does the MaskGIT prior specifically improve sequence modeling compared to MLPs? Are there ablation studies on hyperparameters or architectural choices? 2. What is the exact mechanism of the State Mixer function, and how does it address the limitations of categorical latent states in continuous control? 3. Can the authors provide more quantitative results on the DeepMind Control Suite, such as reward metrics or comparison to prior methods? 4. Are there theoretical insights into why bidirectional transformers (as in MaskGIT) outperform autoregressive ones in this context?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "2hbgKYuao1": {
    "paper_id": "2hbgKYuao1",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces HyMN, a hybrid approach combining Subgraph GNNs with walk-based centrality measures for efficient subgraph selection and structural encodings (SEs). By leveraging centrality scores to prioritize important subgraphs and augment node features, HyMN aims to balance expressiveness and computational efficiency, demonstrating competitive performance on synthetic and real-world tasks with reduced runtime compared to full-bag Subgraph GNNs."
          },
          "strengths": {
            "value": "The paper presents a novel integration of centrality-based subgraph selection and SEs, addressing both efficiency and expressiveness in GNNs. The methodology is grounded in perturbation analysis, offering theoretical justification for centrality-based sampling. Experiments show strong empirical performance, with HyMN outperforming other subgraph sampling methods while maintaining low computational costs. The clarity of the problem statement and the logical flow of the approach are commendable."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of centrality-based sampling versus SEs. The comparison with alternative centrality measures (e.g., between Subgraph Centrality and PageRank) is insufficient, leaving unclear why the chosen measures are optimal. Additionally, the theoretical analysis of expressiveness is brief, and the paper does not thoroughly address potential limitations in scalability or generalization to highly heterogeneous graphs."
          },
          "questions": {
            "value": [
              "Why was Subgraph Centrality specifically chosen over other centrality measures? Could alternative measures (e.g., betweenness, eigenvector centrality) yield better results?",
              "How does HyMN scale to extremely large graphs (e.g., millions of nodes)? Are there hidden computational bottlenecks in the proposed framework?",
              "The paper mentions 'hybrid marking' but does not explicitly define how centrality-based SEs interact with the Subgraph GNN's aggregation mechanism. Could this lead to feature redundancy or conflicts?",
              "What ablation studies were conducted to verify the necessity of combining centrality-based sampling with SEs, as opposed to using them independently?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces HyMN, a hybrid approach that combines subgraph GNNs with walk-based centrality measures to balance efficiency and expressiveness. The method leverages centrality scores for subgraph selection and as structural encodings, reducing computational costs while maintaining competitive performance."
          },
          "strengths": {
            "value": "The paper presents a novel integration of centrality measures into subgraph GNNs, offering a computationally efficient alternative to existing methods. The theoretical connection to perturbation analysis provides a rigorous foundation. Experimental results on synthetic and real-world datasets demonstrate superior performance with reduced runtime. The work addresses key limitations of MPNNs and subgraph GNNs through a principled approach."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the contribution of centrality-based sampling versus structural encodings. The comparison with state-of-the-art methods is limited, with no direct benchmarks against recent GNNs like Graph Transformers. The theoretical analysis of expressiveness is superficial, and the computational complexity analysis is incomplete. The paper also does not address potential limitations of centrality measures in dynamic or heterogeneous graphs."
          },
          "questions": {
            "value": [
              "What specific ablation experiments were conducted to isolate the impact of centrality-based sampling versus structural encodings?",
              "How does HyMN compare to recent GNNs like GraphSAGE or GIN on standard benchmarks?",
              "Are the theoretical claims about expressiveness rigorously proven, or are they based on heuristic arguments?",
              "What is the exact computational complexity of HyMN compared to full-bag subgraph GNNs?",
              "How sensitive is the method to hyperparameters like the number of sampled subgraphs or centrality thresholding?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes HyMN, a hybrid approach combining Subgraph GNNs and Structural Encodings (SEs) using walk-based centrality measures. It leverages centrality scores for efficient subgraph selection and as SEs to enhance expressiveness, claiming reduced computational costs and competitive performance."
          },
          "strengths": {
            "value": "Originality: The integration of walk-based centrality measures for both subgraph selection and SEs is novel. Quality: The connection to perturbation analysis and theoretical claims about expressiveness are well-structured. Clarity: The paper is well-organized, with clear motivation and problem formulation. Significance: Addressing efficiency and expressiveness in GNNs is critical, and the hybrid approach has potential impact."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art methods like Graph Transformers or other efficient subgraph sampling techniques. The theoretical analysis of expressiveness is superficial, with no formal proofs. Experimental results are summarized but lack granular metrics (e.g., exact accuracy numbers, runtime breakdowns). The choice of Subgraph Centrality over other centrality measures is not justified. The truncated conclusion raises concerns about completeness."
          },
          "questions": {
            "value": [
              "What specific centrality measures were evaluated, and why was Subgraph Centrality prioritized?",
              "How does HyMN compare to non-subgraph-based methods like Graph Transformers in terms of accuracy and efficiency?",
              "Are there ablation studies demonstrating the contribution of centrality-based SEs vs. subgraph selection?",
              "What is the exact computational complexity of HyMN compared to full-bag Subgraph GNNs?",
              "How were hyperparameters tuned, and what is the sensitivity of performance to these choices?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "2kfpkTD5ZE": {
    "paper_id": "2kfpkTD5ZE",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a method called Foundation Molecular Grammar (FMG) that leverages multi-modal foundation models (MMFMs) to induce interpretable domain-specific languages (DSLs) for molecular graph generation. The approach involves converting molecules into images, prompting MMFMs to describe them as text, and using tree decomposition to simplify the DSL construction task. The method emphasizes interpretability through 'design narratives' generated by non-expert LLM agents."
          },
          "strengths": {
            "value": "Originality lies in combining MMFMs with molecular DSL induction, particularly through the novel use of tree decomposition and cross-modal consistency. The focus on interpretability and data-efficiency addresses critical gaps in molecular generation. The paper's theoretical framework for integrating prior knowledge into DSL construction is creative. The potential for transparency in molecular discovery workflows is significant."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, with the content cut off mid-section. Key claims about 'significant advantages in synthesizability, diversity, and data-efficiency' are unsupported by quantitative results. The role of 'non-expert LLM agents' as judges is vague—no metrics or training procedures are described. The method's reliance on MMFMs' pretraining on molecular data is unverified, and the tree decomposition approach's effectiveness remains untested. Comparisons to existing DSL induction methods (e.g., Guo et al. 2022b) are absent."
          },
          "questions": {
            "value": "1. How is the tree decomposition algorithm implemented, and what guarantees does it provide for chemical validity? 2. What specific metrics are used to evaluate synthesizability, diversity, and data-efficiency? 3. How are the 'non-expert LLM agents' defined, trained, and validated as judges? 4. What ablation studies confirm the contribution of MMFM components versus other parts of the pipeline? 5. How does FMG handle molecules outside the pretraining distribution of MMFMs?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes Foundation Molecular Grammar (FMG), a method that leverages multi-modal foundation models (MMFMs) to induce interpretable domain-specific languages (DSLs) for molecular generation. By converting molecules into images and text, the approach uses MMFMs to perform tree decomposition of molecular graphs, guided by prompt learning and agent-based evaluation to ensure consistency and correctness."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to DSL induction for molecular generation by combining MMFMs with graph decomposition, addressing challenges in data-efficiency and interpretability. The use of tree decomposition simplifies the problem, while agent-based evaluation of 'design narratives' provides a unique mechanism for ensuring DSL quality. The emphasis on chemical interpretability aligns with practical needs in drug discovery, and the method's reliance on pre-trained MMFMs avoids the need for domain-specific training data. The paper also highlights the potential of MMFMs to generalize across modalities, which is a significant contribution to the intersection of foundation models and molecular design."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about the experimental setup, evaluation metrics, and comparisons to baseline methods incomplete. The proposed method's reliance on MMFMs' zero-shot capabilities lacks concrete technical justification, such as how prompts are designed or how tree decomposition is implemented. The agent-based evaluation of DSL correctness is not explained in detail, raising questions about its reliability. Additionally, the paper does not address potential limitations of using image-text alignment for molecular representation, such as ambiguity in rendering or the impact of pretraining data on MMFM performance."
          },
          "questions": {
            "value": "1. How are the agents trained to evaluate the correctness and persuasiveness of the MMFM's reasoning steps? 2. What specific metrics are used to quantify synthesizability, diversity, and data-efficiency in the experiments? 3. How is the tree decomposition algorithm adapted to work with MMFMs, and what guarantees exist for its soundness? 4. Are there ablation studies demonstrating the contribution of each component (e.g., prompt learning, agent evaluation) to the final performance? 5. How does the method handle molecules with complex or ambiguous structures that may not translate well into images or text?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes a novel method called Foundation Molecular Grammar (FMG) to induce interpretable domain-specific languages (DSLs) for molecular graphs using multi-modal foundation models (MMFMs). The approach involves rendering molecules as images, prompting MMFMs to describe them as text, and leveraging tree decomposition to simplify DSL construction. The method aims to improve data-efficiency, synthesizability, and interpretability without human expertise."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in molecular generation by enabling automated DSL construction without human intervention, which is a significant contribution. The integration of multi-modal reasoning (image and text) with tree decomposition is creative and leverages MMFMs' zero-shot capabilities effectively. The emphasis on interpretability through 'design narratives' aligns with the growing need for transparency in scientific AI. The method's potential for data-efficiency is promising, especially for resource-scarce domains. The paper also thoughtfully contextualizes its work within related fields, demonstrating awareness of prior limitations."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about experiments, baselines, and validation methods incomplete. The reliance on non-expert LLM agents as judges for DSL correctness lacks justification, raising concerns about reliability. The tree decomposition approach's chemical validity and alignment with molecular structure principles are not clearly explained. The paper also does not address how MMFMs' pretraining on RDKit APIs or other molecular data is leveraged, which is crucial for the method's success. Without full experimental results, the claimed advantages in synthesizability and diversity remain unverified."
          },
          "questions": {
            "value": [
              "What specific benchmarks and metrics were used to evaluate FMG's performance, and how does it compare to existing DSL methods or data-driven generative models?",
              "How is the chemical validity of generated molecules ensured, given the reliance on MMFMs' zero-shot reasoning?",
              "What is the exact mechanism by which tree decomposition steps correspond to meaningful molecular substructures, and how are traditional heuristics replaced?",
              "How were the non-expert LLM agents trained or evaluated for their ability to judge DSL correctness and persuasiveness?",
              "Are there ablation studies to demonstrate the contribution of key components (e.g., multi-modal prompting, tree decomposition, or agent-based evaluation)?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "2pEqXce0um": {
    "paper_id": "2pEqXce0um",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a novel approach to Root Cause Analysis (RCA) by connecting it to the Interactive Graph Search (IGS) problem. The authors establish a theoretical lower bound on the number of invariance tests required for RCA and propose an optimal algorithm that leverages causal graphs learned from observational data. They also demonstrate how partial causal knowledge can reduce the number of tests needed, validated through experiments on simulated and production datasets."
          },
          "strengths": {
            "value": "The paper's originality lies in its novel connection between RCA and IGS, along with a rigorous theoretical analysis establishing a lower bound for invariance tests. The methodology is sound, combining causal discovery with algorithmic efficiency. The clarity of the problem formulation and the structured approach to addressing limitations of prior work (e.g., arbitrary DAG conversions) are commendable. The significance is high, as RCA is critical for complex systems, and the paper addresses a key gap in leveraging normal operational data for proactive failure analysis."
          },
          "weaknesses": {
            "value": "The experiments are limited to simulated datasets and a single production-level application, which may not fully reflect real-world complexities. The theoretical bounds are abstract and lack concrete examples of how they translate to practical scenarios. The comparison with state-of-the-art methods like RCD and RUN is cursory, and the paper does not thoroughly address challenges such as noisy observational data or dynamic system changes. The handling of partial causal structures could benefit from more detailed analysis of edge cases."
          },
          "questions": {
            "value": "How does the proposed algorithm handle real-world systems with noisy or incomplete observational data? What are the specific limitations of the IGS reduction when applied to large-scale, dynamic microservice architectures? Could the theoretical lower bound be tightened or adapted for scenarios with multiple root causes? How does the algorithm perform when the causal graph is learned with significant errors from observational data?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a novel algorithm, Root Cause Analysis with Causal Graphs (RCG), which leverages causal discovery from observational data during normal system operations to improve failure diagnosis. The authors establish a theoretical connection between root cause analysis (RCA) and Interactive Graph Search (IGS), derive a lower bound on invariance tests, and propose an optimal algorithm that reduces RCA to IGS. They also address partial causal knowledge scenarios and demonstrate the approach's effectiveness through experiments on simulated and production datasets."
          },
          "strengths": {
            "value": "The paper's originality lies in its novel connection between RCA and IGS, which provides a fresh perspective on failure diagnosis. The theoretical contributions, including the lower bound on invariance tests and the reduction to IGS, are significant. The approach's emphasis on leveraging normal operational data aligns with real-world practicality. The paper also addresses partial causal knowledge, a critical challenge in real-world systems, and proposes a systematic method to handle it. The clarity of the problem formulation and the structured presentation of theoretical results are commendable."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results to substantiate its claims. While it mentions validation on a production-level application, the specifics of the experiments, metrics, and comparisons with state-of-the-art methods like RCD, RUN, and BARO are not provided. The theoretical analysis assumes idealized conditions (e.g., full causal knowledge), which may not hold in practice. Additionally, the paper does not thoroughly discuss the computational complexity of the proposed algorithm or its scalability to large systems."
          },
          "questions": {
            "value": "1. What are the specific results of the experiments on the production-level application? How does RCG compare to existing methods in terms of accuracy, efficiency, and robustness? 2. How does the proposed algorithm handle scenarios where the causal graph is not fully known, and what are the practical limitations of the partial causal structure approach? 3. Are there any empirical validations of the theoretical lower bound on invariance tests, and how does the algorithm perform under finite sample sizes? 4. What are the computational and memory requirements of the C-PC algorithm in large-scale systems?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a novel algorithm, Root Cause Analysis with Causal Graphs (RCG), which leverages causal discovery from observational data to identify failure root causes. It connects RCA to Interactive Graph Search (IGS), establishes theoretical lower bounds on invariance tests, and proposes an optimal algorithm that reduces the problem to IGS. The approach is validated on simulated and production datasets."
          },
          "strengths": {
            "value": "The paper makes a strong theoretical contribution by linking RCA to IGS, providing a novel framework for root cause identification. The analysis of lower bounds on invariance tests is rigorous, and the proposed algorithm's optimality claim is well-motivated. The integration of causal discovery with observational data during normal operations is a significant innovation. The paper also addresses limitations of prior methods, such as reliance on arbitrary assumptions and high-order CI tests."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete (the content is truncated), making it difficult to assess the empirical performance of RCG against baselines like RCD, RUN, and BARO. The theoretical analysis lacks detailed proofs or explanations of the IGS reduction. The paper does not clarify how the C-PC algorithm is adapted for root cause identification, nor does it address potential limitations of the proposed approach in real-world scenarios with noisy data."
          },
          "questions": {
            "value": "1. How exactly is the RCA problem reduced to IGS? What are the specific modifications to the IGS framework? 2. What are the exact experimental results (e.g., accuracy, F1 scores) comparing RCG to baselines on the Sock-shop dataset and production application? 3. How does the algorithm handle cases where the causal graph is only partially known or contains errors? 4. Are there any practical constraints (e.g., computational complexity, data requirements) that limit the applicability of the proposed method?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "2xljvcYOLm": {
    "paper_id": "2xljvcYOLm",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper investigates the relationship between noise samples and generated images in diffusion models, particularly under deterministic DDIM sampling. It identifies that a single-step inference approximates the ZCA de-whitening transform, enabling model-agnostic noise inversion for image generation and editing."
          },
          "strengths": {
            "value": "The paper introduces a novel connection between diffusion models and ZCA whitening, offering a theoretically grounded insight into the noise-image mapping. The experiments demonstrate spatial correlation between noise and images, and the proposed method for model-agnostic noise inversion is promising. The clarity of the problem formulation and the structure of the paper are strong, with relevant references to prior work."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive baseline comparisons (e.g., against existing noise inversion methods like those in SDEdit). The theoretical justification for the ZCA approximation is limited, and the experiments rely heavily on qualitative observations (e.g., Figure 2) without quantitative validation. The claimed 'model-agnostic' approach is not thoroughly tested across diverse diffusion models. The impact of the method on downstream tasks (e.g., image editing) is under-specified, with no ablation studies or statistical significance tests."
          },
          "questions": {
            "value": "1. How does the proposed single-step approximation compare to existing ZCA-based methods in terms of performance? 2. What are the limitations of the fixed-point iteration approach in different diffusion settings? 3. Are the experiments reproducible, and are the datasets/models used publicly available? 4. How does the method handle high-resolution images or non-ImageNet datasets? 5. What is the computational cost of the proposed optimization compared to existing techniques?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper explores the relationship between noise samples and generated images in diffusion models, particularly through deterministic DDIM sampling. It identifies that a single-step approximation of the noise-image mapping corresponds to a ZCA de-whitening transform, enabling model-agnostic noise inversion for image generation and editing."
          },
          "strengths": {
            "value": "The paper introduces a novel theoretical connection between diffusion models and ZCA whitening, which is both mathematically rigorous and practically applicable. The method is model-agnostic, addressing a gap in noise inversion techniques. Experiments demonstrate strong correlations between noise and images, and the approach shows promise for image editing tasks. The writing is clear, and the problem formulation is well-motivated."
          },
          "weaknesses": {
            "value": "The paper lacks extensive comparisons with existing noise inversion methods, making it difficult to assess the practical superiority of the proposed approach. The ZCA approximation is validated primarily on ImageNet, but its generalizability to other datasets or architectures is unexplored. The computational efficiency of the fixed-point iteration method is not discussed, and the paper does not address potential limitations in high-dimensional image spaces."
          },
          "questions": {
            "value": "1. How does the ZCA approximation perform on datasets beyond ImageNet? 2. What are the computational costs of the proposed method compared to existing noise inversion techniques? 3. Can the approach be extended to conditional diffusion models or other modalities (e.g., video, audio)? 4. How sensitive is the method to hyperparameters in the fixed-point iteration?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates the relationship between noise samples and generated images in diffusion models, particularly under deterministic DDIM sampling. The authors identify that a one-step approximation of the noise-image mapping corresponds to a ZCA de-whitening transform, leveraging this insight for model-agnostic noise inversion and image generation."
          },
          "strengths": {
            "value": "The paper demonstrates strong theoretical grounding by connecting diffusion models to ZCA whitening, a well-established technique in signal processing. The methodology is novel in its application to diffusion models, offering a clear and interpretable link between noise and image generation. The experiments showcase practical applications like image variation generation and editing, with potential for broad impact. The paper is well-structured, with clear explanations of key concepts and a logical flow."
          },
          "weaknesses": {
            "value": "The experimental validation is limited, with only partial results presented (e.g., Figure 2's analysis is cut off). The generalizability of the ZCA matrix derived from ImageNet to other datasets or models is unclear. The paper does not thoroughly address how the single-step approximation performs under varying conditions (e.g., different noise schedules or image modalities). Additionally, the comparison to existing methods like SDEdit lacks quantitative analysis."
          },
          "questions": {
            "value": "How was the ZCA matrix computed for ImageNet, and does it generalize to other datasets? What are the computational costs of the proposed simulated annealing optimization? How does the method perform on non-ImageNet datasets or high-resolution images? Are there scenarios where the single-step approximation fails, and how are these handled? What ablation studies were conducted to validate the necessity of the ZCA-based approach?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "30oIfmrcFO": {
    "paper_id": "30oIfmrcFO",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper addresses the issue of representation collapse in intermediate Transformer layers, which limits their performance on complex reasoning tasks. The authors propose Sequential Variance-Covariance Regularization (Seq-VCR) to enhance representation diversity and prevent collapse, combined with dummy pause tokens to simulate chain-of-thought reasoning without explicit supervision. They demonstrate significant improvements on arithmetic reasoning tasks, achieving 99.5% accuracy on a 5x5 multiplication task."
          },
          "strengths": {
            "value": "The paper identifies a novel problem (representation collapse in Transformers for reasoning tasks) and proposes a specific solution (Seq-VCR) with clear experimental validation. The results on arithmetic reasoning tasks are impressive, particularly the 99.5% accuracy on the 5x5 multiplication task. The method combines regularization with dummy pause tokens, offering a practical approach without requiring explicit CoT supervision. The paper also contextualizes its work within existing literature on representation collapse and CoT prompting."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodological descriptions (e.g., exact formulation of Seq-VCR, how variance-covariance is computed). The experimental setup is not fully transparent: it's unclear how the 5x5 multiplication task is constructed, what baselines are compared against, and whether results are statistically significant. The claim that 'models of the same size yield 0% accuracy' is suspicious without evidence of proper training. The paper also does not address whether Seq-VCR generalizes to non-arithmetic tasks or larger models. The connection between representation collapse and arithmetic reasoning is not rigorously justified."
          },
          "questions": {
            "value": [
              "What is the exact mathematical formulation of Seq-VCR? How is variance-covariance regularization applied to intermediate representations?",
              "How is the 5x5 multiplication task constructed? What are the input/output formats, and how is 'exact match accuracy' defined?",
              "What baselines are compared against (e.g., other regularization methods, standard Transformers, or CoT models)? Why are 'models of the same size' assumed to fail completely?",
              "How are dummy pause tokens integrated into the model? Do they modify the architecture, training process, or inference?",
              "Are the results reproducible? What hyperparameters, training details, and evaluation protocols were used?",
              "Does Seq-VCR improve representation diversity in intermediate layers? Are there visualizations or metrics to confirm this?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper identifies representation collapse in intermediate Transformer layers as a key limitation for arithmetic reasoning tasks and proposes Sequential Variance-Covariance Regularization (Seq-VCR) to enhance representation diversity. It combines Seq-VCR with dummy pause tokens to achieve state-of-the-art performance on arithmetic tasks without explicit chain-of-thought supervision."
          },
          "strengths": {
            "value": "Originality is demonstrated by addressing representation collapse in the context of arithmetic reasoning, a less-explored area. The method's novelty lies in combining variance-covariance regularization with dummy pause tokens. Experimental results show significant improvements (e.g., 99.5% accuracy on 5x5 multiplication), highlighting practical significance. The paper is well-structured with clear problem formulation and contextualization of prior work."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed mathematical formulation of Seq-VCR, making it difficult to assess its theoretical grounding. The experiments focus narrowly on arithmetic tasks, leaving the generalizability of the method unclear. Comparisons with baselines are limited (e.g., no ablation studies on Seq-VCR vs. pause tokens). The claim about outperforming GPT-4 lacks contextualization of computational costs or architectural differences."
          },
          "questions": {
            "value": "1. What is the exact mathematical formulation of Seq-VCR, and how does it differ from existing regularization methods like Barlow Twins or VICReg? 2. Are the results reproducible, and what hyperparameters were used for training? 3. How does the method perform on non-arithmetic reasoning tasks (e.g., logical deduction)? 4. What is the computational overhead of adding dummy pause tokens compared to traditional CoT prompting?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper identifies representation collapse in intermediate Transformer layers as a key limitation for arithmetic reasoning tasks and proposes Sequential Variance-Covariance Regularization (Seq-VCR) to enhance representation diversity. The method combines Seq-VCR with dummy pause tokens to achieve state-of-the-art performance on challenging arithmetic tasks, including 99.5% accuracy on 5x5 integer multiplication."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to address representation collapse, a critical issue in Transformer reasoning. The combination of variance-covariance regularization with dummy pause tokens is creative and shows strong empirical results. The experimental validation on specific arithmetic tasks demonstrates significant improvements over baselines and GPT-4. The paper is well-structured, with clear problem formulation and comprehensive literature review."
          },
          "weaknesses": {
            "value": "The paper lacks detailed mathematical formulation of Seq-VCR, making it difficult to assess its theoretical foundation. The implementation of dummy pause tokens is not elaborated, and their exact role in preventing collapse remains unclear. The experiments focus on a narrow set of arithmetic tasks, and the generalizability to other domains is untested. The comparison with GPT-4 lacks methodological details (e.g., exact prompting strategies). Ablation studies or analysis of how Seq-VCR affects intermediate representations are missing."
          },
          "questions": {
            "value": "1. How is the variance-covariance regularization mathematically defined and implemented? 2. What is the exact mechanism by which dummy pause tokens contribute to preventing representation collapse? 3. Are the results reproducible with different random seeds or architectures? 4. How does the method scale to longer sequences or more complex reasoning tasks? 5. What ablation studies were performed to validate the necessity of each component (Seq-VCR vs. pause tokens)?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "31J6aWPnlR": {
    "paper_id": "31J6aWPnlR",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes a method to increase the evasiveness of trojans in deep neural networks by introducing a distribution matching loss inspired by the Wasserstein distance, along with specificity and randomization losses. The approach reduces the effectiveness of model-level detectors while maintaining high attack success rates, and the authors observe that evasive trojans are also harder to reverse-engineer despite not being explicitly designed for this purpose."
          },
          "strengths": {
            "value": "The paper addresses a critical and underexplored problem in trojan detection: the development of evasive trojans that bypass model-level detectors. The method introduces novel technical components (distribution matching, specificity, and randomization losses) that are well-justified. The experimental setup is comprehensive, involving over 6,000 trojaned networks and demonstrating significant reductions in detector performance across multiple datasets. The findings highlight the importance of re-evaluating the offense-defense balance in trojan detection and provide actionable insights for improving detector robustness. The paper is clearly written with illustrative figures and a logical flow."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed comparison with state-of-the-art detectors beyond the ones mentioned, which limits the scope of the evaluation. The white-box threat model assumes full access to training data, which may not reflect real-world scenarios. The mechanism behind the increased reverse-engineering difficulty is not thoroughly analyzed, and the practical implications of the findings (e.g., mitigation strategies) are not discussed. Additionally, the implementation details of the proposed losses and their hyperparameters are not sufficiently described, which could hinder reproducibility."
          },
          "questions": {
            "value": [
              "How do the results hold under different threat models (e.g., black-box or gray-box) compared to the white-box assumption?",
              "What are the specific limitations of the white-box threat model in practical applications, and how could the method be adapted to less restrictive settings?",
              "How does the proposed method scale to larger or more complex neural networks, and what are the computational costs?",
              "Can the increased reverse-engineering difficulty be quantitatively analyzed, and what factors contribute to this phenomenon?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes a method to increase the evasiveness of trojan attacks against model-level detectors by introducing a distribution matching loss inspired by the Wasserstein distance, along with specificity and randomization losses. The authors demonstrate that their evasive trojans significantly reduce the effectiveness of various detection methods while maintaining high attack success rates, and also show that these trojans are harder to reverse-engineer."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in trojan detection research by focusing on model-level detectors, which are less explored compared to input-level or dataset-level methods. The method introduces novel loss functions (distribution matching, specificity, and randomization) to enhance trojan evasiveness. The experiments are extensive, involving over 6,000 trojaned networks, and the results show significant reductions in detection performance across multiple detectors. The work also highlights an unexpected secondary benefit of increased reverse-engineering difficulty, which adds to its significance."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient detail on the implementation of the proposed losses, particularly how the Wasserstein-inspired distribution matching is optimized. The experiments do not compare against state-of-the-art evasion methods, and the evaluation is limited to a narrow set of detectors and datasets. The claim that evasive trojans are harder to reverse-engineer is not thoroughly analyzed, with no ablation studies or quantitative analysis of reverse-engineering difficulty. Additionally, the paper is cut off mid-section, leaving critical details about the methodology and experiments incomplete, which undermines the rigor of the claims."
          },
          "questions": {
            "value": "1. How is the Wasserstein distance-based distribution matching loss implemented and optimized in practice? 2. What specific detectors and datasets were used in the experiments, and how do they compare to existing benchmarks? 3. Are the observed reverse-engineering difficulties due to the proposed method, or are they artifacts of the experimental setup? 4. How generalizable is the method across different network architectures and trojan attack types? 5. Why do the evasive trojans not explicitly aim to resist reverse-engineering, yet achieve this effect?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a method to increase the evasiveness of trojan attacks in deep neural networks against model-level detectors. The approach combines a Wasserstein-inspired distribution matching loss, specificity loss, and randomization loss to create trojans that reduce detection efficacy and hinder reverse-engineering, as demonstrated through extensive experiments on 6,000 trojaned networks."
          },
          "strengths": {
            "value": "The paper addresses a critical and underexplored problem in AI security by proposing a novel method for evading model-level trojan detectors. The experimental validation is comprehensive, with 6,000+ trojaned networks tested across diverse settings. The work highlights the offensive-defense imbalance in trojan detection and contributes to interpretability research. The clear motivation and practical implications for robust monitoring mechanisms strengthen its significance."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparison with prior evasive trojan methods, making it hard to assess novelty. The theoretical justification for the proposed losses is weak, and the mechanism behind improved reverse-engineering resistance is underexplored. The experiments focus on a white-box threat model, but the generalization to other scenarios (e.g., black-box) is unaddressed. The paper also omits ablation studies to isolate the contribution of each loss component."
          },
          "questions": {
            "value": "1. How does the proposed method compare to existing evasive trojan techniques in terms of effectiveness and novelty? 2. What specific properties of the distribution matching loss enable both detection evasion and reverse-engineering resistance? 3. Are the results robust across different network architectures and datasets beyond those tested? 4. How does the method perform under black-box or semi-white-box threat models? 5. Can the reverse-engineering resistance be attributed to the randomization loss, or are there other factors at play?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "3AAXabeZPG": {
    "paper_id": "3AAXabeZPG",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses biases in medical report generation (MRG) by defining visual and textual biases, linking them to frequency bias, and introducing a high-frequency amplification layer (HAL) to enhance model performance. The approach emphasizes improving model sensitivity to abnormal features by amplifying high-frequency signals, validated through experiments on benchmark datasets."
          },
          "strengths": {
            "value": "The paper provides clear, precise definitions of visual and textual biases, which are critical but underexplored in MRG. The proposed HAL method is simple yet effective, offering a novel solution to frequency bias. The experiments are comprehensive, including pseudo-spectrogram analysis and cross-attention visualization, demonstrating the method's efficacy. The work bridges a gap in understanding how biases affect model behavior, contributing to both theoretical and practical advancements in MRG."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of HAL on specific bias types. The justification for associating normal features with low-frequency signals and abnormal features with high-frequency signals is not thoroughly explained. Additionally, the experiments focus on two benchmarks (MIMIC-CXR and IU X-ray), but the generalizability to other datasets or modalities is not addressed. The problem statement is cut off, leaving gaps in the contextualization of the three imbalances and two biases."
          },
          "questions": {
            "value": [
              "How does the HAL method specifically differentiate between low-frequency (normal) and high-frequency (abnormal) signals in the context of MRG? Are there alternative interpretations of this frequency-based distinction?",
              "What are the limitations of the proposed bias definitions compared to existing work? For example, how do the authors address the inconsistency in prior definitions of textual bias?",
              "Could the experiments include comparisons with more state-of-the-art MRG models to better contextualize the performance improvements claimed? Are there cases where HAL might underperform?",
              "The paper mentions 'pseudo-spectrogram analysis' but does not describe the methodology. How is this analysis conducted, and what metrics are used to validate its effectiveness?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper addresses visual and textual biases in medical report generation (MRG) datasets, which lead to frequency bias where models prioritize low-frequency signals. The authors propose the High-Frequency Amplification Layer (HAL) to enhance sensitivity to high-frequency details, aiming to reduce biases and improve MRG performance. They define visual and textual biases rigorously and validate their approach on benchmarks like MIMIC-CXR and IU X-ray."
          },
          "strengths": {
            "value": "The paper provides clear definitions of visual and textual biases, which are critical but underexplored challenges in MRG. The introduction of HAL as a simple yet effective method to address frequency bias is novel. The experimental validation on established benchmarks and the inclusion of pseudo-spectrogram, cross-attention, and representation analyses demonstrate methodological rigor. The work also highlights the importance of debiasing frequency bias, which is a significant contribution to the field."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results comparing HAL to state-of-the-art methods, making it difficult to assess its effectiveness relative to existing approaches. The definitions of visual and textual biases may not be sufficiently distinct from prior work, and the paper does not thoroughly address how these biases interact with other model components. The analysis of frequency bias is theoretical, with limited empirical evidence from the experiments. Additionally, the paper does not discuss potential limitations or failure cases of HAL, which weakens the overall claim of robustness."
          },
          "questions": {
            "value": "1. How is the High-Frequency Amplification Layer (HAL) implemented technically, and what specific components does it modify in the model? 2. Are there ablation studies demonstrating the effectiveness of HAL in isolation versus when combined with other techniques? 3. How does the paper differentiate its definitions of visual and textual biases from prior work, such as the 'textual bias' mentioned in related works? 4. What are the limitations of HAL, and under what conditions might it fail? 5. Can the authors provide more details on the pseudo-spectrogram analysis and how it quantitatively supports the claim of reducing frequency bias?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of bias in medical report generation (MRG) by rigorously defining visual and textual biases and introducing a High-Frequency Amplification Layer (HAL) to mitigate frequency bias. The authors propose HAL to enhance models' sensitivity to high-frequency signals (abnormal features) and demonstrate its effectiveness through experiments on benchmark datasets."
          },
          "strengths": {
            "value": "The paper introduces precise definitions of visual and textual biases, which are critical but underexplored challenges in MRG. The proposed HAL method is novel and directly addresses frequency bias, a key issue in MRG. The experiments include diverse analyses (e.g., pseudo-spectrogram, cross-attention) and show competitive performance on standard benchmarks. The work has clear significance for improving diagnostic accuracy in medical imaging."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, limiting the ability to fully assess the experimental methodology and results. The definition of 'frequency bias' and its connection to visual/textual biases requires deeper justification. The implementation details of HAL are unclear, and the paper lacks ablation studies to validate its components. The analysis of how HAL mitigates specific biases is insufficient."
          },
          "questions": {
            "value": "1. How is the High-Frequency Amplification Layer implemented technically? 2. Are the experiments conducted on the full dataset, or are there limitations in the evaluation? 3. How does HAL handle class imbalance in medical datasets? 4. What ablation studies were performed to validate the effectiveness of individual components of HAL? 5. How does the paper address potential overfitting to high-frequency signals?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "3ENBquM4b4": {
    "paper_id": "3ENBquM4b4",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces SSDE, a method for continual reinforcement learning (RL) that combines structured sparsity with dormant neuron exploration to address the plasticity-stability trade-off. SSDE decomposes network parameters into forward-transfer (frozen) and task-specific (trainable) components via a fine-grained co-allocation strategy and employs sensitivity-guided dormant neurons to enhance exploration. The method demonstrates strong performance on the CW10-v1 benchmark, achieving a 95% success rate."
          },
          "strengths": {
            "value": "The paper presents a novel approach to continual RL by integrating structured sparsity and dormant neuron exploration, addressing a critical challenge in balancing plasticity and stability. The co-allocation strategy explicitly separates parameters for forward transfer and task-specific learning, which is a creative solution to mitigate catastrophic forgetting. The experimental results on the Continual World benchmark are promising, and the paper includes ablation studies and visualizations to support its claims. The method's efficiency in parameter allocation and reduced computational overhead compared to prior work are notable strengths."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art methods, particularly in terms of computational efficiency and scalability. For instance, it is unclear how SSDE's co-allocation strategy compares to CoTASP in terms of parameter utilization or training time. The mechanism for identifying and resetting dormant neurons is not sufficiently explained, raising questions about its practical implementation. Additionally, the theoretical justification for why structured sparsity and dormant neuron exploration improve plasticity is underdeveloped. The experiments focus on a single benchmark (CW10-v1), and the generalizability of the results to other tasks or domains is not thoroughly validated."
          },
          "questions": {
            "value": "1. How is the sensitivity-guided dormant neuron algorithm implemented? What specific metrics are used to identify 'insensitive' parameters? 2. Can the authors provide quantitative comparisons between SSDE's co-allocation strategy and existing methods like CoTASP or PackNet in terms of parameter efficiency and computational cost? 3. How does SSDE handle tasks with overlapping or highly dynamic features, which might challenge the structured sparsity assumption? 4. Are the ablation studies comprehensive enough to isolate the contributions of each component (e.g., co-allocation vs. dormant neuron exploration)? 5. What is the exact definition of 'forward-transfer efficiency' in the context of SSDE, and how is it measured?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces SSDE, a novel method for continual reinforcement learning (RL) that balances plasticity and stability by combining structured sparsity with dormant neuron exploration. SSDE decomposes neural network parameters into frozen (forward-transfer) and task-specific (trainable) components via a fine-grained co-allocation strategy, while a sensitivity-guided dormant neuron algorithm enhances expressiveness in sparse sub-networks. The method achieves 95% success on the CW10-v1 benchmark, demonstrating improved stability and plasticity compared to existing approaches."
          },
          "strengths": {
            "value": "Originality: SSDE introduces a unique combination of structured sparsity and dormant neuron exploration to address plasticity-stability trade-offs in continual RL. The fine-grained co-allocation strategy and sensitivity-guided exploration technique are novel contributions. Quality: The paper presents comprehensive experiments, including ablation studies and comparisons across benchmark versions (v1 and v2). Clarity: The problem statement and methodology are well-structured, with clear separation of allocation, inference, and training components. Significance: Continual RL is a critical challenge, and SSDE's ability to achieve state-of-the-art results while addressing limitations of prior structure-based methods (e.g., parameter capacity reduction) makes it impactful."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with specific baselines (e.g., CoTASP, PackNet) and does not provide sufficient analysis of how SSDE's components interact. The dormant neuron exploration mechanism is vaguely described, with no quantitative validation of its effectiveness. The claim of 'fully preemptive allocation' is not thoroughly justified, and the computational efficiency gains over CoTASP are not empirically supported. The paper is cut off mid-section, leaving critical details about the 'dormant neuron phenomenon' and its integration into SSDE incomplete."
          },
          "questions": {
            "value": [
              "How does SSDE compare to specific baselines like CoTASP or PackNet in terms of performance and parameter efficiency? Are the 95% results statistically significant compared to these methods?",
              "What is the exact mechanism of 'sensitivity-guided dormant neuron exploration'? How are insensitive parameters identified, and what metrics are used to evaluate their reset?",
              "The paper mentions ablation studies but does not describe which components were tested or how their removal affected results. Can the authors provide details on these experiments?",
              "How is the 'trade-off parameter' in the inference function determined? Is it manually tuned or learned dynamically?",
              "The paper claims to 'drastically reduce allocation time' but does not quantify this. What is the computational complexity of SSDE compared to existing methods?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces SSDE, a method for continual reinforcement learning (RL) that combines structured sparsity with dormant neuron exploration to address the plasticity-stability trade-off. SSDE decomposes parameters into frozen (forward-transfer) and task-specific (trainable) components using sparse coding, while a sensitivity-guided dormant neuron algorithm enhances expressiveness. The approach achieves a 95% success rate on the CW10-v1 benchmark."
          },
          "strengths": {
            "value": "Originality: SSDE introduces a novel co-allocation strategy and dormant neuron exploration, addressing gaps in structured sparsity methods. Quality: The experiments include ablation studies, comparisons to baselines, and analysis of parameter utilization. Clarity: The paper is well-structured with clear sections on methodology, related work, and results. Significance: Continual RL is a critical challenge, and SSDE's focus on plasticity-stability trade-offs has broad relevance."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art methods beyond the CW10-v1 benchmark. The dormant neuron exploration technique is under-described, with limited analysis of its sensitivity-guided mechanism. Efficiency claims (e.g., reduced allocation time) lack quantitative validation. The related work section is truncated, making it difficult to assess the novelty relative to prior work like CoTASP or HAT."
          },
          "questions": {
            "value": "1. How does SSDE compare to structure-based methods like PackNet or HAT in terms of performance and efficiency? 2. What are the exact criteria for identifying 'insensitive parameters' in the dormant neuron algorithm? 3. How does the sensitivity-guided exploration interact with the co-allocation strategy during training? 4. Are the computational savings claimed in the paper validated through runtime or resource measurements? 5. Why does SSDE achieve strong performance on CW10-v1 but not other benchmarks (e.g., Meta-World)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "3GMuudWmMV": {
    "paper_id": "3GMuudWmMV",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper evaluates the performance of Aya-101, a multilingual language model, on four NLP tasks (Aspect-Based Sentiment Analysis, Hate Speech Detection, Irony Detection, and Question-Answering) for Brazilian Portuguese, a low-resource language. The study employs a few-shot learning approach using specific datasets and compares Aya's results to Portuguese-specific models, highlighting both strengths and limitations."
          },
          "strengths": {
            "value": "The paper addresses a relevant and important problem in NLP: the disparity in resources between high-resource and low-resource languages. It provides a clear motivation for evaluating multilingual models like Aya in low-resource contexts. The methodology is well-structured, particularly the use of few-shot learning, which is suitable for scenarios with limited labeled data. The comparison with specialized models (e.g., Sabiá-7B) adds practical value. The paper also includes a comprehensive theoretical background for the tasks, which aids in contextualizing the study."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient detail on the methodology, particularly the few-shot setup (e.g., number of examples, prompt engineering, dataset curation). The experiments section is incomplete, making it impossible to assess the rigor of the evaluation. The analysis of Aya's limitations (e.g., struggles with slang, context-dependent features) is superficial, and there is no discussion of potential reasons for its poor performance in Hate Speech Detection. Additionally, the paper does not address broader implications for multilingual model design or propose actionable improvements for Aya."
          },
          "questions": {
            "value": "1. What specific few-shot strategies were used (e.g., number of examples, prompt formatting)? 2. How were the datasets (ABSAPT, ToLD-BR, etc.) adapted for the few-shot setting? 3. Why did Aya underperform in Hate Speech Detection compared to Sabiá-7B? 4. Are the results generalizable to other low-resource languages, or is the analysis limited to Portuguese? 5. What ablation studies were conducted to isolate the impact of multilingual training on Aya's performance?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper evaluates the performance of Aya-101, a multilingual language model supporting 101 languages, on four NLP tasks (Aspect-Based Sentiment Analysis, Hate Speech Detection, Irony Detection, and Question-Answering) for Brazilian Portuguese. The study uses a few-shot learning approach without fine-tuning, highlighting both strengths (e.g., strong QA performance) and limitations (e.g., poor hate speech detection compared to specialized models)."
          },
          "strengths": {
            "value": "The paper addresses an important gap in NLP for low-resource languages, with a clear focus on Brazilian Portuguese. The methodology is well-structured, leveraging few-shot learning to simulate real-world low-resource scenarios. The theoretical background section provides a solid foundation for understanding the tasks. The comparison with specialized models like Sabiá-7B adds context to Aya's performance. The paper also emphasizes the challenges of multilingual models in handling context-dependent language features."
          },
          "weaknesses": {
            "value": "The experiments appear limited in scope, with only a few datasets used for each task. The few-shot approach lacks detailed ablation studies to determine the optimal number of examples or prompt engineering strategies. The paper does not compare Aya with other multilingual models (e.g., mBART, XLM-R) that might be more relevant for low-resource languages. The analysis of why Aya struggles with certain tasks (e.g., hate speech detection) is superficial, and the paper does not propose specific improvements or address the impact of Aya's multilingual training on task-specific performance."
          },
          "questions": {
            "value": [
              "What specific few-shot strategies (e.g., example selection, prompt formatting) were used, and how were they validated? Could the results be sensitive to these choices?",
              "Why was the ABSA task improved when neutral examples were excluded? Were these examples inherently ambiguous or context-dependent?",
              "How does Aya's performance compare to other multilingual models (e.g., mBART, XLM-R) on the same tasks? Are there existing benchmarks for low-resource Portuguese NLP tasks?",
              "What are the limitations of the few-shot approach in this context, and could fine-tuning or domain adaptation improve results?",
              "The paper mentions 'complex slang and context-dependent features' as challenges. Are these issues specific to Brazilian Portuguese, or do they reflect broader limitations of multilingual models?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper evaluates Aya-101, a multilingual language model, on four NLP tasks (Aspect-Based Sentiment Analysis, Hate Speech Detection, Irony Detection, and Question-Answering) for Brazilian Portuguese, a low-resource language. The study employs a few-shot learning approach using existing datasets and highlights Aya's strengths in QA tasks while identifying limitations in handling complex contexts and low-resource scenarios."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in NLP for low-resource languages, which is highly significant. The use of few-shot learning is well-suited for resource-scarce scenarios, and the comparison with specialized models like Sabiá-7B provides meaningful context. The structured methodology and clear organization of sections (e.g., theoretical background, experiments) demonstrate strong quality and clarity. The focus on real-world applicability, such as social media moderation and sentiment analysis, underscores the paper's practical relevance."
          },
          "weaknesses": {
            "value": "The analysis of Aya's limitations lacks depth, particularly in understanding why it struggles with hate speech detection and irony. The few-shot approach is not thoroughly justified or compared to alternative methods (e.g., zero-shot, fine-tuning). The experiments focus on a narrow set of datasets, and the impact of excluding neutral examples in ABSA is not systematically explored. Additionally, the paper does not discuss how Aya's multilingual training could be leveraged to improve performance on Portuguese-specific tasks."
          },
          "questions": {
            "value": "1. How does Aya's performance vary with different few-shot configurations (e.g., varying numbers of examples)? 2. What specific challenges in Portuguese hate speech or irony detection are not addressed by Aya's training data? 3. Could the exclusion of neutral examples in ABSA be generalized to other tasks, and how might this affect broader NLP applications? 4. How do Aya's results compare to monolingual Portuguese models in terms of computational efficiency or scalability?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "3Imf21Jvwh": {
    "paper_id": "3Imf21Jvwh",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces HardNet, a framework for constructing neural networks that inherently satisfy hard constraints (affine/convex) without sacrificing model capacity. This is achieved by appending a differentiable projection layer to the network's output, ensuring constraint satisfaction by construction while retaining universal approximation capabilities. The method is validated across multiple applications including optimization solvers, control policies, and aircraft systems."
          },
          "strengths": {
            "value": "Originality: The paper presents a novel framework (HardNet) that directly addresses the challenge of enforcing hard constraints via differentiable projection layers, which is a significant departure from existing soft-constraint approaches. The universal approximation theorem for constrained networks is a strong theoretical contribution. Quality: The methodology is well-structured, with clear problem formulation and logical progression. Clarity: The abstract, introduction, and contributions are explicitly stated, making the paper accessible. Significance: The work tackles a critical problem in safety-critical applications, where constraint satisfaction is non-negotiable, and provides a generalizable solution."
          },
          "weaknesses": {
            "value": "The paper's experimental validation is incomplete (e.g., the table comparing methods is cut off, and full results are not provided), limiting the ability to assess practical effectiveness. The universal approximation proof is mentioned but lacks detailed exposition, making it hard to evaluate its rigor. The impact of the projection layer on training dynamics, computational efficiency, and scalability to large networks is not discussed. Additionally, the paper does not compare HardNet directly with related methods like C-DGM or RAYEN in the provided content, which weakens the novelty claim."
          },
          "questions": {
            "value": "1. How does the differentiable projection layer affect training stability and convergence? 2. What are the computational costs of the projection layer compared to baseline methods? 3. Can HardNet handle non-convex constraints, and if not, what are the limitations? 4. How does the universal approximation theorem proof account for input-dependent constraints? 5. Are there specific scenarios where HardNet's performance degrades compared to unconstrained networks?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces HardNet, a framework for neural networks that enforce hard constraints (affine/convex) through a differentiable projection layer, ensuring constraint satisfaction while retaining universal approximation capabilities. The authors demonstrate its effectiveness across multiple applications, including safety-critical systems and optimization solvers."
          },
          "strengths": {
            "value": "Originality lies in combining projection layers with neural networks to enforce hard constraints, a novel approach compared to prior soft constraint methods. Theoretical contribution includes a universal approximation theorem under constraints, which is significant for understanding trade-offs between constraints and expressiveness. The paper covers diverse applications, showcasing practical relevance. Clarity is strong in explaining the framework and its guarantees, though some sections are cut off."
          },
          "weaknesses": {
            "value": "The universal approximation proof lacks detailed analysis of how the projection layer preserves approximation power, especially for input-dependent constraints. Experimental validation is limited: the table of related work is incomplete, and comparisons with state-of-the-art methods (e.g., C-DGM, RAYEN) are not fully addressed. The paper does not discuss computational efficiency or scalability for large-scale problems. The claims about 'versatility' are not substantiated with comprehensive benchmarks."
          },
          "questions": {
            "value": "1. How does the projection layer specifically preserve universal approximation properties? What assumptions are made about the constraints? 2. Are there cases where the projection layer might fail to satisfy constraints, and how are these handled? 3. How does HardNet compare to existing methods (e.g., KKT-hPINN, LinSATNet) in terms of accuracy and efficiency? 4. What are the computational costs of the projection layer, and how does it scale with input dimensions? 5. Are the experiments on safety-critical systems reproducible, and what metrics were used to evaluate safety?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces HardNet, a framework for constructing neural networks that inherently satisfy hard constraints (e.g., affine/convex input-output relationships) without sacrificing model capacity. The method uses a differentiable projection layer to enforce constraints, retains universal approximation properties, and demonstrates effectiveness across safety-critical applications."
          },
          "strengths": {
            "value": "The paper's originality lies in its practical framework for hard constraints, which addresses a critical gap in safety-critical applications where soft constraints fail. The theoretical contribution of proving a universal approximation theorem under hard constraints is significant. The clarity of the problem statement, structured presentation, and diverse applications (e.g., control policies, aircraft systems) highlight the paper's broad relevance. The related work section is thorough, contextualizing the novelty of HardNet within existing literature."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as quantitative comparisons with existing methods or ablation studies on constraint types. The universal approximation proof is theoretical but not illustrated with concrete examples. The projection layer's scalability to complex constraints (e.g., non-convex) is unaddressed. Additionally, the paper does not fully discuss computational trade-offs compared to iterative methods like LinSATNet or C-DGM."
          },
          "questions": {
            "value": "How does HardNet handle non-affine/convex constraints? What are the computational costs of the projection layer compared to existing methods? Are there theoretical guarantees for constraint satisfaction in edge cases (e.g., input-dependent constraints with high dimensionality)? How was the universal approximation theorem proven—what assumptions were made about the constraint sets?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "3LFR5N2uv8": {
    "paper_id": "3LFR5N2uv8",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Younger, a dataset of 7,629 unique neural network architectures derived from 174K real-world models across 30 tasks. These architectures are represented as ONNX-compliant DAGs with detailed operator-level information, aiming to support AI-Generated Neural Network Architecture (AIGNNA) research. The dataset emphasizes flexibility by avoiding predefined search spaces, enabling exploration of diverse operator types and data flows."
          },
          "strengths": {
            "value": "Originality is evident in the dataset's focus on real-world architectures and its departure from predefined NAS search spaces. The scale (174K models) and ONNX compatibility offer practical value. The paper includes statistical analyses of architecture diversity and outlines an online platform for continuous expansion. The concept of AIGNNA introduces a novel framework for automation in architecture design, with preliminary experiments demonstrating local optimization potential."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology for filtering isomorphic architectures and ensuring uniqueness. The claim about balancing operator diversity (200 types vs. PyTorch's 2000) is unsupported by quantitative comparisons. Experiments are limited to operator prediction, with no comprehensive evaluation of the dataset's utility for AIGNNA tasks. The online platform's data validation process and privacy measures for parameter exclusion are unclear. The paper fails to compare Younger's diversity to existing NAS datasets like NAS-Bench-*."
          },
          "questions": {
            "value": "1. How exactly were isomorphic architectures filtered out? 2. What metrics were used to quantify the dataset's operator diversity compared to existing benchmarks? 3. How does the online platform ensure the quality of user-uploaded models? 4. What specific privacy mechanisms were applied to exclude model parameters? 5. Are there limitations in the dataset's coverage of tasks or architectures (e.g., bias toward specific domains)?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces Younger, a novel dataset containing 7,629 unique neural network architectures derived from 174K real-world models across 30 tasks. Each architecture is represented as a directed acyclic graph (DAG) using ONNX operators, aiming to support automated neural network architecture generation (AIGNNA). The dataset emphasizes flexibility by avoiding predefined search spaces and includes a platform for continuous updates."
          },
          "strengths": {
            "value": "Originality: Younger addresses a critical gap by providing a dataset free from predefined search spaces, leveraging ONNX operator diversity. The concept of AIGNNA as a research area is novel and timely. Quality: The dataset's construction process (ONNX conversion, filtering isomorphic architectures) demonstrates rigor. Clarity: The paper clearly articulates the motivation, methodology, and potential applications. Significance: By enabling exploration beyond traditional NAS constraints, Younger could catalyze innovations in automated architecture design."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation of the dataset's utility beyond initial local optimization experiments. The filtering process for isomorphic architectures is not described, raising questions about the dataset's uniqueness criteria. The comparison with existing NAS benchmarks (e.g., NAS-Bench-101) is incomplete due to truncated tables. The global AIGNNA paradigm remains underexplored, with no concrete examples of architectures generated from scratch. The paper also omits specifics about the online platform's implementation and data quality controls."
          },
          "questions": {
            "value": "1. How were isomorphic architectures identified and filtered? What metrics or algorithms were used? 2. What criteria were used to select the 174K models from public repositories? 3. Are there quantitative comparisons between Younger and existing NAS datasets in terms of operator diversity or architectural complexity? 4. How does the online platform ensure the quality and consistency of user-uploaded models? 5. What specific challenges remain in implementing the global AIGNNA paradigm, and how might Younger address them?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces Younger, a dataset comprising 7,629 unique neural network architectures derived from 174K real-world models across 30 tasks. The dataset is represented as ONNX-compatible directed acyclic graphs (DAGs) with detailed operator-level information, aiming to support research in Artificial Intelligence-Generated Neural Network Architecture (AIGNNA). It emphasizes flexibility by avoiding predefined search spaces and includes an online platform for continuous updates."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in neural architecture research by providing a diverse, real-world dataset that transcends traditional NAS constraints. The ONNX compatibility and focus on operator-level details enhance framework independence and practical utility. The dataset's balance between operator diversity and manageability is well-justified. The inclusion of an online platform for community contributions and the statistical analysis of architectural diversity are notable strengths. The clarity of the problem statement and the paper's structure are strong."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation of the dataset's utility. While initial experiments on 'operator and dataflow predictions' are mentioned, no results, baselines, or metrics are provided. The filtering process for isomorphic architectures is not described, raising concerns about the dataset's uniqueness. Comparisons with existing datasets (e.g., NAS-Bench-101) are absent, making it difficult to assess Younger's novelty. The challenges in the 'Global' AIGNNA paradigm are acknowledged but not explored in depth, leaving the dataset's applicability to novel architecture generation unclear."
          },
          "questions": {
            "value": "1. What specific statistical analyses were conducted at the operator/component/architecture levels, and what insights did they yield? 2. How are isomorphic architectures identified and filtered, and what criteria ensure the dataset's uniqueness? 3. How does Younger's operator diversity compare to frameworks like PyTorch or TensorFlow, and what trade-offs were made in balancing diversity with manageability? 4. What are the limitations of the dataset in supporting 'Global' AIGNNA tasks, and how might future work address these? 5. What metrics or benchmarks are proposed for evaluating AIGNNA methods using Younger?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "3MDmM0rMPQ": {
    "paper_id": "3MDmM0rMPQ",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces Inverse Prompt Engineering (IPE), a method for creating task-specific safety guardrails for large language models (LLMs) by leveraging existing prompt engineering data. The approach operationalizes the principle of least privilege, restricting LLM functionality to task-specific requirements. The authors evaluate IPE in a chatbot application and on the TensorTrust dataset, reporting significant improvements in robustness against adversarial attacks."
          },
          "strengths": {
            "value": "Originality: IPE presents a novel approach to task-specific safety by repurposing existing prompt engineering data, avoiding the need for additional data collection. Quality: The methodology uses a contrastive reward model training framework, which is theoretically sound. Clarity: The paper is well-structured with clear explanations of the problem, approach, and evaluation. Significance: Task-specific safety guardrails address a critical gap in real-world LLM deployments, offering practical advantages over general-purpose methods."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as specific metrics, baseline comparisons, and ablation studies. The TensorTrust evaluation is described minimally, with no analysis of how IPE's performance varies across attack types. The dependency on high-quality existing prompt engineering data is not thoroughly discussed, leaving open questions about applicability to scenarios without such data. Computational efficiency claims are not supported by quantitative analysis."
          },
          "questions": {
            "value": [
              "What specific metrics were used to measure IPE's performance on the TensorTrust dataset, and how does it compare to baseline methods?",
              "How does IPE handle cases where the existing prompt engineering data is noisy or incomplete?",
              "What are the computational costs of training the reward networks, and how do they scale with task complexity?",
              "Are there any scenarios where IPE's allow-list approach could inadvertently block legitimate outputs?",
              "How was the synthetic prompt collection generated, and what guarantees are there that it covers relevant adversarial patterns?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces Inverse Prompt Engineering (IPE), a method for creating task-specific safety guardrails for large language models (LLMs) by leveraging existing prompt engineering data. The approach operationalizes the principle of least privilege, restricting model functionality to task-specific requirements. IPE is evaluated in a chatbot application and on the TensorTrust dataset, showing significant improvements in robustness against adversarial attacks compared to existing methods."
          },
          "strengths": {
            "value": "Originality is strong, as IPE introduces a novel task-specific safety framework grounded in computer security principles. The method's reliance on existing prompt engineering data avoids additional data collection, addressing a practical limitation. Experiments demonstrate clear effectiveness in real-world settings, with substantial improvements in defense robustness. The paper is well-structured, with clear figures and a logical flow. The significance lies in its potential to improve safety in practical LLM deployments, which are often task-specific."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the components contributing to IPE's performance. The synthetic prompt generation process is not thoroughly explained, raising questions about their quality and relevance. The evaluation on TensorTrust uses pre-collected data, but the paper does not address how IPE generalizes to unseen tasks or data distributions. Additionally, the comparison to other allow-list-based methods is limited, making it harder to assess IPE's unique advantages."
          },
          "questions": {
            "value": "1. How are the synthetic prompts generated, and what criteria ensure their relevance to task-specific safety? 2. Are there ablation studies showing the contribution of specific IPE components (e.g., reward networks vs. prompt engineering data)? 3. How does IPE handle distribution shifts in real-world deployments, such as novel attack patterns not present in the training data? 4. What are the computational costs of training and deploying IPE compared to existing methods?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Inverse Prompt Engineering (IPE), a method for creating task-specific safety guardrails for large language models (LLMs) by leveraging existing prompt engineering data. The approach trains task-specific reward models to act as allow-lists, restricting outputs to those aligned with the intended task, and demonstrates improvements in robustness against adversarial attacks compared to general-purpose methods."
          },
          "strengths": {
            "value": "The paper presents a novel and practical approach to LLM safety by reframing security as a task-specific problem. The key insight—deriving guardrails from existing prompt engineering data—avoids the need for additional data collection, which is a significant advantage. The evaluation on real-world datasets (e.g., TensorTrust) and a chatbot application shows strong empirical results, with substantial improvements in robustness. The connection to the principle of least privilege in computer security is well-justified, and the method's lightweight training process is a practical benefit. The paper also addresses a critical gap in current safety measures by focusing on task-specific rather than general-purpose solutions."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with baseline methods in terms of computational efficiency, inference speed, or scalability. While IPE outperforms existing methods in the evaluated settings, it is unclear how it generalizes to other domains or tasks not tested here. The description of the reward model training process is brief, and it is not clear how the synthetic prompts are generated or why they are effective as negative examples. Additionally, the paper does not discuss potential limitations, such as cases where prompt engineering data might be biased or incomplete, or how IPE interacts with other safety mechanisms. The evaluation on TensorTrust relies on pre-existing defenses, but the exact role of IPE in that context is not fully explained."
          },
          "questions": {
            "value": [
              "How are the synthetic prompts used during reward model training generated? Are they manually crafted, or is there an automated method? What criteria ensure their effectiveness as negative examples?",
              "The paper claims IPE generalizes well to distribution shifts in benign user inputs. Can the authors provide specific examples or metrics to support this claim?",
              "In the TensorTrust evaluation, how is IPE integrated with the existing defenses? Does it act as a standalone system or complement them, and how does this affect overall performance?",
              "What are the computational costs of training and deploying IPE guardrails compared to existing methods? Is the method feasible for resource-constrained applications?",
              "How does IPE handle tasks where the intended functionality is ambiguous or evolves over time? Are there mechanisms to update the guardrails without retraining from scratch?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "3Mq1tY75nv": {
    "paper_id": "3Mq1tY75nv",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the problem of defining and measuring disentanglement in representation learning when factors of variation are not independent. The authors propose an information-theoretic definition of disentanglement, connect it to the concepts of minimal and sufficient representations, and introduce a method to measure disentanglement in non-independent scenarios. They validate their approach on multiple datasets, demonstrating its superiority over existing methods in non-independent settings."
          },
          "strengths": {
            "value": "The paper makes a significant theoretical contribution by generalizing disentanglement definitions beyond the independence assumption, which is a critical limitation in existing work. The connection between the proposed properties and minimal/sufficient representations is mathematically rigorous. The experimental design is comprehensive, showing the method's effectiveness across diverse scenarios. The paper also provides clear contextualization of its contributions relative to prior work, and the writing is logically structured with precise technical definitions."
          },
          "weaknesses": {
            "value": "The experimental validation is somewhat limited in scope, with only a few datasets used to demonstrate the method's effectiveness. The paper could benefit from more detailed ablation studies to isolate the impact of specific components of their framework. Additionally, the theoretical analysis assumes access to ground-truth factor distributions, which may not be practical in real-world applications. The comparison with existing methods is limited to a small subset of baselines, and it's unclear how the proposed method scales to high-dimensional data."
          },
          "questions": {
            "value": [
              "How does the proposed method handle scenarios where the true factor distributions are unknown or estimated from data?",
              "What are the computational limitations of the method when applied to high-dimensional or complex data distributions?",
              "Could the authors provide more intuition about how the information-theoretic properties directly relate to human-understandable disentanglement?",
              "How does the method perform when factors exhibit complex dependencies beyond simple pairwise correlations?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper addresses the limitation of existing disentanglement definitions and metrics in representation learning, which typically assume independence among factors of variation. The authors propose an information-theoretic definition of disentanglement that applies even when factors are dependent. They connect this definition to the properties of minimal and sufficient representations and introduce a method to measure disentanglement in such scenarios, demonstrating its effectiveness through experiments."
          },
          "strengths": {
            "value": "The paper's originality lies in addressing the critical gap of non-independent factors in disentanglement, a common real-world scenario. The theoretical foundation using information theory is rigorous and well-connected to existing literature. The paper clearly articulates the limitations of prior work and provides a structured approach to redefining disentanglement. The potential significance is high, as the proposed method could improve representation learning in practical applications where factor independence is rare."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete due to the paper's truncation, making it difficult to assess the method's efficacy. The paper lacks specific details on how the proposed properties (minimality, sufficiency) are quantified or validated. Additionally, the connection between the information-theoretic framework and practical implementation remains under-explained, which could limit reproducibility. The discussion of real-world applications is also minimal."
          },
          "questions": {
            "value": "1. What specific datasets were used to evaluate the proposed method, and how were non-independent factors incorporated? 2. How does the method handle high-dimensional or complex dependencies between factors? 3. Are there any theoretical guarantees or limitations to the proposed measurement approach? 4. How does the method compare to existing metrics in terms of computational efficiency and scalability?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces an information-theoretic definition of disentanglement that accounts for non-independent factors of variation, which is a critical limitation in existing approaches. The authors propose a method to measure disentanglement by evaluating minimal and sufficient representations, demonstrating its effectiveness on various datasets where traditional metrics fail."
          },
          "strengths": {
            "value": "The paper's originality lies in addressing the critical gap of non-independent factors in disentanglement definitions, which is a common real-world scenario. The connection to minimal and sufficient variables provides a theoretically grounded framework. The clarity of the problem formulation and the structured presentation of related work and metrics are strong. The significance is high, as disentanglement is a core challenge in representation learning, and the proposed approach has practical implications for fairness, interpretability, and generative modeling."
          },
          "weaknesses": {
            "value": "The paper is truncated, making it difficult to assess the completeness of experiments and the robustness of the proposed method. Key details about how the method handles specific types of dependencies (e.g., nonlinear or hierarchical relationships) are missing. The theoretical proofs and derivations of the proposed properties could be more thoroughly explained. Additionally, the paper lacks a detailed comparison with state-of-the-art methods beyond the mentioned baselines, which limits the evaluation of its practical utility."
          },
          "questions": {
            "value": "1. How does the method handle complex dependencies between factors, such as nonlinear or hierarchical relationships? 2. What specific datasets were used in the experiments, and how do they demonstrate the method's effectiveness compared to existing metrics? 3. Are there computational or scalability limitations to the proposed approach? 4. How is the balance between minimality and sufficiency ensured in practice?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "3Pn24GOcQ1": {
    "paper_id": "3Pn24GOcQ1",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper theoretically compares three approaches to achieving invariance in deep linear neural networks: data augmentation, regularization, and hard-wiring. The authors analyze the optimization landscapes of these methods, showing that hard-wiring and data augmentation share identical critical points (all saddles except the global optimum), while regularization introduces additional critical points. They also demonstrate that the regularization path converges to the hard-wired solution."
          },
          "strengths": {
            "value": "The paper makes a clear theoretical contribution by analyzing the loss landscapes of invariant linear networks, which is a well-defined and important problem. The originality lies in comparing data augmentation, regularization, and hard-wiring within a unified framework. The analysis is mathematically rigorous, with precise characterizations of critical points. The clarity is strong, with logical flow and structured presentation of results. The significance is high, as understanding optimization landscapes for invariant models has broad implications for deep learning theory and practice."
          },
          "weaknesses": {
            "value": "The focus on linear networks limits the applicability of findings to real-world nonlinear models. The paper lacks empirical validation, which could have strengthened the theoretical claims. The comparison between methods is primarily theoretical, and the practical implications of the critical point analysis are not fully explored. Additionally, the discussion of rank constraints and their real-world relevance (e.g., in VAEs or LoRA) is superficial and could be expanded."
          },
          "questions": {
            "value": "How do the theoretical findings translate to nonlinear networks, which are the primary focus of modern deep learning? What are the practical consequences of the increased number of critical points in the regularized case? Could the equivalence between data augmentation and hard-wiring be leveraged to design more efficient training algorithms? Are there scenarios where regularization might still be preferable despite its more complex landscape?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper investigates the optimization landscapes of three methods for achieving invariance in deep linear neural networks: data augmentation, regularization, and hard-wiring. It focuses on rank-constrained linear maps and demonstrates that hard-wiring and data augmentation share identical critical points (all saddles except the global optimum), while regularization introduces additional critical points. The regularization path is shown to converge to the hard-wired solution."
          },
          "strengths": {
            "value": "The paper addresses a timely and important question about the theoretical differences between invariance-inducing methods, which has practical implications for model design. The focus on rank-constrained linear networks is novel, as these models cannot be reparameterized as standard linear models, offering fresh insights into optimization landscapes. The theoretical analysis is rigorous, and the connection between data augmentation, regularization, and hard-wiring provides a cohesive framework for understanding their equivalence in certain limits. The work also highlights the relevance of rank constraints in practical architectures like VAEs and LoRA, bridging theory and application."
          },
          "weaknesses": {
            "value": "The analysis is limited to linear networks, which may restrict the generalizability of the findings to nonlinear models. The paper lacks empirical validation, which is critical for substantiating theoretical claims about optimization landscapes. Additionally, the comparison with related work on loss landscapes is thorough but could delve deeper into how the rank constraints and invariance properties differentiate this study from prior results. The practical implications of the findings are not fully explored, and the paper does not address how these insights might translate to real-world nonlinear networks."
          },
          "questions": {
            "value": "How do the theoretical results extend to nonlinear networks, which are the primary focus of modern deep learning? Are there empirical experiments or numerical validations to support the claims about critical points and the regularization path? What are the practical consequences of the observed differences in critical point structures for optimization dynamics? How do the rank constraints interact with other architectural choices (e.g., depth, width) in more complex models?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper theoretically compares three approaches to achieving invariance in deep linear neural networks: data augmentation, regularization, and hard-wiring. The authors analyze the optimization landscapes of these methods, showing that hard-wiring and data augmentation share the same critical points (all saddles except the global optimum), while regularization introduces more critical points. They also demonstrate the continuity of the regularization path converging to the hard-wired solution."
          },
          "strengths": {
            "value": "The paper makes a clear theoretical contribution by systematically analyzing the optimization landscapes of invariant models, which is a foundational question in deep learning. The originality lies in comparing data augmentation, regularization, and hard-wiring within a unified framework of rank-constrained linear networks. The methodology is rigorous, leveraging the simplicity of linear networks to derive precise characterizations of critical points. The significance is high, as understanding optimization landscapes is critical for improving model design and generalization. The paper is well-structured, with clear contributions and a thorough review of related work."
          },
          "weaknesses": {
            "value": "The focus on linear networks limits the practical applicability of the findings, as real-world models use nonlinearities. The theoretical analysis lacks empirical validation, which could strengthen the claims. The paper does not address how rank constraints interact with nonlinear activation functions or real-world data distributions. Additionally, the comparison between the three methods is somewhat abstract, and the practical implications of the critical point analysis are not fully explored. The truncated section on related work may also omit key prior studies."
          },
          "questions": {
            "value": "How do the theoretical findings extend to nonlinear networks with activation functions? What are the practical implications of the critical point analysis for model training? Are the assumptions about rank constraints valid in real-world scenarios with complex data? How does the regularization path's continuity hold under varying data augmentation strategies or network depths? Could the results be generalized to other invariance groups or loss functions?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "3UKOzGWCVY": {
    "paper_id": "3UKOzGWCVY",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces LEARN-BY-INTERACT, a data-centric framework for adapting large language models (LLMs) to realistic environments without human annotations. The framework generates agent data through self-instructed task generation and 'backward construction'—a process that abstracts interaction trajectories into instructions. It demonstrates effectiveness in improving performance on agentic tasks across multiple benchmarks like SWE-bench and WebArena."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel data synthesis approach that eliminates human annotation requirements, leveraging self-instruct and backward construction. The backward construction method is creatively designed to generate high-quality instructions from interaction histories. Quality: The experiments span diverse benchmarks (SWE-bench, WebArena, etc.), showing consistent improvements in both training and in-context learning (ICL). Clarity: The framework is well-structured, with clear descriptions of the synthesis pipeline and retrieval methods. Significance: The work addresses a critical challenge in real-world LLM deployment, offering a scalable solution for agent adaptation."
          },
          "weaknesses": {
            "value": "The paper is truncated, limiting visibility into detailed experimental setups (e.g., baseline comparisons, statistical significance of results). The backward construction process lacks depth on how it ensures instruction quality, especially in complex environments. The ablation studies are briefly mentioned but not elaborated, leaving questions about the robustness of the method. The paper also does not discuss potential limitations, such as scalability to highly dynamic or unstructured environments."
          },
          "questions": {
            "value": "1. How are the baselines defined for the reported performance improvements (e.g., 23.1% for Codestral-22B)? Are results statistically significant? 2. What specific challenges does backward construction address, and how is its effectiveness validated beyond qualitative examples? 3. Are there scenarios where the framework fails, and how does the authors' approach handle them? 4. How does the framework scale to environments with high state-space complexity or sparse documentation?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces LEARN-BY-INTERACT, a data-centric framework for self-adaptive LLM agents that synthesizes high-quality agent-environment interaction data without human annotations. The framework uses self-instruct to generate task instructions and 'backward construction' to create instructions from sub-trajectories, improving data quality. It demonstrates effectiveness across multiple benchmarks with significant performance gains in both training-free ICL and training-based settings."
          },
          "strengths": {
            "value": "The paper presents a novel data-centric approach for agent adaptation, avoiding costly human annotations. The backward construction method addresses a key challenge in synthesizing aligned instruction-trajectory pairs, leveraging LLMs' summarization capabilities. The experiments span diverse benchmarks (SWE-bench, WebArena, etc.), showing consistent improvements. The methodology is well-structured with clear algorithms and figures, and the significance of enabling self-adaptive agents in real-world environments is emphasized."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies on backward construction's components (e.g., how much improvement stems from summarization vs. abstraction). The experimental section is sparse on baseline comparisons (e.g., how does the framework compare to prior data synthesis methods?). The reliance on documentation for instruction generation may limit applicability to environments without such resources. The claims about 'high-quality' data are not rigorously validated with quantitative metrics beyond performance gains."
          },
          "questions": {
            "value": [
              "How is the backward construction process implemented? What specific LLM capabilities (e.g., summarization, abstraction) are leveraged, and how are sub-trajectories structured for this purpose?",
              "What are the exact metrics used in ablation studies? For instance, how much of the 10.6% improvement in training stems from backward construction versus other components?",
              "How does the framework handle environments with limited or no documentation? Are there fallback mechanisms or alternative strategies for such cases?",
              "Are there quantitative comparisons between the synthesized data and human-annotated data in terms of diversity, coverage, or alignment with task goals?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces LEARN-BY-INTERACT, a data-centric framework for synthesizing agent data without human annotations by leveraging self-instruct and LLMs to generate task instructions and interaction trajectories. The key innovation is 'backward construction,' where sub-trajectories are summarized to create instructions, improving data quality. The framework demonstrates effectiveness across multiple benchmarks with significant performance gains in both training-based and in-context learning (ICL) settings."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in agent adaptation—high annotation costs—by proposing a novel data-centric approach. The use of self-instruct and backward construction is innovative, leveraging LLMs' summarization capabilities to improve data quality. The experiments are comprehensive, covering diverse benchmarks (SWE-bench, WebArena, etc.), and the framework's versatility in training-free and training-based settings is a strength. The claims about performance improvements (e.g., 23.1% for training) are supported by empirical results."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology for backward construction, making it unclear how sub-trajectories are summarized. Experimental comparisons to existing data synthesis methods (e.g., RAG) are insufficient, weakening the novelty claim. The ablation studies are briefly mentioned but not elaborated, leaving questions about variable control. The paper also does not address potential limitations of using self-instruct-generated instructions, such as bias or coverage gaps. Claims about 'high-quality' data are not rigorously validated beyond downstream performance."
          },
          "questions": {
            "value": "1. Can the authors elaborate on the backward construction process? Specifically, how are sub-trajectories abstracted into instructions (e.g., techniques, models, or heuristics used)? 2. How is the quality of synthesized data filtered, and what metrics are used? 3. What baselines were compared against in the experiments (e.g., RAG, other data synthesis methods)? 4. How do the authors ensure the diversity and coverage of self-instruct-generated instructions? 5. Are there cases where backward construction fails, and how is this mitigated?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "3bcN6xlO6f": {
    "paper_id": "3bcN6xlO6f",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces Video Action Differencing (VidDiff), a novel task for identifying subtle differences between videos of the same action. The authors create VidDiffBench, a dataset with 549 video pairs annotated with fine-grained differences and timestamps. They propose the VidDiff method, an agentic workflow using specialized models for difference proposal, keyframe localization, and frame differencing, and evaluate it on both open-source and proprietary large multimodal models."
          },
          "strengths": {
            "value": "The paper addresses a novel and practical task with clear applications in skill learning and coaching. The VidDiffBench dataset is comprehensive, covering diverse domains and providing detailed annotations. The proposed method leverages specialized models for structured problem decomposition, showing promise for tackling localization and fine-grained comparison challenges. The work highlights critical gaps in existing models and provides a foundation for future research."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the effectiveness of each stage in the VidDiff method. The comparison with baselines is limited, particularly for the 'closed' setting where target differences are provided. The analysis of failure cases is superficial, and the paper does not thoroughly address how the proposed method generalizes across domains. Additionally, the evaluation of zero-shot performance on proprietary models (e.g., GPT-4o) lacks transparency regarding training data and fine-tuning details."
          },
          "questions": {
            "value": "1. How were the annotations for VidDiffBench validated for consistency and inter-rater reliability? 2. What specific limitations of existing LMMs were identified through failure case analysis, and how does VidDiff address them? 3. Are there quantitative results comparing the VidDiff method against simpler baselines (e.g., frame-level feature matching)? 4. How does the agentic workflow handle temporal alignment between video pairs, especially when sub-action durations vary? 5. What are the computational costs and scalability of the proposed method for large-scale video analysis?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Video Action Differencing (VidDiff), a novel task for identifying subtle differences between videos of the same action. The authors propose VidDiffBench, a benchmark dataset with 549 video pairs annotated with fine-grained differences and timestamps. They also present the VidDiff method, an agentic workflow combining large language models, CLIP, and VLMs to address localization and fine-grained comparison challenges."
          },
          "strengths": {
            "value": "Originality: The task of video action differencing and the associated benchmark address a critical gap in skill learning applications. The structured agentic workflow with specialized models for localization and comparison is innovative. Quality: The dataset is comprehensive, with detailed annotations across diverse domains. Clarity: The motivation and technical approach are well-explained, with clear examples. Significance: The work has practical implications for fitness, surgery, and other domains requiring skill refinement through video analysis."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with existing video comparison methods, making it hard to assess the novelty of the proposed approach. The benchmark's domain coverage (fitness, sports, surgery) is limited, potentially restricting generalizability. The evaluation metrics focus on accuracy without addressing qualitative aspects of difference descriptions. The failure case analysis is brief, and the paper does not thoroughly discuss limitations of the agentic workflow (e.g., error propagation across stages)."
          },
          "questions": {
            "value": [
              "How were the video pairs selected for VidDiffBench? Are they balanced across domains and difficulty levels?",
              "What metrics were used to evaluate the quality of generated difference descriptions beyond accuracy? How do these align with human preferences?",
              "How does the agentic workflow handle temporal misalignment between videos, which is critical for sub-action localization?",
              "Are the annotations validated by domain experts, and what measures were taken to ensure inter-annotator agreement?",
              "How does the closed setting (with provided difference strings) differ from the open setting in terms of technical challenges and evaluation criteria?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Video Action Differencing (VidDiff), a novel task for identifying subtle differences between videos of the same action. The authors create VidDiffBench, a benchmark with 549 video pairs annotated with 4,469 fine-grained differences and 2,075 timestamps. They propose the VidDiff method, an agentic workflow combining large multimodal models (LMMs) for localization and frame comparison, and demonstrate that existing LMMs struggle with this task."
          },
          "strengths": {
            "value": "The paper addresses a novel and practical task with clear real-world applications in skill learning and coaching. The VidDiffBench dataset is comprehensive, with detailed annotations and a taxonomy of action differences that aligns with domain expertise. The method's structured agentic workflow leverages specialized models for each stage, showing promise for tackling localization and fine-grained comparison challenges. The work also highlights critical limitations of existing LMMs, providing valuable insights for future research."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the effectiveness of individual components in the VidDiff method. The experiments focus on zero-shot performance but do not explore fine-tuning or domain adaptation. The benchmark's diversity is mentioned, but specific details about the domains (e.g., fitness, surgery) and how they contribute to the task's difficulty are unclear. The comparison to related work, such as Nagarajan & Torresani (2024), is brief and does not fully address differences in granularity or methodology."
          },
          "questions": {
            "value": [
              "How does the VidDiff method handle domain shifts between the benchmark's diverse applications (e.g., fitness vs. surgery)?",
              "What is the exact process for the agentic workflow's three stages, and how are the specialized models integrated?",
              "Are there limitations in the benchmark's annotation process that could affect the reliability of the results?",
              "How does the method perform on actions with minimal visual differences versus those with obvious disparities?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "3ep9ZYMZS3": {
    "paper_id": "3ep9ZYMZS3",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper proposes HyPER, a model-agnostic framework that combines neural surrogates, reinforcement learning (RL), and physics simulators to reduce rollout errors in neural networks trained on limited data. HyPER uses RL to decide when to invoke a physics simulator for 'knowledge-guided corrections,' improving accuracy and adaptability in modeling transient dynamics governed by PDEs."
          },
          "strengths": {
            "value": "Originality lies in the novel integration of RL with physics-based simulators for dynamic error correction, addressing data paucity and rollout errors in neural surrogates. The approach is model-agnostic and simulator-agnostic, broadening its applicability. The paper highlights significant error reduction (47%-78%) and adaptability to noise, demonstrating potential impact. Clarity is strong in the abstract and introduction, with a clear problem statement and motivation. The significance is high, as it tackles critical challenges in scientific machine learning and surrogate modeling."
          },
          "weaknesses": {
            "value": "The paper is truncated, making it impossible to evaluate the experimental methodology, baselines, or statistical significance of results. Key details about the RL policy training (e.g., reward formulation, environment design) and integration with non-differentiable simulators are missing. The claim of 'cost-aware' correction lacks quantitative analysis of computational trade-offs. Without full experiments, it is unclear how HyPER compares to existing hybrid models or surrogate-only approaches."
          },
          "questions": {
            "value": "1. What specific baselines were used to validate the 47%-78% error reduction? How does HyPER compare to existing hybrid models or physics-informed neural networks? 2. How is the RL policy trained when the simulator is non-differentiable? What reward function is used to balance accuracy and computational cost? 3. Are the experiments tested on diverse PDEs (e.g., stiff vs. non-stiff) and noise levels? 4. How is the 'cost-aware' aspect quantified—e.g., wall-clock time, energy, or simulator calls? 5. What are the limitations of the current framework, and how might they affect real-world deployment?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes HyPER, a model-agnostic framework combining neural surrogates, reinforcement learning (RL), and physics simulators to reduce rollout errors in neural surrogates trained with limited data. HyPER uses an RL decision model to dynamically invoke a physics simulator for 'knowledge-guided correction' during inference, aiming to improve accuracy and adaptability."
          },
          "strengths": {
            "value": "Originality: The integration of RL for dynamic simulator invocation in hybrid modeling is novel, addressing limitations of prior methods that rely on differentiable simulators or static couplings. The model-agnostic design allows flexibility across surrogate architectures. The problem of rollout error in data-poor settings is significant, and the approach's adaptability to noise and changing conditions highlights its practical relevance. The paper's clear motivation and technical framing provide a strong foundation for the proposed solution."
          },
          "weaknesses": {
            "value": "The paper is truncated, omitting critical details about experimental validation, baseline comparisons, and technical implementation of the RL component. The claimed 47-78% error reduction lacks supporting data, making it difficult to assess the empirical impact. The RL policy's reward function, training methodology, and cost-awareness mechanism are not explained. The paper also does not address how HyPER scales to complex PDEs or non-differentiable simulators, which are central to its claims. The absence of ablation studies or comparisons to state-of-the-art hybrid models undermines the novelty and effectiveness claims."
          },
          "questions": {
            "value": [
              "How is the RL policy trained? What is the reward function, and how does it balance error reduction with computational cost?",
              "What specific baselines were compared against (e.g., surrogate-only models, existing hybrid methods, or PDE-based solvers)?",
              "How does HyPER handle simulators without gradients? What are the computational overheads of dynamic simulator invocation?",
              "Are there ablation studies demonstrating the contribution of each component (surrogate, RL, simulator) to the overall performance?",
              "How does the framework adapt to different types of PDEs or physical systems? Are there limitations in the scope of the experiments?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces HyPER, a model-agnostic framework that combines neural surrogates, reinforcement learning (RL), and physics simulators to reduce rollout errors in neural surrogates, particularly under data scarcity. HyPER uses RL to decide when to correct surrogate predictions with a physics-based simulator, aiming to improve accuracy and adaptability to dynamic conditions."
          },
          "strengths": {
            "value": "The paper presents a novel approach by integrating RL with hybrid modeling to address rollout errors in neural surrogates, which is a critical challenge in scientific machine learning (SciML). The model-agnostic design and adaptability to non-differentiable simulators are significant advantages. The work highlights practical relevance for applications with limited data and transient dynamics. The clarity of the problem statement and framework description is strong, with a logical structure and illustrative figure."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as specific metrics, baselines for comparison, or ablation studies to isolate the contributions of RL and simulator integration. The claimed 47-78% error reduction is not contextualized against existing methods, and the experiments appear to be limited to a single case study. The RL policy training mechanism and cost function are under-described, raising questions about scalability and practical implementation. The related work section does not sufficiently contrast HyPER with existing hybrid models that also use simulators without differentiability constraints."
          },
          "questions": {
            "value": "1. How is the RL policy trained? What is the reward function, and how does it balance accuracy and computational cost? 2. What specific PDEs or benchmarks were used for evaluation, and how do they reflect real-world scientific challenges? 3. Are there ablation studies showing the individual contributions of the neural surrogate, simulator, and RL component? 4. How does HyPER compare to non-RL hybrid models that use static coupling between surrogates and simulators? 5. What is the computational overhead of the RL policy, and how does it scale with problem complexity?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "3fGwTRRudc": {
    "paper_id": "3fGwTRRudc",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces FocalLens, a method for generating conditional image representations using natural language instructions. By leveraging vision instruction tuning data and contrastive learning, FocalLens adapts a pretrained vision encoder to produce task-specific visual features, demonstrated to outperform generic encoders like CLIP on downstream tasks."
          },
          "strengths": {
            "value": "Originality: FocalLens addresses a novel problem of task-specific conditional image representation, moving beyond fixed generic features. Quality: The paper presents extensive experiments on challenging benchmarks (e.g., SugarCrepe, MMVP-VLM) with measurable gains. Clarity: The motivation and high-level approach are well-articulated, with clear figures illustrating the method's application. Significance: The ability to condition visual features on natural language instructions opens new possibilities for downstream tasks requiring task-specific focus."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of the contrastive tuning process and how instructions are integrated into the vision encoder. Comparisons with existing conditional vision methods (e.g., InstructBLIP) are superficial, and the ablation studies are not sufficiently comprehensive. The claims about zero-shot generalization lack empirical validation, and the paper does not address potential limitations in handling ambiguous or complex instructions."
          },
          "questions": {
            "value": "1. How is the contrastive learning objective specifically designed to align image features with natural language instructions? 2. What architectural modifications are made to the vision encoder to incorporate instruction conditioning? 3. Are the performance gains consistent across diverse instruction types, or are they limited to specific domains? 4. How does FocalLens handle instructions that require multi-step reasoning or external knowledge?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces FocalLens, a method for generating conditional image representations by leveraging natural language instructions. The approach uses vision instruction tuning data and contrastive learning to adapt a pretrained vision encoder, enabling it to produce tailored image features based on specific tasks or contexts described in text. Experiments show improvements over models like CLIP on downstream tasks such as image retrieval and classification."
          },
          "strengths": {
            "value": "The paper presents a novel approach to conditional visual feature extraction by integrating natural language instructions, addressing a gap in task-specific representation learning. The method's potential to generalize across diverse downstream tasks is promising. The experimental results demonstrate measurable performance gains on benchmark datasets, suggesting practical relevance. The work also contributes to the broader discussion on adaptive vision foundation models."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of the contrastive tuning process and how the model handles varying instruction types. The experimental section appears incomplete, with limited comparison to alternative approaches like MLLMs or other conditional representation methods. The ablation studies and analysis of failure cases are missing, which hampers the evaluation of the method's robustness. Additionally, the paper does not thoroughly address how the approach scales to more complex or diverse instruction domains."
          },
          "questions": {
            "value": [
              "How exactly is the contrastive learning objective formulated, and what specific components of the model are modified during instruction tuning?",
              "What are the limitations of the current instruction tuning data in terms of task diversity and coverage?",
              "How does FocalLens handle ambiguous or multi-faceted instructions that could lead to conflicting feature prioritization?",
              "Are there quantitative analyses comparing FocalLens to other conditional representation methods (e.g., InstructBLIP) on the same benchmarks?",
              "What is the computational cost of the proposed method compared to standard vision encoders like CLIP?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces FocalLens, a method for generating conditional image representations using natural language instructions. By leveraging vision instruction tuning data and contrastive learning, FocalLens produces task-specific visual features that emphasize relevant aspects of an image. The approach is evaluated on downstream tasks, showing performance improvements over standard vision encoders like CLIP."
          },
          "strengths": {
            "value": "The paper addresses a relevant problem in visual feature extraction by enabling task-specific representations through natural language conditioning, which is both original and practical. The experimental results on benchmark datasets demonstrate significant improvements, and the method's flexibility in handling diverse instructions is a key strength. The presentation is clear, with illustrative figures and a well-structured approach to contrastive tuning."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of key components (e.g., instruction tuning vs. contrastive learning). The comparison with baselines is limited, as it only mentions CLIP without including other state-of-the-art models. The theoretical justification for the effectiveness of the approach is underdeveloped, and the paper does not address potential limitations, such as handling complex or ambiguous instructions. Additionally, the experimental setup for downstream tasks is not thoroughly described, raising questions about reproducibility."
          },
          "questions": {
            "value": [
              "What specific vision instruction tuning data was used, and how was it curated? How does this data differ from standard vision-language pretraining corpora?",
              "How does FocalLens handle instructions that require multi-step reasoning or context beyond simple object descriptions?",
              "Are there ablation studies demonstrating the contribution of contrastive learning versus instruction tuning alone?",
              "Why was CLIP chosen as the base model? How would the results differ with other vision encoders (e.g., ViT, ALIGN)?",
              "What are the computational costs of the instruction tuning process, and how does it scale to large datasets?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "3g2iyFU8gA": {
    "paper_id": "3g2iyFU8gA",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes MFSC, a method for learning compact and task-relevant state representations from multi-view observations in reinforcement learning (RL). It combines self-attention mechanisms with bisimulation metric learning to fuse information across views, incorporates a mask-based latent reconstruction task for robustness to missing views, and evaluates performance on Meta-World and Pybullet benchmarks."
          },
          "strengths": {
            "value": "Originality is evident in the integration of self-attention and bisimulation principles for multi-view fusion, addressing two key challenges (high-dimensional redundancy and view-specific information aggregation). The method's modular design allows seamless integration into existing RL frameworks. Experiments on standard benchmarks demonstrate effectiveness in capturing task-relevant details and handling missing views. The paper also provides a clear motivation for the problem and connects to relevant prior work."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with state-of-the-art multi-view RL methods (e.g., recent works on cross-view attention or contrastive learning). The mask-based reconstruction task is not thoroughly explained, and its impact on performance is not quantified. The ablation studies are limited, and the computational efficiency of the method is not discussed. The theoretical justification for combining self-attention with bisimulation is underdeveloped, and the paper does not address potential limitations in scalability to more views."
          },
          "questions": {
            "value": [
              "How does MFSC compare to recent multi-view RL approaches like (Jangir et al., 2022) or (Hwang et al., 2023) in terms of performance and efficiency?",
              "What is the exact implementation of the mask-based latent reconstruction task? How is the mask generated, and what loss function is used for reconstruction?",
              "Are there experiments demonstrating MFSC's performance with varying numbers of views (e.g., 2 vs. 5 views)?",
              "How does the method handle extreme cases of missing views (e.g., all views except one are missing)?",
              "What are the computational costs of MFSC compared to baselines? Is the self-attention mechanism scalable to high-resolution images?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes MFSC, a method for learning compact and task-relevant state representations from multi-view observations in reinforcement learning (RL). It combines self-attention mechanisms, bisimulation metric learning, and a mask-based latent reconstruction task to fuse information across views, while also enabling robustness to missing views. The approach is evaluated on Meta-World and Pybullet benchmarks."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in multi-view RL: balancing information richness with computational efficiency. The integration of self-attention and bisimulation principles is novel, offering a structured way to prioritize task-relevant features. The mask-based reconstruction task introduces a practical solution for handling missing views, which is a significant practical contribution. The experiments on standard benchmarks demonstrate the method's effectiveness, and the clear separation of components (e.g., fusion, reconstruction, robustness) enhances interpretability."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art multi-view RL methods, making it hard to assess the magnitude of improvement. The ablation studies and analysis of hyperparameter sensitivity are missing, which limits understanding of the method's robustness. The theoretical justification for combining self-attention and bisimulation is superficial, and the paper does not thoroughly analyze how the mask strategy affects representation quality. Additionally, the truncation of the paper raises concerns about incomplete experimental results and missing details in the related work section."
          },
          "questions": {
            "value": "1. What specific baselines were compared against MFSC on Meta-World and Pybullet? 2. How does the method scale with an increasing number of views, and what is the computational overhead? 3. Are the attention weights explicitly analyzed to confirm they prioritize task-relevant features? 4. How is the mask applied during training versus inference, and what criteria determine masked regions? 5. Could the latent reconstruction task inadvertently suppress useful information rather than enhance it?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces MFSC, a method for learning compact and task-relevant state representations from multi-view observations in reinforcement learning (RL). It combines self-attention with bisimulation metric learning to fuse multi-view data, incorporates a mask-based latent reconstruction task to reduce redundancy, and enables handling of missing views through additional mask tokens. The approach is evaluated on Meta-World and Pybullet benchmarks."
          },
          "strengths": {
            "value": "The paper addresses critical challenges in multi-view RL, including high-dimensional data and informative aggregation, with a novel architecture. The integration of self-attention and bisimulation metric learning is theoretically grounded. The mask-based reconstruction task and handling of missing views demonstrate practical relevance. The experiments on standard benchmarks show competitive performance, and the code is publicly available. The related work section is comprehensive, and the problem formulation is clear."
          },
          "weaknesses": {
            "value": "The paper lacks ablation studies to isolate the contributions of individual components (e.g., self-attention vs. bisimulation). The theoretical justification for combining bisimulation with self-attention is underdeveloped. The comparison with state-of-the-art multi-view RL methods is limited, and the paper does not address how task-relevance is quantified. The experiments on Pybullet are brief, and the computational efficiency of MFSC is not discussed. The paper also does not clarify how the 'task-relevant details' are determined during training."
          },
          "questions": {
            "value": [
              "How does MFSC compare to existing multi-view RL methods like multi-view autoencoders or generative models in terms of performance and efficiency?",
              "What ablation studies were conducted to validate the necessity of self-attention, bisimulation, and the mask-based reconstruction components?",
              "How is task-relevance of the fused representations quantified or validated in the experiments?",
              "What is the impact of varying the number of views on MFSC's performance, and how does it scale to more than two views?",
              "How does the mask token mechanism handle partial or complete absence of views during inference, and what are the limitations of this approach?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "3n6DYH3cIP": {
    "paper_id": "3n6DYH3cIP",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces an extendable structure learning strategy for Bayesian networks that efficiently incorporates new variables into an existing graph without retraining from scratch. It proposes two algorithms for updating the graph and an iterative method to build the network incrementally, demonstrating significant runtime improvements over traditional methods like PC."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in Bayesian network structure learning by enabling dynamic updates with new variables, which is highly relevant for real-world applications. The empirical results show impressive runtime reductions (up to 1300×) while maintaining accuracy, validated on benchmark datasets. The iterative approach is novel and practical for large-scale problems. The theoretical analysis of complexity and the clear problem formulation enhance the paper's rigor. The contributions are significant for scalability in Bayesian network learning."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed discussion of scenarios where the existing graph is not a perfect map (P-map), which could invalidate the assumption that adding a new variable only removes edges. The theoretical guarantees are limited, and the analysis of edge cases (e.g., when new edges are necessary) is absent. The experiments focus on structural Hamming distance but omit other metrics like BIC or log-likelihood for comprehensive evaluation. The comparison with incremental learning methods is not sufficiently detailed, and the practical limitations of the approach (e.g., dependency on initial graph quality) are underexplored."
          },
          "questions": {
            "value": "How does the method handle cases where the original graph is not a P-map, potentially requiring new edges for the added variable? What are the specific conditions under which the assumption of edge deletions only holds? How does the iterative approach scale with very large datasets or high-dimensional variables? Are there any constraints on the structure of the initial graph (e.g., sparsity, degree limits) that affect the method's performance? How does the method compare to online learning approaches that update models with new data instances rather than variables?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces an extendable structure learning strategy for Bayesian networks that efficiently incorporates new variables into an existing P-map graph without retraining from scratch. The method leverages the existing structure to reduce computational overhead, achieving up to 1300x runtime improvements. Additionally, the authors propose an iterative algorithm that incrementally adds variables to build a full P-map, maintaining accuracy while improving efficiency compared to traditional methods."
          },
          "strengths": {
            "value": "The paper's originality lies in addressing a critical gap in Bayesian network structure learning: dynamic updates when new variables are introduced. The extendable algorithm's theoretical foundation, such as the claim that adding a new variable only deletes edges (not adds them), offers a novel perspective. The empirical results demonstrate significant computational gains, which is a strong practical contribution. The clarity of the problem statement, definitions (e.g., d-separation), and algorithm descriptions is commendable. The significance of enabling real-time updates in high-dimensional settings makes this work relevant for applications in social science, finance, and other domains with evolving data."
          },
          "weaknesses": {
            "value": "The paper lacks detailed theoretical justification for the key claim that adding a new variable only deletes edges in the existing graph. This assumption may not hold in all scenarios, and its validity requires further analysis. The empirical evaluation, while promising, is limited to a small set of benchmark datasets—more extensive experiments on diverse or larger-scale data would strengthen the claims. Additionally, the paper does not compare the proposed iterative approach to existing incremental learning methods (e.g., online or streaming algorithms) that might address similar challenges. The complexity analysis focuses on the constraint-based algorithm but omits details about the score-based variant's performance."
          },
          "questions": {
            "value": [
              "How does the method ensure that edge deletions do not compromise the faithfulness of the resulting P-map? Are there guarantees that the reduced search space still contains the optimal DAG?",
              "What are the assumptions about the data distribution (e.g., faithfulness, Markov condition) required for the edge deletion claim to hold? How does the method handle violations of these assumptions?",
              "The paper mentions runtime improvements but does not discuss memory usage or scalability to very large networks. How does the approach perform in terms of resource constraints?",
              "How does the iterative algorithm handle variables with complex dependencies that might require re-optimizing earlier parts of the graph when new variables are added?",
              "Are there specific scenarios where the extendable algorithm might fail or degrade in performance compared to retraining from scratch? How does the authors' approach compare to alternative methods for dynamic Bayesian network learning?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes an extendable structure learning strategy for Bayesian networks that efficiently incorporates new variables into an existing graph without retraining from scratch. It introduces two algorithms for extending the graph and an iterative approach to build the full structure incrementally, claiming significant computational efficiency gains while maintaining accuracy."
          },
          "strengths": {
            "value": "The paper's originality lies in its extendable framework for Bayesian network structure learning, addressing a critical gap in dynamic or large-scale applications. The theoretical analysis of edge deletion when adding new variables is novel. The experimental results show impressive runtime improvements (up to 1300×), which could be impactful for real-time applications. The iterative approach demonstrates practical utility, and the paper is well-structured with clear problem formulations."
          },
          "weaknesses": {
            "value": "The claim that adding a new variable only deletes edges (and never adds new ones) lacks theoretical justification or empirical validation. The experiments rely on limited benchmark datasets and do not compare against state-of-the-art methods beyond PC. The runtime improvement claims are not contextualized against alternative approaches like incremental learning. The paper also omits discussions on handling non-faithful distributions or violations of Markov assumptions, which are critical for Bayesian networks."
          },
          "questions": {
            "value": "1. What is the theoretical basis for the edge deletion assumption when adding new variables? How does the method handle cases where the new variable introduces novel dependencies? 2. Are the experimental results reproducible? What specific datasets and baselines were used? 3. How does the proposed method compare to existing incremental learning approaches (e.g., Kocacaban & Cussens 2019)? 4. What are the limitations of the iterative approach in terms of scalability or accuracy? 5. How does the method ensure correctness when the original graph is not a perfect map?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "3tukjsVyrE": {
    "paper_id": "3tukjsVyrE",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes a novel method for scaling speech-text pre-training by generating synthetic interleaved speech-text data from text corpora, eliminating the need for parallel speech-text datasets. The approach includes a supervised speech tokenizer and a two-stage training process, achieving state-of-the-art results on spoken question answering and an end-to-end spoken chatbot."
          },
          "strengths": {
            "value": "Originality is evident in the synthetic data generation approach, which addresses data scarcity in speech-text pre-training. The method's efficiency in bypassing actual speech synthesis and leveraging text-to-token models is innovative. Experiments demonstrate significant performance improvements (e.g., 31% on spoken QA), and the architecture design (e.g., 12.5Hz tokenizer) shows careful trade-off analysis. Clarity is strong, with clear figures and structured methodology. The significance lies in advancing end-to-end speech models and reducing dependency on scarce speech data."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies on the synthetic data quality (e.g., how TTS-generated tokens compare to real speech). The supervised tokenizer's performance metrics (e.g., tokenization accuracy, semantic preservation) are not thoroughly analyzed. The comparison to baselines like Moshi is limited to a single task, and the chatbot's real-world performance (e.g., latency, robustness) is not evaluated. The claim about 'competitive performance' lacks quantitative benchmarks against existing speech chatbots."
          },
          "questions": {
            "value": "1. How was the quality of the synthetic interleaved data validated? Were human or automated metrics used to assess speech-text alignment? 2. What specific metrics were used to evaluate the supervised tokenizer's semantic preservation at 12.5Hz? 3. How does the chatbot's performance compare to non-end-to-end systems (e.g., cascaded ASR+LLM+TTS) in terms of task accuracy and speech quality? 4. Were there any challenges in training the text-to-token model with TTS datasets, and how were they mitigated?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper proposes a method to scale speech-text pre-training by generating synthetic interleaved speech-text data from text corpora, eliminating the need for parallel speech-text datasets. It introduces a supervised speech tokenizer and demonstrates state-of-the-art performance on speech language modeling and spoken question answering tasks, as well as an end-to-end spoken chatbot."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in speech-language modeling—data scarcity—by leveraging synthetic data, which is a novel and impactful approach. The method's scalability (1 trillion tokens) and practical application in a spoken chatbot highlight its significance. The two-stage training process (pre-training on synthetic data + fine-tuning on dialogue data) is well-structured. The supervised speech tokenizer with 12.5Hz sampling rate balances semantic preservation and efficiency, showcasing technical creativity. The results on spoken QA tasks (31% accuracy) significantly outperform prior work (13%), demonstrating strong empirical validation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies on key components, such as the impact of different text-to-token models or sampling rates (e.g., 6.25Hz vs. 50Hz). The choice of 12.5Hz as optimal is not thoroughly justified, and the trade-offs between semantic retention, efficiency, and reconstruction quality remain underexplored. The synthetic data generation process relies on TTS datasets, but the paper does not compare this approach to alternatives (e.g., direct speech synthesis). Additionally, the chatbot's performance metrics are not compared to strong baselines in detail, limiting the clarity of its competitive advantage."
          },
          "questions": {
            "value": "1. How does the quality of the text-to-token model (trained on TTS datasets) affect the downstream performance of the SpeechLM? 2. What ablation results are available for different sampling rates (e.g., 6.25Hz vs. 12.5Hz)? 3. How does the flow-matching decoder compare to traditional autoregressive or non-autoregressive speech synthesis methods in terms of efficiency and quality? 4. Are there any limitations in the synthetic data's diversity or realism that could bias the model's performance? 5. How does the chatbot's performance on standard speech dialogue benchmarks (e.g., MultiWOZ) compare to existing speech-based systems?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes a method to scale speech-text pre-training by generating synthetic interleaved speech-text data from text corpora, eliminating the need for parallel speech-text datasets. The approach uses a text-to-token model to create interleaved data, a supervised speech tokenizer trained with ASR, and two-stage training to achieve state-of-the-art performance on spoken QA tasks and an end-to-end spoken chatbot."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in speech-language modeling: data scarcity. The synthetic interleaved data approach is innovative, avoiding reliance on parallel speech-text datasets. The supervised speech tokenizer with ASR-based training shows promise for semantic preservation at low frame rates. The results on spoken QA (31% accuracy) and the end-to-end chatbot demonstrate practical impact. The two-stage training framework provides a clear methodology for scaling pre-training to 1 trillion tokens."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies on the text-to-token model's design and the 12.5Hz sampling rate selection. Experimental comparisons with alternative synthetic data approaches (e.g., pure TTS vs. interleaved) are missing. The chatbot's baseline comparisons are vague—how do they compare to speech-only or text-based baselines? The computational cost of generating 600B tokens is not discussed. The paper also doesn't address domain generalization (e.g., non-English speech) or potential biases in the text corpora used for synthesis."
          },
          "questions": {
            "value": [
              "How was the text-to-token model evaluated for fidelity in converting text spans to speech tokens? Were there quantitative metrics for speech reconstruction quality?",
              "What specific ablation studies were conducted to justify the 12.5Hz sampling rate? How do other rates (e.g., 6.25Hz vs. 50Hz) perform on downstream tasks?",
              "Are there comparisons with existing methods that use real speech data (e.g., Moshi) on the same benchmarks? How does the synthetic data approach compare in terms of model efficiency?",
              "How does the chatbot's performance vary across different speech modalities (e.g., noisy environments, non-native accents)?",
              "What is the exact architecture of the flow-matching based decoder? How does it differ from standard speech synthesis methods?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "3usdM1AuI3": {
    "paper_id": "3usdM1AuI3",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces BRAID, a deep learning framework for modeling nonlinear neural-behavioral dynamics while explicitly incorporating external inputs. It addresses challenges in disentangling intrinsic dynamics from input-driven effects, prioritizing behaviorally relevant dynamics, and handling nonlinearities through multi-step forecasting and a multi-stage optimization scheme."
          },
          "strengths": {
            "value": "Originality: BRAID introduces a novel framework that combines input-driven modeling, multi-step forecasting, and behaviorally relevant dynamics prioritization, addressing gaps in existing methods. Quality: The approach is theoretically grounded with clear methodological components (e.g., predictor/generative representations, multi-stage learning). Clarity: The paper is well-structured, with detailed explanations of key innovations and comparisons to related work. Significance: The work has potential impact on neuroscience and neurotechnology by improving neural-behavioral data modeling and decoding."
          },
          "weaknesses": {
            "value": "The experimental validation lacks sufficient detail on baseline comparisons (e.g., how BRAID outperforms IPSID/DPAD/TNDM quantitatively). The application to NHP data is limited to a single task, with no analysis of generalizability. The multi-stage optimization's theoretical guarantees are not discussed. The paper does not address computational complexity or scalability to larger datasets. The interpretation of 'behaviorally relevant dynamics' remains somewhat abstract without clear metrics."
          },
          "questions": {
            "value": [
              "How were hyperparameters for the multi-stage optimization determined? Were they validated on a separate dataset?",
              "What specific metrics were used to quantify improvements in neural-behavioral prediction compared to baselines?",
              "How does BRAID handle missing or noisy input data? Is there ablation analysis for input robustness?",
              "Are the 'behavior-specific dynamics' explicitly defined, or is their dissociation based on indirect measures?",
              "What is the computational cost of BRAID compared to linear methods like IPSID? Can it scale to high-dimensional neural recordings?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces BRAID, a deep learning framework for modeling nonlinear neural-behavioral dynamics while explicitly incorporating external inputs. The method disentangles intrinsic neural dynamics from input-driven effects through multi-step forecasting and a multi-stage optimization scheme, aiming to improve neural-behavioral prediction and interpretation."
          },
          "strengths": {
            "value": "BRAID addresses multiple critical challenges in neural-behavioral modeling, including nonlinear dynamics, input disentanglement, and behavior-specific relevance. The multi-stage optimization and dual representation (predictor/generative) offer a structured approach to learning intrinsic dynamics. The paper's motivation is well-aligned with neuroscience goals, and the framework's explicit handling of inputs is a notable contribution. The method's potential to improve closed-loop neurotechnologies is promising."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient experimental validation, with only partial results described (e.g., motor cortical data). Key baselines like nonlinear models incorporating inputs are not thoroughly compared. The novelty over existing methods (e.g., IPSID, DPAD) is unclear, as the paper does not convincingly demonstrate how BRAID's multi-stage approach or forecasting objective uniquely advances prior work. Theoretical justification for the multi-step forecasting objective and the generative representation's interpretability is underdeveloped. The paper also does not address potential limitations of nonlinear models in terms of overfitting or generalizability."
          },
          "questions": {
            "value": "1. How does BRAID's multi-stage optimization explicitly prioritize behaviorally relevant dynamics compared to supervised methods like those in Sani et al. (2021)? 2. What specific advantages does the multi-step forecasting objective provide over 1-step-ahead prediction in DPAD? 3. How were the preprocessing/post-hoc stages validated, and what metrics were used to quantify their effectiveness? 4. Are there quantitative comparisons with nonlinear models that include inputs (e.g., non-linear extensions of IPSID)? 5. How does BRAID handle high-dimensional input spaces or noisy data, which are common in neural recordings?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces BRAID, a deep learning framework for modeling nonlinear neural-behavioral dynamics while explicitly incorporating external inputs. It addresses challenges in disentangling intrinsic neural dynamics from input-driven effects and prioritizing behaviorally relevant dynamics through multi-stage optimization and multi-step forecasting."
          },
          "strengths": {
            "value": "Originality: BRAID's integration of input-driven forecasting, multi-stage optimization, and explicit disentanglement of intrinsic dynamics from inputs represents a novel approach to neural-behavioral modeling. Quality: The method is well-motivated with clear technical contributions, including a multi-step-ahead prediction objective and a generative representation of intrinsic dynamics. Clarity: The paper is structured logically, with a thorough discussion of related work and a clear problem statement. Significance: The work addresses critical gaps in neuroscience modeling, particularly in interpreting neural dynamics and improving neurotechnological applications like closed-loop controllers."
          },
          "weaknesses": {
            "value": "The experimental validation is limited: While simulations are described, the real-world NHP data analysis lacks quantitative metrics (e.g., prediction error, variance explained) and comparison with more diverse baselines. The multi-stage optimization framework's effectiveness is not ablated or thoroughly justified. The paper does not address how BRAID scales to high-dimensional neural data or handles missing inputs. Theoretical analysis of the model's stability or convergence is absent."
          },
          "questions": {
            "value": [
              "What specific quantitative metrics were used to evaluate BRAID's performance on the NHP data, and how do they compare to baselines?",
              "How does the multi-stage optimization framework disentangle behaviorally relevant dynamics from other components? Are there ablation studies demonstrating its necessity?",
              "What is the computational complexity of BRAID, and how does it scale to large neural datasets?",
              "How does the paper address potential overfitting, especially given the nonlinearity and multi-step forecasting components?",
              "Are there theoretical guarantees for the identifiability of the intrinsic dynamics under BRAID's formulation?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "3zw9NhLhBM": {
    "paper_id": "3zw9NhLhBM",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper's content is incomplete or missing, making it impossible to determine its main contributions or approach."
          },
          "strengths": {
            "value": "Not applicable due to lack of paper content."
          },
          "weaknesses": {
            "value": "The paper is not available for review, preventing assessment of its strengths, weaknesses, or validity."
          },
          "questions": {
            "value": "The authors should provide the full paper content for proper evaluation. Key questions about the paper's methodology, experiments, and claims cannot be addressed without access to the full text."
          },
          "rating": {
            "value": 1
          },
          "confidence": {
            "value": 1
          },
          "soundness": {
            "value": 1
          },
          "presentation": {
            "value": 1
          },
          "contribution": {
            "value": 1
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper appears to be incomplete, as only the first page with the number '1' is provided. No meaningful content, contributions, or methodology are available for review."
          },
          "strengths": {
            "value": "The paper does not provide sufficient information to assess its strengths. No originality, quality, clarity, or significance can be evaluated due to the lack of content."
          },
          "weaknesses": {
            "value": "The paper is fundamentally incomplete and non-readable. It lacks a title, abstract, introduction, methodology, experiments, or any other substantive content. This makes it impossible to evaluate its scientific merit, novelty, or validity."
          },
          "questions": {
            "value": "The authors should provide the full paper content for review. It is unclear whether this is an error in submission or an intentional omission. Additional details are required to assess the work."
          },
          "rating": {
            "value": 1
          },
          "confidence": {
            "value": 1
          },
          "soundness": {
            "value": 1
          },
          "presentation": {
            "value": 1
          },
          "contribution": {
            "value": 1
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper content provided is incomplete, containing only the number '1' on page 1. No meaningful technical content, contributions, or methodology is available for evaluation."
          },
          "strengths": {
            "value": "Unable to assess strengths due to lack of paper content."
          },
          "weaknesses": {
            "value": "The paper is non-existent in terms of technical substance. No experiments, analysis, or theoretical framework is provided. The submission appears to be a placeholder or error."
          },
          "questions": {
            "value": [
              "Please confirm that the paper content was correctly uploaded and provide the full text for review.",
              "If this is a placeholder, please resubmit the complete paper for evaluation."
            ]
          },
          "rating": {
            "value": 1
          },
          "confidence": {
            "value": 1
          },
          "soundness": {
            "value": 1
          },
          "presentation": {
            "value": 1
          },
          "contribution": {
            "value": 1
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "41HlN8XYM5": {
    "paper_id": "41HlN8XYM5",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Contextual Decomposition for Transformers (CD-T), a novel mathematical method for efficiently discovering interpretable circuits in transformers. CD-T enables automated, fine-grained circuit discovery at the attention head level with significantly reduced runtime compared to existing methods like ACDC and EAP, while achieving high ROC AUC scores and demonstrating superior faithfulness in replicating model behavior."
          },
          "strengths": {
            "value": "Originality: CD-T presents a mathematically grounded approach to circuit discovery, extending contextual decomposition (CD) from CNNs/RNNs to transformers, which is novel for mechanistic interpretability. Quality: The method is rigorously defined with clear mathematical formulations, and experiments demonstrate substantial runtime improvements (seconds vs. hours). Clarity: The paper is well-structured, with detailed explanations of CD-T's principles and comparisons to baselines. Significance: The work addresses critical challenges in interpretable AI for large language models, with potential applications in high-stakes domains like medicine and science."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of CD-T's components. The definition of 'nodes' (e.g., attention heads vs. outputs) is ambiguous, which could affect reproducibility. Faithfulness metrics are not fully contextualized—comparisons to alternative baselines (e.g., SAEs) are missing. Theoretical guarantees for CD-T's decomposition principles are not discussed, and the generalization to non-English or specialized transformer architectures is unexplored. The experimental setup for ROC AUC computation requires more methodological transparency."
          },
          "questions": {
            "value": "1. How does CD-T handle transformers with non-standard architectures (e.g., causal vs. bidirectional)? 2. What is the exact definition of 'nodes' in the experiments, and how does this affect circuit interpretability? 3. Can CD-T's decomposition principles be theoretically proven to capture causal relationships? 4. How does the faithfulness metric account for variations in circuit size and structure? 5. Are there cases where CD-T fails to recover manually curated circuits, and what are the failure modes?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces Contextual Decomposition for Transformers (CD-T), a novel method for efficiently discovering interpretable circuits in large language models. CD-T uses mathematical decomposition to isolate feature contributions, enabling fine-grained circuit identification with significantly reduced runtime compared to existing methods like ACDC and EAP. The approach is compatible with all transformer architectures and requires no training or manual examples."
          },
          "strengths": {
            "value": "Originality is strong, as CD-T extends contextual decomposition to transformers, addressing a critical gap in automated mechanistic interpretability. The method's efficiency (runtime reduction from hours to seconds) and compatibility with diverse transformer architectures are significant innovations. Experiments on three standard datasets demonstrate superior performance (97% ROC AUC) and robust faithfulness metrics. The paper's clarity in explaining the mathematical framework and practical implications is commendable, and the potential impact on understanding large language models is substantial."
          },
          "weaknesses": {
            "value": "The paper lacks detailed mathematical formulations of CD-T's decomposition principles, making it difficult to assess the novelty and technical rigor. The faithfulness evaluation relies on comparisons to random circuits, but the exact methodology for measuring faithfulness is under-specified. The runtime comparisons with baselines are qualitative, and quantitative benchmarks (e.g., FLOPS, computational complexity) are missing. The claim of compatibility with 'all transformer types' is not substantiated with examples or ablation studies on different architectures."
          },
          "questions": {
            "value": [
              "Please provide the exact mathematical equations defining CD-T's decomposition principles and clarify how they differ from prior CD methods for CNNs/RNNs.",
              "How is 'faithfulness' quantitatively defined? What metrics are used to ensure CD-T circuits are not merely statistically correlated with model behavior?",
              "What specific pruning strategies are employed to reduce runtime, and how do they avoid introducing approximation errors?",
              "Can you provide examples of CD-T's application to non-BERT/GPT architectures (e.g., encoder-only or hybrid models) to validate its compatibility claims?",
              "How does CD-T handle dynamic computation graphs in transformers (e.g., varying sequence lengths or attention patterns)?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces Contextual Decomposition for Transformers (CD-T), a method for efficiently discovering interpretable circuits in transformer models. CD-T uses mathematical decomposition to isolate feature contributions, enabling automated circuit discovery with fine-grained attention head-level granularity. The approach significantly reduces runtime compared to existing methods like ACDC and EAP, while achieving superior performance on standard evaluation datasets."
          },
          "strengths": {
            "value": "Originality: CD-T presents a novel mathematical framework for circuit discovery in transformers, extending contextual decomposition techniques from CNNs/RNNs. The method's ability to achieve fine-grained attention head-level circuits with minimal computational overhead represents a significant advancement. Quality: The paper includes comprehensive experiments on three benchmark datasets, demonstrating superior performance in both accuracy (97% ROC AUC) and efficiency. Clarity: The methodology is well-structured, with clear explanations of the decomposition process and circuit pruning. Significance: The work addresses a critical challenge in transformer interpretability, with potential applications in high-stakes domains like medicine and scientific research."
          },
          "weaknesses": {
            "value": "The paper lacks detailed mathematical formulations of the CD-T algorithm, making it difficult to assess the theoretical foundations. While runtime improvements are claimed, specific numerical comparisons (e.g., exact seconds saved) are not provided. The faithfulness metric's calculation method is not thoroughly explained, and the 80% improvement over random circuits lacks statistical validation. The paper also does not address potential limitations in handling different transformer architectures beyond BERT/GPT-like structures."
          },
          "questions": {
            "value": "1. Can the authors provide the exact mathematical equations for CD-T's decomposition principle? 2. How was the faithfulness metric quantitatively defined and validated? 3. What specific hardware/software configurations were used for the runtime comparisons? 4. Are there any cases where CD-T fails to identify circuits, and how does it handle such scenarios? 5. How does CD-T's performance scale with larger transformer models beyond the tested architectures?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "44WiKy8THW": {
    "paper_id": "44WiKy8THW",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a novel flow matching approach for non-autoregressive text generation by leveraging KL divergence geodesics for interpolation in logit space. The method formulates a loss function to maximize conditional likelihood, with theoretical analysis showing its connection to flow matching velocities. While initial experiments on TinyStories showed suboptimal results, an empirical sampling scheme using a pretrained denoiser significantly improved performance on larger datasets like Fine Web and Lamini Instruction."
          },
          "strengths": {
            "value": "The paper presents a creative integration of geodesic interpolation with flow matching, addressing the geometric challenges of discrete sequence modeling. The theoretical analysis linking likelihood maximization to flow matching velocities is a strong contribution. The empirical sampling scheme demonstrates practical improvements, and the work opens new directions for non-autoregressive text generation by combining continuous and discrete modeling techniques. The clarity of the problem formulation and the connection to prior work on discrete flow matching are notable."
          },
          "weaknesses": {
            "value": "The experiments on TinyStories are limited and lack detailed ablation studies to isolate the impact of the proposed methods. The theoretical justification for the empirical sampling scheme is incomplete, leaving questions about its generalizability. The paper does not thoroughly compare against state-of-the-art non-autoregressive models or provide quantitative metrics on the hybrid approach's performance. The connection between the logit-space interpolation and downstream generation quality requires deeper analysis."
          },
          "questions": {
            "value": "1. How does the empirical sampling scheme specifically leverage the pretrained denoiser? What architecture or training procedure is used for the denoiser? 2. Can the authors provide ablation studies showing the contribution of geodesic interpolation versus linear interpolation on the TinyStories dataset? 3. What are the exact metrics used to evaluate performance on Fine Web and Lamini Instruction datasets, and how do they compare to existing non-autoregressive baselines? 4. How does the hybrid approach balance computational efficiency with the complexity of geodesic interpolation?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a novel flow matching approach for non-autoregressive text generation by leveraging Kullback-Leibler (KL) divergence geodesics for interpolation in logit space. It proposes a theoretical framework linking likelihood maximization to flow matching velocity, an empirical sampling scheme to improve performance on complex datasets, and demonstrates results on Fine Web and Lamini Instruction datasets."
          },
          "strengths": {
            "value": "Originality is evident in applying KL geodesics to discrete sequence modeling, a novel geometric perspective for flow matching. The theoretical analysis connecting likelihood maximization to flow velocity is rigorous and contributes to the foundational understanding of the method. The empirical sampling scheme shows practical improvements, and the paper addresses a relevant problem in NLP. The writing is clear, with logical organization and proper contextualization of prior work."
          },
          "weaknesses": {
            "value": "The experiments on the TinyStories dataset are described as suboptimal, raising concerns about the method's generalizability. The theoretical justification for the empirical sampling scheme is incomplete, and the paper lacks detailed ablation studies or comparisons with state-of-the-art non-autoregressive models. The scalability to longer sequences or other domains is not addressed. The experimental results on complex datasets are promising but lack comprehensive analysis of failure cases or quantitative comparisons."
          },
          "questions": {
            "value": "How does the KL geodesic interpolation compare to linear interpolation in terms of performance on standard benchmarks? What specific limitations of the TinyStories experiments prevent the method from achieving better results? Can the empirical sampling scheme be theoretically justified, and how does it interact with the flow matching objective? How does the method handle out-of-distribution inputs or rare tokens?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a novel flow matching approach for non-autoregressive text generation by leveraging Kullback-Leibler (KL) divergence geodesics for interpolating between discrete distributions in logit space. The method involves a theoretical analysis of the flow matching velocity under logit interpolation and an empirical sampling scheme to improve performance on complex datasets."
          },
          "strengths": {
            "value": "Originality: The application of KL divergence geodesics for discrete sequence modeling is novel, particularly in the context of non-autoregressive text generation. The theoretical justification for the loss function's maximizer as the flow matching velocity provides a rigorous foundation. Quality: The paper presents a clear mathematical formulation of the geodesic interpolation and vector field derivation. Clarity: The introduction and background sections are well-structured, though some technical details in the methods section are dense. Significance: Non-autoregressive models are critical for efficient text generation, and this work addresses a key challenge in discrete sequence modeling."
          },
          "weaknesses": {
            "value": "The experiments on the TinyStories dataset are described as suboptimal, with limited quantitative results to support the claims. The empirical sampling scheme lacks theoretical justification and is not thoroughly explained. The paper does not compare the proposed method to existing flow-based or non-autoregressive text generation approaches, making it difficult to assess its relative performance. The hybrid approach's implementation details are vague, and there is no ablation study to validate its components. The theoretical analysis is limited to single-token scenarios, with no discussion of its generalization to sequences."
          },
          "questions": {
            "value": [
              "What specific implementation details of the hybrid approach enable its strong performance on complex datasets like Fine Web and Lamini Instruction?",
              "How does the empirical sampling scheme based on the pretrained denoiser work in practice, and what are its computational costs compared to existing methods?",
              "Why did the initial experiments on TinyStories fail, and how does the proposed sampling scheme address these limitations? Are there quantitative results to support this?",
              "What are the exact comparisons with existing flow-based or non-autoregressive models, and how does the proposed method outperform them?",
              "Is the theoretical analysis of the loss function's maximizer applicable to multi-token sequences, or are there unaddressed limitations in this generalization?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "44z7HL4mfX": {
    "paper_id": "44z7HL4mfX",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces INSTRUCT-SKILLMIX, an automated pipeline for generating high-quality instruction-following data using two stages: (1) skill extraction via a powerful LLM to identify core instruction-following skills, and (2) data generation by randomly combining these skills to create diverse (instruction, response) pairs. The method achieves strong performance on benchmarks with minimal data (4K examples) at low cost ($<600)."
          },
          "strengths": {
            "value": "Originality: The pipeline leverages LLM metacognition for skill extraction and random skill combinations to ensure diversity, which is a novel approach compared to prior synthetic data methods. Quality: The experiments show significant performance gains on benchmarks with minimal data. Clarity: The methodology is well-explained, and the paper addresses key challenges in instruction tuning. Significance: The low-cost, automated approach could democratize high-quality instruction tuning for smaller models."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons to established synthetic data methods like Self-Instruct or UltraChat, making it hard to assess relative effectiveness. The skill extraction process is not thoroughly validated—how are skills defined, and are they representative of real-world instruction tasks? The ablation study on low-quality data is superficial, with no details on how 'shirkers' were generated or evaluated. The cost estimate ($600) is not justified with API pricing details or computational resource breakdowns."
          },
          "questions": {
            "value": "1. How does the skill extraction process ensure coverage of critical instruction-following skills rather than trivial ones? 2. What metrics were used to validate the quality of extracted skills? 3. Can the pipeline be generalized to non-English languages or specialized domains? 4. How were the 4K examples selected—were they human-validated, or does the model's own judgment drive the selection? 5. What is the exact implementation of the 'random skill combination' in data generation, and how is diversity quantified?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces INSTRUCT-SKILLMIX, a two-stage pipeline for generating high-quality synthetic instruction-following datasets. The first stage extracts 'skills' from a large language model (LLM) through metacognitive prompting, while the second stage generates diverse (instruction, response) pairs by combining random skill pairs. The approach achieves strong performance on benchmarks with minimal data, costing under $600 to create a 4K dataset."
          },
          "strengths": {
            "value": "Originality lies in leveraging LLM metacognition for skill extraction and random skill combination to ensure diversity, which differs from prior synthetic data methods. The methodology is well-structured, with clear experimental validation showing competitive results against state-of-the-art models. The paper emphasizes cost-effectiveness and automation, addressing key challenges in instruction tuning. Clarity is strong, with logical organization and contextualization of prior work."
          },
          "weaknesses": {
            "value": "The skill extraction process lacks detailed technical description, making it hard to replicate. Experiments focus narrowly on specific models (e.g., LLaMA-3-8B-Base) and benchmarks, limiting generalizability. Comparisons with alternative synthetic data methods (e.g., Self-Instruct, UltraChat) are absent. The cost estimation assumes GPT-4 API usage but doesn't account for variability in pricing or other implementation factors. The ablation study on low-quality data is mentioned but not thoroughly analyzed."
          },
          "questions": {
            "value": "How are 'skills' defined and extracted? Are there specific examples of extracted skills? What metrics are used to validate skill relevance? How does the method scale to tasks beyond simple instruction-following? Can the pipeline be adapted to other domains without retraining? What is the exact breakdown of the $600 cost estimate?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces INSTRUCT-SKILLMIX, a two-stage pipeline for generating synthetic instruction-following data using large language models (LLMs). The first stage extracts 'skills' by prompting a strong LLM, while the second stage generates diverse (instruction, response) pairs by randomly combining these skills. The method achieves strong performance on benchmarks with minimal data (e.g., 4K examples) and emphasizes automation with minimal human intervention."
          },
          "strengths": {
            "value": "Originality lies in leveraging LLM metacognition for skill extraction and using random skill combinations to enhance diversity. The automated pipeline reduces reliance on human-designed datasets, addressing challenges in open instruction-tuning. The experiments demonstrate significant performance gains with minimal data, highlighting efficiency. Clarity is maintained through structured descriptions of the methodology and results. The work's significance stems from its potential to democratize high-quality instruction tuning without expensive human annotation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology for skill extraction (e.g., specific prompts, criteria for skill selection). The ablation study on 'shirkers' is limited and does not address other factors like skill overlap or task complexity. The comparison to existing methods (e.g., Self-Instruct) is superficial. The cost estimate ($600) is unclear in terms of model used (e.g., GPT-4 vs. other LLMs). Generalizability to safety/alignment tasks is speculative without empirical validation."
          },
          "questions": {
            "value": "1. What specific prompts were used for skill extraction and question generation? 2. How are skills defined and validated (e.g., inter-rater reliability)? 3. Are there quantitative metrics for skill diversity in the generated data? 4. How does the method perform on non-English or domain-specific tasks? 5. What is the exact evaluation protocol for AlpacaEval 2.0 (e.g., reference model, scoring mechanism)? 6. How does the pipeline handle conflicting skills or ambiguous instructions?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "49jkevjF6x": {
    "paper_id": "49jkevjF6x",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Abstractive Event Extraction (AEE), a novel task that shifts from span-based event extraction to ontology-grounded, abstract event representation. The authors release LEMONADE, a multilingual expert-annotated dataset covering 16 languages, and propose a zero-shot model (ZEST) and a supervised baseline, achieving 57.2% and 71.6% F1 scores respectively."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in real-world event extraction by proposing a novel AEE task that moves beyond surface-level span annotations. The LEMONADE dataset is a significant contribution, offering multilingual coverage for underrepresented languages and aligning with domain-specific ontologies. The problem formulation is clear, and the motivation for abstract event representation is well-justified. The paper also highlights practical applications in socio-political analysis, which adds relevance to the work."
          },
          "weaknesses": {
            "value": "The paper lacks detailed discussion of the dataset's annotation process, inter-annotator agreement, and quality control measures. The experimental evaluation is incomplete (the text is cut off), leaving critical questions about the baseline models' design, comparison to existing methods, and generalization across languages. The claim that AEE enables implicit argument annotation is not thoroughly demonstrated with examples. Additionally, the paper does not address how the proposed approach handles low-resource languages or domain-specific ontologies beyond the ACLED context."
          },
          "questions": {
            "value": "1. How was the LEMONADE dataset annotated, and what measures were taken to ensure inter-annotator agreement? 2. What specific design choices differentiate ZEST from existing zero-shot information extraction methods? 3. How does the paper address the challenge of domain-specific entity normalization in low-resource languages? 4. Are the F1 scores reported on LEMONADE normalized against baseline models or random guessing? 5. How does the paper validate the practical utility of AEE for real-world applications like conflict monitoring?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces Abstractive Event Extraction (AEE), a novel task that shifts from span-based event extraction to ontology-grounded, abstract event representation. The authors release LEMONADE, a multilingual dataset with 41,148 events across 16 languages, and propose a zero-shot system (ZEST) achieving 57.2% F1 and a supervised model with 71.6% F1."
          },
          "strengths": {
            "value": "Originality is strong through the AEE task formulation, which addresses limitations of traditional span-based extraction. The LEMONADE dataset is comprehensive, covering underrepresented languages and emphasizing expert annotation. The paper highlights real-world applications in socio-political monitoring and tackles critical issues like entity normalization and multilingual support. The problem definition and motivation are well-articulated, with clear alignment to practical challenges in event extraction."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation. Key baselines (e.g., span-based models, prior AEE approaches) are not compared, making it hard to assess the significance of the 57.2% and 71.6% F1 scores. The zero-shot system's generalization across languages and schemas is unexplored. Annotation methodology for LEMONADE is under-specified (e.g., inter-annotator agreement, domain-specific entity normalization). The paper also fails to address scalability of ZEST or its dependency on the predefined ontology."
          },
          "questions": {
            "value": [
              "What specific criteria were used to select experts for annotating LEMONADE, and what was the inter-annotator agreement?",
              "How does ZEST handle languages with limited training data in LEMONADE? Are there ablation studies for low-resource languages?",
              "What baselines were used for comparison (e.g., span-based models, prior zero-shot IE methods)?",
              "How is the ontology in Definition 1 implemented in practice? Does it require manual schema engineering?",
              "Are the F1 scores reported on LEMONADE comparable to existing span-based benchmarks (e.g., ACE, KBP)?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a novel abstractive event extraction (AEE) task that moves beyond span-based annotations to enable more holistic event understanding. It presents LEMONADE, a multilingual expert-annotated dataset covering 16 languages with 41,148 events, and proposes a zero-shot system (ZEST) and a supervised model achieving 57.2% and 71.6% F1 scores respectively."
          },
          "strengths": {
            "value": "Originality: The AEE task formulation addresses critical gaps in traditional event extraction by emphasizing ontology-based abstraction and implicit argument detection. Quality: The LEMONADE dataset is a significant contribution, particularly for low-resource languages, with expert annotations from a reputable source. Clarity: The problem definition and dataset description are well-structured, though some technical details are sparse. Significance: The work addresses real-world challenges in event analysis for domains like political violence and public health, with potential for impactful applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing state-of-the-art methods for event extraction, making it hard to assess the novelty of ZEST. The zero-shot approach's methodology is under-described, and the paper does not analyze failure cases or error patterns. The dataset's annotation process and quality control mechanisms are not thoroughly explained. Additionally, the paper does not address how the proposed AEE framework handles schema variations or domain shifts."
          },
          "questions": {
            "value": "1. How does ZEST differ from existing zero-shot information extraction methods? What specific innovations enable its performance? 2. What baselines were used for comparison (e.g., span-based models, existing AEE approaches)? 3. Can the authors elaborate on the annotation process for LEMONADE, particularly for low-resource languages? 4. How does the AEE framework handle event arguments that require external knowledge (e.g., caste systems in the example)? 5. Are there any limitations in LEMONADE's coverage of event types or languages?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "49ti6LOUw5": {
    "paper_id": "49ti6LOUw5",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes UnoLoRA, a parameter-efficient multi-task learning method that uses a single shared LoRA module for all tasks, significantly reducing trainable parameters while maintaining performance. The enhanced variant, UnoLoRA*, introduces a shared hypernetwork to generate task-specific embeddings, improving convergence. The approach leverages LoRA's implicit regularization properties to enable knowledge sharing between tasks."
          },
          "strengths": {
            "value": "Originality: The concept of a single shared LoRA module for multi-task learning is novel, offering a new perspective on parameter efficiency. Quality: The method is theoretically grounded in LoRA's implicit regularization properties, with a clear mathematical formulation. Clarity: The paper is well-structured, with logical flow from problem formulation to experiments. Significance: The potential to reduce memory requirements for multi-task learning is impactful for resource-constrained applications."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with state-of-the-art methods like HyperFormer, making it hard to assess relative performance. The analysis of A/B matrix roles is based on limited PCA visualization without statistical validation. The hypernetwork component's design and training procedure are under-specified. The GLUE results are incomplete, with no detailed ablation studies or analysis of negative transfer mitigation. The theoretical justification for the shared LoRA approach remains superficial."
          },
          "questions": {
            "value": [
              "How does the shared hypernetwork generate task-specific embeddings? Please provide the architectural details of H(t, s, p).",
              "What ablation studies were conducted to validate the contribution of the shared LoRA module vs. task-specific adapters?",
              "How does UnoLoRA* compare to HyperFormer in terms of parameter efficiency and task performance on GLUE?",
              "Are there quantitative results showing reduced negative transfer with the shared LoRA approach?",
              "What is the exact implementation of the PCA visualization in Figure 3? How were the A/B matrices analyzed across tasks?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes UnoLoRA, a parameter-efficient multi-task learning approach using a single shared LoRA module for large language models. It introduces UnoLoRA*, which employs a hypernetwork to generate task-specific embeddings, aiming to reduce trainable parameters to 0.05% per task while maintaining performance on GLUE. The work analyzes the complementary roles of LoRA matrices A and B in capturing general and task-specific features."
          },
          "strengths": {
            "value": "The paper presents a novel architecture for multi-task learning with a shared LoRA module, addressing parameter efficiency. The empirical evaluation on GLUE demonstrates competitive performance, and the analysis of LoRA matrices provides insights into their roles. The method leverages implicit regularization, aligning with recent trends in parameter-efficient fine-tuning. The clear problem formulation and structured methodology contribute to its strengths."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with state-of-the-art methods like HyperFormer, making it hard to assess relative improvements. The parameter efficiency claim (0.05% per task) is not thoroughly justified with concrete numbers or ablation studies. The analysis of A/B matrices relies on a single PCA visualization, which may not be statistically rigorous. The hypernetwork's design and its impact on performance are under-explained. Experiments on GLUE are incomplete, and the paper does not address negative transfer explicitly."
          },
          "questions": {
            "value": "1. How does UnoLoRA* compare to HyperFormer in terms of parameter count and performance? 2. What ablation studies validate the hypernetwork's contribution? 3. How is the 0.05% parameter efficiency calculated, and what baseline does it reference? 4. Are the results consistent across different GLUE tasks? 5. How does the shared LoRA module prevent negative transfer? 6. What is the role of position information in the hypernetwork's task embeddings?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces UnoLoRA, a parameter-efficient multi-task learning method that uses a single shared Low-Rank Adaptation (LoRA) module for all tasks, reducing trainable parameters to 0.05% per task. It also proposes UnoLoRA*, which incorporates a shared hypernetwork to generate task-specific embeddings. The approach is evaluated on the GLUE benchmark, showing competitive performance with existing methods."
          },
          "strengths": {
            "value": "Originality: The paper presents a novel architecture for multi-task learning using a single shared LoRA module, which is less explored compared to task-specific adapters. Quality: The methodology is well-structured, with clear equations and a logical flow. Clarity: The problem formulation and key contributions are clearly stated, though some technical details are sparse. Significance: Reducing parameter overhead in multi-task learning is critical for resource-constrained applications, making this work relevant for practical deployment."
          },
          "weaknesses": {
            "value": "The hypernetwork's design and implementation details are under-specified, making it hard to assess its contribution. The comparison with existing methods like HyperFormer lacks quantitative analysis of parameter efficiency and performance trade-offs. The analysis of A/B matrix roles relies on PCA visualization without deeper statistical validation. The paper does not address negative transfer, a key challenge in multi-task learning. Experiments are limited to GLUE, with no evaluation on other benchmarks or task diversity."
          },
          "questions": {
            "value": [
              "How is the shared hypernetwork architecture designed? What is the exact mechanism for generating task-specific embeddings?",
              "What ablation studies were conducted to validate the necessity of the hypernetwork in UnoLoRA*?",
              "How does UnoLoRA* compare to HyperFormer in terms of parameter count, training stability, and task-specific performance?",
              "What is the impact of varying the rank $ r $ on the trade-off between parameter efficiency and task performance?",
              "How does the paper address negative transfer between tasks in the shared LoRA framework?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "4A9IdSa1ul": {
    "paper_id": "4A9IdSa1ul",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces FreDF, a novel approach to time series forecasting that addresses label autocorrelation in the Direct Forecast (DF) paradigm by operating in the frequency domain. The method aims to reduce estimation bias caused by ignoring temporal dependencies in multi-step predictions, with claims of significant performance improvements over existing methods."
          },
          "strengths": {
            "value": "Originality: The paper identifies label autocorrelation as a critical, underexplored issue in DF and proposes a frequency-domain approach to address it, which is a novel combination of concepts. Quality: The theoretical analysis of DF's bias and the motivation for frequency-domain transformation are logically structured. Clarity: The problem statement and contributions are clearly articulated, though the paper is truncated. Significance: Addressing autocorrelation in forecasting has broad implications for time series modeling, and the proposed method's compatibility with various models enhances its practical relevance."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete due to the paper being cut off, making it impossible to assess the rigor of the results. The theoretical justification for why frequency-domain learning mitigates label autocorrelation lacks depth, with no mathematical proofs or detailed ablation studies. The paper also does not provide a clear comparison against baseline methods beyond vague claims of 'state-of-the-art' performance. The practical implementation details of FreDF (e.g., how frequency-domain transformations are integrated into existing models) are not sufficiently explained."
          },
          "questions": {
            "value": "1. What specific datasets and metrics were used to evaluate FreDF's performance? 2. How does FreDF compare quantitatively to existing DF and iterative forecast methods? 3. Are there ablation studies demonstrating the impact of frequency-domain learning on bias reduction? 4. What is the computational overhead of FreDF compared to standard DF? 5. How is the frequency-domain transformation applied to different model architectures (e.g., Transformers, MLPs)?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces FreDF, a novel forecasting approach that addresses label autocorrelation in time series modeling by learning in the frequency domain. The authors argue that existing Direct Forecast (DF) paradigms suffer from biased learning objectives due to ignoring label autocorrelation and propose FreDF to mitigate this by aligning forecasts with label sequences in the frequency domain."
          },
          "strengths": {
            "value": "The paper identifies a critical yet underexplored issue in time series forecasting: label autocorrelation's impact on DF paradigms. The theoretical analysis of DF's bias is a strong point, and the proposal of FreDF as a frequency-domain solution demonstrates originality. The method's compatibility with various models and the claim of improved performance over state-of-the-art approaches highlight its practical significance. The paper also contextualizes its work within existing literature on frequency analysis in forecasting."
          },
          "weaknesses": {
            "value": "The experimental validation lacks critical details: it is unclear which baselines were compared, which datasets were used, or how FreDF's performance was quantitatively evaluated. The theoretical justification for the frequency-domain approach is brief and does not thoroughly explain how frequency alignment reduces label autocorrelation. The paper also omits ablation studies to isolate the contribution of frequency-domain learning. Additionally, the claim of being 'the first' to use frequency analysis for forecasting paradigms is questionable given prior work like FedFormer (Zhou et al., 2022)."
          },
          "questions": {
            "value": [
              "What specific datasets and baselines were used in the experiments? How does FreDF compare to frequency-domain methods like FedFormer?",
              "How is label autocorrelation quantitatively measured, and what evidence supports the claim that frequency-domain learning reduces it?",
              "Are there ablation studies demonstrating the necessity of frequency-domain transformation for FreDF's performance gains?",
              "What is the computational overhead of frequency-domain processing, and how does it affect scalability?",
              "How does FreDF handle non-stationary time series or varying input lengths, which are common challenges in forecasting?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces FreDF, a novel forecasting approach that addresses label autocorrelation in multi-step time series prediction by operating in the frequency domain. The authors theoretically show that the Direct Forecast (DF) paradigm is biased due to ignored label autocorrelation and propose FreDF to mitigate this by aligning forecasts and labels in the frequency domain, improving performance across various models."
          },
          "strengths": {
            "value": "The paper presents a fresh perspective by highlighting label autocorrelation as a critical yet underexplored issue in time series forecasting. The theoretical analysis of DF's bias due to label autocorrelation is original and insightful. FreDF's integration of frequency-domain analysis into the DF paradigm is a creative solution with potential broad applicability. The paper's structure is clear, and the problem statement is well-motivated with relevant references."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete in the provided content, making it difficult to assess the empirical claims. The paper lacks detailed ablation studies to isolate the contribution of frequency-domain learning. The theoretical analysis of bias reduction in the frequency domain is not sufficiently elaborated. Additionally, the compatibility of FreDF with specific model architectures (e.g., Transformers vs. MLPs) requires further clarification."
          },
          "questions": {
            "value": [
              "What specific datasets and baseline models were used in the experiments? How do the results generalize across different time series characteristics (e.g., seasonality, noise levels)?",
              "How does FreDF handle non-stationary time series, where frequency-domain representations might be less stable?",
              "Are there any ablation studies demonstrating the effectiveness of frequency-domain learning versus traditional time-domain approaches in FreDF?",
              "What is the computational overhead of frequency-domain operations compared to standard DF methods? How does this scale with sequence length?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "4BFzTrIjPN": {
    "paper_id": "4BFzTrIjPN",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces CONGO, a framework for sparse online convex optimization that leverages compressive sensing to improve gradient estimation efficiency. The authors propose three variants (CONGO-B, CONGO-Z, CONGO-E) that reduce sample complexity by exploiting gradient sparsity, achieving dimension-independent regret bounds and demonstrating superior performance in both synthetic and real-world scenarios."
          },
          "strengths": {
            "value": "Originality: The paper innovatively applies compressive sensing to zeroth-order online convex optimization, addressing a novel problem formulation where gradient sparsity is exploited for sample efficiency. Quality: The theoretical analysis rigorously establishes regret bounds and sample complexity improvements, supported by comprehensive experiments across multiple domains. Clarity: The paper is well-structured with clear motivation, problem formulation, and algorithmic descriptions. Significance: The work addresses a critical challenge in large-scale optimization for resource-constrained systems, with practical relevance to queueing networks and microservices."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed comparison with state-of-the-art sparse optimization methods beyond ZORO and SPSA, which could strengthen the novelty claim. The theoretical analysis assumes idealized conditions for compressive sensing recovery, but the paper does not fully address robustness to measurement noise or non-ideal sparsity. The real-world microservices experiments, while promising, lack statistical significance testing and detailed ablation studies on sparsity levels."
          },
          "questions": {
            "value": [
              "How does CONGO handle non-ideal sparsity patterns (e.g., approximately sparse gradients) in practical scenarios?",
              "What are the exact computational overheads of the proposed compressive sensing-based gradient estimation compared to baseline methods?",
              "The paper mentions a 'dimension-independent' regret bound, but the dependence on sparsity parameter s is not explicitly quantified in the theoretical results.",
              "Are there specific constraints on the measurement matrices (e.g., Gaussian vs. Bernoulli) that affect CONGO's performance in different applications?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces CONGO, a framework for zeroth-order online convex optimization that leverages compressive sensing to exploit sparse gradients. By reducing the number of function evaluations needed for gradient estimation, CONGO achieves dimension-independent regret bounds and demonstrates superior performance in synthetic, queueing system, and microservices benchmarks compared to non-sparse methods."
          },
          "strengths": {
            "value": "The paper addresses a relevant and practical problem in online optimization with sparse gradients, which is critical for applications like queueing networks. The integration of compressive sensing with zeroth-order methods is novel, combining SPSA and compressive sensing techniques to achieve efficient gradient estimation. Theoretical contributions include dimension-independent regret bounds and improved sample complexity for sparse settings. The experimental validation spans diverse scenarios, including real-world microservices, enhancing the paper's credibility. The writing is clear, with well-structured sections and contextualization of related work."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed comparison with other sparse gradient methods (e.g., ZORO) in terms of specific metrics like regret or sample efficiency. The analysis of sparsity assumptions in real-world settings (e.g., queueing systems) is insufficient, and the practical implications of compressive sensing failures are not thoroughly discussed. Theoretical guarantees rely on assumptions about gradient sparsity and measurement matrices that are not fully explored. Additionally, the paper truncates key sections (e.g., Problem Formulation), which may affect the completeness of the analysis."
          },
          "questions": {
            "value": "How does CONGO handle scenarios where the gradient sparsity is not exactly known or varies over time? What are the practical limitations of the measurement matrix design (e.g., Gaussian vs. Bernoulli) in real-world applications? How do the constants in the regret bounds scale with the sparsity parameter s, and what impact does this have on empirical performance? Can the framework be extended to non-convex settings or more complex constraint sets?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces CONGO, a framework for sparse online convex optimization that leverages compressive sensing to estimate gradients efficiently. By exploiting sparsity in the gradient, CONGO reduces sample complexity and achieves dimension-independent regret bounds. The authors propose three variants (CONGO-B, CONGO-Z, CONGO-E) and validate their approach on synthetic data, queueing systems, and real-world microservices benchmarks."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by applying compressive sensing to zeroth-order online convex optimization, a novel combination that addresses a critical gap in handling sparse gradients. The theoretical analysis is rigorous, with clear regret bounds and sample complexity improvements over non-sparse methods. The empirical validation covers diverse settings, including real-world applications, showcasing practical relevance. The work also provides a clear motivation rooted in queueing networks and resource allocation, which strengthens its significance."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed comparison with state-of-the-art methods beyond the mentioned ZORO and SPSA, which limits the assessment of CONGO's relative advantages. The theoretical guarantees assume exact sparsity, which may not hold in real-world scenarios with noisy or approximate sparsity. Additionally, the experiments focus on low-dimensional settings, and the scalability to extremely high-dimensional problems (e.g., $d > 10^5$) is not thoroughly explored. The analysis of compressive sensing failure cases is minimal, leaving gaps in understanding robustness."
          },
          "questions": {
            "value": [
              "How does CONGO handle scenarios where the gradient is not exactly sparse, but only approximately sparse? What guarantees exist for such cases?",
              "The paper assumes the sparsity level $s$ is known. How sensitive is the algorithm to errors in estimating $s$?",
              "What are the computational costs of the proposed methods compared to baseline approaches, especially for large $d$?",
              "The experiments focus on synthetic and microservices data. How does CONGO perform on other real-world problems with different sparsity patterns?",
              "The regret bounds depend on the compressive sensing recovery success. How does the paper address the probability of failure in this step?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "4D0f16Vwc3": {
    "paper_id": "4D0f16Vwc3",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces ReMoE, a fully differentiable Mixture-of-Experts (MoE) architecture that replaces the traditional TopK+Softmax routing with ReLU-based routing. The method aims to address the non-differentiability of vanilla TopK routers while maintaining computational efficiency through sparsity regulation and load balancing. Experiments on LLaMA-like architectures show consistent performance improvements across various model sizes and expert configurations."
          },
          "strengths": {
            "value": "Originality: The use of ReLU as a continuous, differentiable routing mechanism represents a novel approach to MoE design, avoiding the discontinuities of TopK. Quality: The paper provides a clear technical formulation of ReMoE, including sparsity control via L1 regularization and load balancing. Clarity: The motivation for ReLU routing is well-explained through comparisons with TopK, and the architecture is described with mathematical rigor. Significance: MoE scalability is a critical challenge in large model development, and ReMoE's potential to improve this area is impactful for the field."
          },
          "weaknesses": {
            "value": "The paper lacks a comprehensive comparison with other differentiable MoE approaches (e.g., Soft MoE, Lory) beyond brief mentions. The experimental validation is limited to LLaMA-like architectures, with insufficient analysis of domain-specific performance or edge cases. The theoretical justification for ReLU's superiority over TopK is minimal, relying mostly on empirical results. The load-balancing mechanism's effectiveness is not thoroughly evaluated, and the adaptive L1 regularization coefficient's tuning process is under-specified."
          },
          "questions": {
            "value": "1. How does ReMoE handle tokens with varying complexity compared to TopK? Are there specific scenarios where ReLU routing underperforms?\n2. What ablation studies were conducted to isolate the contribution of ReLU routing vs. sparsity regularization?\n3. How does the adaptive L1 coefficient work in practice? Is it tuned manually or learned during training?\n4. Were experiments conducted on non-autoregressive tasks to validate the method's generalizability beyond LLaMA?\n5. How does ReMoE's computational efficiency compare to TopK in terms of FLOPs and memory usage?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces ReMoE, a fully differentiable Mixture-of-Experts (MoE) architecture that replaces the traditional TopK+Softmax routing with a ReLU-based routing mechanism. The method aims to address the non-differentiability of vanilla TopK routers while maintaining computational efficiency through sparsity control and load balancing."
          },
          "strengths": {
            "value": "The paper presents a novel approach to MoE routing by leveraging ReLU's continuous nature, which could enable more efficient training. The method's simplicity as a 'drop-in replacement' for TopK is promising. The experimental claims of superior scalability and performance across various model configurations suggest potential practical value. The paper also addresses key MoE challenges like sparsity control and load balancing."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing fully differentiable MoE methods (e.g., Lory) in terms of training dynamics and convergence. The theoretical justification for ReLU routing's superiority over TopK is underdeveloped. The experimental results rely on a single architecture (LLaMA) without ablation studies on hyperparameters like the L1 regularization coefficient. The claimed 'domain specialization' lacks quantitative analysis. The paper also does not address potential issues with gradient propagation through ReLU gates."
          },
          "questions": {
            "value": [
              "How does the proposed L1 regularization with adaptive coefficients compare to other sparsity-inducing methods in terms of training stability?",
              "What specific metrics were used to quantify ReMoE's 'domain specialization' and how do they correlate with task performance?",
              "Why is the sparsity parameter (1 - k/E) sufficient to match TopK's computational efficiency without explicit expert selection?",
              "How does ReMoE handle gradient vanishing/exploding issues inherent to ReLU activation in deep MoE architectures?",
              "Are the claimed scalability benefits reproducible on non-LLaMA architectures or downstream tasks beyond the experiments described?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes ReMoE, a fully differentiable Mixture-of-Experts (MoE) architecture that replaces the traditional TopK+Softmax routing with ReLU-based routing. The method aims to address the non-differentiability of standard MoE routers while maintaining computational efficiency and scalability. Experiments show ReMoE outperforms TopK-routed MoE across various model configurations and demonstrates superior scalability with more experts."
          },
          "strengths": {
            "value": "Originality: ReMoE introduces a novel approach to MoE routing by leveraging ReLU's continuity, which is a creative rethinking of the routing mechanism. The paper also proposes a load-balancing regularization method to control sparsity. Quality: The experiments are comprehensive, covering multiple model sizes, expert counts, and granularity levels. The comparison against TopK routing is thorough. Clarity: The motivation for ReLU routing is well-explained through mathematical analysis and visualizations. Significance: The work addresses a critical limitation in MoE architectures, enabling more efficient and scalable model training, which is highly relevant for large-scale language models."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with other differentiable MoE approaches like Soft MoE or Lory, which limits the assessment of ReMoE's relative advantages. The theoretical analysis of why ReLU routing improves performance over TopK is superficial, relying heavily on empirical results. The load-balancing regularization's hyperparameter tuning is not thoroughly discussed, raising questions about its generalizability. Additionally, the paper does not address potential issues with ReLU's non-differentiability at zero, which could affect gradient propagation."
          },
          "questions": {
            "value": [
              "How does ReMoE compare to other differentiable MoE methods like Soft MoE or Lory in terms of performance and efficiency? The paper focuses on TopK but omits these alternatives.",
              "The paper mentions 'domain specialization' but does not provide quantitative evidence or examples of how ReMoE achieves this compared to TopK routing.",
              "The load-balancing regularization uses an adaptively tuned coefficient. How is this coefficient determined, and how sensitive is the performance to its value?",
              "What are the limitations of ReLU routing in terms of gradient flow? Does the non-differentiability at zero affect training stability?",
              "The experiments focus on LLaMA-like architectures. How generalizable are the results to other model types (e.g., encoder-decoder models or vision transformers)?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "4ExwvWAy9b": {
    "paper_id": "4ExwvWAy9b",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces FactCheckmate, a framework for preemptively detecting and mitigating hallucinations in language models (LMs) by analyzing their internal hidden states. The method uses a lightweight classifier to predict hallucinations before decoding begins and intervenes by adjusting hidden states to improve factual outputs. Experiments show over 70% detection accuracy and a 34.4% improvement in factual outputs across multiple LM families and datasets."
          },
          "strengths": {
            "value": "The paper presents a novel proactive approach to hallucination detection, leveraging internal LM representations rather than post-hoc analysis. The method is lightweight, with minimal inference overhead, and demonstrates consistent performance across diverse models and tasks. The experimental evaluation is comprehensive, covering multiple domains and LM architectures. The work provides practical insights into LM internals and addresses a critical problem in NLP."
          },
          "weaknesses": {
            "value": "The intervention mechanism is under-specified—how exactly are hidden states adjusted? The paper lacks ablation studies on key design choices (e.g., layer selection, aggregation methods). The 3.16s overhead claim is not compared to baseline methods, making it hard to assess efficiency. The evaluation relies on GPT-4o as a judge, which may not be a standard metric. The classifier is trained per-model, but scalability and generalization across models are not thoroughly discussed."
          },
          "questions": {
            "value": "1. How is the intervention model's adjustment of hidden states implemented? What specific modifications are made to the hidden states? 2. Are there ablation studies on the impact of different layers or aggregation methods (e.g., average vs. max pooling)? 3. How is 'factual' defined in the evaluation? Are there human-labeled benchmarks for comparison? 4. What is the exact mechanism for selecting the optimal layer $ l $, and how does this vary across models? 5. Are there cases where the intervention fails, and what patterns emerge in such cases?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces FactCheckmate, a system that preemptively detects and mitigates hallucinations in language models (LMs) by analyzing their internal hidden states. It proposes a lightweight classifier to predict hallucinations before decoding and an intervention model to adjust hidden states for factual outputs. The method is evaluated across multiple LM families and datasets, showing over 70% detection accuracy and a 34.4% improvement in factual output quality."
          },
          "strengths": {
            "value": "The paper addresses a critical problem of hallucinations in LMs with a novel approach leveraging internal representations. The method is lightweight and efficient, with minimal inference overhead. The experimental validation across diverse models and datasets demonstrates practical effectiveness. The work provides actionable insights into LM internals and offers a proactive alternative to reactive mitigation strategies."
          },
          "weaknesses": {
            "value": "The intervention mechanism lacks detailed technical description, making it unclear how hidden states are adjusted. The hallucination detection relies on a simplistic average of hidden states, which may not capture nuanced patterns. The paper does not compare with state-of-the-art proactive detection methods. The evaluation metrics (e.g., exact match) may not fully capture factual accuracy. The 3.16s overhead is significant for real-time applications, and the paper lacks ablation studies to validate key components."
          },
          "questions": {
            "value": [
              "How exactly does the intervention model modify the hidden states? What are the specific operations applied to the last token's hidden state?",
              "Why is the average of hidden states used instead of more sophisticated aggregation methods (e.g., attention mechanisms)?",
              "What are the baselines for comparison with existing proactive hallucination detection methods?",
              "How are the gold answers determined for datasets like MMLU and MedMCQA, given their multiple-choice format?",
              "Are the results statistically significant across different LM families and datasets?",
              "What is the exact definition of 'factual outputs' used for evaluation?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces FactCheckmate, a method for preemptively detecting and mitigating hallucinations in language models (LMs) by analyzing their internal hidden states. The approach involves a lightweight classifier that predicts hallucinations before decoding begins and an intervention model that adjusts hidden states to improve factual outputs. The method is evaluated across multiple LM architectures and datasets, showing high detection accuracy and improved factual output quality with minimal inference overhead."
          },
          "strengths": {
            "value": "Originality: FactCheckmate introduces a novel preemptive approach to hallucination detection and mitigation, leveraging internal LM representations rather than reactive post-hoc methods. Quality: The experiments are comprehensive, covering diverse LM families (Llama, Mistral, Gemma) and domains (QA, STEM, medical). Clarity: The paper is well-structured with clear explanations of the pipeline, mathematical formulations, and experimental setup. Significance: Addressing hallucinations is critical for LM reliability, and the lightweight intervention mechanism offers practical benefits over existing methods."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of the classifier's training process, including how hallucination labels are generated (e.g., exact match vs. semantic similarity). The intervention mechanism's exact technical implementation (e.g., how hidden states are adjusted) is not fully explained. The 3.16-second inference overhead, while described as minimal, is not contextualized against baseline methods. The evaluation focuses on QA tasks, leaving open questions about generalizability to other modalities or tasks."
          },
          "questions": {
            "value": [
              "How are hallucination labels generated for training the classifier? Are they based on exact matches, semantic similarity, or other criteria?",
              "What is the precise mechanism of the intervention model (g_φ)? How does adjusting the last token's hidden state influence the final output?",
              "Are there cases where FactCheckmate fails to mitigate hallucinations, and what patterns emerge in such cases?",
              "How does the 3.16-second overhead compare to other mitigation methods (e.g., iterative sampling)?",
              "What ablation studies were conducted to validate the choice of middle layers for hidden state aggregation?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "4FRUNLuY54": {
    "paper_id": "4FRUNLuY54",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Dragonfly, a vision-language model (VLM) that enhances visual detail extraction by using multi-resolution zoom-in encoding. The approach involves processing images at resolutions beyond their native size, generating multiple sub-crops, and aggregating features via mean-pooling to manage computational complexity. Dragonfly achieves strong performance on general and biomedical benchmarks, particularly excelling in tasks requiring fine-grained image understanding."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel multi-crop zoom-in strategy that extends beyond conventional high-resolution techniques, addressing limitations in vision transformers (ViTs) for fine-grained detail extraction. Quality: The experiments are well-designed, with competitive results on diverse benchmarks, including biomedical tasks where Dragonfly sets new state-of-the-art scores. Clarity: The paper is structured logically, with clear explanations of the methodology and contributions. Significance: The work highlights a critical challenge in fixed-resolution ViTs and offers a practical, efficient solution with broad applicability to both general and specialized domains."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed analysis of why mean-pooling outperforms more complex token-reduction methods, such as learnable aggregation techniques. While the experiments show strong results, the ablation studies on the zoom-in mechanism and its impact on specific tasks (e.g., text recognition in charts) are insufficient. Additionally, the description of the curated datasets is vague, with limited details on data curation strategies, balancing criteria, and potential biases. The comparison with larger models or those trained on bigger datasets could be more concrete, with explicit metrics for direct head-to-head performance."
          },
          "questions": {
            "value": "1. How does the mean-pooling aggregation compare to alternative methods (e.g., attention-based or learnable pooling) in terms of preserving critical visual details? 2. What specific challenges in biomedical imaging (e.g., histopathology, radiology) does the zoom-in approach address that standard ViTs fail to capture? 3. Can the authors provide more details on the dataset curation process, particularly for the biomedical domain, including how they ensured task diversity and reduced redundancy? 4. Are there any limitations to the zoom-in approach in scenarios with extreme resolution increases or specific image modalities (e.g., low-light images)?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 5
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces Dragonfly, a vision-language model (VLM) that enhances fine-grained visual understanding by using multi-resolution zoom-in encoding. The method processes images at resolutions beyond their native size, extracting features from multiple sub-crops and aggregating them via mean-pooling to manage computational complexity. Dragonfly achieves strong performance on general and biomedical benchmarks, particularly in tasks requiring detailed image analysis."
          },
          "strengths": {
            "value": "Originality: The multi-crop zoom-in approach extends beyond conventional high-resolution techniques, addressing limitations of fixed-resolution vision transformers (ViTs). Quality: The paper demonstrates competitive results on diverse benchmarks, including biomedical tasks, with claims of state-of-the-art performance. Clarity: The methodology and contributions are clearly articulated, with a focus on practical applications. Significance: The work highlights the importance of high-resolution visual encoding for specialized domains like biomedicine, where fine-grained details are critical."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the effectiveness of mean-pooling compared to other token-reduction methods (e.g., learnable pooling). The experimental section is insufficiently comprehensive, with no comparison to advanced approaches like attention-based or dynamic token selection. The zoom-in mechanism (e.g., zoom factor, crop selection strategy) is not explicitly described, making it difficult to assess its novelty. The biomedical dataset curation details (e.g., balancing, deduplication) are vague, and the paper does not address potential biases or limitations in the data. Claims about outperforming larger models lack quantitative analysis of computational efficiency or parameter scalability."
          },
          "questions": {
            "value": [
              "How exactly is the 'zoom-in' process implemented? What is the zoom factor, and how are sub-crops selected (e.g., random, grid-based, or object-aware)?",
              "What ablation studies were conducted to compare mean-pooling with other token-aggregation methods (e.g., attention, dynamic selection)?",
              "The paper mentions a biomedical instruction-tuning dataset—what are its key characteristics (e.g., domain distribution, image modalities, task diversity)?",
              "How does Dragonfly handle the increased computational load from high-resolution crops? Are there trade-offs in inference speed or memory usage?",
              "What is the exact architecture of Dragonfly? How does it differ from existing VLMs like LLaVA or Med-PaLM?",
              "The claims about outperforming larger models (e.g., Med-Gemini) require direct comparisons on the same metrics and datasets—were these conducted?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Dragonfly, a vision-language model (VLM) that enhances fine-grained visual understanding by processing images through multi-resolution zoom-in crops beyond their native resolution. The approach uses mean-pooling to aggregate features from these high-resolution crops, reducing computational complexity while preserving critical details. Dragonfly achieves strong performance on general and biomedical benchmarks, particularly in tasks requiring detailed visual analysis."
          },
          "strengths": {
            "value": "The paper presents a novel multi-resolution zoom-in strategy to address limitations of fixed-resolution vision transformers (ViTs) in capturing fine-grained details. The experimental results demonstrate competitive performance on diverse benchmarks, including state-of-the-art results on biomedical tasks. The clarity of the problem statement and methodology is strong, with a clear connection to prior work. The contribution is significant for domains like biomedicine, where high-resolution visual understanding is critical."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the effectiveness of key components, such as the zoom-in mechanism and mean-pooling strategy. The comparison with existing methods is limited, particularly for larger models or those trained on more data. The dataset curation process for biomedical tasks is not thoroughly described, raising concerns about reproducibility. Additionally, the paper does not address potential limitations of mean-pooling in preserving spatial information compared to more sophisticated token-reduction techniques."
          },
          "questions": {
            "value": "1. How is the 'zoom-in' process implemented technically? Does it involve image scaling, patch extraction, or a different mechanism? 2. What specific challenges in biomedical tasks necessitate multi-resolution zoom-in, and how does Dragonfly address them? 3. Are there quantitative comparisons showing the trade-off between computational cost and performance gains from mean-pooling versus other aggregation methods? 4. How was the biomedical instruction-tuning dataset curated, and what measures were taken to ensure task diversity and quality?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "4FWAwZtd2n": {
    "paper_id": "4FWAwZtd2n",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates the effectiveness of scaling test-time compute versus pre-training compute for improving LLM reasoning capabilities. The authors propose a 'compute-optimal' strategy that adaptively allocates test-time resources based on prompt difficulty, demonstrating significant efficiency gains over baselines like best-of-N sampling. They evaluate this approach on math reasoning tasks using PaLM-2 models, showing that test-time compute can outperform larger models in certain scenarios."
          },
          "strengths": {
            "value": "The paper introduces a novel compute-optimal strategy for adaptive test-time computation, which addresses a critical gap in understanding how to effectively utilize inference-time resources. The experimental design is rigorous, with FLOPs-matched comparisons and systematic analysis of different test-time mechanisms (revisions vs. PRM search). The work provides valuable insights into the trade-offs between test-time and pre-training compute, with clear implications for model deployment and efficiency. The writing is clear, and the figures effectively summarize key findings."
          },
          "weaknesses": {
            "value": "The paper's methodology is described as 'fairly naïve,' suggesting room for improvement in the adaptive allocation algorithm. The experiments are limited to math reasoning tasks, raising questions about generalizability to other domains. The PRM verifier's performance is not thoroughly analyzed, and the paper lacks ablation studies to validate the compute-optimal strategy's components. Additionally, the FLOPs-matched comparison does not address how results scale with varying computational budgets or hardware constraints."
          },
          "questions": {
            "value": "How is the 'compute-optimal' strategy implemented? What specific metrics are used to determine when to apply revisions versus PRM search? How is the question difficulty metric defined and validated? Are there cases where the adaptive strategy fails? What is the computational overhead of the adaptive allocation mechanism? How does the approach perform on non-math tasks or more complex reasoning scenarios?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper explores the effectiveness of scaling test-time computation in large language models (LLMs) for reasoning tasks, proposing a 'compute-optimal' strategy that adaptively allocates inference-time resources based on prompt difficulty. The authors demonstrate that this approach can outperform parameter scaling in certain scenarios, particularly for math reasoning problems, by using revisions and PRM (Process-based Reward Model) search mechanisms."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem in LLM research: optimizing test-time computation for reasoning. It introduces a unified framework for understanding test-time strategies (proposer vs. verifier) and presents compelling empirical results showing that compute-optimal scaling can outperform parameter scaling in FLOPs-matched settings. The experiments on the MATH dataset are rigorous, and the insights into prompt difficulty-driven adaptation are novel. The work also provides practical implications for model deployment and pretraining trade-offs."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of how the 'compute-optimal' strategy is implemented, making it difficult to replicate or generalize the approach. The experiments are focused on math reasoning, limiting the scope of conclusions. The FLOPs-matched comparison assumes a simplified model of compute trade-offs, which may not capture real-world complexities. Additionally, the paper is truncated, leaving key methodological details (e.g., question difficulty metrics, adaptation logic) underspecified."
          },
          "questions": {
            "value": "1. How is the 'compute-optimal' strategy determined for each prompt? What criteria or models guide the allocation of resources?\n2. What specific metrics or features define 'question difficulty' from the base LLM's perspective?\n3. How does the approach handle prompts where the initial proposal distribution is already highly accurate?\n4. Are there cases where the compute-optimal strategy fails, and what are the limitations of the current implementation?\n5. How does the method scale to non-math reasoning tasks or other modalities?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates the effectiveness of scaling test-time compute for improving large language model (LLM) performance on reasoning tasks, comparing it to scaling model parameters. The authors propose a 'compute-optimal' strategy that adaptively allocates test-time computation based on prompt difficulty, demonstrating significant efficiency gains over baselines like best-of-N sampling. They also show that test-time compute can outperform larger models in FLOPs-matched evaluations, challenging the assumption that parameter scaling is always superior."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem in LLM research: optimizing inference-time computation for reasoning tasks. The contributions include a novel compute-optimal strategy that adapts to prompt difficulty, supported by rigorous experiments on the MATH dataset. The methodology is well-structured, with clear comparisons between test-time and pre-training compute. The work's significance lies in its practical implications for deploying efficient models and rethinking the trade-off between pre-training and inference compute. The clarity of explanations and figures (e.g., Figure 1) enhances readability."
          },
          "weaknesses": {
            "value": "The paper lacks depth in explaining how the 'compute-optimal' strategy dynamically adapts to prompt difficulty, which is central to its contribution. The PRM verifier mechanism is not thoroughly detailed, and the paper does not fully address potential limitations of this approach (e.g., computational overhead or scalability). Additionally, the FLOPs-matched evaluation with a 14× larger model is promising but requires more specifics on how the FLOPs were balanced. The analysis of why certain strategies outperform others on easy vs. hard prompts is superficial, leaving room for deeper theoretical insights."
          },
          "questions": {
            "value": [
              "How is 'question difficulty' quantified from the base LLM's perspective, and what features does it rely on?",
              "What specific metrics or criteria are used to determine the optimal allocation of test-time compute per prompt?",
              "Can the PRM verifier's effectiveness be validated through ablation studies or comparisons with alternative verification methods?",
              "How does the compute-optimal strategy handle real-world constraints, such as latency or resource limits during inference?",
              "What are the limitations of the FLOPs-matched evaluation in terms of generalizability to other tasks or model architectures?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 5
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "4GD7a9Bo9A": {
    "paper_id": "4GD7a9Bo9A",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates positional bias in text embedding models, demonstrating that APE- and RoPE-based models disproportionately prioritize the beginning of input texts. Through ablation studies and regression analysis, the authors quantify this bias and propose a novel data augmentation method, Position-Aware Data Sampling (PADS), to mitigate it."
          },
          "strengths": {
            "value": "The paper addresses a novel and important problem in embedding models, which has been underexplored compared to generative models. The methodology is rigorous, combining ablation studies with regression analysis to isolate positional bias. The proposed PADS technique offers a practical solution with clear experimental validation. The writing is clear, and the significance of the findings for IR systems is well-articulated."
          },
          "weaknesses": {
            "value": "The paper lacks specific details about the eight embedding models tested, making it difficult to assess the generality of findings. The ablation studies could benefit from more controlled experiments, such as varying input lengths or evaluating model-specific mechanisms. The regression analysis's methodology is not fully explained, and the paper does not compare PADS to existing bias-mitigation techniques. Additionally, the theoretical connection between preprocessing strategies and bias remains underdeveloped."
          },
          "questions": {
            "value": "1. Which eight embedding models were tested, and what are their key architectural differences? 2. How were the datasets selected to ensure generalizability across domains? 3. What ablation controls were used to rule out confounding factors like input length? 4. How does PADS compare to alternative methods for mitigating positional bias? 5. Can the authors provide additional analysis on how preprocessing strategies (e.g., chunking) interact with positional encoding mechanisms?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper investigates positional bias in text embedding models, particularly those using APE and RoPE positional encodings, which disproportionately prioritize the beginning of input texts. The authors conduct ablation studies and regression analysis to quantify this bias and propose a data augmentation method called PADS to mitigate it."
          },
          "strengths": {
            "value": "The paper addresses a critical and underexplored issue in embedding models, which has direct implications for IR and semantic similarity tasks. The ablation studies and regression analysis are methodologically sound, and the hypothesis linking bias to preprocessing and positional encoding is novel. The work highlights the importance of addressing positional sensitivity in long-context models, which is a significant contribution to the field."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental setups, such as the specific models tested (only mentioned as 'eight embedding models') and the exact implementation of PADS. The regression analysis's methodology for isolating position bias from other factors (e.g., sentence length, content) is unclear. The claims about the 12.3% similarity drop lack statistical rigor, and the paper does not compare PADS against existing mitigation strategies. Additionally, the experiments appear to focus on a narrow range of tasks, limiting generalizability."
          },
          "questions": {
            "value": "1. What are the specific embedding models used in the experiments, and how do they differ in their positional encoding mechanisms? 2. How is PADS implemented, and what are its key components? 3. Are the regression results statistically significant, and how were confounding variables (e.g., sentence length, content) controlled? 4. How does PADS compare to existing methods for mitigating positional bias in embeddings? 5. Are the findings consistent across different datasets or only specific to the one used?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper investigates positional bias in text embedding models, demonstrating that models like APE and RoPE disproportionately prioritize the beginning of input texts. Through ablation studies and regression analysis, they quantify this bias and propose a data augmentation method (PADS) to mitigate it, highlighting implications for retrieval systems."
          },
          "strengths": {
            "value": "The paper addresses a novel and practical issue in embedding models, which is underexplored despite their critical role in IR. The experimental design is rigorous, combining ablation studies and regression analysis to isolate positional bias. The hypothesis linking pre-processing strategies to bias is theoretically grounded. The work has clear significance for improving retrieval systems and long-context models."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing methods for mitigating positional bias, such as alternative augmentation techniques or model architectures. The proposed PADS method is described only briefly, with no implementation details or ablation studies validating its effectiveness. The experiments focus on a limited set of models (eight total) and may not generalize to other architectures. The hypothesis about pre-processing strategies is not empirically validated. The paper is cut off mid-section, leaving critical details (e.g., full experimental results, implementation of PADS) incomplete."
          },
          "questions": {
            "value": "1. How does PADS differ from existing data augmentation techniques, and what specific mechanisms address positional bias? 2. Are the results consistent across diverse embedding models (e.g., encoder-only vs. decoder-only)? 3. What exact pre-processing strategies (e.g., truncation, padding) are hypothesized to cause bias, and how were they validated? 4. How was the regression analysis conducted (e.g., feature selection, control variables)? 5. What are the limitations of the ablation studies (e.g., dataset-specific effects, model-specific behaviors)?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "4KKqHIb4iG": {
    "paper_id": "4KKqHIb4iG",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a backpropagation-free method for training neural PDE solvers by separating spatial and temporal variables, using random sampling of hidden layer parameters (ELMs/SWIM), and solving the resulting ODEs with classical solvers. It claims significant improvements in accuracy and training time over PINNs and mesh-based methods."
          },
          "strengths": {
            "value": "Originality: Novel combination of ELMs/SWIM with ODE solvers and space-time separation. Quality: Experiments cover diverse PDEs with high-frequency dynamics, shocks, and high-dimensionality. Clarity: Logical structure with clear problem motivation and method description. Significance: Addresses critical challenges in PDE solving, with potential impact on computational science."
          },
          "weaknesses": {
            "value": "Experiments lack detailed comparisons with state-of-the-art methods (e.g., neural operators, PINNs with advanced optimizers). Theoretical justification for the random sampling approach is minimal. Boundary condition handling is briefly described without analysis of its limitations. Claims of 1-5 order-of-magnitude improvements require more rigorous validation with quantitative metrics."
          },
          "questions": {
            "value": "1. What specific PDEs were used in experiments? Are results reproducible with standard benchmarks? 2. How does the method handle Dirichlet/Neumann boundary conditions compared to PINNs? 3. What is the exact computational complexity of the ODE solver approach vs. backpropagation? 4. Are there cases where the random sampling strategy fails? 5. How does the method scale to higher-dimensional PDEs compared to FEM?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes a backpropagation-free method for training neural PDE solvers by separating spatial and temporal variables and using random sampling of hidden layer parameters (ELMs/SWIM). The approach reformulates PDEs as ODEs in time, leveraging classical ODE solvers for improved accuracy and efficiency, particularly for high-frequency dynamics and long-time simulations."
          },
          "strengths": {
            "value": "The paper introduces a novel backpropagation-free framework that addresses key limitations of gradient-based PDE solvers, such as training instability and inefficiency. The integration of space-time separation with randomized sampling (ELMs/SWIM) is creative and aligns with recent trends in scalable neural methods. The method's potential for mesh-free, high-dimensional PDE solving is significant. The paper also highlights practical benefits like spectral convergence and compatibility with complex geometries, which could broaden applicability in computational science."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete (the paper is cut off), making it difficult to assess the claimed 1-5 order-of-magnitude improvements. The paper lacks detailed ablation studies to isolate the contributions of space-time separation vs. random sampling. The boundary condition satisfaction technique is mentioned but not elaborated. Additionally, the theoretical justification for the method's superiority over PINNs and traditional methods is underdeveloped, and comparisons to other backpropagation-free approaches (e.g., neural Galerkin methods) are missing."
          },
          "questions": {
            "value": "1. What specific PDEs were used in the experiments, and how do they represent high-frequency dynamics, shocks, and high-dimensionality? 2. How exactly are boundary conditions enforced in the proposed method, and what are the limitations of this approach? 3. Are there comparisons to other backpropagation-free methods (e.g., neural Galerkin, randomized basis functions) in the full paper? 4. What is the exact implementation of the ODE solver, and how is it integrated with the neural network's spatial basis functions? 5. How does the method handle non-autonomous PDEs or time-dependent boundary conditions?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes a backpropagation-free method for training neural PDE solvers by separating space and time variables and using random sampling of hidden layer parameters (ELM/SWIM) combined with classical ODE solvers. It claims significant improvements in accuracy and training time over PINNs and mesh-based methods."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to PDE solving by decoupling space and time, leveraging randomized neural networks (ELM/SWIM) instead of backpropagation. The method addresses key limitations of PINNs, such as training instability and inefficiency for high-frequency dynamics. The problem formulation is clear, and the potential significance of avoiding backpropagation for long-time simulations is promising. The paper also highlights novel boundary condition techniques and mesh-free advantages for high-dimensional problems."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete and lacks critical details. The claims of 1-5 orders of magnitude improvement in accuracy and 4 orders in training time are unsubstantiated without specific metrics, baselines, or comparisons to state-of-the-art methods. The paper does not clarify how the proposed boundary condition techniques work in practice or provide ablation studies. The truncated content suggests missing results, and the theoretical analysis of convergence or error bounds is absent. Additionally, the paper fails to address potential limitations, such as scalability to extremely high-dimensional problems or sensitivity to hyperparameters."
          },
          "questions": {
            "value": [
              "What specific PDEs were tested, and what quantitative metrics (e.g., L2 error, convergence rates) were used to demonstrate the 1-5 orders of magnitude improvement?",
              "How do the proposed boundary condition techniques generalize to different types of PDEs (e.g., Dirichlet, Neumann, mixed conditions)?",
              "What baselines were compared against (e.g., PINNs, FEM, other backpropagation-free methods), and how do the results compare in terms of accuracy and efficiency?",
              "Are there any cases where the method fails, and what are the limitations of the space-time separation approach?",
              "How does the random sampling of weights/biases affect the stability of the ODE solver, and what guarantees exist for the resulting solutions?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "4NsYCAxubi": {
    "paper_id": "4NsYCAxubi",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces fPLSA, a method that combines traditional Probabilistic Latent Semantic Analysis (PLSA) with foundation models to iteratively cluster and tag document segments based on contextual information. The approach aims to capture latent semantic structures in document collections, with applications in text reconstruction and hierarchical sampling. Experiments suggest fPLSA outperforms existing tagging methods in reconstructing original texts and generating diverse outputs."
          },
          "strengths": {
            "value": "Originality: The integration of foundation models with PLSA represents a novel approach to unsupervised semantic structure learning. Quality: The method is theoretically grounded in EM algorithms and leverages recent advancements in LLMs. Clarity: The paper is well-structured, with clear explanations of the framework and experimental setup. Significance: The potential applications in document analysis, task guideline generation, and diverse text sampling are impactful for NLP."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art LLM-based topic modeling methods, such as those using prompt engineering or neural topic models. Experimental validation is limited to qualitative claims (e.g., 'higher reconstruction likelihood') without quantitative metrics or statistical significance tests. The role of the foundation model in the clustering process is not sufficiently explained, raising concerns about reproducibility. The paper also does not address potential limitations, such as scalability to large datasets or sensitivity to hyperparameters."
          },
          "questions": {
            "value": "1. How exactly does the foundation model contribute to the clustering/tagging process? Are specific LLM capabilities (e.g., contextual understanding, zero-shot generalization) leveraged? 2. What baseline methods were compared against in the experiments? Are there quantitative results (e.g., ROUGE scores, perplexity) to support the claims? 3. How is the 'diversity' of hierarchical sampling measured? Are there ablation studies to validate the effectiveness of fPLSA's tagging strategy? 4. What are the computational costs and scalability properties of fPLSA compared to traditional PLSA or other methods?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces fPLSA, a method combining Probabilistic Latent Semantic Analysis (PLSA) with foundation models to iteratively cluster and tag document segments based on contextual information. It claims that fPLSA improves text reconstruction and hierarchical sampling compared to existing tagging methods, particularly on story writing, math, and reasoning tasks."
          },
          "strengths": {
            "value": "The paper presents a novel integration of traditional PLSA with foundation models, addressing limitations of prior work that rely on manual topic interpretation or bag-of-words representations. The iterative EM framework is theoretically grounded, and the method's potential for capturing semantic structures beyond lexical features is promising. The experiments suggest improvements in reconstruction likelihood and sampling diversity, which could have practical implications for NLP tasks."
          },
          "weaknesses": {
            "value": "The paper lacks critical experimental details, such as specific metrics for reconstruction quality, comparison to strong baselines (e.g., neural topic models or LLM-based methods), and ablation studies to isolate the contribution of foundation models. The claims about 'higher likelihood of hitting the correct answer' are vague without quantitative justification. Additionally, the method's scalability and computational efficiency are not discussed, and the paper does not address potential overfitting to specific datasets."
          },
          "questions": {
            "value": "1. What specific metrics were used to evaluate text reconstruction quality (e.g., BLEU, ROUGE, or likelihood scores)? 2. How do the results compare to state-of-the-art neural topic models or LLM-based approaches like those in [Pham et al. 2024]? 3. What is the computational cost of integrating foundation models into the PLSA framework, and how does it scale to large datasets? 4. Are the tags generated by fPLSA interpretable, and how do they differ from existing semantic labeling methods? 5. How does the iterative clustering process avoid overfitting or collapsing into trivial clusters?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces fPLSA, a method that combines Probabilistic Latent Semantic Analysis (PLSA) with foundation models to discover latent semantic structures in document collections. The approach uses iterative clustering and tagging of document segments based on context, aiming to improve text reconstruction and hierarchical sampling for diverse outputs."
          },
          "strengths": {
            "value": "The paper addresses a significant problem in NLP: automating the discovery of semantic structures without human supervision. It bridges traditional topic modeling (PLSA) with foundation models, which is a promising direction. The motivation is clear, and the experimental claims (e.g., better reconstruction and diversity) are relevant to practical applications. The related work section is comprehensive, highlighting gaps that fPLSA aims to fill."
          },
          "weaknesses": {
            "value": "The paper lacks technical details on how foundation models are integrated into the PLSA framework. For example, it is unclear how LLMs contribute to the E/M steps or how their outputs are incorporated into the probabilistic model. The experimental evaluation is briefly mentioned but not described in depth, making it hard to assess rigor. Additionally, the paper does not compare against state-of-the-art LLM-based methods or provide ablation studies to validate the necessity of the proposed components."
          },
          "questions": {
            "value": "1. How exactly are foundation models utilized in the iterative clustering and tagging process? Are they used for generating segment embeddings, guiding topic assignments, or another role? 2. What specific modifications were made to PLSA to incorporate foundation models, and how do these differ from prior LLM-based topic modeling approaches? 3. Are the experiments on story writing, math, and reasoning datasets reproducible? What metrics were used beyond reconstruction likelihood and diversity? 4. How does fPLSA scale to large document collections, and what are its computational limitations?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "4Sv5MQ931E": {
    "paper_id": "4Sv5MQ931E",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes MSR-ViR, a framework for Video Question Answering (VideoQA) that enhances interpretability by integrating a Modularized Spatial-Temporal Grounding (MoST-Grounding) module and a reinforcement learning-based Alternate Self-reflection Training strategy. The approach uses a question parser LLM to generate execution policies, enabling modular reasoning and visual evidence localization, while the self-reflection training optimizes both the parser and multimodal LLM."
          },
          "strengths": {
            "value": "Originality is evident in combining modular reasoning with self-reflection for interpretability in VideoQA. The methodology is rigorous, with a clear architecture for spatial-temporal grounding and a novel training strategy. Experiments on standard datasets demonstrate performance improvements and accurate localization. The paper addresses a critical gap in transparency for multimodal LLMs, making it significant for explainable AI. Clarity is maintained through structured explanations of components and their roles."
          },
          "weaknesses": {
            "value": "The paper lacks ablation studies to validate the contribution of individual components (e.g., MoST-Grounding vs. self-reflection training). The comparison with existing modular methods (e.g., MoReVQA, ViperGPT) is superficial, and the paper does not address potential limitations of the question parser LLM. The reinforcement learning formulation is not detailed enough to assess its novelty or effectiveness. Additionally, the related work section is incomplete, omitting recent advancements in grounded VideoQA."
          },
          "questions": {
            "value": [
              "How does the MoST-Grounding module handle complex, multi-step questions that require integrating multiple spatial-temporal cues?",
              "What is the exact architecture and training setup of the question parser LLM, and how is it pretrained on VideoQA tasks?",
              "Are there any cases where the self-reflection training strategy fails, and how does the framework handle such failures?",
              "How does the paper ensure that the generated execution policies are aligned with the video content, given the parser LLM's potential for errors?",
              "What are the computational costs of the proposed framework compared to baseline methods, and how scalable is it to longer videos?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes MSR-ViR, a framework for Video Question Answering (VideoQA) that enhances interpretability by integrating a Modularized Spatial-Temporal Grounding (MoST-Grounding) module and a reinforcement learning-based self-reflection training strategy. The MoST-Grounding module uses a question parser LLM to generate execution policies, enabling temporal and spatial localization of video evidence, while the self-reflection strategy alternates between fine-tuning the multimodal LLM and the parser LLM to improve performance."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in VideoQA by introducing a modular framework with explicit interpretability mechanisms, which is a significant contribution to the field. The methodology combines existing techniques (e.g., visual adapters, LLMs) in a novel way, and the experiments on standard datasets demonstrate improved performance. The clarity of the problem statement, framework design, and experimental setup is strong, with clear separation of components. The significance lies in advancing both task performance and model transparency in video understanding."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons against state-of-the-art grounding-based methods, making it difficult to assess the magnitude of the improvement. The question parser LLM's training methodology is under-specified, and there is no analysis of its reliability when unsupervised. The reinforcement learning component is described abstractly without concrete implementation details. Additionally, the paper does not address potential limitations, such as scalability to longer videos or handling ambiguous questions. Ablation studies to validate the contribution of individual components (e.g., MoST-Grounding vs. self-reflection) are missing."
          },
          "questions": {
            "value": [
              "How does the question parser LLM perform on tasks without supervision, and what measures are taken to ensure the quality of generated execution policies?",
              "What specific baselines were compared against in the experiments (e.g., Video-LLaMA, VTimeLLM, or modular methods like MoReVQA)?",
              "Can the authors provide more details on the reinforcement learning setup, such as reward functions or training dynamics?",
              "Are there any limitations in the framework's ability to handle complex or ambiguous questions, and how are these addressed?",
              "How does the computational efficiency of MSR-ViR compare to existing methods, especially given the modular and iterative training process?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces MSR-ViR, a modularized self-reflected video reasoner for Video Question Answering (VideoQA). The framework combines a Modularized Spatial-Temporal Grounding (MoST-Grounding) module with a reinforcement learning-based Alternate Self-reflection Training strategy. MoST-Grounding uses a question parser LLM to generate interpretable execution policies, while invoking small modules to localize temporal-spatial evidence in videos. The method achieves state-of-the-art performance on VideoQA benchmarks while providing transparent reasoning paths and visual evidence."
          },
          "strengths": {
            "value": "The paper presents a novel approach to improving interpretability in VideoQA by combining modular reasoning with reinforcement learning. The MoST-Grounding module introduces a structured way to localize video evidence, addressing a key limitation of black-box multimodal LLMs. The alternate self-reflection training strategy offers a promising method for joint optimization of the question parser and multimodal LLM. The experiments demonstrate significant performance improvements on multiple benchmarks, and the clear separation of modules enhances the framework's transparency. The work also effectively connects recent advances in modular reasoning and grounded video understanding."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the contribution of individual components (e.g., the question parser vs. the grounding module). The reinforcement learning implementation details (e.g., reward function design, training dynamics) are not sufficiently explained. The comparison with existing modular VideoQA methods is limited - it would be valuable to see direct comparisons with approaches like MoReVQA or ViperGPT. The paper also doesn't address how the framework handles videos with complex temporal structures or long sequences. The visual evidence visualization method is not described in detail."
          },
          "questions": {
            "value": [
              "How is the question parser LLM trained? Does it require explicit supervision on VideoQA datasets?",
              "What specific rewards are used in the reinforcement learning objective? How are they balanced with the supervised finetuning loss?",
              "How does the framework handle videos with varying lengths or frame rates? Is there any temporal resolution adaptation mechanism?",
              "Can the execution policies generated by the question parser be visualized or analyzed to assess their quality?",
              "What is the computational cost of the alternate self-reflection training compared to standard finetuning approaches?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "4VHiptx7xe": {
    "paper_id": "4VHiptx7xe",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces STRAP, a method for robot policy learning that retrieves sub-trajectories from large datasets using vision foundation models and dynamic time warping (DTW). The approach focuses on sub-trajectory-level similarity rather than full trajectories, aiming to improve data utilization, generalization, and robustness in few-shot imitation learning scenarios."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in multi-task robot learning by proposing a novel sub-trajectory retrieval framework. The use of pre-trained vision models (e.g., DINOv2) for object-centric feature extraction and DTW for length-invariant alignment is both original and practical. The method's ability to automatically extract sub-trajectories without manual segmentation is a significant advantage. The paper also demonstrates strong theoretical motivation and clear connections to related work, with experiments on benchmark datasets like LIBERO showing promising results."
          },
          "weaknesses": {
            "value": "The paper is truncated, leaving key details about experimental results, baseline comparisons, and implementation specifics incomplete. The reliance on pre-trained vision models may limit applicability to domains with significant visual domain shifts. The sub-trajectory extraction mechanism lacks technical depth, and the paper does not address potential limitations in computational efficiency or scalability. Additionally, the real-world evaluation details are insufficient to assess the method's practical impact."
          },
          "questions": {
            "value": [
              "How exactly is sub-trajectory extraction automated, and what criteria are used to define sub-trajectory boundaries?",
              "What specific baselines were compared against (e.g., full-trajectory retrieval, multi-task learning), and how were they implemented?",
              "How does STRAP handle cases where DINOv2 features fail to capture task-relevant visual distinctions?",
              "What are the computational requirements for real-time deployment, and how does the method scale to very large datasets?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces STRAP, a method for robot policy learning that retrieves sub-trajectories from large pre-collected datasets during deployment. By leveraging vision foundation models (e.g., DINOv2) and dynamic time warping, STRAP focuses on sub-trajectory-level similarity rather than full-trajectory matching, aiming to improve data utilization, generalization, and robustness in few-shot imitation learning scenarios."
          },
          "strengths": {
            "value": "Originality: The focus on sub-trajectory retrieval instead of full-trajectory matching addresses a critical gap in multi-task policy learning, where negative transfer limits performance. The integration of off-the-shelf vision models and dynamic time warping for alignment is a novel combination. Quality: The paper provides clear motivation for the problem of negative transfer in multi-task policies and proposes a method with strong theoretical grounding. Clarity: The approach is well-explained, with logical flow from problem statement to solution. Significance: The method has practical implications for real-world robotics, where data collection is costly and deployment environments are dynamic."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental comparisons with state-of-the-art retrieval methods, making it difficult to assess the magnitude of improvements claimed. The real-world experiments are not sufficiently described, and it is unclear how STRAP performs under varying environmental conditions. The automatic sub-trajectory extraction mechanism is not elaborated, leaving questions about its robustness. Additionally, the paper does not address computational efficiency, which is critical for real-time deployment."
          },
          "questions": {
            "value": "How is sub-trajectory extraction automated, and what are the criteria for segmenting trajectories? What ablation studies validate the contribution of DINOv2 features versus other embedding methods? How does STRAP handle scenarios where sub-trajectories from different tasks have conflicting objectives? Are there quantitative results demonstrating the reduction in negative transfer compared to multi-task learning? What are the limitations of the method in domains with extreme visual differences?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces STRAP, a method for robot policy learning that retrieves sub-trajectories from large pre-collected datasets during deployment. By leveraging pre-trained vision models and dynamic time warping, STRAP enables robust policy adaptation to novel tasks with minimal in-domain data, outperforming multi-task learning and prior retrieval methods in simulations and real-world experiments."
          },
          "strengths": {
            "value": "Originality: STRAP's focus on sub-trajectory retrieval rather than full-trajectory matching addresses a critical gap in robotic policy learning, offering a novel approach to mitigate negative transfer. Quality: The method combines established techniques (DINOv2, DTW) with a clear rationale for improving data utilization. Clarity: The paper is well-structured, with a logical flow from problem formulation to experiments. Significance: The work has practical implications for real-world robotic deployment, where data collection is costly and tasks are dynamic."
          },
          "weaknesses": {
            "value": "The experimental validation is limited in scope, with insufficient comparison to state-of-the-art retrieval methods. The real-world experiments lack quantitative metrics, relying on qualitative descriptions. The paper does not address computational efficiency or scalability to extremely large datasets. The theoretical analysis of why sub-trajectory retrieval outperforms full-trajectory methods is superficial, leaving key mechanisms underexplored."
          },
          "questions": {
            "value": [
              "How does STRAP handle sub-trajectories with significantly different lengths or temporal structures compared to the query? What are the limitations of dynamic time warping in this context?",
              "What specific ablation studies were conducted to validate the contribution of DINOv2 features versus alternative embeddings?",
              "The paper claims to outperform multi-task learning, but which baselines were used for comparison? Were results compared to recent methods like DROID or other sub-trajectory approaches?",
              "How is the automatic sub-trajectory extraction framework implemented? What criteria are used to segment trajectories, and how robust is this to noise or varying task structures?",
              "What are the computational requirements of STRAP for real-time deployment? How does it scale with the size of the training corpus?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "4XHyThqt1C": {
    "paper_id": "4XHyThqt1C",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces Alternating Optimized Stochastic Vector Quantization (AOSVQ), a novel method for neural compression that addresses train-test mismatch and suboptimal encoder gradients. It combines an encoder-decoder alternating optimization strategy with a sphere-noise based stochastic approximation to improve rate-distortion (RD) optimization. The approach theoretically analyzes the roles of encoder and decoder in quantization boundaries and centers, validated through experiments on vector sources and natural images."
          },
          "strengths": {
            "value": "The paper demonstrates strong theoretical contributions by reinterpreting neural compression as source-space vector quantization, clarifying the roles of encoder and decoder in determining quantization boundaries and centers. The alternating optimization strategy effectively mitigates train-test mismatch by decoupling encoder and decoder training. The sphere-noise approximation method is innovative, leveraging high-dimensional geometry to align encoder gradients with RD loss differences. The experiments show consistent improvements across diverse tasks, and the writing is structured with clear motivation and technical depth."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art methods like STE or soft quantization, which limits the assessment of its relative effectiveness. The sphere-noise implementation details (e.g., radius determination, computational overhead) are not fully explained. The experimental section is incomplete (cut-off at Page 3), leaving critical results (e.g., ablation studies, quantitative comparisons) unverified. The theoretical analysis assumes idealized conditions that may not hold in practical neural compression systems."
          },
          "questions": {
            "value": [
              "How does the sphere-noise method handle high-dimensional latent spaces, and what is the computational cost compared to existing approximations?",
              "What is the exact mechanism for determining the sphere radius in practice, and how does it adapt to varying quantization granularities?",
              "Are there comparisons with advanced methods like multi-stage VQ or lattice-based quantization that address similar challenges?",
              "How does the alternating optimization strategy affect training stability and convergence speed?",
              "What ablation studies validate the individual contributions of the alternating optimization and sphere-noise components?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes Alternating Optimized Stochastic Vector Quantization (AOSVQ) for neural compression, addressing train-test mismatch and suboptimal encoder gradients in vector quantization (VQ). The method combines alternating encoder-decoder optimization with a sphere-noise based stochastic approximation to improve rate-distortion (RD) optimization."
          },
          "strengths": {
            "value": "The paper provides a novel theoretical analysis of how encoder and decoder gradients affect RD optimization, framing VQ as a source-space quantizer. The alternating optimization strategy is creative for decoupling encoder and decoder training, and the sphere-noise approximation introduces a new perspective on gradient alignment. The problem formulation is clear and addresses key limitations in existing neural compression methods."
          },
          "weaknesses": {
            "value": "The paper lacks concrete experimental results (e.g., PSNR, bitrate comparisons) to validate the proposed method, despite claiming 'comprehensive experiments.' The sphere-noise approximation is described conceptually but not mathematically formalized or implemented. The related work section contains repeated citations (e.g., multiple references to Agustsson2020) and incomplete references, undermining rigor. The figure captions reference missing figures (e.g., Figure~\ref{fig:1} without the actual image)."
          },
          "questions": {
            "value": [
              "How is the sphere-noise approximation implemented in practice? What are the hyperparameters governing the hypersphere radius and distribution?",
              "What are the specific quantitative results comparing AOSVQ to existing methods like STE or additive noise? Are there ablation studies to isolate the contributions of the alternating optimization and sphere-noise components?",
              "Why is the alternating optimization strategy more effective than joint optimization? Is there theoretical justification for the convergence of this approach?",
              "How does the sphere-noise method address the 'suboptimal encoder gradients' issue compared to prior work? What is the mathematical basis for the claimed gradient alignment with RD loss differences?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper addresses challenges in neural compression caused by vector quantization (VQ) by proposing a novel method called Alternating Optimized Stochastic Vector Quantization (AOSVQ). The approach combines an encoder-decoder alternating optimization strategy to reduce train-test mismatch and a sphere-noise based stochastic approximation to improve encoder gradients for better rate-distortion (RD) optimization."
          },
          "strengths": {
            "value": "The paper introduces a novel framework for neural compression that tackles two critical issues in VQ: train-test mismatch and suboptimal encoder gradients. The alternating optimization strategy is creative, as it decouples encoder and decoder training to align with their distinct roles in quantization. The sphere-noise approximation method is theoretically grounded, with a clear connection to optimal entropy-constrained vector quantization (ECVQ). The work is significant for its potential to improve RD performance in neural compression, and the paper's structure and problem formulation are well-organized. The experimental claims on diverse data types suggest broad applicability."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results and comparisons with state-of-the-art methods, which weakens the evidence for its claims. The related work section is incomplete and contains repeated citations (e.g., multiple mentions of Agustsson2020), which may indicate poor organization. The theoretical analysis of the sphere-noise method is promising but requires more rigorous validation. Additionally, the paper does not address potential computational costs or scalability to high-dimensional data, which are critical for real-world deployment."
          },
          "questions": {
            "value": "1. What specific datasets and baselines were used in the experiments? How does AOSVQ compare to methods like ECVQ or recent neural compression techniques (e.g., \\cite{balle2020end}) in terms of RD performance? 2. How is the sphere-noise radius determined in practice, and what are the hyperparameters for the stochastic approximation? 3. Are there any ablation studies to isolate the contributions of the alternating optimization strategy versus the sphere-noise method? 4. How does the method handle high-dimensional data, and what is its computational overhead compared to existing approaches?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "4YzVF9isgD": {
    "paper_id": "4YzVF9isgD",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces HyperFace, a method for generating synthetic face recognition datasets by optimizing embeddings on a hypersphere to maximize inter-class variation. The approach formulates dataset generation as a spherical packing problem, solves it via gradient descent, and uses a conditional generator to synthesize images from optimized embeddings. Experiments show competitive performance on benchmark datasets."
          },
          "strengths": {
            "value": "Originality is strong, as the hypersphere packing formulation addresses a novel angle for synthetic dataset generation. The methodology is well-structured, combining optimization and generative modeling. Clarity is good, with clear sections on problem formulation, method, and experiments. The significance is high, given the ethical concerns around real-world face datasets and the potential impact of synthetic data on model training."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contribution of the hypersphere optimization versus the generator model. The experimental evaluation could be more comprehensive, such as comparing against a broader range of synthetic datasets or evaluating downstream tasks beyond standard benchmarks. The computational cost of the optimization is not discussed, which is critical for scalability. Additionally, the paper does not address how the optimized embeddings avoid mode collapse or ensure diversity in generated faces."
          },
          "questions": {
            "value": "1. How does the paper ensure that the optimized embeddings on the hypersphere correspond to realistic face variations rather than just geometrically spaced points? 2. What specific metrics (e.g., FID, face recognition accuracy) were used to evaluate the quality of the generated datasets? 3. Are there ablation studies demonstrating the effectiveness of the regularization term in the optimization? 4. How does the computational efficiency of HyperFace compare to existing synthetic dataset generation methods, especially for large-scale datasets?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces HyperFace, a method for generating synthetic face recognition datasets by optimizing the packing of embeddings on a hypersphere. The approach formulates dataset generation as a spherical code optimization problem to maximize inter-class variation, followed by image synthesis from optimized embeddings. Experiments show competitive performance compared to existing synthetic datasets."
          },
          "strengths": {
            "value": "The paper presents a novel approach by applying spherical code optimization (Tammes problem) to face embedding spaces, addressing a critical gap in inter-class variation for synthetic datasets. The method is theoretically grounded, with clear problem formulation and optimization strategy. The practical implementation using a conditional generator and evaluation on benchmark datasets demonstrates relevance. The work tackles important ethical concerns in face recognition data collection."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art synthetic datasets (e.g., how does HyperFace outperform existing methods like those in [melzi2024synthetic]?). The computational feasibility for large-scale datasets (e.g., 10,000+ identities) is not thoroughly discussed. The regularization term's impact is not quantitatively analyzed. Experimental results focus on a limited set of benchmarks, and ablation studies on key components (e.g., distance functions, optimization steps) are missing."
          },
          "questions": {
            "value": "1. How does HyperFace handle the computational complexity of optimizing 10,000+ embeddings on a high-dimensional hypersphere? 2. What specific distance function (e.g., cosine, Euclidean) was used in the optimization, and how does it affect results? 3. Are there ablation studies showing the contribution of the regularization term to inter-class variation? 4. How does the generated dataset perform on tasks beyond face recognition (e.g., gender classification)? 5. What are the limitations of the conditional generator in capturing intra-class variation compared to specialized face synthesis models?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces HyperFace, a method for generating synthetic face recognition datasets by optimizing embeddings on a hypersphere. The approach formulates dataset generation as a spherical code optimization problem to maximize inter-class variation, followed by image synthesis from optimized embeddings. Experiments suggest that models trained on HyperFace achieve state-of-the-art performance on real-world benchmarks."
          },
          "strengths": {
            "value": "The paper addresses a critical problem of privacy in face recognition datasets with a novel approach leveraging hypersphere packing. The connection to established optimization problems (e.g., Tammes problem) provides theoretical grounding. The method's focus on inter-class variation, which is often overlooked in synthetic data generation, is significant. The experimental results demonstrate competitive performance against existing synthetic datasets, suggesting practical relevance."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the effectiveness of the proposed regularization term and optimization strategy. The computational scalability of the method for large-scale datasets (e.g., 10,000+ identities) is not addressed, despite being a stated goal. The comparison to baselines is vague—specific metrics and baselines (e.g., StyleGAN-based methods) are not clearly defined. The paper also does not discuss potential limitations of the generator model in capturing fine-grained variations."
          },
          "questions": {
            "value": "How is the regularization term designed to maintain embeddings on the face embedding manifold? What specific conditional face generator is used, and does it have limitations in generating diverse faces? Are the synthetic datasets evaluated on multiple benchmarks beyond the ones mentioned? How does the method scale to very large n_id (e.g., 10,000 identities) in terms of computation and quality? What is the impact of different distance functions (e.g., cosine vs. Euclidean) on the optimization results?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "4ZX2a3OKEV": {
    "paper_id": "4ZX2a3OKEV",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a surrogate loss-based approach to solve hidden monotone variational inequalities (VIs) in deep learning, leveraging the structure of model outputs. The method employs an alpha-descent condition for convergence guarantees and demonstrates effectiveness in min-max optimization and reinforcement learning tasks, including a novel TD(0) variant."
          },
          "strengths": {
            "value": "The paper presents a novel extension of surrogate losses to VI problems, which addresses a critical gap in deep learning for non-scalar optimization. The convergence analysis under realistic assumptions (e.g., hidden monotonicity, interpolation) is theoretically rigorous. The unification of preconditioning methods via the Gauss-Newton connection is insightful. Experimental results on min-max and value prediction tasks, along with a sample-efficient TD(0) variant, highlight practical relevance. The algorithm's compatibility with existing optimizers like ADAM is a significant practical advantage."
          },
          "weaknesses": {
            "value": "The convergence guarantees rely on strong assumptions (e.g., hidden monotonicity, sufficient surrogate optimization) that may not hold in general VI scenarios. The paper lacks comparisons with state-of-the-art VI solvers (e.g., extragradient, optimistic methods) in experiments. The empirical validation for the TD(0) variant is limited to specific settings, with unclear generalization. The alpha-descent condition's practical tuning and robustness to hyperparameters are not thoroughly analyzed. Theoretical analysis of the interplay between surrogate loss minimization and VI solution quality is incomplete."
          },
          "questions": {
            "value": [
              "How do the authors justify the assumption of hidden monotonicity in practical VI problems? Are there empirical validations of this assumption in their experiments?",
              "What are the limitations of the alpha-descent condition when applied to non-monotone VIs? How does the method handle violations of the assumed structure?",
              "Why is the proposed TD(0) variant more compute/sample efficient than existing methods? Are there ablation studies demonstrating this claim?",
              "How does the surrogate loss approach compare to established VI solvers (e.g., extragradient, mirror descent) in terms of convergence speed and stability?",
              "What is the theoretical basis for the claim that surrogate losses improve data efficiency in VI settings? Are there formal guarantees for this?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces a surrogate loss-based algorithm called α-descent to solve hidden monotone variational inequalities (VIs) in deep learning. The approach leverages the hidden structure of model outputs to ensure convergence under realistic assumptions, unifies existing preconditioning methods, and demonstrates effectiveness in min-max optimization and reinforcement learning tasks."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in deep learning for VI problems, which are common in min-max optimization and reinforcement learning. The α-descent condition is a novel contribution that avoids restrictive assumptions like summable errors. The unification of preconditioning methods under the surrogate loss framework provides a fresh perspective. The experimental validation on min-max and TD(0) variants shows practical relevance, though details are limited."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous experimental comparisons with state-of-the-art VI solvers, making it hard to assess the empirical advantages of the proposed method. The convergence analysis relies on assumptions (e.g., hidden monotonicity, interpolation) that are not thoroughly justified in practical scenarios. The TD(0) variant's improvements are not quantified with concrete metrics. Additionally, the surrogate loss formulation's scalability to large-scale deep learning tasks remains unexplored."
          },
          "questions": {
            "value": [
              "How do the authors validate the practicality of their hidden monotonicity assumption in real-world VI problems?",
              "What are the specific computational or sample efficiency gains of the proposed TD(0) variant compared to existing methods?",
              "Can the α-descent condition handle non-monotone VIs, and how is this addressed in the analysis?",
              "Are the experiments on min-max optimization conducted on standard benchmarks, and how do they compare to baselines like GAN training or adversarial robustness methods?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of solving variational inequalities (VIs) in deep learning settings, where traditional gradient-based methods often fail. The authors propose a surrogate loss-based approach with an α-descent condition to ensure convergence under realistic assumptions. They unify preconditioning methods, demonstrate effectiveness in min-max optimization and reinforcement learning, and introduce a novel TD(0) variant."
          },
          "strengths": {
            "value": "The paper tackles a critical problem in deep learning—solving VIs—which has practical relevance for applications like GANs and RL. The α-descent condition is a novel contribution that avoids restrictive assumptions in prior work. The unification of preconditioning methods provides theoretical clarity. The experimental validation in diverse VI scenarios, including a new TD(0) variant, demonstrates practical utility. The writing is structured and accessible, with clear algorithmic descriptions."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with existing VI solvers, making it hard to assess relative performance. The convergence analysis assumes 'hidden monotonicity' and sufficient surrogate optimization, but the practicality of these assumptions is not thoroughly discussed. The experimental section is truncated, leaving gaps in understanding the TD(0) variant's innovations. The novelty of the surrogate loss approach compared to prior work (e.g., \\cite{vaswani2021surrogate}) is not sufficiently justified."
          },
          "questions": {
            "value": "How does the α-descent condition compare to existing convergence criteria for VIs? What are the exact assumptions required for convergence, and how do they hold in non-convex settings? Are there specific VI problems where the method fails, and why? How does the new TD(0) variant differ from prior work in terms of updates and theoretical guarantees? What is the empirical impact of the α-descent condition on training stability?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "4anfpHj0wf": {
    "paper_id": "4anfpHj0wf",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces POINT SET DIFFUSION, a diffusion-based latent variable model for representing arbitrary point processes on general metric spaces without relying on intensity functions. The approach learns stochastic interpolation between data and noise point sets, enabling efficient sampling and flexible conditional generation. Experiments show state-of-the-art performance on synthetic and real-world datasets for spatial and spatiotemporal point processes."
          },
          "strengths": {
            "value": "Originality: The paper presents a novel framework for point processes by bypassing intensity functions, leveraging diffusion models and point process properties like superposition and thinning. Quality: The method addresses a key limitation of existing models (reliance on intensity functions) and demonstrates practical benefits (faster sampling). Clarity: The paper provides a structured overview of background, methodology, and contributions, with clear technical definitions. Significance: Point processes are foundational in many applications, and the approach opens new possibilities for modeling complex interactions without restrictive assumptions."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental comparisons with state-of-the-art methods for point processes, such as likelihood-based models or other diffusion approaches. The evaluation framework is mentioned but not elaborated, leaving unclear how it assesses generative quality. Theoretical analysis of the model's convergence or guarantees is missing. The ablation studies and analysis of hyperparameter sensitivity are not described, which limits understanding of the method's robustness."
          },
          "questions": {
            "value": [
              "What specific baselines were compared against in the experiments? How does POINT SET DIFFUSION outperform existing methods like INGP or Hawkes processes?",
              "How does the model handle varying densities or large-scale point sets? Are there scalability experiments?",
              "Can the paper provide more details about the proposed evaluation framework? How does it quantify conditional generation quality?",
              "Are there theoretical guarantees for the learned stochastic interpolation process? How is the reverse diffusion process defined mathematically?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces POINT SET DIFFUSION, a diffusion-based latent variable model that represents arbitrary point processes on general metric spaces without relying on intensity functions. By learning stochastic interpolation between data and noise point sets, the method enables efficient sampling and conditional generation, achieving state-of-the-art results on synthetic and real-world datasets."
          },
          "strengths": {
            "value": "The paper's originality lies in its novel application of diffusion models to point processes, bypassing traditional intensity function dependencies. The methodology is well-grounded in point process theory, leveraging superposition and thinning properties. The experiments demonstrate strong performance on both synthetic and real-world tasks, with clear claims of faster sampling. The model-agnostic evaluation framework is a valuable contribution. The writing is clear, and the technical details are well-explained."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art methods for point processes, such as likelihood-based models or other diffusion approaches. While experiments on synthetic data are thorough, real-world applications are limited to a few examples, and the scalability to high-dimensional spaces is not discussed. The theoretical analysis of the diffusion process's convergence or stability is minimal. Additionally, the paper does not address how the model handles extreme density variations or edge cases."
          },
          "questions": {
            "value": [
              "How does the model scale to high-dimensional metric spaces, and what are the computational trade-offs compared to existing methods?",
              "What specific limitations exist in the current formulation, and how might they be addressed in future work?",
              "How is the noise point process defined, and how sensitive is the model to its choice?",
              "Are there scenarios where the model's performance degrades, and how are these handled?",
              "How does the model's efficiency compare to other diffusion-based or likelihood-based approaches in terms of training time and memory usage?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces POINT SET DIFFUSION, a diffusion-based latent variable model that represents arbitrary point processes on general metric spaces without relying on intensity functions. The method learns to stochastically interpolate between noise and data point sets, enabling efficient parallel sampling and flexible conditional generation. Experiments show state-of-the-art performance on synthetic and real-world datasets for spatial and spatiotemporal point processes."
          },
          "strengths": {
            "value": "The paper presents a novel approach to modeling point processes by eliminating reliance on intensity functions, addressing a key limitation in existing methods. The method's ability to enable parallel sampling and conditional generation is a significant practical advantage. The theoretical foundation leverages superposition and thinning properties of point processes, and the proposed model-agnostic evaluation framework adds value. The experiments demonstrate strong performance on diverse tasks, with clear contributions to both methodology and applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with alternative state-of-the-art methods beyond the referenced ADD-THIN model. The theoretical analysis of the diffusion process's convergence or distributional accuracy is limited, with proofs only referenced in an appendix (not fully visible). The experimental validation appears focused on specific datasets, with insufficient exploration of scalability or edge cases. The paper does not explicitly address how the model handles high-dimensional metric spaces or complex dependencies beyond the examples provided."
          },
          "questions": {
            "value": [
              "How does the model ensure distributional accuracy when the underlying metric space has non-Euclidean structure?",
              "What ablation studies were conducted to verify the necessity of key components (e.g., superposition/thinning properties)?",
              "Are there theoretical guarantees for the convergence of the diffusion process in general metric spaces?",
              "How does the computational complexity scale with the size of the point sets or dimensionality of the metric space?",
              "What specific challenges were encountered when applying the method to real-world datasets, and how were they addressed?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "4dAgG8ma3B": {
    "paper_id": "4dAgG8ma3B",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces CHEMGUIDE, a method that leverages quantum chemistry as a non-differentiable oracle to guide diffusion models for molecular generation. By integrating zeroth-order optimization with quantum chemistry calculations (e.g., xTB), the approach improves molecular stability, reduces atomic forces, and enables compatibility with both explicit and implicit guidance frameworks."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in molecular generation: the scarcity of labeled data. The use of quantum chemistry as a non-differentiable oracle is innovative and well-motivated. The method combines zeroth-order optimization with diffusion models, offering a novel solution to guide generation without relying on neural network-based property predictors. The experiments demonstrate practical benefits, including improved stability and generalization. The clarity of the problem statement and the structured presentation of contributions are strong."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art methods that use similar principles (e.g., physics-informed diffusion models). The integration of the non-differentiable oracle into the diffusion process is not thoroughly explained, particularly how zeroth-order gradients are estimated and scaled. The experimental section appears truncated, making it difficult to assess the full scope of results. Additionally, the bilevel optimization framework is mentioned but not elaborated on, leaving questions about its implementation and effectiveness."
          },
          "questions": {
            "value": "1. How are the zeroth-order gradients from quantum chemistry calculations incorporated into the diffusion process? What specific optimization steps are taken? 2. Are there ablation studies to validate the contribution of the non-differentiable oracle versus other components? 3. How does CHEMGUIDE handle the computational cost of quantum chemistry evaluations during inference? 4. What metrics are used to quantify 'stability' and 'validity' of generated molecules, and how do they compare to existing benchmarks?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces CHEMGUIDE, a method that leverages quantum chemistry as a non-differentiable oracle to guide diffusion models for molecular generation. By using zeroth-order optimization techniques, CHEMGUIDE provides gradient estimates from quantum chemistry calculations (e.g., xTB) to steer diffusion processes toward stable molecular configurations without requiring labeled data. The approach demonstrates improvements in molecular stability, compatibility with both explicit/implicit guidance, and generalization to tasks beyond stability optimization."
          },
          "strengths": {
            "value": "Originality: The integration of non-differentiable quantum chemistry calculations into diffusion models is novel, addressing data scarcity challenges in molecular generation. Quality: The method is theoretically grounded, with clear connections to existing diffusion literature and bilevel optimization frameworks. Clarity: The paper is well-structured, with detailed explanations of the core ideas and experimental setup. Significance: The work has potential impact in computational chemistry and drug discovery by enabling stable molecular generation without reliance on large labeled datasets."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contribution of quantum chemistry guidance versus other components. The integration of non-differentiable oracles (e.g., xTB) into diffusion models is not thoroughly explained, particularly how gradient estimates are computed and stabilized. Experimental comparisons are limited to specific baselines (e.g., GeoLDM), with insufficient analysis of performance relative to state-of-the-art methods. The generalization claims (e.g., to tasks beyond stability) lack quantitative validation."
          },
          "questions": {
            "value": "1. How are the non-differentiable gradients from quantum chemistry calculations (e.g., xTB) approximated and stabilized during diffusion? 2. What are the computational costs of integrating quantum chemistry oracles compared to traditional neural network-based guidance? 3. Are there specific molecular properties or tasks where CHEMGUIDE underperforms, and why? 4. How does the bilevel optimization framework handle conflicting objectives between molecular properties and stability during training?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces CHEMGUIDE, a method that leverages quantum chemistry as a non-differentiable oracle to guide diffusion models for molecular generation. Instead of relying on neural networks for property prediction, it uses zeroth-order optimization to estimate gradients from quantum chemistry calculations (e.g., xTB) to improve molecular stability and validity."
          },
          "strengths": {
            "value": "The paper presents a novel approach by integrating quantum chemistry as a non-differentiable oracle into diffusion models, addressing the challenge of limited labeled data in molecular generation. The methodology is theoretically grounded, with clear connections to zeroth-order optimization and bilevel optimization frameworks. The experiments demonstrate practical benefits, such as reduced atomic forces and compatibility with both explicit/implicit guidance. The paper also highlights generalization to tasks beyond stability optimization, which is significant for molecular design."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of computational costs and scalability when using quantum chemistry software (e.g., xTB/Gaussian), which are computationally intensive. The integration of non-differentiable gradients into the diffusion process is not thoroughly explained, particularly how the zeroth-order optimization is adapted for high-dimensional molecular spaces. The experiments focus on QM9 and GEOM datasets but do not compare with other non-differentiable guidance methods or baseline approaches. Additionally, the paper does not discuss potential limitations of quantum chemistry software (e.g., accuracy, applicability to diverse molecules) or how CHEMGUIDE handles noisy/incorrect oracle feedback."
          },
          "questions": {
            "value": "1. How does the paper address the computational expense of quantum chemistry calculations during diffusion sampling? 2. What is the exact implementation of the zeroth-order optimization method, and how is it adapted for high-dimensional molecular geometries? 3. Are there ablation studies to validate the contribution of each component (e.g., gradient estimation, bilevel optimization)? 4. How does CHEMGUIDE handle cases where quantum chemistry software provides inaccurate or inconsistent gradients? 5. What are the specific metrics used to quantify 'improved stability' and 'validity' of generated molecules?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "4es2oO9tw1": {
    "paper_id": "4es2oO9tw1",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper addresses compute-constrained fine-tuning of large language models (LLMs) by formalizing data selection as a cost-aware optimization problem. It explores trade-offs between data selection methods, model sizes, and training compute, finding that simpler approaches like sparse retrieval outperform complex methods (e.g., perplexity/gradient-based) in compute-optimal scenarios."
          },
          "strengths": {
            "value": "The paper's strengths include a rigorous experimental design with 600+ model training runs across diverse settings, clear formalization of a novel cost-aware utility function, and practical insights for resource-constrained fine-tuning. The empirical analysis of compute scalability and theoretical modeling of selection-to-training ratios demonstrate significant methodological depth. The work also highlights an important gap in prior literature by explicitly accounting for data selection's own computational costs."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of specific variables (e.g., hyperparameters, data characteristics). The theoretical analysis of FLOP inefficiency for complex methods is underdeveloped, with limited mathematical justification. The extrapolation of parametric models for compute-optimal ratios is not thoroughly validated across all scenarios. Additionally, the paper doesn't address potential biases in the selected datasets or the generalizability of findings to non-instruction-tuning settings."
          },
          "questions": {
            "value": [
              "How were the parametric models for compute-optimal ratios validated? Were they tested on out-of-distribution scenarios or only the same dataset/task configurations?",
              "What specific metrics were used to quantify FLOP efficiency for different data selection methods? Were actual training times or theoretical FLOP counts used?",
              "How were hyperparameters for the 600+ experiments standardized? Were there systematic comparisons of hyperparameter sensitivity across methods?",
              "The paper claims 'sparse retrieval' dominates, but what defines 'sparse retrieval' in this context? How does it differ from other non-model-based methods in the experiments?",
              "Are the findings applicable to scenarios beyond instruction-tuning? How might the results generalize to other downstream tasks or data modalities?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates the trade-off between data selection costs and training gains in compute-constrained LLM finetuning. The authors formalize a cost-aware utility function, conduct extensive experiments across 600+ models with varying sizes and data selection methods, and find that simpler statistical approaches often outperform complex model-based methods in compute-optimal scenarios."
          },
          "strengths": {
            "value": "The paper addresses a critical practical problem in LLM finetuning with clear theoretical motivation. The experimental scope is impressive, covering 6 data selection methods, 3 tasks, and 7-70B parameter models. The empirical findings provide actionable insights for practitioners. The paper's framework for analyzing compute trade-offs is novel and well-structured."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons with recent state-of-the-art data selection methods (e.g., those using contrastive learning or active learning). The theoretical analysis of compute efficiency is superficial, with limited mathematical justification for the 5x/10x model size ratios. The experimental design doesn't account for varying hardware architectures or distributed training efficiencies. The claim about 'Pareto-optimality' lacks formal definitions and validation."
          },
          "questions": {
            "value": "1. Why were recent methods like contrastive data selection or active learning not included in the comparison? 2. How were the 5x/10x model size ratios derived - through theoretical analysis or empirical curve-fitting? 3. What specific hardware constraints were considered in the compute budget calculations? 4. How do the authors address potential selection bias in the datasets used for evaluation? 5. Can the framework be extended to multi-task learning scenarios?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper addresses compute-constrained fine-tuning of large language models (LLMs) by formalizing data selection as a cost-aware optimization problem. It experiments with 600 models across varying sizes, tasks, and data selection methods, concluding that simpler approaches like sparse retrieval outperform complex ones (e.g., perplexity/gradient-based) in compute efficiency. The work provides empirical guidelines for optimal training-to-selection model size ratios."
          },
          "strengths": {
            "value": "The paper introduces a novel cost-aware framework for data selection, which is critical for practical LLM fine-tuning under compute constraints. The comprehensive experimental setup (600 models, 6 methods, 3 tasks) and empirical validation of theoretical claims demonstrate high quality. The clarity of problem formulation and practical insights make the work significant for resource-constrained applications. The connection to real-world instruction-tuning scenarios adds relevance."
          },
          "weaknesses": {
            "value": "The paper lacks depth in theoretical analysis of why simpler methods outperform complex ones. The experimental design does not fully address how data selection effectiveness varies across tasks or domains. The proposed model size ratios (5x/10x) are not validated across diverse architectures or training regimes. The paper also does not discuss limitations of their empirical approach or potential biases in the selected datasets."
          },
          "questions": {
            "value": "1. How were the 600 models' configurations (e.g., training tokens, compute budgets) systematically varied? 2. What metrics were used to quantify 'compute-optimal' performance beyond perplexity? 3. Are the proposed model size ratios generalizable to non-instruction-tuning tasks? 4. How does the paper address cases where complex data selection methods might still be preferred despite higher compute costs? 5. What ablation studies were conducted to isolate the impact of data selection vs. other variables?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "4ftMNGeLsz": {
    "paper_id": "4ftMNGeLsz",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes FedGO, a federated ensemble distillation method that leverages principles from generative adversarial networks (GANs) to address data heterogeneity in federated learning. The key idea is to use client-side discriminators, trained on a generator distributed by the server, to assign optimal weights to client predictions for pseudo-labeling. The method theoretically justifies its optimality via GAN theory and demonstrates performance improvements over baselines on image classification tasks."
          },
          "strengths": {
            "value": "The paper introduces a novel approach by connecting GAN theory to federated ensemble distillation, which is a fresh perspective in the field. The theoretical analysis provides a clear justification for the weighting mechanism, and the experiments show consistent performance gains across multiple datasets. The work also addresses practical concerns like communication cost and privacy leakage, which adds to its significance. The clarity of the problem statement and the structured presentation of contributions are commendable."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the necessity of key components, such as the role of discriminators versus other weighting schemes. The theoretical analysis assumes idealized settings (e.g., perfect generators/discriminators) that may not hold in practice. The experiments, while comprehensive, do not thoroughly explore edge cases (e.g., extreme data heterogeneity or non-IID scenarios). Additionally, the communication and computational overhead analysis is superficial, with no quantitative trade-off analysis between performance and efficiency."
          },
          "questions": {
            "value": "1. How are the client discriminators trained in practice? What is the relationship between the server's generator and the client discriminators? 2. Are there limitations to the theoretical optimality guarantees under non-ideal conditions (e.g., noisy gradients or partial client participation)? 3. How does FedGO handle scenarios where the server's generator is not representative of the client data distributions? 4. What is the exact mechanism for balancing the trade-off between communication cost and model performance?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes FedGO, a federated ensemble distillation method that leverages GAN-based optimality to address data heterogeneity in federated learning. The method uses client discriminators trained on a generator from the server to assign optimal weights for pseudo-labels, improving model performance. The authors claim theoretical guarantees and demonstrate improvements over baselines on image classification tasks."
          },
          "strengths": {
            "value": "The paper introduces a novel weighting strategy for federated ensemble distillation, inspired by GAN theory, which addresses a critical challenge in pseudo-label quality under data heterogeneity. The experimental section includes multiple image datasets (CIFAR-10/100, ImageNet100) and covers scenarios with and without server-held datasets. The analysis of communication cost, privacy leakage, and computational burden adds practical relevance. The method's theoretical foundation, linking GANs to optimal model ensembles, shows creativity in combining domains."
          },
          "weaknesses": {
            "value": "The paper lacks detailed theoretical proofs for the claimed optimality, relying instead on high-level references to GAN literature. The experiments omit comparisons with key baselines like FedDF, Fed-ET, and FedDS, which are cited in the introduction. The ablation studies on the impact of client discriminators or generator quality are missing. The analysis of privacy leakage and communication cost is superficial, with no quantitative metrics provided. The system model section is incomplete (cut off mid-sentence), raising concerns about methodological rigor."
          },
          "questions": {
            "value": [
              "Please provide the full theoretical derivation showing how GAN-based results justify the optimality of the weighting method.",
              "Why are key baselines like FedDF, Fed-ET, and FedDS not included in the experiments, despite being cited in the introduction?",
              "What ablation studies were conducted to isolate the contribution of client discriminators versus the generator-based approach?",
              "How is privacy leakage quantified? What specific mechanisms (e.g., differential privacy) are used to mitigate it?",
              "The system model section is incomplete. Please clarify the federated learning setup, particularly how the generator is distributed and trained."
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes FedGO, a federated ensemble distillation method that leverages generative adversarial network (GAN) theory to address data heterogeneity in federated learning. The key idea is to use client-side discriminators trained on a server-distributed generator to assign optimal weights for ensemble distillation, theoretically justified by GAN principles. The method demonstrates improved performance over baselines in image classification tasks under various scenarios."
          },
          "strengths": {
            "value": "Originality: The integration of GAN theory into federated ensemble distillation is novel, particularly the use of client discriminators for weight optimization. Quality: The theoretical analysis connects GAN optimality conditions to ensemble weighting, and experiments are comprehensive across multiple datasets. Clarity: The paper is well-structured with clear problem formulation and method description. Significance: Addressing data heterogeneity in FL is critical, and the method's practical improvements in performance and convergence are impactful."
          },
          "weaknesses": {
            "value": "Theoretical justification relies on assumptions about the relationship between GAN optimality and ensemble weighting that require deeper justification. The experimental analysis of communication cost and privacy leakage is superficial, lacking quantitative comparisons with baselines. The paper does not fully address how client discriminators are trained in practice or how they interact with the server's generator. The claim of 'provably near-optimal' weights needs stronger empirical validation through ablation studies."
          },
          "questions": {
            "value": "1. How exactly does the GAN-based optimality condition translate to the weighting scheme? What guarantees does the theory provide about the convergence of the weighting process? 2. The paper mentions client discriminators trained on a server generator - what is the exact training procedure and how is it integrated with the ensemble distillation process? 3. The communication cost analysis is limited - could you provide specific metrics comparing FedGO's communication overhead to existing methods? 4. How does the method handle non-IID data distributions with extreme heterogeneity that might break the GAN assumptions?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "4fyg68nmd7": {
    "paper_id": "4fyg68nmd7",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates scaling laws for aligning artificial neural networks (ANNs) with the primate visual ventral stream (VVS) by training over 600 models across varied architectures and datasets. It identifies that while behavioral alignment improves with scaling, neural alignment saturates, and advocates for prioritizing data over model size in compute allocation."
          },
          "strengths": {
            "value": "The paper's systematic evaluation of 600 models across multiple benchmarks demonstrates rigorous empirical methodology. The use of standardized Brain-Score benchmarks ensures reproducibility and comparability. The contribution to understanding scaling limits in brain alignment is significant, and the open-source release of code and checkpoints enhances transparency. The findings about saturation in neural alignment and the proposed scaling recipe address critical gaps in neuro-inspired model design."
          },
          "weaknesses": {
            "value": "The paper lacks detailed statistical validation for the saturation of neural alignment (e.g., confidence intervals, p-values). The claim that data scaling outperforms model scaling is not sufficiently supported by ablation studies or quantitative analysis of efficiency gains. The definition of 'higher-level visual areas' and their measurement is vague. The scaling recipe is presented without explicit derivation from the experiments. The paper also does not address potential biases in the Brain-Score benchmarks or the generalizability of results to other brain regions."
          },
          "questions": {
            "value": [
              "How was the saturation of neural alignment statistically validated? Were there specific thresholds or significance tests applied?",
              "What metrics were used to quantify the efficiency gains of data vs. model scaling, and how were they derived?",
              "How were 'higher-level visual areas' operationalized in the analysis, and what evidence supports their differential response to scaling?",
              "What criteria were used to determine the optimal compute allocation in the scaling recipe, and how was it validated?",
              "Were the Brain-Score benchmarks tested for robustness to dataset biases or confounding variables?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates scaling laws for aligning artificial neural networks (ANNs) with the primate visual ventral stream (VVS) by systematically evaluating over 600 models across architectures and datasets. It finds that behavioral alignment improves with scale, while neural alignment saturates, and advocates for prioritizing data over model size in compute allocation. The work provides a scaling recipe, benchmarks, and open-source resources."
          },
          "strengths": {
            "value": "The paper introduces a novel systematic exploration of scaling laws for brain alignment, which is a critical gap in current research. The scale of experimentation (600+ models) and use of standardized benchmarks like Brain-Score add rigor. The practical contribution of releasing code, checkpoints, and evaluation pipelines enables reproducibility and future work. The findings on higher-level visual areas and compute efficiency offer actionable insights for model design. The work bridges ML scaling trends with neuroscience, highlighting interdisciplinary value."
          },
          "weaknesses": {
            "value": "The paper lacks depth in explaining why neural alignment saturates despite scaling, which is central to its claims. The comparison to prior work on scaling laws (e.g., power-law exponents) is superficial, and the paper does not address potential confounding factors (e.g., architectural differences, training procedures). The proposed scaling recipe is descriptive rather than prescriptive, with limited validation. The experimental setup could clarify how subsampled datasets were selected and whether results generalize across architectures."
          },
          "questions": {
            "value": "1. How do the authors explain the saturation of neural alignment? Could this reflect inherent limitations of current architectures or dataset quality? 2. What specific inductive biases (e.g., convolutional vs. transformer structures) contribute to compute efficiency, and how were they quantified? 3. Are the Brain-Score benchmarks robust to different data splits, and how were hyperparameters chosen for linear regression/feature extraction? 4. How does the proposed scaling recipe perform when applied to non-convolutional architectures (e.g., transformers)? 5. What are the implications of these findings for future brain-like model design, beyond compute allocation?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates scaling laws for artificial neural networks (ANNs) in modeling the primate visual ventral stream (VVS). By training over 600 models across controlled conditions, the authors find that behavioral alignment with primate core object recognition (COR) improves with scaling, while neural alignment with brain regions like V1, V2, V4, and IT saturates. They propose a scaling recipe prioritizing data over model size and highlight the need for novel strategies beyond scaling for brain-like models."
          },
          "strengths": {
            "value": "The paper's systematic evaluation of 600+ models across diverse architectures and datasets demonstrates rigorous empirical methodology. It leverages established benchmarks (e.g., Brain-Score) for neural and behavioral alignment, ensuring reproducibility and relevance. The open-sourcing of code and checkpoints enhances transparency. The findings address a critical gap in understanding scaling limits for brain modeling, with clear implications for future research. The paper also provides actionable insights into compute allocation strategies."
          },
          "weaknesses": {
            "value": "The analysis of why neural alignment saturates lacks depth, relying on empirical observations without mechanistic explanations. The paper does not fully address potential confounding factors (e.g., training duration, optimization techniques) that could influence scaling trends. The experimental scope is limited to specific brain regions (V1-V4/IT) and object classification tasks, raising questions about generalizability. The theoretical framework for scaling laws is not sufficiently elaborated, and the paper's cut-off nature prevents full assessment of methodology and results."
          },
          "questions": {
            "value": [
              "What specific architectures and datasets were included in the 600+ models, and how were they selected to ensure diversity?",
              "How were the 'compute-efficient' models (e.g., those with stronger inductive bias) defined and validated?",
              "What metrics were used to quantify 'neural alignment saturation,' and how were they normalized across brain regions?",
              "How were hyperparameters (e.g., learning rates, training epochs) controlled across models with varying scales?",
              "Could the observed saturation in neural alignment be attributed to limitations in current datasets or model architectures, and how might this be addressed?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "4g0PUEAHg0": {
    "paper_id": "4g0PUEAHg0",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates the theoretical capability of transformers to autoregressively learn Bayesian networks in-context. The authors prove the existence of a transformer model that can estimate conditional probabilities from a context of Bayesian network samples and generate new samples based on these estimates. They also present empirical results demonstrating that such a model can be trained effectively."
          },
          "strengths": {
            "value": "The paper makes a novel contribution by extending prior work on in-context learning to Bayesian networks, a domain not previously explored. The theoretical analysis is clean and provides a clear framework for understanding transformer capabilities. The empirical section includes experiments across different Bayesian network structures (chains, trees, general graphs), showing robustness. The paper also contextualizes its work well within the broader literature on transformers and in-context learning."
          },
          "weaknesses": {
            "value": "The theoretical proofs lack concrete details about the transformer architecture, such as the number of layers, attention mechanisms, or training procedures. The empirical experiments are described but lack specific implementation details (e.g., hyperparameters, baseline comparisons, ablation studies). The paper does not address how the transformer generalizes to Bayesian networks with higher in-degrees or more complex structures. Additionally, the claim that the transformer 'can be effectively obtained through training' is not supported by sufficient experimental evidence."
          },
          "questions": {
            "value": "1. What specific transformer architecture was used in the experiments, and how was it trained (e.g., loss function, optimization details)? 2. How does the model handle Bayesian networks with variables having more than one parent, which the prior work by Huang et al. limited to single-parent cases? 3. Are there baseline comparisons to other models or methods for Bayesian network learning? 4. What ablation studies were conducted to verify the importance of key components (e.g., attention mechanisms, autoregressive generation)? 5. How does the model's performance scale with the size of the Bayesian network or the number of context samples?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates the theoretical and empirical capability of transformers to learn Bayesian networks in-context. The authors claim that a transformer can estimate conditional probabilities of a Bayesian network from observed samples and autoregressively generate new samples. They provide theoretical proofs and empirical validation across different network structures."
          },
          "strengths": {
            "value": "The paper offers clear theoretical analysis of transformers' ability to approximate Bayesian network learning, which is a novel contribution to understanding in-context learning. The empirical section demonstrates results across diverse Bayesian network structures (chain, tree, general graph), showing practical feasibility. The work contextualizes itself well within existing literature on in-context learning and transformer capabilities."
          },
          "weaknesses": {
            "value": "The theoretical analysis is incomplete, as the paper cuts off mid-proof and lacks detailed mathematical rigor. The empirical section lacks critical baselines (e.g., traditional Bayesian network learning methods) and quantitative metrics to validate the transformer's performance. The paper does not address how the transformer's architecture is specifically tailored for this task, nor does it discuss limitations in handling complex Bayesian networks with multiple parents per variable. The claims about 'robust empirical studies' are under-specified."
          },
          "questions": {
            "value": "1. How does the transformer's architecture differ from standard models to enable Bayesian network learning? 2. What specific training procedures or hyperparameters were used to achieve the empirical results? 3. How are variables with multiple parents handled in the theoretical framework? 4. What baseline methods were compared against, and what quantitative metrics were used to evaluate performance? 5. Are there ablation studies to validate the importance of key components (e.g., attention mechanisms, autoregressive generation)?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates the theoretical and empirical capability of transformers to autoregressively learn Bayesian networks in-context. The authors prove the existence of a transformer model that can estimate conditional probabilities from context samples and generate new samples based on these estimates. They further validate this through experiments on Bayesian networks with different structures (chains, trees, general graphs)."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by bridging transformers with Bayesian network learning, an underexplored area. The theoretical analysis is clean and intuitive, providing clear proofs of the transformer's capabilities. The empirical validation is robust, covering diverse Bayesian network structures. The work addresses a significant gap in understanding transformers' in-context learning abilities, contributing to both theoretical and practical knowledge."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons with alternative methods for Bayesian network learning, which would strengthen the empirical analysis. The theoretical model assumes a specific structure for Bayesian networks (e.g., ordered parents), which may limit generalizability. The experimental results could benefit from more detailed quantitative metrics (e.g., KL divergence, log-likelihood) to assess the quality of estimated distributions. The transformer architecture details (e.g., number of layers, attention mechanisms) are not explicitly described, leaving room for ambiguity."
          },
          "questions": {
            "value": [
              "How does the proposed method handle Bayesian networks with high in-degree nodes, where the parent set size exceeds the model's capacity to process?",
              "What specific transformer architecture (e.g., number of layers, attention heads) was used in the experiments, and how was it trained (e.g., hyperparameters, loss function)?",
              "Are the theoretical guarantees dependent on the assumption that variables are ordered with parents having smaller indices? How would the results change for arbitrary graph structures?",
              "Could the authors provide ablation studies to isolate the contribution of different components (e.g., attention vs. feedforward layers) in the transformer's performance?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "4hFT4rfG40": {
    "paper_id": "4hFT4rfG40",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a plug-and-play framework for controllable generation using discrete masked models, enabling efficient sampling from controlled distributions without task-specific fine-tuning. The method leverages mean-field approximation and iterative masking-unmasking to generate samples that satisfy constraints or optimize reward functions, demonstrated across domains like protein design and sequence generation."
          },
          "strengths": {
            "value": "The paper presents a novel framework for controllable generation in discrete masked models, addressing a critical gap in the literature. The approach is theoretically grounded in importance sampling and mean-field approximation, offering a generalizable solution that avoids training. The experiments highlight versatility across tasks, and the method's efficiency (e.g., 10 model queries) is promising. The writing is clear, with well-defined notation and structured problem formulation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental comparisons with existing discrete controllable generation methods, making it hard to assess relative performance. The theoretical analysis is limited—no convergence guarantees or bounds on approximation errors. The unmasking schedule and Monte Carlo sampling strategy are not thoroughly justified. Additionally, the paper is cut off mid-section, leaving key details (e.g., algorithm pseudocode, scalability analysis) unresolved."
          },
          "questions": {
            "value": "1. How does the proposed method compare to task-specific fine-tuning approaches in terms of sample quality and computational cost? 2. What are the theoretical guarantees for the mean-field approximation's accuracy in high-dimensional discrete spaces? 3. How is the unmasking schedule (γ) determined, and what impact does it have on performance? 4. Are there limitations to the types of constraints/rewards the framework can handle (e.g., non-continuous or combinatorial constraints)? 5. What is the scalability of the method for very long sequences (e.g., D > 1000)?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a plug-and-play framework for controllable generation using discrete masked models, enabling efficient sampling from constrained or reward-optimized distributions without task-specific fine-tuning. The method leverages iterative unmasking, mean-field approximation, and importance sampling to generate samples while maintaining computational efficiency."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in controllable generation for discrete models, offering a novel, fine-tuning-free approach with broad applicability. The methodology is theoretically grounded, with clear steps for iterative unmasking and Monte Carlo sampling. The experiments demonstrate versatility across domains like protein design, and the framework's efficiency (10 model queries and 1000 samples) is promising. The problem formulation and notation are well-structured, enhancing clarity."
          },
          "weaknesses": {
            "value": "The theoretical analysis of the mean-field approximation's convergence and error bounds is insufficient. The experiments lack detailed comparisons with existing methods (e.g., task-specific fine-tuning or continuous-domain approaches) and metrics for evaluation (e.g., FID, diversity scores). The computational efficiency claims are not validated with runtime benchmarks. The paper also does not discuss limitations of the reward function assumptions or scalability to longer sequences."
          },
          "questions": {
            "value": "1. What guarantees exist for the convergence of the iterative unmasking process under the mean-field approximation? 2. How does the framework handle non-differentiable reward functions in practice, given the importance sampling step? 3. Are there specific constraints or reward functions where the method fails, and why? 4. How do the claimed efficiency metrics (10 queries, 1000 samples) scale with sequence length or complexity? 5. What baseline methods were compared against in the protein design experiments, and what metrics were used?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a plug-and-play framework for controllable generation using discrete masked models, enabling efficient sampling from constrained or reward-optimized distributions without task-specific fine-tuning. The method leverages importance sampling and mean-field approximations for iterative unmasking/remasking, demonstrated on tasks like protein design and constrained sequence generation."
          },
          "strengths": {
            "value": "Originality is strong, addressing a critical gap in discrete masked models compared to continuous-domain plug-and-play methods. The framework's independence from gradient information and adaptability to non-differentiable rewards is novel. The methodology is theoretically grounded, with clear motivation for efficiency. The experiments demonstrate versatility across domains, though more detailed comparisons are needed."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous comparison with existing learning-based approaches for discrete controllable generation, making it hard to assess relative efficacy. The mean-field approximation's limitations (e.g., independence assumptions) are not thoroughly analyzed. Computational efficiency claims are supported by anecdotal evidence (e.g., '10 queries'), but no quantitative benchmarks against alternatives. Theoretical guarantees for convergence or error bounds are absent."
          },
          "questions": {
            "value": "How does the method handle high-dimensional constraints (e.g., global properties in protein design)? What are the exact computational costs compared to baselines? Are there cases where the mean-field approximation fails? How is the reward function integrated into the importance sampling framework? Can the approach handle dynamic constraints during generation?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "4hp2bVdaHU": {
    "paper_id": "4hp2bVdaHU",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces YES training bounds, a framework for real-time, data-aware certification and monitoring of deep neural network training. The approach evaluates data utilization efficiency and optimization dynamics, offering deterministic guarantees without randomization. It includes a cloud-based system with color-coded training status indicators and claims to provide insights beyond traditional local optimization perspectives."
          },
          "strengths": {
            "value": "The paper presents a novel framework (YES bounds) with clear practical applications, such as real-time monitoring via a cloud-based system. The deterministic certification approach is a significant advantage over randomized methods. The work addresses the critical need for reliable training quality assurance in high-stakes AI applications, and the theoretical foundation for single-layer networks (Section 3) suggests potential for broader generalization. The paper also contextualizes its contributions within the growing AI safety literature."
          },
          "weaknesses": {
            "value": "The paper lacks critical details about the theoretical derivation of YES bounds, particularly how they extend from single-layer to deep networks. The experimental validation is incomplete (the paper is cut off), so it is unclear whether the claims about outperforming local optimization perspectives are substantiated. The cloud-based system's implementation details, scalability, and integration with existing training pipelines are not discussed. Additionally, the paper does not compare YES bounds to existing monitoring tools or provide ablation studies to isolate their effectiveness."
          },
          "questions": {
            "value": "1. How are the YES bounds mathematically derived, and what assumptions underlie their validity? 2. What specific mechanisms enable the bounds to detect suboptimal training plateaus beyond local optimization? 3. How does the cloud-based system handle large-scale or distributed training scenarios? 4. Are the bounds applicable to non-convex or highly non-linear architectures? 5. What baselines were used to validate the claims about deterministic certification and improved monitoring? 6. How is 'data-awareness' operationalized in the bounds—what aspects of the dataset are considered?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces YES training bounds, a framework for real-time, data-aware certification and monitoring of neural network training. These bounds evaluate data utilization efficiency and optimization dynamics, offering insights beyond traditional local optimization perspectives. The authors propose a cloud-based system with color-coded visualizations to assess training quality, validated on synthetic and real-world tasks like image denoising."
          },
          "strengths": {
            "value": "Originality is evident in the data-aware, deterministic approach to training monitoring, which contrasts with existing statistical methods. The practical cloud-based visualization system addresses a real-world need for real-time training evaluation. The paper highlights the importance of global optimization perspectives and determinism for standardization, which could influence future training practices. The significance lies in improving trust and reliability in deep learning systems, aligning with growing AI safety concerns."
          },
          "weaknesses": {
            "value": "The paper lacks detailed mathematical formulations of the YES bounds, making it difficult to assess their theoretical foundation. Experimental validation is superficial, with no specific metrics, baselines, or datasets provided. The claims about surpassing local optimization perspectives are not substantiated with concrete examples. The connection between the theoretical bounds and the cloud system's implementation is unclear. Additionally, the paper is cut off mid-section, preventing full evaluation of its contributions."
          },
          "questions": {
            "value": "1. What is the mathematical definition of the YES bounds, and how are they derived? 2. How are the bounds validated experimentally, and what metrics are used to quantify their effectiveness? 3. What specific datasets and baselines were used in the experiments? 4. How do the YES bounds differ from existing methods like gradient-based analysis or loss landscape studies? 5. What are the computational costs and scalability of the proposed framework?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces YES training bounds, a novel framework for real-time, data-aware certification and monitoring of deep learning training. These bounds aim to assess training progress, detect suboptimal behavior, and provide deterministic guarantees without relying on randomization. The approach is integrated into a cloud-based system with color-coded training status indicators, targeting reliability and safety in critical AI applications."
          },
          "strengths": {
            "value": "Originality: The YES bounds framework addresses a novel problem of real-time, data-aware training certification, which is distinct from traditional optimization analyses. The deterministic nature of the bounds and their focus on global training insights over local optimization perspectives are promising contributions. Quality: The paper references prior work on neural network capacity and optimization, and the proposed methodology aligns with established deep learning principles. Clarity: The abstract and introduction are well-structured, clearly stating the problem, approach, and potential impact. Significance: The focus on AI safety, reliability, and standardization resonates with current research priorities, particularly in high-stakes applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed mathematical formulations of the YES bounds, making it difficult to assess their theoretical foundation. Experimental validation is superficial, with no specifics on datasets, baselines, or quantitative results to support claims of superior performance over existing methods. The cloud-based monitoring system's design and implementation details are absent, limiting reproducibility. The deterministic certification claim requires stronger evidence, as the paper does not address potential edge cases or limitations. The connection between the bounds and practical training adjustments is not clearly explained."
          },
          "questions": {
            "value": "1. What is the mathematical definition of the YES bounds, and how are they derived? 2. Which specific datasets and baselines were used in the experiments, and what metrics were evaluated? 3. How do the YES bounds compare to existing methods like loss landscape analysis or Bayesian optimization? 4. What are the computational costs of the cloud-based system, and how is it integrated into training pipelines? 5. How are the color-coded thresholds (red/yellow/green) determined, and what guarantees do they provide?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "4mqt6QxSUO": {
    "paper_id": "4mqt6QxSUO",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes a novel framework for automated SARS-CoV-2 detection from CT scans, combining submodular optimization, Riemannian geometry-inspired attention mechanisms, and adversarial domain adaptation. The approach integrates theoretical concepts from statistical learning, optimal transport, and information geometry, with claims of convergence guarantees and improved performance over existing methods."
          },
          "strengths": {
            "value": "The paper demonstrates high originality by synthesizing advanced mathematical theories (submodularity, Riemannian geometry, optimal transport) into a unified framework for medical imaging. The theoretical analysis is rigorous, with proofs of submodularity and DR-submodularity, and the methodology addresses critical challenges like domain shift through innovative use of Wasserstein-Fisher-Rao distance. The work has significant potential impact in medical diagnostics and contributes to foundational ML theory."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about experimental setup, implementation of the Riemannian attention mechanism, and empirical validation incomplete. The theoretical claims about generalization bounds and convergence require deeper scrutiny without full proofs. The connection between the proposed framework and practical medical imaging challenges (e.g., computational efficiency, interpretability) is underdeveloped. The comparison with existing methods lacks specific metrics and baseline details."
          },
          "questions": {
            "value": "1. How is the Riemannian attention mechanism implemented computationally, and what are its training dynamics? 2. What specific datasets and metrics were used to evaluate performance, and how do they compare to state-of-the-art methods? 3. How does the framework handle class imbalance in CT scan data? 4. What are the limitations of the theoretical analysis in practical scenarios? 5. Can the adversarial domain adaptation technique be validated on real-world datasets with varying imaging protocols?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes a novel framework for SARS-CoV-2 detection from CT scans, combining submodular optimization, Riemannian geometry, and adversarial domain adaptation. It integrates concepts from statistical learning theory, optimal transport, and information geometry to address challenges in image selection, feature extraction, and domain shift."
          },
          "strengths": {
            "value": "The paper demonstrates strong theoretical foundations, including rigorous proofs of submodularity and DR-submodularity, and novel applications of Riemannian geometry and optimal transport in medical imaging. The integration of multiple advanced techniques into a unified framework is promising, and the focus on domain adaptation addresses a critical challenge in medical AI. The methodology is mathematically structured and well-motivated."
          },
          "weaknesses": {
            "value": "The paper lacks concrete empirical results, such as performance metrics, comparison with state-of-the-art models, or analysis of computational efficiency. The theoretical claims (e.g., generalization bounds) are not validated with experiments. The practical implementation details of the Riemannian attention mechanism and adversarial domain adaptation are unclear. The scope is narrowly focused on SARS-CoV-2, with limited discussion of broader applicability."
          },
          "questions": {
            "value": "1. What specific datasets were used for evaluation, and how do they compare to existing benchmarks? 2. How does the proposed method compare to established deep learning approaches (e.g., 3D CNNs) in terms of accuracy and robustness? 3. Are the theoretical guarantees (e.g., convergence, generalization) validated in practice? 4. What are the computational costs and scalability of the proposed framework? 5. How is the Riemannian attention mechanism implemented, and what are its key design choices?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "本文提出了一种基于黎曼几何、最优传输和信息几何的统一框架，用于从CT扫描中自动检测SARS-CoV-2。方法包括子模优化的图像选择、黎曼几何启发的注意力机制、基于Bregman散度的决策框架以及对抗域适应技术。理论分析涵盖收敛性保证和泛化边界，实验表明优于现有方法。"
          },
          "strengths": {
            "value": "原创性体现在将子模优化、黎曼几何和最优传输理论结合用于医学影像分析；理论部分提供了严格的数学证明（如子模函数性质）；结构清晰，方法描述完整；对医疗影像自动化诊断有实际意义，且理论贡献可能推动机器学习与最优传输的交叉研究。"
          },
          "weaknesses": {
            "value": "实验部分缺乏具体细节（如数据集规模、对比基线的实现细节、统计显著性检验）；定理3.1的证明被截断，可能影响理论严谨性；对抗域适应的Wasserstein-Fisher-Rao距离与Gromov-Wasserstein正则化的结合缺乏直观解释；未明确讨论方法的计算复杂度和可扩展性。"
          },
          "questions": {
            "value": [
              "子模优化中如何计算肺部覆盖函数L？是否考虑了CT图像的三维结构？",
              "定理3.1的证明被截断，能否补充完整？特别是DR-submodularity的证明是否严谨？",
              "实验中对比的基线方法具体有哪些？是否与最新方法（如3D CNN或Transformer）进行比较？",
              "对抗域适应中的Wasserstein-Fisher-Rao距离与Gromov-Wasserstein正则化如何协同工作？是否有理论依据？",
              "方法在不同CT设备或扫描参数下的泛化能力如何验证？"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "4sJ2FYE65U": {
    "paper_id": "4sJ2FYE65U",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes a graph-image multimodal fusion (GIMF) framework for neural multi-objective combinatorial optimization (MOCO). The approach integrates graph and image representations of problem instances through three components: instance image construction, problem-size adaptive resolution, and modality-specific bottlenecks. The method is evaluated on classic MOCO problems, showing improvements over existing neural MOCO methods."
          },
          "strengths": {
            "value": "The paper introduces a novel multimodal approach to MOCO, addressing a gap in existing methods that rely solely on graph representations. The problem-size adaptive resolution strategy and modality-specific bottlenecks are specific innovations with clear motivation. The experimental results demonstrate the framework's effectiveness, and the paper is well-structured with clear explanations of key components. The significance of improving MOCO methods for real-world applications is well justified."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the contribution of individual components (e.g., PSAR, MSB). The instance image construction method is not thoroughly explained, leaving unclear how spatial structures are encoded. The generalization claims are not supported by quantitative metrics on out-of-distribution problem sizes. Computational efficiency and scalability are not discussed, which is critical for practical deployment. The comparison with baselines is limited to two backbones, and the paper does not address potential limitations of the multimodal fusion approach."
          },
          "questions": {
            "value": [
              "How is the instance image constructed? What specific spatial features or transformations are used to map graph data to images?",
              "Are there ablation studies demonstrating the individual contributions of PSAR and MSB to the overall performance?",
              "What metrics are used to evaluate generalization across different problem sizes? How does GIMF compare to baselines on out-of-distribution instances?",
              "How does the computational complexity of GIMF compare to existing neural MOCO methods, particularly for large-scale problems?",
              "Are there cases where the multimodal fusion fails, and how does the framework handle conflicting information between graph and image modalities?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a graph-image multimodal fusion (GIMF) framework to enhance neural multi-objective combinatorial optimization (MOCO) methods. The approach integrates graph and image representations of problem instances through three components: instance image construction, problem-size adaptive resolution, and modality-specific bottlenecks. The framework is evaluated on classic MOCO problems, demonstrating improved performance and generalization compared to existing methods."
          },
          "strengths": {
            "value": "The paper presents a novel multimodal approach to MOCO, addressing a key limitation of prior methods that rely solely on graph representations. The three proposed components (instance image, PSAR strategy, MSB mechanism) show clear innovation in leveraging complementary modalities. The experimental results on classic MOCO problems validate the framework's effectiveness, and the integration with two state-of-the-art backbones highlights its versatility. The work advances the field by improving optimality and generalization in neural MOCO."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how the instance image is constructed and what specific features it captures, which limits understanding of its representational power. The PSAR strategy's implementation and theoretical justification are underdeveloped, making it unclear how it enables cross-size generalization. The multimodal fusion mechanism's design (e.g., bottlenecks) is not thoroughly explained, and ablation studies are missing to isolate the contributions of each component. Additionally, the experiments focus on classical problems without real-world validation."
          },
          "questions": {
            "value": [
              "How is the instance image generated? What specific spatial or structural features are encoded in the image modality?",
              "Can the authors provide more details on the PSAR strategy? How does it adapt resolution during image construction, and what metrics were used to evaluate its effectiveness?",
              "What is the role of modality-specific bottlenecks in the fusion mechanism? How were their hyperparameters determined?",
              "Are there ablation studies demonstrating the individual impact of the three components (instance image, PSAR, MSB) on performance?",
              "How does the framework handle computational overhead from multimodal fusion, especially for large-scale MOCO problems?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes a graph-image multimodal fusion (GIMF) framework to enhance neural multi-objective combinatorial optimization (MOCO) by integrating graph and image representations of problem instances. The framework includes three components: instance image construction, problem-size adaptive resolution (PSAR), and modality-specific bottlenecks (MSB) for fusion. The authors demonstrate its effectiveness on classic MOCO problems, claiming superior performance and generalization compared to existing methods."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to MOCO by combining graph and image modalities, addressing a gap in existing methods that rely solely on graph data. The three proposed components (instance images, PSAR, MSB) show creative problem-solving for multimodal representation learning. The experimental results on classic MOCO problems suggest practical significance, and the framework's versatility is validated by testing with two state-of-the-art backbones. The work aligns with recent trends in neural combinatorial optimization and offers a structured methodology for multimodal fusion."
          },
          "weaknesses": {
            "value": "The paper lacks detailed explanations of how instance images are constructed from graph data, which is critical for reproducibility. The experimental evaluation is limited in scope, with no comparison to non-neural MOCO methods or ablation studies on the proposed components. The claim of 'superior generalization' is not thoroughly supported, as the PSAR strategy's effectiveness across problem sizes remains unclear. Additionally, the paper is cut off mid-section, leaving key details about the framework's implementation and results incomplete."
          },
          "questions": {
            "value": [
              "How exactly are the instance images generated from the graph data? What transformation or encoding method is used?",
              "Which specific classic MOCO problems were tested, and how do they compare to existing benchmarks?",
              "What is the computational cost of the GIMF framework compared to baseline methods, and how does PSAR impact training efficiency?",
              "Are there any limitations to the types of MOCO problems the framework can handle (e.g., problem size, objective functions)?",
              "What ablation studies were conducted to validate the contribution of each component (instance images, PSAR, MSB)?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "4ub9gpx9xw": {
    "paper_id": "4ub9gpx9xw",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a method to measure the faithfulness of large language model (LLM) explanations by defining 'causal concept faithfulness' as the alignment between concepts implied by explanations and those empirically influencing model outputs. The approach uses an auxiliary LLM to generate counterfactual inputs and a Bayesian hierarchical model to quantify concept effects at example and dataset levels. Experiments on social bias and medical tasks reveal patterns of unfaithfulness, such as hidden biases and misleading evidence claims."
          },
          "strengths": {
            "value": "The paper offers a novel, rigorously defined metric for faithfulness grounded in causal inference, addressing a critical gap in AI interpretability. The method combines counterfactual generation and hierarchical modeling, enabling both quantitative scores and semantic pattern analysis. Experiments on real-world tasks demonstrate practical significance, uncovering new insights about LLM unfaithfulness. The work is well-motivated, with clear connections to safety and trustworthiness concerns in high-stakes domains."
          },
          "weaknesses": {
            "value": "The reliance on an auxiliary LLM for counterfactual generation introduces potential biases, as the quality of counterfactuals depends on the auxiliary model's capabilities. The Bayesian hierarchical model's complexity may limit reproducibility and interpretability. The experiments focus on specific datasets and LLMs, leaving questions about generalizability. The definition of 'concepts' remains somewhat abstract, with limited discussion of how they are identified or validated."
          },
          "questions": {
            "value": [
              "How is the auxiliary LLM's role in counterfactual generation validated? Are there ablation studies comparing its performance to alternative methods?",
              "What guarantees exist that the Bayesian hierarchical model accurately captures causal effects rather than spurious correlations?",
              "How are 'concepts' defined and extracted in practice? Are there criteria for determining which concepts are relevant to the task?",
              "Are the observed patterns of unfaithfulness unique to the tested LLMs, or do they generalize to other models?",
              "How does the method handle cases where multiple concepts interact in complex ways, potentially confounding the analysis?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a novel method to measure the faithfulness of large language model (LLM) explanations by defining 'causal concept faithfulness' as the alignment between concepts implied by explanations and those empirically influencing model outputs. The approach combines an auxiliary LLM for generating counterfactual inputs and a Bayesian hierarchical model to estimate causal effects at both example- and dataset-levels. Experiments on social bias and medical QA tasks reveal patterns of unfaithfulness, such as hidden gender bias and misleading evidence attribution."
          },
          "strengths": {
            "value": "The paper's originality lies in its rigorous causal inference-based definition of faithfulness, which addresses a critical gap in LLM interpretability. The method's ability to uncover semantic patterns of unfaithfulness (e.g., hidden safety measures in social bias tasks) demonstrates practical significance. The experiments are well-designed, with clear examples (e.g., Table 1) and validation on multiple LLMs. The work's emphasis on semantic patterns (item 3 in the introduction) adds value beyond existing quantitative metrics. The writing is structured and accessible, with a clear motivation for the problem."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough comparison with existing faithfulness metrics, making it unclear how the proposed method advances the state of the art. The auxiliary LLM's role is under-specified: its training methodology, capabilities, and limitations are not discussed. The Bayesian hierarchical model's assumptions (e.g., independence across questions) are not justified, and its robustness to noisy counterfactuals is unexplored. The experiments focus on specific datasets, and the generalizability of findings to other domains or LLMs is not addressed. The paper also does not propose solutions to mitigate the identified unfaithfulness, only tools to measure it."
          },
          "questions": {
            "value": [
              "How is the auxiliary LLM trained? Is it fine-tuned on the target task, or is it a pre-trained model? What metrics are used to evaluate its ability to generate realistic counterfactuals?",
              "What are the assumptions of the Bayesian hierarchical model? How are hyperparameters chosen, and how sensitive are results to these choices?",
              "Are there ablation studies showing the contribution of each component (counterfactual generation vs. Bayesian modeling) to the final faithfulness estimates?",
              "How does the method handle concept ambiguity or overlapping concepts in inputs? Are there cases where the auxiliary LLM misidentifies concepts?",
              "What is the computational cost of the method? How does it scale to large datasets or complex models?",
              "Are the identified 'semantic patterns of unfaithfulness' actionable for developers? How could they be used to improve model transparency?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a novel method to measure the faithfulness of large language model (LLM) explanations by comparing the concepts implied by explanations to those that are empirically influential. The approach uses an auxiliary LLM to generate counterfactual inputs and a Bayesian hierarchical model to quantify causal effects at both dataset and question levels. The method identifies semantic patterns of unfaithfulness, such as hidden social biases or misleading evidence claims, across tasks like social bias detection and medical QA."
          },
          "strengths": {
            "value": "The paper's originality lies in formalizing 'causal concept faithfulness' through causal inference and proposing a method that combines counterfactual generation with hierarchical modeling. The experiments on real-world tasks (e.g., social bias, medical QA) demonstrate practical significance, revealing new insights about LLM unfaithfulness. The clarity of the problem statement, examples (e.g., gender bias in hiring), and structured contributions are strong. The work addresses a critical gap in LLM safety and trustworthiness, with potential impact on both users and developers."
          },
          "weaknesses": {
            "value": "The method's reliance on an auxiliary LLM for counterfactual generation introduces potential biases, but the paper does not thoroughly analyze how variations in this LLM's performance affect results. The Bayesian hierarchical model's assumptions (e.g., priors, data structure) are not fully justified, and its robustness to dataset-specific challenges (e.g., class imbalance) is unclear. The experiments focus on specific tasks, limiting generalizability, and the comparison across the three LLMs (GPT-3.5, GPT-4o, Claude-3.5-Sonnet) lacks depth in explaining performance differences."
          },
          "questions": {
            "value": [
              "How sensitive are the faithfulness scores to the choice of auxiliary LLM for counterfactual generation? Could this introduce unintended biases?",
              "What statistical validation is provided for the Bayesian hierarchical model's assumptions (e.g., model convergence, prior sensitivity)?",
              "The paper mentions 'hiding the influence of safety measures' in social bias tasks—how is this quantified, and what defines a 'safety measure' in this context?",
              "Are the counterfactual inputs generated by the auxiliary LLM sufficiently realistic to capture true causal relationships, or do they risk introducing artifacts?",
              "How do the authors address potential confounding factors in the medical QA task (e.g., domain-specific terminology, ambiguous evidence)?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "4xBew7kuYB": {
    "paper_id": "4xBew7kuYB",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates whether the coherence of small language models (SLMs) trained on children's stories stems from high readability or other data properties. Through empirical analysis, the authors validate readability measures, generate synthetic datasets with varying complexity, and demonstrate that SLMs trained on complex language achieve comparable coherence to those trained on simple language. They also release datasets and show open-source LMs can evaluate readability effectively."
          },
          "strengths": {
            "value": "The paper addresses a critical question about SLM training data with rigorous methodology. It combines multiple readability measures (classic formulas, constituency parse statistics, and LM-based evaluations) and introduces LLM-as-a-Judge for quality assessment. The empirical analysis is thorough, and the release of synthetic datasets enhances reproducibility. The discussion of knowledge distillation as a potential confound adds depth, and the findings challenge assumptions about the necessity of 'developmentally inspired' data for SLM coherence."
          },
          "weaknesses": {
            "value": "The synthetic datasets with complex language are not sufficiently described, leaving unclear how complexity was quantified or controlled. The paper lacks a detailed analysis of how low diversity in TinyStories might still influence results. The comparison to human readability judgments is limited, and the paper does not fully address whether complex data could introduce new biases. The connection between readability and coherence is not thoroughly disentangled, as the experiments focus on generation quality rather than learning dynamics."
          },
          "questions": {
            "value": "1. How was the LlamaTales-GRE dataset generated, and what specific linguistic complexities were introduced compared to TinyStories? 2. Were the SLMs evaluated on tasks beyond coherence and perplexity (e.g., factual accuracy, logical reasoning)? 3. How were the open-source LM readability evaluations validated against gold-standard metrics? 4. Could the observed coherence in complex data be due to hidden redundancy or structural similarity to TinyStories?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates whether the coherence of small language models (SLMs) trained on children's stories (e.g., TinyStories) stems from the data's high readability or other properties. The authors validate readability measures, generate synthetic datasets with varying readability levels, and demonstrate that SLMs trained on complex language achieve similar coherence as those trained on simple language. They also release datasets and propose using open-source LMs for readability evaluation."
          },
          "strengths": {
            "value": "The paper addresses a critical question about the role of readability in SLM training, which is both timely and relevant. The empirical approach is rigorous, with multiple readability measures (including LM-based evaluations) and quality proxies (coherence and perplexity). The release of datasets and data-generation methodology enhances reproducibility. The use of 'LLM-as-a-Judge' for coherence evaluation is innovative. The work connects to broader themes like developmental language learning and knowledge distillation, showing strong contextual awareness."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about the synthetic dataset (LlamaTales-GRE) and experimental validation incomplete. The claims about 'complex language' SLMs achieving coherence lack concrete evidence, as the methodology for generating such datasets is not fully described. The analysis of readability vs. coherence correlation is preliminary, with limited discussion of potential confounding factors (e.g., dataset diversity, structural patterns). The comparison between high/low readability data is not sufficiently detailed to assess robustness."
          },
          "questions": {
            "value": "1. How was the LlamaTales-GRE dataset generated, and what specific metrics were used to control for readability? 2. What measures were taken to ensure the synthetic datasets differ meaningfully in complexity while controlling for other variables (e.g., diversity, structure)? 3. How were the 'coherence' and 'perplexity' metrics quantitatively validated against human judgments or other benchmarks? 4. Were there any statistical analyses to confirm the significance of the observed effects (e.g., p-values, effect sizes)? 5. How does the paper address potential biases in the LM-as-a-Judge evaluations?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates whether the coherence of small language models (SLMs) trained on children's stories (e.g., TinyStories) stems from the data's high readability or other properties. The authors validate readability measures, create synthetic datasets with varying readability, and demonstrate that SLMs trained on complex language achieve similar coherence as those trained on simple language. They also release datasets and show open-source LMs can evaluate readability effectively."
          },
          "strengths": {
            "value": "The paper addresses a relevant and underexplored question about the role of readability in SLM training, challenging assumptions in prior work. It employs a systematic approach, combining multiple readability measures (e.g., LM-based evaluations) and validating against human judgments. The clarity of the methodology and the release of datasets for reproducibility are strong points. The significance lies in its implications for efficient SLM training and the potential to replace human annotators with LMs for readability assessment."
          },
          "weaknesses": {
            "value": "The study focuses narrowly on readability as the primary variable, without thoroughly exploring other data properties (e.g., diversity, semantic structure, or syntactic complexity) that might influence SLM performance. The synthetic data generation process is not fully detailed, and the paper is cut off mid-section, leaving critical methodological details incomplete. Additionally, the experiments lack comparisons with non-synthetic or real-world datasets, limiting the generalizability of findings."
          },
          "questions": {
            "value": "How did the authors ensure the synthetic datasets (e.g., LlamaTales-GRE) maintain diversity while varying readability? What ablation studies were conducted to isolate the effect of readability from other data properties (e.g., vocabulary size, syntactic complexity)? Are there limitations in using LM-as-a-Judge for readability evaluation that the authors acknowledge? How do the authors address potential biases in their synthetic data generation pipeline?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "4y6Q98hJzr": {
    "paper_id": "4y6Q98hJzr",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper identifies a 'stability gap' phenomenon in domain continual pre-training of large language models (LLMs), where initial performance drops before recovery. The authors adapt the concept of stability gap from vision models to explain this behavior and propose three strategies—multi-epoch training on subsets, quality-aware data sampling, and distribution-matched data mixing—to mitigate the gap. They validate these methods on Llama-family models, achieving improved medical and legal task performance with reduced compute budgets."
          },
          "strengths": {
            "value": "Originality is strong through the adaptation of the stability gap concept from vision to NLP, offering a novel framework for understanding LLM behavior during continual pre-training. The methodology is rigorous, with experiments across multiple model sizes and domains. Clarity is excellent, with well-structured problem formulation and strategies. Significance is high, as efficient domain adaptation is critical for real-world LLM deployment, and the proposed techniques address computational constraints effectively."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how the stability gap mechanism specifically affects LLMs, relying heavily on analogies to vision models. Experimental validation is limited to Llama-family models, raising questions about generalizability. The comparison with existing continual pre-training methods is superficial, and the paper does not address potential trade-offs between the proposed strategies (e.g., data quality vs. diversity). The claim of 'no forgetting' in general tasks requires stronger evidence."
          },
          "questions": {
            "value": [
              "How does the stability gap in LLMs differ mechanistically from that in vision models? What evidence supports the analogy?",
              "Are the proposed strategies effective across non-medical/legal domains, or are they tailored to specific task distributions?",
              "What is the exact relationship between epoch count and stability gradient improvement? Is there an optimal number of epochs?",
              "How are 'high-quality tokens' defined and selected? Could this introduce bias toward specific data sources?",
              "What metrics were used to quantify 'no forgetting' in general tasks, and how do they compare to baseline methods?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces the concept of a 'stability gap' in large language models (LLMs) during domain continual pre-training, where initial performance drops before recovery. The authors propose strategies to mitigate this gap by adjusting training epochs, data quality, and distribution alignment, demonstrating improvements on medical and legal tasks with reduced computational budgets."
          },
          "strengths": {
            "value": "The paper's originality lies in applying the stability gap concept from vision models to NLP, offering a novel framework for understanding LLM behavior during continual pre-training. The experiments are comprehensive, with specific quantitative results (e.g., 4.5% improvement in medical tasks). The practical strategies (multi-epoch training, quality data selection, distribution alignment) are well-motivated and validated. The significance is high, as domain adaptation is critical for real-world LLM deployment."
          },
          "weaknesses": {
            "value": "The theoretical analysis of the stability gap remains superficial, with limited evidence linking the V-shaped performance curve to stability/plasticity gradients. The experiments focus on Llama-family models, raising questions about generalizability. Key claims (e.g., 'no forgetting' in general tasks) lack detailed ablation studies or comparison with baseline methods like learning rate re-warming. The paper also fails to address how the stability gap interacts with other factors like model size or data quality."
          },
          "questions": {
            "value": "1. How was the stability gap quantified? Was it measured through weight update analysis or task-specific metrics? 2. What is the exact mechanism linking the V-shaped curve to stability/plasticity gradients? 3. How do the proposed strategies compare to existing methods (e.g., knowledge distillation or replay)? 4. Are the improvements robust across different domain sizes or data distributions? 5. What metrics were used to assess 'no forgetting' in general tasks?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper identifies a 'stability gap' phenomenon in domain continual pre-training of LLMs, where initial performance drops before recovery. It proposes strategies to mitigate this by adjusting training epochs, data quality, and distribution shifts, demonstrating improvements on medical and legal tasks with reduced compute budgets."
          },
          "strengths": {
            "value": "Originality is strong by applying the stability gap concept from vision to NLP, offering a novel framework for understanding LLM behavior during continual pre-training. The experiments show practical improvements on medical tasks, and the focus on compute efficiency aligns with real-world constraints. The paper highlights a significant problem in domain adaptation, with potential implications for large-scale model training."
          },
          "weaknesses": {
            "value": "The paper lacks detailed empirical validation of the stability gap mechanism (e.g., no ablation studies on weight updates or quantitative measures of stability/plasticity gradients). The proposed strategies are vague (e.g., 'proper subset size' and 'high-quality tokens' are not defined). Comparisons to prior work (e.g., learning rate re-warming) are superficial. The claim of outperforming GPT-4 lacks rigorous benchmarking. The related work section is incomplete, missing key references."
          },
          "questions": {
            "value": "How was the stability gap quantified? What metrics were used to define 'high-quality tokens' or 'proper subset size'? How do the proposed strategies compare to existing methods like knowledge distillation or replay? Are the results reproducible with alternative datasets or model architectures? What is the exact definition of 'distribution shift' in this context?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "4ytRL3HJrq": {
    "paper_id": "4ytRL3HJrq",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Nova, a generative language model for assembly code that addresses challenges like low information density and compiler optimization diversity through a hierarchical attention mechanism and contrastive learning objectives. Nova achieves state-of-the-art results on binary decompilation and similarity detection tasks."
          },
          "strengths": {
            "value": "The paper presents a novel hierarchical attention mechanism tailored for assembly code, which addresses the unique challenges of low information density and long-sequence dependencies. The contrastive learning objectives for optimization-aware training are innovative. The experimental results demonstrate significant improvements over existing methods, and the work highlights the importance of assembly code analysis in security applications. The paper is well-structured and clearly contextualizes its contributions within related work."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the effectiveness of individual components (e.g., hierarchical attention vs. standard attention). The experimental setup is not fully described, including the assembly corpus curation process, evaluation metrics, and comparison against strong baselines. The claims about performance improvements (e.g., 14.84–21.58% higher Pass@1) require statistical significance analysis. The contrastive learning objectives are not sufficiently explained, making it difficult to assess their novelty and implementation."
          },
          "questions": {
            "value": [
              "What specific baselines were used for comparison, and how do they relate to prior work like CodeArt or SLaDe?",
              "How was the assembly corpus curated, and what are its characteristics (e.g., size, compiler optimizations, architectures)?",
              "Are the reported performance improvements statistically significant, and what is the variance across multiple runs?",
              "Can the authors provide more details on the contrastive learning objectives (e.g., negative sampling strategies, loss functions)?",
              "How does the hierarchical attention mechanism compare to existing approaches like LongCoder's window attention on assembly code?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes Nova, a generative language model for assembly code that addresses challenges like low information density and compiler optimization diversity through hierarchical attention mechanisms and contrastive learning objectives. Nova demonstrates improvements in binary decompilation and similarity detection tasks compared to existing methods."
          },
          "strengths": {
            "value": "The paper introduces a novel hierarchical attention mechanism with three levels of granularity, which is a significant contribution to handling assembly code's unique characteristics. The contrastive learning objectives for encoding assembly semantics are well-motivated. The experimental results show measurable improvements in key metrics (Pass@1, Recall@1), and the release of pre-trained models enhances reproducibility. The work addresses an important problem in binary analysis with clear practical relevance."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the effectiveness of individual components (e.g., hierarchical attention vs. contrastive learning). The experimental comparisons are limited to a few baselines (CodeArt, SLaDe, LLM4Decompile) without comprehensive evaluation against state-of-the-art methods. The paper does not analyze the computational efficiency or scalability of Nova. The design rationale for the three attention levels is under-explained, and the contrastive learning objectives lack technical depth. The truncated section on attention mechanisms suggests incomplete coverage of related work."
          },
          "questions": {
            "value": "1. How were the three hierarchical attention levels (intra-instruction, preceding-instruction, inter-instruction) determined? What ablation studies were conducted to validate their necessity? 2. The paper mentions 'functionality contrastive learning' but provides no details on its implementation or how it differs from standard contrastive learning. 3. How does Nova handle assembly code from different compilers beyond the 00/01 optimization flags shown in Figure 1? 4. What is the size and composition of the training corpus for Nova? 5. Are there any limitations in the evaluation datasets that could affect generalizability?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Nova, a generative language model tailored for assembly code analysis. The key innovations include a hierarchical self-attention mechanism to capture semantics across different instruction granularities and contrastive learning objectives to handle assembly's diverse optimizations. Nova achieves state-of-the-art results on binary decompilation and similarity detection tasks."
          },
          "strengths": {
            "value": "Originality is evident in the hierarchical attention design, which addresses assembly code's low information density and long-sequence challenges through multi-level attention mechanisms. The contrastive learning objectives for optimization-aware training are novel. The experiments are well-structured, with clear metrics (Pass@1, Recall@1) demonstrating significant improvements over existing methods. The paper effectively contextualizes its contributions within the security domain's critical needs. The release of pre-trained models enhances reproducibility and practical impact."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the necessity of each component in the hierarchical attention mechanism. The contrastive learning objectives are described conceptually but not thoroughly explained in terms of implementation details (e.g., negative sample construction, loss weighting). Comparisons are limited to a few baseline models (e.g., CodeArt, SLaDe), and it's unclear how Nova performs against more recent assembly-specific models. The experiments focus on specific optimization flags (00/01) but don't evaluate diverse compiler optimizations or edge cases."
          },
          "questions": {
            "value": "1. How were the hierarchical attention layers trained? Were they jointly optimized with the contrastive learning objectives? 2. What specific assembly features does the functionality contrastive learning target (e.g., control flow, data dependencies)? 3. Are there quantitative results showing Nova's robustness across different compiler optimizations beyond the 00/01 flags? 4. How does Nova's performance scale with varying assembly code lengths compared to baseline models? 5. What is the computational cost of the hierarchical attention mechanism compared to standard self-attention?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "54XlM8Clkg": {
    "paper_id": "54XlM8Clkg",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces 'point cluster' as a compact message unit for communication-efficient collaborative perception, proposing a framework (CPPC) that decouples structure and semantic information to improve bandwidth efficiency and perception accuracy. It addresses challenges in collaborative perception by leveraging sparse representations and novel aggregation mechanisms."
          },
          "strengths": {
            "value": "Originality: The point cluster concept offers a novel approach to message unit design, combining sparse structure and semantic information. Quality: The framework includes well-defined modules (packing and aggregation) with clear motivations. Clarity: The paper is structured logically, with detailed explanations of the problem and methodology. Significance: The method achieves state-of-the-art results on benchmarks, demonstrating practical relevance for real-world applications like autonomous driving."
          },
          "weaknesses": {
            "value": "The experiments lack detailed ablation studies to isolate the contributions of individual components (e.g., graph optimization vs. speed estimation). The comparison with prior work is limited to three benchmarks; additional datasets or baselines would strengthen validity. The handling of noise and latency is mentioned but not thoroughly analyzed. The theoretical analysis of communication efficiency versus performance trade-offs is insufficient."
          },
          "questions": {
            "value": [
              "How does the point cluster packing module dynamically adjust the number of points under varying bandwidth constraints? Are there quantitative results on bandwidth reduction?",
              "What specific metrics were used to evaluate the robustness to pose errors and time latency? How do these compare to existing methods?",
              "The paper mentions 'parameter-free solutions' for noise adaptation—what are the exact mechanisms, and how do they generalize across different noise levels?",
              "Are there any limitations of the point cluster representation (e.g., in handling dynamic scenes or occlusions) that were not addressed in the experiments?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces Point Cluster, a novel compact message unit for communication-efficient collaborative perception, and proposes the CPPC framework. The approach decouples structure and semantic information in sparse point clusters to reduce bandwidth usage while maintaining perception accuracy. Experiments demonstrate state-of-the-art performance on collaborative perception benchmarks."
          },
          "strengths": {
            "value": "Originality: The paper presents a fresh perspective by introducing point clusters as a message unit that explicitly preserves structural information, addressing limitations in existing BEV map-based methods. Quality: The framework includes thoughtful designs for robustness against noise and latency. Clarity: The problem formulation and key contributions are well-articulated. Significance: The work addresses a critical challenge in collaborative perception with potential impact on real-world applications like autonomous driving."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the effectiveness of specific components (e.g., graph optimization, speed estimation). Experimental comparisons are limited to a few benchmarks, and the paper does not thoroughly analyze the trade-offs between communication cost and perception accuracy. The theoretical analysis of why point clusters outperform BEV maps is underdeveloped."
          },
          "questions": {
            "value": [
              "How does the point cluster framework handle dynamic environments with rapidly moving objects?",
              "What is the exact mechanism for the parameter-free solutions against pose errors and time latency?",
              "Are there any limitations in the current design of point clusters for specific scenarios (e.g., dense urban environments)?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces 'point cluster' as a compact message unit for communication-efficient collaborative perception, proposing a framework (CPPC) that decouples structural and semantic information to reduce bandwidth usage while maintaining performance. The method uses sparse point clusters for object representation, enabling efficient aggregation and robustness to noise, with experiments showing state-of-the-art results on benchmarks."
          },
          "strengths": {
            "value": "The paper presents a novel message unit (point cluster) that combines structural and semantic information, addressing the communication-perception trade-off. The framework's design for low-bandwidth operation and robustness to noise is innovative. The experiments demonstrate significant performance gains on benchmarks, and the introduction of new metrics for fine-grained analysis is valuable. The problem formulation and motivation are well-justified."
          },
          "weaknesses": {
            "value": "The paper lacks critical details about the implementation of point clusters (e.g., how clusters are generated, how semantic/structural information is separated). The experimental section is incomplete, with no ablation studies, comparison with baseline methods, or analysis of computational efficiency. The theoretical justification for the point cluster design is weak, and the paper does not address potential limitations (e.g., scalability to large-scale scenarios). The claims about 'state-of-the-art' performance are unsubstantiated without full experimental results."
          },
          "questions": {
            "value": [
              "How are point clusters generated from raw data? What criteria are used to decouple structure and semantics?",
              "What baselines were compared against, and why are the reported performance gains significant over existing methods?",
              "How does the framework handle dynamic environments with varying numbers of agents or objects?",
              "What is the computational complexity of the point cluster aggregation module, and how does it scale with scene complexity?",
              "Are the proposed metrics for evaluating message units reproducible and widely applicable?",
              "How does the graph optimization for pose consistency handle extreme noise levels without parameter tuning?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 2
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "57xboRTbwI": {
    "paper_id": "57xboRTbwI",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a framework to analyze bias shifts in unconditional image generative models by comparing attribute distributions between training and generated data. It defines bias as the difference between observed and ideal reference distributions, proposes a taxonomy of attributes based on classifier decision boundaries, and compares diffusion models and GANs in terms of bias reduction."
          },
          "strengths": {
            "value": "The paper presents a well-structured framework for quantifying bias shifts, which addresses a critical gap in understanding generative model fairness. The methodology is innovative in its focus on inductive bias, and the experiments on real datasets (CelebA and DeepFashion) provide concrete evidence. The taxonomy of subjective/non-subjective attributes based on classifier confidence is a novel contribution. The comparison between diffusion models and GANs highlights practical implications for model selection. The work is methodologically rigorous and addresses an important societal concern."
          },
          "weaknesses": {
            "value": "The framework relies on ground-truth labels, limiting its applicability to datasets without such annotations. The experiments are restricted to two datasets, which may not generalize to other domains. The paper does not propose mitigation strategies for identified biases, focusing only on analysis. The classification-based approach for attribute labeling could introduce errors if the classifier is not sufficiently accurate. The taxonomy of subjective attributes lacks quantitative validation, and the paper does not address potential confounding factors like dataset biases or architectural differences beyond inductive bias."
          },
          "questions": {
            "value": [
              "How were the attribute classifiers trained? Were they evaluated for accuracy, and how does this affect the reliability of bias shift measurements?",
              "What specific attributes (e.g., gender, race) were tested, and how were they defined? Were there any challenges in labeling these attributes?",
              "How is the 'ideal reference distribution' determined? Could this introduce subjectivity into the bias analysis?",
              "The paper mentions that diffusion models show reduced bias shifts, but how do these results compare to other model architectures or training paradigms?",
              "Are there any limitations in the framework when applied to attributes that are inherently ambiguous or context-dependent?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a framework to analyze bias shifts in unconditional image generative models by comparing attribute distributions between training data and generated samples. The authors define bias as the discrepancy between observed and ideal reference distributions, propose a taxonomy of attributes based on classifier decision boundary density, and compare diffusion models and GANs in terms of bias reduction."
          },
          "strengths": {
            "value": "The paper presents a structured framework for evaluating bias shifts in generative models, which addresses a critical gap in understanding fairness in AI. The use of attribute classifiers for label prediction is practical and scalable. The taxonomy of subjective vs. non-subjective attributes based on decision boundary density is a novel contribution. The empirical analysis on real datasets (CelebA, DeepFashion) and the comparison between diffusion models and GANs provide actionable insights. The work highlights the disconnect between traditional generation metrics and bias metrics, which is significant for model evaluation."
          },
          "weaknesses": {
            "value": "The methodology relies on pre-trained classifiers for attribute prediction, but the paper lacks details on how these classifiers were trained (e.g., data sources, training procedures). The definition of 'subjective' attributes is vague and not rigorously validated. The experiments are limited to two datasets, and the generalizability of findings is unclear. The comparison between diffusion models and GANs is superficial, with no quantitative analysis of bias reduction. The paper also does not address how bias shifts could be mitigated, focusing only on analysis."
          },
          "questions": {
            "value": "1. How were the attribute classifiers trained? Were they trained on the same dataset as the generative models, or on external data? 2. What specific criteria were used to classify attributes as 'subjective' or 'non-subjective' based on decision boundary density? 3. How were the 'bias shift' metrics quantified for diffusion models across training steps? 4. What is the relationship between the proposed taxonomy and existing definitions of bias in NLP or computer vision literature?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces a standardized framework to analyze bias shifts in unconditional image generative models by comparing attribute distributions between training and generated data. It identifies varying sensitivity of attributes to distribution shifts, categorizes them as 'subjective' or 'non-subjective' based on classifier decision boundaries, and demonstrates diffusion models' superiority in reducing bias shifts compared to GANs."
          },
          "strengths": {
            "value": "Originality: The work systematically addresses bias transfer in unconditional generative models, a less-explored aspect compared to conditional models. It introduces a novel taxonomy for attribute sensitivity based on classifier decision boundaries. Quality: The methodology includes rigorous experiments on real-world datasets (CelebA, DeepFashion) and comparative analysis of diffusion models vs. GANs. Clarity: The framework is well-structured, with clear definitions of bias shift and visualizations (e.g., Figure 1). Significance: The findings highlight critical fairness concerns in generative AI, with practical implications for model evaluation and development."
          },
          "weaknesses": {
            "value": "The study relies on a limited set of datasets (CelebA and DeepFashion), which may not generalize to other domains. The classifier-based approach for attribute labeling lacks validation of its reliability, particularly for subjective attributes. The 'subjective/non-subjective' taxonomy is under-theorized, with no quantitative criteria for determining decision boundary density. The comparison between diffusion models and GANs is superficial, omitting key baselines (e.g., other diffusion variants, recent GAN architectures). The paper does not address how bias shifts correlate with specific model components (e.g., architecture, training objectives)."
          },
          "questions": {
            "value": [
              "How was the classifier's reliability validated, especially for subjective attributes where decision boundaries may be ambiguous?",
              "What specific criteria were used to determine whether a decision boundary lies in a high/low-density region? Were these criteria tested for robustness?",
              "Why were only BigGAN and diffusion models compared? How do other GAN variants (e.g., StyleGAN3) perform on bias shifts?",
              "How do the authors account for pre-existing biases in the training datasets (e.g., CelebA's demographic imbalances)?",
              "What is the relationship between training duration and bias shifts in diffusion models? Are the observed trends consistent across different architectures?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "590yfqz1LE": {
    "paper_id": "590yfqz1LE",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates 'non-adversarial reproduction' in large language models (LLMs), where model outputs overlap with public internet text under natural, benign prompts. The authors quantify this overlap (up to 15% of generated text) and compare it to human-generated text, showing LLMs reproduce longer snippets more frequently. They also explore prompting strategies to mitigate this phenomenon."
          },
          "strengths": {
            "value": "The paper addresses a critical and underexplored problem in LLM safety and privacy, with practical implications. It uses diverse, real-world datasets (e.g., WildChat, LMSYS-Chat-1M) and establishes a clear methodology for measuring overlap with public internet text. The comparison to human outputs provides valuable context. The work highlights a gap in current LLM defenses and motivates further research on mitigating unintended data reproduction."
          },
          "weaknesses": {
            "value": "The proxy for training data (public internet) introduces uncertainty, as it may not accurately reflect the models' actual training distributions. The 50-character threshold for measuring overlaps is arbitrary and lacks justification. The paper does not fully address how it distinguishes between memorization and coincidental similarity (e.g., common phrases). The analysis of prompting strategies is limited, with no systematic evaluation of different prompting techniques or ablation studies. The claim of '100% overlap' requires more rigorous validation, as the paper is cut off mid-explanation."
          },
          "questions": {
            "value": [
              "How was the public internet used as a proxy for training data validated? Were there controls for overfitting to specific sources (e.g., Wikipedia, code repositories)?",
              "What statistical methods were used to determine the significance of overlap rates, and how were false positives (e.g., common ngrams) accounted for?",
              "How were cases of '100% overlap' verified? Were these generated texts checked against multiple sources, and were they confirmed to be exact matches?",
              "What specific prompting strategies were tested, and how were they designed to reduce reproduction? Were there ablation studies to isolate the effectiveness of different techniques?",
              "How were human-written baselines selected? Were they sampled from the same task distributions as the LLM outputs, and how was their similarity to internet text controlled for?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces the concept of 'non-adversarial reproduction' to quantify how much large language models (LLMs) reproduce text from the public internet when responding to natural, benign prompts. The authors measure overlaps between model outputs and web snippets, finding up to 15% overlap in some cases, and compare these results with human-generated text to highlight the gap in reproduction rates."
          },
          "strengths": {
            "value": "The paper presents a novel framework for analyzing data reproduction in LLMs under non-adversarial conditions, addressing a critical gap in understanding model behavior during routine interactions. The methodology is rigorous, combining real-world datasets (e.g., WildChat, LMSYS-Chat-1M) with manual prompt creation. The experiments are comprehensive, comparing LLMs to human baselines and exploring mitigation strategies. The significance of the work lies in its implications for privacy, copyright, and model accountability, which are pressing concerns in the field."
          },
          "weaknesses": {
            "value": "The paper lacks clarity on how the public internet is used as a proxy for training data, raising questions about the validity of this assumption. The threshold for 'non-adversarial' reproduction (e.g., 50-character snippets) is not thoroughly justified, and potential false positives (e.g., common phrases or factual knowledge) are not addressed. The study focuses on specific tasks and may not generalize to all use cases. Additionally, the analysis of mitigation strategies (e.g., prompting) is superficial, with limited exploration of technical solutions to prevent worst-case reproductions."
          },
          "questions": {
            "value": [
              "How was the public internet selected as a proxy for training data? Are there biases or gaps in this proxy that could affect the results?",
              "What measures were taken to distinguish between legitimate knowledge reuse (e.g., common phrases) and problematic reproduction?",
              "How were the 50-character thresholds determined, and what is the impact of varying this threshold on the results?",
              "Are the observed overlaps indicative of memorization, or could they result from the models learning to generate common patterns present in the training data?",
              "What are the limitations of using human-generated text as a baseline for comparison, given that humans may also inadvertently reproduce web content?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces the concept of 'non-adversarial reproduction' to quantify how much large language models (LLMs) reproduce verbatim text from the internet when responding to natural, benign prompts. The authors analyze outputs from state-of-the-art conversational models across various tasks, finding that 8-15% of generated text overlaps with internet snippets, with extreme cases reaching 100% overlap. They compare these results with human-written baselines and explore prompting strategies to mitigate reproduction."
          },
          "strengths": {
            "value": "The paper addresses an important and timely problem: understanding unintended data leakage in LLMs during natural interactions. Its novel concept of 'non-adversarial reproduction' fills a gap between adversarial extraction and natural n-gram memorization. The experimental setup is comprehensive, combining curated prompts, real-world conversations, and comparative analysis with human baselines. The work has clear implications for privacy, copyright, and model accountability, making it significant for both research and practice."
          },
          "weaknesses": {
            "value": "The proxy of using public internet data to represent training data lacks validation, as the true training data of commercial models is unknown. The 50-character threshold for measuring reproduction is arbitrary and may not capture meaningful overlaps. The human baseline comparison is limited to specific tasks and lacks statistical rigor (e.g., sample size, diversity of human writers). The paper does not address how model architecture or training data composition affects reproduction rates. Additionally, the analysis of prompting strategies is superficial and lacks systematic evaluation."
          },
          "questions": {
            "value": "1. How was the public internet data selected as a proxy for training data? Were there any validation steps to ensure its representativeness? 2. What criteria were used to determine the 50-character threshold, and how does this affect the interpretation of results? 3. How were human-written baselines collected? Were they from the same domain as the LLM tasks, and how was their diversity ensured? 4. Could the observed reproduction rates be influenced by the specific datasets (e.g., WildChat, LMSYS-Chat-1M) used for analysis? 5. What are the limitations of the prompting strategies tested, and how might they be improved?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "599F4CZ0HB": {
    "paper_id": "599F4CZ0HB",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Bench-O-Matic, an automated pipeline for curating high-quality, open-ended benchmarks from crowdsourced data using LLMs. The approach leverages LLMs for prompt curation, validation, and evaluation, resulting in Eval-O-Matic—a benchmark with 500 prompts that demonstrates superior model separation and alignment with human preferences compared to existing benchmarks."
          },
          "strengths": {
            "value": "The paper presents a novel automated pipeline for benchmark curation, addressing the critical challenge of manual effort in creating high-quality evaluation sets. The proposed metrics for measuring benchmark quality (e.g., model separability and human alignment) are theoretically grounded and validated experimentally. The work is well-motivated, with clear comparisons to existing benchmarks and a strong emphasis on scalability and cost-effectiveness. The open-sourcing of both the pipeline and benchmark further enhances its practical value."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving key details about the seven indicators for prompt quality, the LLM-as-a-judge methodology, and the specific implementation of the proposed metrics incomplete. The experimental validation lacks depth, such as ablation studies or comparisons against baseline methods for the curation pipeline itself. The claim of 98.6% correlation with human preferences requires stronger justification, including statistical significance and error analysis. The cost comparison to other methods (e.g., GPQA) is superficial and lacks a detailed breakdown of expenses."
          },
          "questions": {
            "value": "1. What are the seven indicators used to evaluate prompt quality, and how are they weighted in the curation process? 2. How does the LLM-as-a-judge framework handle biases in model evaluations, and what specific mitigation strategies are proposed? 3. Are the proposed metrics for separability and alignment generalizable to other benchmarking scenarios, or are they tailored to the specific datasets used? 4. How does Bench-O-Matic ensure diversity and representativeness in the curated prompts, and what safeguards prevent overfitting to specific datasets like Chatbot Arena? 5. What are the limitations of the automated pipeline in handling edge cases or ambiguous prompts, and how are these addressed?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Bench-O-Matic, an automated pipeline for curating high-quality, open-ended benchmarks from crowdsourced data using LLMs. It proposes Eval-O-Matic, a benchmark with 500 prompts that achieves high model separation and human preference alignment at low cost."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in LLM evaluation by proposing an automated, scalable solution to benchmark curation, reducing reliance on manual efforts. The methodology leverages LLMs for prompt filtering and evaluation, which is novel in its integration of multiple quality metrics. The experimental results show promising improvements in model separation and human preference correlation compared to existing benchmarks. The paper also provides clear comparisons to related work and outlines practical benefits like cost reduction."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation of the proposed metrics for separability and alignment. The claims about 98.6% correlation with human rankings are not substantiated with statistical significance tests or error margins. The methodology for filtering prompts from crowd-sourced data is not sufficiently described, raising concerns about reproducibility. Additionally, the paper does not address potential biases in LLM-as-a-judge evaluations or provide ablation studies to validate the effectiveness of individual components of Bench-O-Matic."
          },
          "questions": {
            "value": "1. How exactly are the seven quality indicators (e.g., specificity, domain knowledge) operationalized in the prompt filtering process? 2. What is the exact methodology for validating the 98.6% correlation with Chatbot Arena rankings? 3. Are there any manual validation steps in the benchmark curation pipeline, or is it fully automated? 4. How does Eval-O-Matic handle edge cases or ambiguous prompts that may skew model evaluations? 5. What specific biases in LLM-as-a-judge evaluations were addressed, and how were they mitigated?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces Bench-O-Matic, an automated pipeline for curating high-quality, open-ended benchmarks from crowdsourced data using LLMs. The system generates Eval-O-Matic, a benchmark with 500 prompts that demonstrates superior model separation and alignment with human preferences compared to existing benchmarks like MT-Bench, while significantly reducing curation costs."
          },
          "strengths": {
            "value": "The paper presents a novel automated solution to a critical problem in LLM evaluation, addressing the scalability and cost issues of manual curation. The proposed metrics for benchmark quality (separability and alignment) are well-motivated and validated experimentally. The work's practical impact is strong, with a 3x improvement in model separation and a low curation cost. The clear comparison table and focus on open-ended tasks differentiate it from static benchmarks. The integration of LLM-as-a-judge for automated evaluation is a compelling innovation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of Bench-O-Matic's seven quality indicators and how they are weighted or implemented. The evaluation of LLM-as-a-judge reliability is superficial, with no analysis of specific bias sources or mitigation strategies. The comparison to existing benchmarks is limited to correlation metrics without deeper analysis of task diversity or domain coverage. The paper also does not address how Bench-O-Matic handles edge cases or rare prompt types in crowd-sourced data."
          },
          "questions": {
            "value": "1. How are the seven quality indicators for prompt selection operationalized and prioritized in Bench-O-Matic? 2. What specific biases in LLM evaluations were identified, and how do the proposed mitigation strategies address them? 3. How does Eval-O-Matic perform on specialized domains or tasks not covered by the tested datasets? 4. Are there any cases where automated curation failed to capture high-quality prompts, and how were these handled?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "5AB33izFxP": {
    "paper_id": "5AB33izFxP",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a novel method for simultaneous online system identification and trajectory tracking control of nonlinear systems using composite adaptive Lyapunov-based deep neural networks (DNNs). The approach employs a composite adaptation law that integrates tracking error and prediction error from a dynamic state-derivative observer, ensuring uniform ultimate boundedness of errors and exponential convergence under the persistence of excitation (PE) condition. The method is validated through simulations on a two-link manipulator and an unmanned underwater vehicle (UUV) with intermittent state feedback loss, demonstrating superior performance compared to baseline approaches."
          },
          "strengths": {
            "value": "The paper's originality lies in its first-of-its-kind approach to simultaneous online system identification and control with full DNN layer updates, addressing a critical gap in prior work. The theoretical framework is robust, combining Lyapunov-based stability analysis with a novel prediction error formulation. The clarity of problem formulation, mathematical derivations, and experimental setup is strong, while the significance is underscored by practical applications in robotics and autonomous systems. The paper also provides a detailed analysis of the challenges posed by DNNs' nonlinear-in-parameters structure."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough discussion on ensuring the PE condition in practical scenarios, which is critical for the exponential convergence guarantees. The simulations are limited to synthetic environments, and real-world validation is missing. The use of first-order Taylor expansion for DNN approximation may introduce inaccuracies, and the computation of the DNN Jacobian for adaptive laws is not explicitly addressed. Additionally, the paper does not compare against recent DNN-based control methods that may offer similar capabilities."
          },
          "questions": {
            "value": "1. How does the paper ensure the PE condition is satisfied in practical implementations, especially in the absence of state-derivative measurements? 2. What are the computational implications of updating all DNN layers online, and how does this affect real-time performance? 3. Are there specific constraints on the DNN architecture (e.g., depth, activation functions) for the proposed method to work? 4. How does the method handle non-idealities like noise or delays in state feedback during the PE condition?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces a novel method for simultaneous online system identification and trajectory tracking control of nonlinear systems using adaptive deep neural networks (DNNs). The approach employs a composite adaptation law that integrates tracking error and prediction error, derived from a dynamic state-derivative observer, to update all DNN layers. A Lyapunov-based stability analysis ensures uniform ultimate boundedness (UUB) of tracking, observer, and parameter estimation errors, with exponential convergence under the persistence of excitation (PE) condition. The method is validated on a two-link manipulator and an unmanned underwater vehicle (UUV) with intermittent state feedback loss."
          },
          "strengths": {
            "value": "The paper presents a significant advancement in combining system identification and control for nonlinear systems, addressing a critical gap in existing DNN-based adaptive control methods. The theoretical framework is rigorous, with a novel composite adaptation law that incorporates both tracking and prediction errors. The Lyapunov-based stability analysis is comprehensive, and the practical validation on real-world systems (e.g., manipulators, UUVs) demonstrates the method's applicability. The paper also provides a detailed discussion of challenges in extending composite adaptation laws to nonlinear-in-parameters DNNs, showcasing methodological creativity."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art baselines, such as other adaptive control methods or DNN-based controllers that might incorporate similar principles. The PE condition's practical verification in simulations is not explicitly described, raising questions about its feasibility in real-world scenarios. The theoretical analysis assumes idealized conditions (e.g., known control effectiveness matrix g), which may limit the method's applicability. Additionally, the dynamic state-derivative observer's robustness to noise or model inaccuracies is not thoroughly discussed, and the paper does not address computational complexity or real-time implementation constraints."
          },
          "questions": {
            "value": "1. What specific baseline methods were used for comparison, and how do they differ in architecture or adaptation mechanisms? 2. How was the PE condition enforced or verified in the simulations, and what are the implications for systems where PE is not guaranteed? 3. Are there practical limitations to the dynamic state-derivative observer (e.g., sensitivity to noise, computational overhead)? 4. How does the method handle non-smooth or discontinuous dynamics, which are common in real-world systems? 5. What are the theoretical guarantees for the Taylor series approximation used in the prediction error formulation?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a novel approach for simultaneous online system identification and trajectory tracking control of nonlinear systems using composite adaptive Lyapunov-based deep neural networks (DNNs). The method employs a composite adaptation law that integrates tracking error and prediction error, leveraging a dynamic state-derivative observer and DNN Jacobian to ensure uniform ultimate boundedness (UUB) of errors. Under the persistence of excitation (PE) condition, exponential convergence to a neighborhood of the origin is achieved, enabling system identification and enhanced control performance. The approach is validated on two robotic systems with intermittent state feedback loss."
          },
          "strengths": {
            "value": "The paper presents a highly original contribution by combining system identification and trajectory tracking in an online DNN-based control framework, addressing a critical gap in existing literature. The theoretical analysis is rigorous, with a novel composite adaptation law that integrates tracking and prediction errors, supported by a combined Lyapunov-based stability proof. The method's potential for real-world applications, such as underwater vehicles with intermittent feedback, is significant. The clarity of problem formulation, mathematical notation, and logical flow of ideas are strong, with clear connections to prior work on Lyapunov-based control and DNNs."
          },
          "weaknesses": {
            "value": "The experimental validation is limited in scope, with only brief mentions of results on two systems. Key details about the PE condition's practical verification, the design of the dynamic state-derivative observer, and the handling of noisy or incomplete state measurements are not sufficiently elaborated. The paper lacks ablation studies to isolate the contribution of the composite adaptation law versus traditional tracking-error-based methods. Additionally, the theoretical guarantees depend on the PE condition, but its applicability to real-world systems with intermittent feedback remains unclear."
          },
          "questions": {
            "value": "1. How is the persistence of excitation (PE) condition practically ensured in the simulations, especially under intermittent state feedback loss? 2. What specific metrics were used to quantify the performance improvement of the proposed method compared to baseline controllers? 3. Could the dynamic state-derivative observer's design and stability analysis be further clarified, particularly in the context of nonlinear DNN dynamics? 4. How does the method handle scenarios where the PE condition is not satisfied, and what are the practical implications for real-world deployment?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "5AoOHSickG": {
    "paper_id": "5AoOHSickG",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces FoundationForensics, a novel method for tracing back poisoned inputs in vision foundation models after a backdoor attack is detected. The approach calculates maliciousness scores for pre-training inputs based on their contribution to backdoor behavior and identifies outliers as poisoned data. The work includes theoretical analysis and empirical evaluations across multiple models, datasets, and attack types."
          },
          "strengths": {
            "value": "Originality: The paper addresses a critical gap in backdoor forensics for foundation models, proposing a first-of-its-kind method. Quality: Theoretical analysis and extensive empirical evaluations on diverse models and datasets demonstrate rigor. Clarity: The problem statement, methodology, and contributions are well-structured. Significance: Backdoor attacks on foundation models pose a major security risk, and this work provides a valuable tool for forensic analysis."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology for computing maliciousness scores, which is central to the approach. The theoretical analysis is briefly mentioned but not elaborated, making it hard to assess its validity. Experimental results are described generically without specific metrics (e.g., precision, recall) or comparisons to baselines. The use of Median Absolute Deviation (MAD) for outlier detection is standard but not justified in the context of foundation models. The paper also does not address computational scalability for large pre-training datasets."
          },
          "questions": {
            "value": [
              "How exactly is the maliciousness score calculated? What specific aspects of the pre-training process are tracked to quantify contribution to backdoor behavior?",
              "What are the details of the 'eight adaptive' backdoor attacks used in the experiments? How do they differ from existing attacks?",
              "How does FoundationForensics handle different types of triggers (e.g., image-based vs. text-based)? Are there limitations in trigger diversity?",
              "What is the impact of the hyperparameter k on detection performance? How is it tuned in practice?",
              "Are there any false positives or negatives in the poisoning detection? How does the method perform under noisy or overlapping triggers?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces FoundationForensics, the first forensics method to trace back poisoned pre-training inputs in vision foundation models after a backdoor attack has been detected. The approach involves calculating a 'maliciousness score' for each pre-training input based on its contribution to backdoor behavior and identifying outliers as poisoned inputs. The work provides theoretical analysis and empirical evaluations across multiple foundation models, datasets, and backdoor attack scenarios."
          },
          "strengths": {
            "value": "Originality: The paper addresses a novel problem of tracing back poisoned inputs in foundation models, which is critical for forensic analysis. The method of using maliciousness scores and outlier detection (e.g., Median Absolute Deviation) is creative and tailored to the unique challenges of foundation models. Quality: The theoretical analysis and extensive empirical evaluations on diverse models and attacks demonstrate rigor. Clarity: The problem statement and methodology are well-structured, though some technical details may require deeper explanation. Significance: Backdoor attacks on foundation models pose a major threat to AI ecosystems, and this work provides a crucial tool for forensic analysis and mitigation."
          },
          "weaknesses": {
            "value": "Theoretical analysis is limited: While the paper claims to prove that poisoned inputs have higher maliciousness scores, the proof details are not provided, and the assumptions may not cover all attack scenarios. Outlier detection method: The use of Median Absolute Deviation (MAD) is standard but may not be optimal for high-dimensional embedding spaces. Scalability: The paper does not discuss how the method scales with large pre-training datasets, which is critical for real-world applications. Evaluation scope: While the experiments cover multiple models and attacks, they focus on synthetic backdoor triggers, and real-world scenarios or adaptive attacks may not be fully addressed."
          },
          "questions": {
            "value": "1. How is the maliciousness score calculated in practice? Does it involve gradient-based contributions or other techniques? 2. Are there cases where clean inputs might have high maliciousness scores, and how does the method mitigate false positives? 3. How does the choice of hyperparameter k affect performance, and what guidelines are provided for tuning it? 4. The paper mentions extending to foundation models, but how does FoundationForensics compare to existing classifier forensics methods in terms of precision and recall? 5. What are the computational and memory costs of the method, especially for large-scale pre-training data?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces FoundationForensics, a novel method to trace back poisoned pre-training inputs in vision foundation models after a backdoor attack is detected. The approach calculates a 'maliciousness score' for each pre-training input based on its contribution to the backdoor behavior and identifies outliers as poisoned inputs using Median Absolute Deviation. The work includes theoretical analysis, empirical evaluation on multiple models and datasets, and comparisons to existing classifier-based forensics methods."
          },
          "strengths": {
            "value": "Originality: The paper addresses a critical gap in backdoor attack forensics for foundation models, which are increasingly used in critical applications. The maliciousness score mechanism and outlier detection approach are novel for this domain. Quality: The theoretical analysis proves that poisoned inputs have higher maliciousness scores than clean ones, and the empirical evaluation covers diverse models, datasets, and attack types. Clarity: The paper is well-structured with clear explanations of the methodology and contributions. Significance: Backdoor attacks on foundation models pose severe risks, and the ability to trace poisoned inputs is crucial for security and accountability."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing foundation model-specific forensics methods beyond classifier-based approaches. The theoretical analysis assumes linear relationships in model behavior, which may not hold for complex foundation models. The outlier detection method (Median Absolute Deviation) is standard but not novel, and the paper does not explore more advanced techniques. The computational efficiency of the method is not discussed, which is critical for large-scale pre-training data. The evaluation focuses on single-modal and multi-modal models but does not address other foundation model types (e.g., audio-visual)."
          },
          "questions": {
            "value": [
              "How does FoundationForensics handle adaptive attacks that specifically target the maliciousness score calculation mechanism?",
              "What is the computational overhead of calculating maliciousness scores for large-scale pre-training datasets?",
              "Why was Median Absolute Deviation chosen over other outlier detection methods (e.g., Isolation Forest, DBSCAN) for this task?",
              "How does the method perform when multiple backdoor triggers are present in the pre-training data?",
              "Are there any cases where clean inputs might accidentally be flagged as poisoned due to high maliciousness scores?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "5B6eSE6l4M": {
    "paper_id": "5B6eSE6l4M",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates performance heterogeneity in graph-level tasks for both message-passing and transformer-based GNNs. The authors introduce heterogeneity profiles to analyze individual graph performance, demonstrate that graph topology alone cannot explain heterogeneity, and propose a selective rewiring approach and depth-optimization heuristic based on spectral analysis and Tree Mover's Distance."
          },
          "strengths": {
            "value": "Originality: The systematic analysis of performance heterogeneity in graph-level tasks is novel, especially the use of Tree Mover's Distance to link class-distance ratios with performance. Quality: The methodology is rigorous, combining theoretical analysis with practical experiments. Clarity: The paper is well-structured with clear explanations of concepts like heterogeneity profiles. Significance: The findings address a critical gap in GNN design for heterogeneous graph data, with practical implications for preprocessing and architecture choices."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of selective rewiring versus other factors. The experimental validation is limited to standard benchmarks without exploring edge cases or more complex graph structures. The theoretical connection between Tree Mover's Distance and performance heterogeneity requires deeper justification. The proposed depth-optimization heuristic (Fiedler value) is not thoroughly compared to existing methods."
          },
          "questions": {
            "value": [
              "How is 'performance heterogeneity' quantitatively defined and measured in the experiments?",
              "What criteria determine which graphs are selected for rewiring in the proposed method?",
              "Are the results generalizable across different types of graph data (e.g., social networks vs. molecular graphs)?",
              "How does the Fiedler value-based depth heuristic compare to other spectral-based approaches in the literature?",
              "Were there any cases where selective rewiring worsened performance, and how were they handled?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates performance heterogeneity in graph neural networks (GNNs) for graph-level tasks, analyzing how model and data characteristics interact to cause variations in performance. The authors introduce heterogeneity profiles, demonstrate that graph topology alone cannot explain heterogeneity, and propose a selective rewiring approach and spectrum-aware depth selection heuristic."
          },
          "strengths": {
            "value": "The paper makes original contributions by systematically analyzing graph-level performance heterogeneity, which has been less studied than node-level tasks. The use of Tree Mover's Distance to link class-distance ratios with performance is novel. The proposed selective rewiring and depth heuristic address practical challenges in GNN design. The experiments cover multiple benchmarks and architectures, showing practical utility of the methods."
          },
          "weaknesses": {
            "value": "The analysis of why topology alone fails to explain heterogeneity lacks depth; more quantitative ablation studies on topological vs. feature-based factors would strengthen this. The selective rewiring approach's implementation details are under-specified, and its effectiveness is not compared against existing rewiring methods. The spectrum-aware depth heuristic relies on the Fiedler value, but the connection to optimal depth is not theoretically justified. The paper could better contextualize its contributions relative to prior work on over-smoothing and graph rewiring."
          },
          "questions": {
            "value": [
              "How are heterogeneity profiles defined quantitatively, and how do they account for variations in graph size and structure?",
              "What specific limitations of existing rewiring methods motivated the proposed selective approach, and how does it differ from Barbero et al. (2023)?",
              "Can the spectrum-aware depth heuristic be generalized to non-structural tasks or different GNN architectures?",
              "How sensitive are the results to the choice of Tree Mover's Distance parameters?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates performance heterogeneity in graph-level tasks for Graph Neural Networks (GNNs), introducing heterogeneity profiles to analyze variations in model performance across individual graphs. It identifies that graph topology alone cannot explain heterogeneity, leverages the Tree Mover’s Distance to link class-distance ratios to performance, and proposes selective rewiring and depth-heuristics to address these issues."
          },
          "strengths": {
            "value": "Originality: The paper systematically analyzes graph-level performance heterogeneity, a gap in prior work focused mainly on node-level tasks. The introduction of heterogeneity profiles and the use of Tree Mover’s Distance for joint topological-feature analysis are novel. Quality: The methodology is well-structured, with experiments on standard benchmarks. Clarity: The paper is logically organized, with clear problem formulations and technical explanations. Significance: Addressing heterogeneity in graph-level tasks has practical implications for real-world applications like drug discovery and biochemistry."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with state-of-the-art methods for handling heterogeneity, such as advanced rewiring techniques or adaptive depth strategies. The selective rewiring approach is proposed but not thoroughly validated with ablation studies or sensitivity analyses. The heuristic for network depth based on the Fiedler value is not rigorously justified or tested across diverse datasets. Additionally, the experiments focus on a limited set of benchmarks, and the impact of the proposed methods on downstream tasks (e.g., regression) is underexplored."
          },
          "questions": {
            "value": "1. How is the threshold for selective rewiring determined, and what criteria ensure its effectiveness? 2. Are there limitations to the Tree Mover’s Distance in capturing heterogeneity for specific graph types (e.g., molecular graphs)? 3. How does the selective rewiring approach scale to large graphs or datasets with high computational constraints? 4. What ablation studies demonstrate the necessity of the proposed depth heuristic over fixed-depth baselines?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "5EuAMDMPRK": {
    "paper_id": "5EuAMDMPRK",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces POROver, a method to improve the safety and reduce overrefusal in large language models (LLMs) by combining overgeneration of training data using advanced teacher models (e.g., GPT-4o) and preference optimization. The approach involves generating diverse completions for both general-purpose and toxic prompts, followed by preference optimization to align model outputs with human preferences while maintaining safety."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem in LLM safety and usability. The proposed method combines overgeneration and preference optimization, which are promising directions for improving model alignment. The experiments demonstrate measurable improvements in safety and usefulness metrics (e.g., F1 score, Not-Overrefusal Rate). The work is well-structured, with clear motivation and a logical flow from problem statement to methodology and results. The authors also make their datasets publicly available, which is a valuable contribution."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodological descriptions, such as how overgeneration is implemented (e.g., specific prompts, sampling strategies, or criteria for selecting completions). The experiments are not thoroughly described, including unclear definitions of metrics like 'Not-Overrefusal Rate' and limited comparisons to baseline methods. The claims about safety improvements are not substantiated with rigorous ablation studies or analysis of trade-offs. Additionally, the paper does not address potential limitations of using advanced teacher models (e.g., bias propagation or computational costs)."
          },
          "questions": {
            "value": "1. How exactly is overgeneration implemented? What specific prompts, sampling strategies, or selection criteria are used to generate completions? 2. What preference optimization techniques (e.g., DPO, RLHF) are employed, and how are preference data curated? 3. Are there ablation studies to isolate the contributions of overgeneration vs. preference optimization? 4. How are safety and usefulness metrics defined and calculated? 5. What are the computational costs and scalability challenges of the proposed method?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes POROver, a method combining overgeneration of training data using advanced teacher models and preference optimization to improve the safety and usefulness of large language models. The approach addresses overrefusal by generating diverse completions for general-purpose and toxic prompts, followed by preference-based alignment to reduce overrefusal while maintaining safety."
          },
          "strengths": {
            "value": "The paper tackles a critical challenge in LLMs: balancing safety and usefulness. The integration of overgeneration with advanced teacher models (e.g., GPT-4o) and preference optimization is novel, offering a practical solution to overrefusal. The experimental results demonstrate significant improvements in safety-usefulness trade-offs, with clear metrics (F1 scores, overrefusal rates). The public release of datasets enhances reproducibility and community impact. The work also provides a structured analysis of how teacher model quality affects model behavior."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how overgenerated completions are evaluated for quality or alignment with safety goals. The comparison to baseline methods (e.g., traditional instruction tuning or other preference optimization approaches) is insufficient, making it hard to assess the novelty of POROver. The experiments focus on specific metrics (F1, overrefusal rates) but do not address qualitative aspects of model behavior. Additionally, the paper does not discuss potential limitations of overgeneration, such as computational costs or bias amplification in generated data."
          },
          "questions": {
            "value": [
              "How were the overgenerated completions validated for alignment with safety and usefulness criteria? Were human evaluations or automated metrics used?",
              "What baselines were compared against POROver (e.g., standard instruction tuning, DPO without overgeneration)?",
              "How does the method handle varying levels of toxicity in prompts? Are there cases where overgeneration might introduce new safety risks?",
              "What is the computational cost of overgeneration and preference optimization compared to existing methods?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces POROver, a method combining overgeneration of training data using advanced teacher models (e.g., GPT-4o) and preference optimization to improve the safety and usefulness of instruction-following language models. The approach aims to reduce overrefusal while maintaining safety, with experiments showing significant improvements in metrics like F1 scores and Not-Overrefusal Rates."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in LLMs—balancing safety and usefulness—through a novel combination of overgeneration and preference optimization. The methodology is grounded in existing techniques (instruction finetuning, DPO) but applies them to a specific problem (overrefusal) with clear empirical results. The public release of datasets enhances reproducibility and community impact. The work also highlights practical trade-offs between safety and usefulness, which is significant for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing methods for reducing overrefusal, making it hard to assess the novelty of POROver. The evaluation metrics (e.g., F1 scores, Not-Overrefusal Rates) are not thoroughly defined, and it is unclear how safety is quantified. The use of GPT-4o as a teacher model is mentioned, but the paper does not clarify whether this model was actually used or if it is a hypothetical example. Additionally, the experiments on toxic prompts and preference optimization lack ablation studies or analysis of hyperparameter sensitivity."
          },
          "questions": {
            "value": "1. How were the teacher models (e.g., GPT-4o) selected, and what criteria were used to ensure their reliability for overgeneration? 2. What specific metrics were used to calculate safety and usefulness, and how do they align with established benchmarks? 3. How does POROver compare to existing post-training methods for reducing overrefusal (e.g., RLHF, DPO variants)? 4. What is the exact definition of 'Not-Overrefusal Rate,' and how was it measured in the experiments? 5. Are the improvements in safety and usefulness statistically significant, and what is the sample size of the evaluation datasets?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "5GI6BGToyw": {
    "paper_id": "5GI6BGToyw",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces AtmosArena, the first multi-task benchmark for evaluating foundation models in atmospheric sciences. The benchmark includes diverse tasks spanning weather forecasting, climate modeling, and atmospheric chemistry, supported by standardized datasets, evaluation metrics, and baselines. The authors evaluate state-of-the-art models like ClimaX and Stormer, demonstrating the utility of AtmosArena for comparative analysis and highlighting opportunities for future model development."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in atmospheric sciences by proposing a comprehensive, standardized benchmark for foundation models, which is essential for systematic progress. The task diversity (e.g., weather forecasting, climate downscaling, extreme event detection) and inclusion of both deep learning and traditional baselines show thoroughness. The open-source commitment enhances reproducibility, and the comparison with existing works (e.g., ClimateLearn, ClimaX) provides context. The paper also highlights the importance of multi-task evaluation, which aligns with trends in NLP and vision benchmarks."
          },
          "weaknesses": {
            "value": "The experiments are limited in scope, with only two models (ClimaX and Stormer) evaluated, and no detailed analysis of why certain models underperform. The benchmark's tasks may not fully capture real-world complexities (e.g., data sparsity, domain shifts). The paper lacks ablation studies or insights into the relative importance of different tasks. Additionally, the comparison with traditional methods is superficial, and the paper does not discuss potential biases in the datasets (e.g., ERA5, ClimateBench) or their generalizability."
          },
          "questions": {
            "value": "1. How were the tasks in AtmosArena selected to ensure coverage of critical atmospheric science challenges? 2. Are there plans to expand the benchmark to include additional domains (e.g., oceanography) or more diverse data sources? 3. What specific challenges did the evaluated models face in tasks like climate model emulation or extreme weather detection? 4. How does AtmosArena handle domain adaptation across different datasets (e.g., ERA5 vs. ClimateBench)? 5. Could the paper provide more details on the fine-tuning protocols and hyperparameters used for the baselines?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces AtmosArena, a multi-task benchmark for evaluating foundation models in atmospheric sciences. It includes diverse tasks spanning atmospheric physics and chemistry, along with datasets, evaluation metrics, and baselines. The authors evaluate state-of-the-art models like ClimaX and Stormer, highlighting the need for standardized benchmarks to assess generalization across tasks."
          },
          "strengths": {
            "value": "Originality is strong as AtmosArena addresses the lack of a comprehensive, standardized benchmark for atmospheric foundation models. The quality of the benchmark is high, with diverse tasks, datasets, and metrics. Clarity is good, with a structured presentation of tasks and experiments. Significance is evident, as the benchmark could accelerate progress in atmospheric science by enabling systematic evaluation, similar to benchmarks in NLP and computer vision."
          },
          "weaknesses": {
            "value": "The paper lacks depth in experimental analysis, such as statistical significance tests or ablation studies to validate the effectiveness of the benchmark. The comparison with existing benchmarks (e.g., ClimateLearn, ClimaX) is superficial, and the rationale for task selection is underdeveloped. The related work section is truncated, limiting context for prior efforts. Additionally, the paper does not address potential biases in the datasets or the practical challenges of deploying foundation models in atmospheric applications."
          },
          "questions": {
            "value": [
              "How were the tasks in AtmosArena selected to ensure representativeness of real-world atmospheric challenges?",
              "What specific baselines (e.g., traditional methods or other deep learning models) were used for comparison, and why were they chosen?",
              "How does AtmosArena handle domain-specific challenges, such as spatiotemporal dependencies or data scarcity in certain tasks?",
              "Are there any limitations to the datasets (e.g., ERA5, ClimateBench) that could affect the benchmark's generalizability?",
              "What computational resources were required to run the experiments, and how does this impact the accessibility of the benchmark?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces AtmosArena, the first multi-task benchmark for evaluating foundation models in atmospheric sciences. It covers diverse tasks like weather forecasting, climate downscaling, and extreme event detection, accompanied by extensive experiments comparing deep learning models (e.g., ClimaX, Stormer) and traditional baselines. The benchmark aims to standardize evaluation and accelerate research in the field."
          },
          "strengths": {
            "value": "Originality is strong, as AtmosArena addresses a critical gap by creating the first comprehensive multi-task benchmark for atmospheric foundation models. The quality of the work is high, with a well-structured suite of tasks, datasets, and metrics. Clarity is excellent, with clear explanations of the benchmark's design and experiments. Significance is evident, as standardized benchmarks have historically driven progress in fields like NLP and computer vision, and this paper positions AtmosArena to do the same for atmospheric sciences."
          },
          "weaknesses": {
            "value": "The paper lacks depth in analyzing why pretrained models outperform baselines in some tasks but not others. While experiments are extensive, they focus on comparative performance rather than probing model limitations or failure modes. The benchmark's task selection could benefit from more justification, such as how it addresses domain-specific challenges (e.g., spatiotemporal complexity). Additionally, the paper does not discuss potential biases or limitations in the datasets used (e.g., ERA5, ClimateBench)."
          },
          "questions": {
            "value": "1. How were the tasks in AtmosArena selected to ensure coverage of critical atmospheric science challenges? 2. Are there plans to incorporate real-time or dynamic data streams into the benchmark? 3. How does AtmosArena address domain-specific challenges like physical consistency or conservation laws in atmospheric models? 4. Could the authors elaborate on the trade-offs between multi-source pretraining and single-source pretraining in their experiments?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "5GauLpaNGC": {
    "paper_id": "5GauLpaNGC",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces TCMRL, a framework for offline meta-reinforcement learning (meta-RL) that improves generalization by capturing both task characteristic and task contrastive information. The method includes a task characteristic extractor using attention mechanisms and a task contrastive loss to emphasize distinguishing features between tasks, demonstrated to enhance adaptation to unseen tasks."
          },
          "strengths": {
            "value": "The paper presents a novel approach to address context shift in offline meta-RL by explicitly modeling task characteristic and contrastive information, which are critical for generalization. The motivation is well-justified with clear examples (e.g., 'Door-Open' tasks), and the framework is structured with specific components (attention-based extractor, contrastive loss). The experiments on standard benchmarks (MuJoCo, Meta-World) suggest practical relevance, and the focus on improving context quality aligns with key challenges in meta-RL."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, limiting the ability to assess experimental completeness, ablation studies, and comparison with all relevant methods. The task characteristic extractor's design details (e.g., how attention weights are computed) are not fully explained. The task contrastive loss's implementation and its relationship to trajectory subsequences require clarification. The paper lacks analysis of how the proposed method directly mitigates context shift, and the theoretical justification for the contrastive loss is underdeveloped."
          },
          "questions": {
            "value": "1. How does the task characteristic extractor specifically identify 'characteristic transitions' (e.g., thresholds, criteria)? 2. What is the exact formulation of the task contrastive loss, and how are trajectory subsequences used to capture interrelations? 3. Are there ablation studies demonstrating the individual contributions of the extractor and contrastive loss? 4. How does TCMRL address the risk of overfitting to the meta-training data, which is a core issue in offline RL? 5. What are the computational costs of the proposed method compared to baseline approaches?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes TCMRL, a framework for offline meta-reinforcement learning (meta-RL) that addresses context shift by capturing both task characteristic and task contrastive information. It introduces a task characteristic extractor using attention mechanisms and a task contrastive loss to enhance context generalization. The approach aims to improve adaptation to unseen tasks by emphasizing distinguishing features and inter-task relationships."
          },
          "strengths": {
            "value": "Originality is evident in combining attention mechanisms with contrastive learning for context generation in meta-RL. The framework addresses a critical gap in existing methods by explicitly modeling task characteristics and contrastive information. Clarity is strong in the problem formulation and motivation, with clear distinctions between task characteristic and contrastive information. Significance is high, as improving generalization in offline meta-RL has broad implications for real-world applications where online interaction is limited."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation. It does not specify which baselines were compared (e.g., Dorfman et al. 2021, Gao et al. 2023) or provide ablation studies to isolate the contributions of the task characteristic extractor and contrastive loss. The proposed context-based reward estimator and loss functions are not thoroughly explained, leaving questions about their implementation and theoretical justification. Additionally, the paper does not address computational efficiency or scalability, which are critical for practical deployment."
          },
          "questions": {
            "value": [
              "What specific baselines were used for comparison, and how do they align with state-of-the-art methods in offline meta-RL?",
              "Are there ablation studies demonstrating the individual contributions of the task characteristic extractor and contrastive loss?",
              "How is the context-based reward estimator trained, and what metrics are used to evaluate its effectiveness?",
              "What are the computational costs of TCMRL compared to existing methods, and how does it scale to larger environments?",
              "How does the framework handle tasks with sparse rewards or highly variable dynamics?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper addresses generalization challenges in offline meta-reinforcement learning (meta-RL) by introducing a framework called TCMRL that captures both task characteristic and contrastive information. The framework includes a task characteristic extractor using attention mechanisms and a task contrastive loss to improve context quality, enabling better adaptation to unseen tasks."
          },
          "strengths": {
            "value": "The paper identifies a critical gap in existing context-based offline meta-RL methods, specifically the failure to capture task-specific and contrastive information. The proposed TCMRL framework is well-structured, with clear components (task characteristic extractor and contrastive loss) that address the problem. The experiments on standard benchmarks (MuJoCo and Meta-World) demonstrate performance improvements, and the motivation is grounded in concrete examples (e.g., 'Door-Open' tasks). The work contributes to a well-defined problem in meta-RL and offers actionable insights for improving generalization."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough comparison with existing methods, as the related work section is incomplete. The task characteristic extractor's design and implementation details (e.g., how transitions are identified as 'characteristic') are under-specified. The task contrastive loss is vaguely described, with unclear mechanisms for capturing interrelations among trajectory subsequences. The experiments are not fully detailed, making it difficult to assess the robustness of the claims. Additionally, the paper does not discuss computational efficiency, scalability, or ablation studies to validate individual components."
          },
          "questions": {
            "value": "1. How does the task characteristic extractor differentiate between task-specific and redundant transitions? What criteria or metrics are used for this distinction? 2. What are the specific loss functions introduced for the context-based reward estimator, and how do they address sparsity in attention weights? 3. How is the task contrastive loss implemented, and what are the criteria for selecting trajectory subsequences? 4. Are there ablation studies to validate the contribution of each component (e.g., characteristic extractor vs. contrastive loss)? 5. How does TCMRL handle tasks with varying numbers of trajectories or sparse data?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "5I39Zvlb3Y": {
    "paper_id": "5I39Zvlb3Y",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Collu-Bench, a benchmark for predicting code hallucinations in large language models (LLMs) across code generation (CG) and automated program repair (APR) tasks. The benchmark includes 13,234 hallucination instances from 11 LLMs, with detailed features like log probabilities, token types, and execution feedback. Experiments using traditional ML and neural networks achieve 22.03–33.15% accuracy, highlighting challenges in localizing and predicting code hallucinations."
          },
          "strengths": {
            "value": "The paper addresses a novel and important problem: code hallucinations in LLMs, which has been underexplored compared to text/image hallucinations. The benchmark is comprehensive, with 13,234 instances from diverse datasets and LLMs, and includes granular features (e.g., per-step log probabilities, token types) that enable detailed analysis. The automated pipeline for collecting hallucination data is innovative, and the experiments provide actionable insights into hallucination patterns. The work is well-structured and clearly motivated."
          },
          "weaknesses": {
            "value": "The experimental results are modest, with accuracy ranging from 22% to 33%, which suggests the task remains highly challenging. The paper lacks comparison with existing code hallucination detection methods, making it hard to assess the novelty of their approach. The automated pipeline's effectiveness is not thoroughly validated, and the paper does not address how to improve the low accuracy. Additionally, the analysis of token types (e.g., Keywords, Identifiers) is superficial, with no deeper investigation into why these tokens are more prone to hallucinations."
          },
          "questions": {
            "value": "1. How does the automated pipeline ensure the accuracy of hallucination token localization? Are there manual validations or gold-standard annotations? 2. Why is the accuracy of the proposed methods relatively low (22–33%)? Are there specific challenges in code hallucination detection that make this task harder than text/image hallucination? 3. How do the features (log probabilities, token types, execution feedback) contribute to the model's performance? Are there ablation studies to quantify their individual impact? 4. The paper mentions 'per-token' and 'per-sample' prediction tasks—what are the practical implications of these settings for real-world LLM deployment?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces Collu-Bench, a benchmark for predicting code hallucinations in large language models (LLMs) across code generation and automated program repair tasks. It includes 13,234 hallucination instances from 11 LLMs, along with features like per-step log probabilities, token types, and execution feedback. The authors conduct experiments using traditional ML and neural networks, achieving 22.03–33.15% accuracy, and identify patterns in hallucination behavior."
          },
          "strengths": {
            "value": "The paper addresses a critical but underexplored problem: code hallucinations in LLMs, which can lead to severe financial and operational risks. The creation of Collu-Bench is a significant contribution, providing a structured dataset with rich features for analyzing and mitigating code hallucinations. The experiments are comprehensive, covering both traditional and neural methods, and the findings about LLM confidence levels and token types are insightful. The paper also clarifies the distinction between hallucination detection and localization, which is novel in the code domain."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons with existing code-specific benchmarks like HalluCode (Liu et al., 2024a), which limits the assessment of Collu-Bench's novelty and utility. The automated pipeline for collecting hallucination instances is not sufficiently detailed, making it hard to replicate or validate the methodology. The experimental results (22–33% accuracy) are modest, suggesting that the problem remains largely unsolved, but the paper does not thoroughly analyze why current methods fail or how Collu-Bench's features could enable future improvements. Additionally, the paper does not address class imbalance in the dataset or evaluate metrics beyond accuracy, which is critical for imbalanced classification tasks."
          },
          "questions": {
            "value": "1. How does Collu-Bench compare to existing code hallucination benchmarks like HalluCode in terms of scope, granularity, and evaluation metrics? 2. What specific techniques were used for sampling equivalent code and program normalization in the automated pipeline? 3. Why were traditional ML methods (e.g., random forest) and neural networks (e.g., LSTM) chosen over other approaches like transformers or graph-based models? 4. How were the features (log probabilities, token types, execution feedback) selected, and what ablation studies were conducted to validate their importance? 5. Are the results generalizable across different LLMs and code domains, or are they heavily dependent on the specific datasets used?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Collu-Bench, a benchmark for predicting code hallucinations in large language models (LLMs) across code generation (CG) and automated program repair (APR) tasks. The benchmark includes 13,234 hallucination instances with detailed features like per-step log probabilities, token types, and execution feedback. The authors conduct experiments using traditional ML and neural networks, achieving 22.03–33.15% accuracy, highlighting the challenges of localizing code hallucinations."
          },
          "strengths": {
            "value": "Originality is strong, as code hallucinations are underexplored compared to text/image hallucinations. The benchmark's detailed features (log probabilities, token types, execution feedback) enable granular analysis, addressing a critical gap. Experiments are comprehensive, covering multiple models (random forest, LSTM) and data splits (All-in-one, One-per-dataset, One-per-LLM). The paper clearly articulates the significance of code hallucinations in real-world applications, and the methodology for constructing Collu-Bench (e.g., program normalization) demonstrates creativity."
          },
          "weaknesses": {
            "value": "The paper lacks comparison with existing code-specific benchmarks like HalluCode (Liu et al., 2024a), which could contextualize Collu-Bench's novelty. The experimental models (e.g., random forest, LSTM) are not state-of-the-art, and the paper does not explore more advanced architectures (e.g., transformers) that might better capture code structure. The low accuracy (22–33%) is not thoroughly analyzed—e.g., why certain token types (keywords, identifiers) are more prone to hallucination remains unclear. Additionally, the paper does not discuss potential biases in the dataset or how the automated pipeline ensures accurate hallucination labeling."
          },
          "questions": {
            "value": [
              "How does Collu-Bench differ from existing code hallucination benchmarks like HalluCode? What specific gaps does it address?",
              "Why were traditional ML models (e.g., random forest) and simple NNs (e.g., LSTM) chosen over more advanced architectures (e.g., transformer-based models) for the experiments?",
              "The paper mentions that hallucinated tokens have lower log probabilities and higher entropy. How were these patterns validated statistically, and what is their generalizability across different LLMs/datasets?",
              "How was the automated pipeline for collecting hallucination instances validated? Are there cases where the pipeline might mislabel non-hallucinatory code as hallucinatory?",
              "What are the limitations of using execution feedback as a signal for hallucination detection? Could false positives/negatives arise from incomplete or incorrect test cases?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "5KgKa96PUG": {
    "paper_id": "5KgKa96PUG",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper explores the saddle point reformulation of Vertical Federated Learning (VFL) using the classical Lagrangian function. It proposes deterministic and stochastic algorithms, including modifications for compression, partial participation, and coordinate selection, alongside convergence analysis and empirical validation. The work aims to demonstrate the advantages of saddle point formulations over standard minimization approaches in VFL."
          },
          "strengths": {
            "value": "The paper introduces a novel perspective on VFL through saddle point reformulation, which is theoretically grounded and addresses practical challenges like communication efficiency and asynchronous updates. The proposed algorithms (e.g., ExtraGradient-based methods) are well-motivated, and the paper provides convergence guarantees. The exploration of multiple practical modifications (compression, partial participation, coordinate selection) adds value. The work also bridges theoretical optimization techniques (ADMM, saddle point problems) with federated learning, which is significant."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive experimental comparisons with state-of-the-art VFL methods beyond ADMM, making it difficult to assess the practical superiority of the proposed approach. The theoretical analysis assumes convexity, which may not hold in real-world non-convex settings. The modifications for practical scenarios (e.g., compression) are mentioned but not thoroughly evaluated in terms of their impact on convergence or privacy. Additionally, the paper is cut off mid-sentence, leaving critical details about alternative reformulations and non-convex extensions incomplete."
          },
          "questions": {
            "value": [
              "How does the saddle point reformulation handle non-convex VFL problems in practice, given the theoretical analysis focuses on convex cases?",
              "What specific datasets and baselines were used in the experiments? How do the proposed methods compare to other VFL approaches (e.g., FedProx, FedAvg) beyond ADMM?",
              "Are the practical modifications (e.g., compression, partial participation) rigorously analyzed for their effect on convergence guarantees and privacy?",
              "The paper mentions 'alternative reformulations' but does not elaborate. What are their exact formulations, and how do they compare to the saddle point approach in terms of performance and complexity?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper explores the saddle point reformulation of Vertical Federated Learning (VFL) using classical Lagrangian methods, proposing deterministic and stochastic algorithms with practical modifications such as compression, partial participation, and coordinate selection. The authors claim superior performance over existing methods like ADMM through theoretical convergence analysis and empirical validation."
          },
          "strengths": {
            "value": "The paper introduces a novel perspective on VFL by leveraging saddle point reformulations, which is a fresh approach compared to traditional minimization formulations. The theoretical contributions, including convergence guarantees for the proposed algorithms, are significant. The practical modifications (e.g., compression, asynchronous communication) address real-world challenges in distributed learning. The clarity of the problem setup and the structured presentation of technical preliminaries enhance readability. The work's potential to advance privacy-preserving distributed optimization makes it impactful for the ML community."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art VFL methods beyond ADMM, which limits the evaluation of the proposed approach's competitiveness. The experimental section appears incomplete (e.g., truncated content), raising questions about the thoroughness of empirical validation. The theoretical analysis focuses on convex settings, and the non-convex extension is not sufficiently explored. Some claims about the advantages of the saddle point reformulation (e.g., enabling specific modifications) lack concrete justification or ablation studies."
          },
          "questions": {
            "value": "1. How does the saddle point reformulation specifically enable the proposed modifications (e.g., compression, coordinate selection) compared to standard minimization formulations? 2. What are the exact datasets and baselines used in the experiments, and how do they compare to recent VFL works (e.g., [Liu et al., 2024])? 3. Can the authors provide additional insights into the non-convex extension, particularly regarding convergence guarantees and practical implementation? 4. How do the alternative saddle point reformulations (mentioned in the contributions) differ in terms of trade-offs between computational efficiency, memory usage, and privacy guarantees?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates Vertical Federated Learning (VFL) through a saddle point reformulation using the classical Lagrangian function. It proposes deterministic and stochastic algorithms, along with practical modifications like compression, partial participation, and coordinate selection, to address communication and computation challenges in VFL. The work also explores non-convex extensions and alternative reformulations."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel saddle point reformulation for VFL, distinguishing it from standard minimization approaches. It proposes practical modifications (compression, partial participation) that are theoretically grounded. Quality: Theoretical convergence analysis is provided for each algorithm, and the paper addresses non-convex cases. Clarity: The structure is logical, with clear sections on preliminaries, reformulation, and contributions. Significance: Solving communication bottlenecks and improving efficiency in VFL has broad applicability in privacy-sensitive domains."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation due to incomplete content (e.g., cut-off experiments section). Theoretical guarantees are limited to specific assumptions (e.g., convexity), and the non-convex extension is not thoroughly analyzed. The comparison with existing methods like ADMM is superficial, and practical modifications (e.g., compression) are not empirically evaluated. The paper also does not address potential privacy trade-offs in the proposed reforms."
          },
          "questions": {
            "value": [
              "What specific deterministic methods are used to solve the saddle point problem, and how do they compare to ADMM in terms of computational complexity?",
              "How are the stochastic modifications (e.g., compression, partial participation) implemented, and what are their theoretical convergence guarantees under non-convex settings?",
              "Are the numerical experiments conducted on standard VFL benchmarks (e.g., healthcare, finance datasets), and how do they compare to state-of-the-art methods like FedProx or FedAvg?",
              "What is the impact of the proposed compression techniques on model utility and privacy, and are there any trade-offs between communication efficiency and accuracy?",
              "How are the alternative saddle point reformulations (e.g., easier stepsize estimation) validated, and what are their memory or computational requirements?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "5MNJKgaj54": {
    "paper_id": "5MNJKgaj54",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces Latent Exploration Score (LES), a differentiable constraint for Latent Space Optimization (LSO) to mitigate over-exploration. LES leverages the VAE decoder's data distribution approximation to identify valid regions in latent space, improving solution quality without additional training or architectural changes. Evaluation across 30 tasks and 22 VAE models demonstrates its effectiveness in generating valid solutions while maintaining high objective values."
          },
          "strengths": {
            "value": "Originality: LES presents a novel, differentiable score for LSO that addresses over-exploration, offering a practical solution for structured discrete optimization. Quality: The experiments are comprehensive, covering diverse tasks and VAE models, with clear claims of performance improvements. Clarity: The problem statement and motivation are well-articulated, with concrete examples (e.g., arithmetic expressions, SMILES) to contextualize the issue. Significance: Addressing over-exploration in LSO has practical implications for applications like drug discovery and molecular design, where validity is critical."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with alternative methods, such as uncertainty-based constraints or specialized VAE architectures, making it hard to assess LES's relative advantages. The computational efficiency claims (e.g., 80% faster) are not substantiated with rigorous benchmarks or ablation studies. The scalability of LES (cubic complexity in latent dimension) is not thoroughly discussed, particularly for high-dimensional data. Additionally, the paper does not clarify how LES generalizes across diverse VAE architectures or sequence space structures beyond the examples provided."
          },
          "questions": {
            "value": [
              "How does LES compare to uncertainty-based constraints (e.g., the method in Notin et al. 2021) in terms of computational efficiency and effectiveness? Are there cases where LES fails to outperform these approaches?",
              "What specific VAE models were used in the experiments, and how do their architectures or training data affect LES's performance? Are there any VAEs where LES is less effective?",
              "The paper mentions LES's cubic complexity but claims it is faster than existing methods. Can the authors provide concrete runtime comparisons or theoretical analysis to support this claim?",
              "How was the threshold for LES scores determined to enforce validity constraints? Is this threshold task-specific, and how does it impact the trade-off between solution validity and objective value?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces Latent Exploration Score (LES), a method to mitigate over-exploration in Latent Space Optimization (LSO) by leveraging the decoder's approximation of the data distribution. LES acts as a constraint in LSO optimization, enhancing solution validity without requiring additional training or architectural changes to the VAE. The approach is evaluated across 30 tasks, demonstrating improved performance over existing methods."
          },
          "strengths": {
            "value": "Originality: LES provides a novel, differentiable constraint for LSO that addresses over-exploration without modifying the VAE. Quality: The experiments are comprehensive, covering 22 VAEs and 5 benchmark tasks, with clear quantitative results. Clarity: The paper is well-structured, with detailed examples and intuitive explanations of LES. Significance: The method has practical value for discrete optimization tasks in scientific domains, such as molecule design, by improving solution validity and sample efficiency."
          },
          "weaknesses": {
            "value": "The paper lacks theoretical analysis explaining why LES effectively identifies valid regions in latent space. The cubic computational complexity of LES with respect to latent dimension may limit scalability for high-dimensional data. The comparison to existing methods like uncertainty-based constraints is limited, with no ablation studies on LES components (e.g., decoder vs. prior distribution). The evaluation focuses on synthetic tasks; real-world applications (e.g., drug discovery) are not explicitly demonstrated."
          },
          "questions": {
            "value": "1. How does LES theoretically relate to the data distribution approximated by the VAE decoder? 2. Are there ablation studies showing the contribution of specific components (e.g., decoder vs. prior) to LES performance? 3. How does LES compare to uncertainty-based constraints in terms of computational efficiency and effectiveness? 4. Can the authors provide examples of real-world applications where LES significantly improves LSO outcomes?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces the Latent Exploration Score (LES), a method to mitigate over-exploration in Latent Space Optimization (LSO) by leveraging the decoder's approximation of the data distribution. LES is designed to constrain LSO to generate valid solutions without requiring additional training, architectural changes, or access to training data. The approach is evaluated across multiple tasks and VAE models, demonstrating improved solution quality and objective values."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in LSO—generating invalid solutions due to over-exploration—by proposing a practical and generalizable solution. LES's differentiability and computational tractability make it easily integrable into existing LSO pipelines. The experimental evaluation is extensive, covering 30 tasks, 22 VAEs, and five benchmarks, showing consistent improvements over baseline methods. The motivation and problem formulation are clear, and the paper highlights the importance of structure preservation in discrete optimization tasks."
          },
          "weaknesses": {
            "value": "The paper does not thoroughly analyze LES's scalability, particularly its cubic complexity with latent dimension, which may limit applicability to high-dimensional spaces. The experiments lack detailed comparisons of computational efficiency and fail to explain the exact metrics used to quantify 'high objective values.' The discussion of LES's theoretical properties is brief, and the paper does not address potential edge cases where LES might fail. Additionally, the lack of ablation studies on LES components limits understanding of its effectiveness."
          },
          "questions": {
            "value": "1. How does LES handle latent spaces with very high dimensions, given its cubic complexity? 2. What is the exact definition of the LES score, and how is it computed in practice? 3. Are there scenarios where LES might not improve performance, and how are these handled? 4. How does the numerical stability of the optimization procedure compare to existing methods? 5. What specific metrics are used to evaluate 'solution quality' and 'objective values,' and how do they align with prior work?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "5RZoYIT3u6": {
    "paper_id": "5RZoYIT3u6",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces PruneNet, a calibration-free model compression method that reformulates pruning as a policy learning process. By decoupling pruning from model architecture and leveraging intrinsic model properties, PruneNet achieves high compression ratios (30%) with minimal performance loss (80% zero-shot accuracy retention on LLaMA-2-7B) without relying on external calibration data."
          },
          "strengths": {
            "value": "Originality is strong through the policy learning formulation for pruning, which is a novel approach compared to traditional structured/unstructured methods. The experiments demonstrate significant practical improvements (e.g., 50% faster compression than SliceGPT) with clear metrics. The paper's structure is logical, and the problem statement addresses a critical challenge in LLM deployment. The claim of calibration-free compression has high significance for real-world applications."
          },
          "weaknesses": {
            "value": "The mechanism for preserving spectral structure is under-explained, making it hard to assess how this contributes to performance retention. The paper lacks ablation studies to isolate the impact of key components like the policy learner. The comparison with SliceGPT is limited - additional baselines (e.g., quantization methods) would strengthen claims. The theoretical justification for why policy learning outperforms calibration-based approaches is insufficient."
          },
          "questions": {
            "value": "1. How is the stochastic pruning policy learned? What specific model properties are used as inputs? 2. Can the authors elaborate on how spectral structure preservation is achieved mathematically? 3. Are there ablation studies showing the contribution of different components (e.g., policy learning vs. spectral preservation)? 4. How does PruneNet handle different model architectures beyond LLaMA and OPT? 5. What are the computational requirements for training the policy learner?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces PruneNet, a calibration-free model compression method that reformulates pruning as a policy learning process. It eliminates the need for external calibration datasets by learning a stochastic pruning policy based on intrinsic model properties, preserving spectral structure to minimize information loss. PruneNet achieves high compression ratios with minimal performance degradation on LLaMA-2-7B and multitask benchmarks."
          },
          "strengths": {
            "value": "Originality: The policy learning formulation for pruning is novel, decoupling pruning from model architecture. Quality: Experiments on LLaMA-2-7B demonstrate significant speed (15 minutes) and performance retention (80% zero-shot accuracy at 30% compression). Clarity: The problem statement and key results are well-articulated, with a structured comparison table. Significance: Addresses critical challenges in model compression for deployment on resource-constrained devices."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving key methodological details (e.g., how intrinsic properties are defined, spectral structure preservation) and experiments incomplete. The absence of calibration data is claimed as a strength, but the mechanism for determining parameter importance without external data is under-explained. Ablation studies, comparisons with additional baselines (e.g., SparseGPT, LLM-Pruner), and analysis of failure cases are missing. The claim of 84% zero-shot accuracy lacks detailed statistical validation."
          },
          "questions": {
            "value": "1. How exactly are 'intrinsic model properties' quantified for parameter importance assessment? 2. What specific spectral properties are preserved, and how is this enforced during pruning? 3. Are there ablation studies showing the contribution of policy learning vs. other components? 4. How does PruneNet handle varying model architectures beyond LLaMA-2-7B? 5. What is the computational overhead of the policy learner during pruning? 6. How does the method scale to even higher compression ratios (e.g., 50%)?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces PruneNet, a calibration-free model compression method that reformulates pruning as a policy learning process. By decoupling pruning from model architecture and using intrinsic properties to assess parameter importance, PruneNet achieves high compression ratios (e.g., 30% with 80% zero-shot performance retention on LLaMA-2-7B) without external calibration data. The approach preserves spectral structures to minimize information loss and claims faster compression compared to existing methods like SliceGPT."
          },
          "strengths": {
            "value": "Originality is strong through the policy learning formulation of pruning, which addresses calibration data dependency—a critical limitation in prior work. The method's efficiency (15-minute compression) and performance retention (84% zero-shot accuracy) are significant contributions. Clarity is maintained through structured sections, a comparative table, and clear problem statements. The significance lies in enabling scalable, hardware-agnostic compression for resource-constrained deployments, with empirical results demonstrating robustness across tasks."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical exposition of the policy learning framework, such as how the stochastic policy is trained without calibration data or the specific intrinsic properties used for parameter importance. Experimental comparisons are limited to SliceGPT, omitting other structured pruning methods like LLM-Pruner or Layer Collapse. The claim of 15-minute compression for LLaMA-2-7B requires validation with computational complexity analysis. The spectral structure preservation mechanism is vaguely described, and ablation studies to isolate key components are missing."
          },
          "questions": {
            "value": [
              "How is the stochastic pruning policy trained without calibration data? What objective function or reward mechanism is used?",
              "What specific intrinsic model properties (e.g., weight distributions, gradient statistics) are leveraged to assess parameter importance?",
              "Are there ablation studies demonstrating the contribution of spectral structure preservation versus other components?",
              "How does PruneNet handle varying compression ratios without retraining? Is the policy reusable across different sparsity levels?",
              "What is the exact computational complexity of PruneNet, and how does it achieve 50% faster compression than SliceGPT?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "5V8d2dVF1F": {
    "paper_id": "5V8d2dVF1F",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates the vulnerabilities of large language models (LLMs) to social bias attacks, including gender, racial, and religious biases. It evaluates techniques like prefix injection, refusal suppression, and learned attack prompts across models such as LLaMA2, GPT-3.5, and GPT-4, finding higher susceptibility to gender bias and increased vulnerability in larger models. The work also introduces cross-bias and multiple-bias attacks and proposes a simple defense mechanism."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely issue in AI ethics, focusing on bias vulnerabilities in LLMs. Its comprehensive evaluation of multiple models and attack techniques demonstrates practical relevance. The introduction of cross-bias and multiple-bias attacks adds novel dimensions to bias research. The ethical considerations and proposed defense method highlight the importance of responsible AI development."
          },
          "weaknesses": {
            "value": "The paper lacks detailed descriptions of attack implementations, such as how prefix injections or refusal suppression were specifically applied. Key metrics (e.g., 'jailbreak rate') are not rigorously defined or validated. The defense method is described as 'simple' without empirical evidence of its effectiveness. Experiments appear limited in scope (e.g., few models, no ablation studies) and lack comparisons to state-of-the-art bias mitigation techniques. The ethical safeguards mentioned are vague, raising concerns about potential misuse of the findings."
          },
          "questions": {
            "value": [
              "How were the attack prompts generated, and what criteria were used to ensure their effectiveness? Are they publicly available for reproducibility?",
              "What specific metrics were used to quantify 'jailbreak rate' and how were human assessments conducted? Were inter-rater reliability checks performed?",
              "How does the proposed defense method compare to existing bias mitigation approaches (e.g., adversarial training, post-hoc filtering)?",
              "What steps were taken to ensure the evaluation dataset is representative and free from inherent biases?",
              "How were ethical safeguards implemented beyond the generic statement about limiting data usage? Were there third-party reviews of the methodology?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates the vulnerability of large language models (LLMs) to social bias attacks, focusing on gender, racial, and religious biases. The authors propose three attack techniques—prefix injection, refusal suppression, and learned attack prompts—and evaluate their effectiveness across models like LLaMA2, GPT-3.5, and GPT-4. Key findings include higher susceptibility to gender bias, increased vulnerability in larger models, and the transferability of biases across dimensions."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely issue in AI ethics, offering a comprehensive evaluation of bias vulnerabilities. Its originality lies in exploring cross-bias and multiple-bias attacks, which are less studied. The methodology combines automated metrics (e.g., jailbreak rate) and human assessment, enhancing credibility. The clarity of the problem statement and structure is strong, with clear contributions and practical implications for model developers."
          },
          "weaknesses": {
            "value": "The paper lacks depth in explaining why larger models are more vulnerable, which could be due to architectural complexity or training data biases. The proposed defense method is briefly mentioned but not thoroughly analyzed or compared to existing approaches. The experimental scope is limited to a fixed set of models and bias types, with no discussion of generalizability. Additionally, the ethical safeguards described are vague, raising concerns about potential misuse of the findings."
          },
          "questions": {
            "value": [
              "How were the attack prompts generated? Were they manually crafted or learned through adversarial training?",
              "What specific criteria were used for human assessment of biased responses? How was inter-rater reliability ensured?",
              "How does the proposed defense method work, and how does it compare to existing bias mitigation techniques?",
              "Were there any limitations in the choice of models (e.g., exclusion of newer or proprietary models) that could affect the generalizability of results?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates the susceptibility of large language models (LLMs) to social bias attacks, focusing on gender, racial, and religious biases. The authors evaluate three attack techniques (prefix injection, refusal suppression, learned attack prompts) across models like LLaMA2, GPT-3.5, and GPT-4, finding that gender bias attacks are more effective and larger models exhibit higher vulnerability. They also introduce cross-bias and multiple-bias attacks and propose a simple defense method."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem in AI ethics, with significant implications for model safety. The experimental design is comprehensive, covering multiple models, bias types, and attack techniques. The introduction of cross-bias and multiple-bias attacks demonstrates originality in exploring transferability of vulnerabilities. The clarity of the structure and motivation is strong, though the ethical considerations are acknowledged thoughtfully."
          },
          "weaknesses": {
            "value": "The paper lacks detailed descriptions of the defense method, making it difficult to assess its effectiveness. Experimental results are not sufficiently contextualized (e.g., statistical significance, baseline comparisons). The human evaluation methodology is vague, and the paper does not address potential confounding factors in bias measurement. The ethical safeguards mentioned are overly generic, and the paper does not discuss limitations of the attack techniques or their real-world applicability."
          },
          "questions": {
            "value": "1. What is the exact implementation of the proposed defense method, and how was its effectiveness evaluated? 2. How were the human assessments conducted (e.g., sample size, criteria, calibration)? 3. Are the results statistically significant, and how do they compare to prior work on bias mitigation? 4. How were the attack prompts generated, and what measures were taken to ensure validity? 5. What are the limitations of the cross-bias and multiple-bias attack frameworks?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "5VK1UulEbE": {
    "paper_id": "5VK1UulEbE",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes FredNormer, a frequency-domain normalization module for non-stationary time series forecasting. The authors theoretically analyze how time-domain normalization uniformly scales frequency components, leading to suboptimal performance. They address this by introducing a stability metric based on the Coefficient of Variation (CV) to adaptively weight frequency components, improving forecasting accuracy while maintaining efficiency."
          },
          "strengths": {
            "value": "The paper provides a clear theoretical analysis of how time-domain normalization affects frequency components, which is a novel perspective. The proposed method is simple, model-agnostic, and claims to be efficient. The experiments demonstrate significant improvements on benchmark datasets, and the problem formulation of frequency-based normalization for non-stationary time series is impactful. The paper is well-structured with a focus on practical applicability."
          },
          "weaknesses": {
            "value": "The theoretical analysis lacks depth, particularly in proving why uniform scaling of frequencies is suboptimal. The stability metric (CV-based) is not thoroughly justified, and its effectiveness compared to alternative metrics is unclear. The experiments compare to baseline normalization methods but do not include state-of-the-art frequency-domain approaches, limiting the evaluation of true novelty. The paper also omits ablation studies to validate the contribution of individual components (e.g., the statistical metric vs. the weighting layer). The claimed 33.3% and 55.3% improvements lack detailed statistical significance testing."
          },
          "questions": {
            "value": [
              "How does the CV-based stability metric explicitly capture frequency importance compared to other metrics (e.g., variance, entropy)?",
              "What specific frequency components (e.g., high vs. low frequency) are being emphasized, and how does this align with the non-stationary characteristics of the data?",
              "Are there ablation studies showing the contribution of the statistical metric versus the learnable weighting layer?",
              "Why does the paper not compare to other frequency-domain methods (e.g., Fourier-based models) that explicitly model temporal patterns?",
              "How does FredNormer handle varying time series lengths or missing data, which are common in real-world scenarios?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes FredNormer, a frequency-domain normalization method for non-stationary time series forecasting. It addresses limitations of time-domain normalization by theoretically analyzing how existing methods uniformly scale frequency components and introduces a stability metric based on the Coefficient of Variation (CV) to adaptively weight key frequencies. The method is presented as a plug-and-play module with empirical improvements on benchmark datasets."
          },
          "strengths": {
            "value": "The paper presents a novel problem formulation by addressing non-stationarity through frequency-domain analysis, which is underexplored in existing normalization methods. The theoretical analysis of time-domain normalization's limitations is a strong contribution, and the proposed stability metric (CV-based) offers a fresh perspective. The method's simplicity, efficiency, and model-agnostic design are practical advantages. The experiments demonstrate significant performance gains on specific datasets, suggesting potential value for forecasting tasks."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous baseline comparisons, such as against frequency-aware models or alternative normalization techniques beyond z-score. The theoretical analysis is superficial, with no detailed proofs or derivations provided. Experimental validation is limited to a narrow set of datasets (e.g., ETTm2, Traffic) and does not include ablation studies or analysis of edge cases. The claims about efficiency improvements (60-70%) are unsupported by concrete computational metrics. The figures and methodology descriptions are incomplete, with placeholder images and truncated sections."
          },
          "questions": {
            "value": [
              "What specific baselines were used for comparison, and how do they relate to prior work on frequency-domain modeling?",
              "How was the stability metric (CV-based) validated against alternative metrics, and what justifies its superiority?",
              "Are there any limitations to the CV-based approach in handling different types of non-stationarity?",
              "What is the computational complexity of FredNormer, and how does it scale with larger time series?",
              "How was the linear projection layer designed, and what ablation studies were conducted to verify its necessity?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces FredNormer, a frequency-domain normalization method for non-stationary time series forecasting. It addresses limitations of time-domain normalization by analyzing frequency components and adaptively weighting key frequencies using a stability metric. The approach shows significant improvements in MSE on benchmark datasets compared to existing normalization methods."
          },
          "strengths": {
            "value": "The paper's originality lies in applying frequency-domain analysis to normalization, a novel approach for non-stationary time series. The theoretical analysis of how time-domain normalization affects frequency components is rigorous. The method's efficiency and model-agnostic design (as a plug-and-play module) are strong practical advantages. The experimental results demonstrate substantial performance gains, particularly on challenging datasets like ETTm2, and the paper clearly articulates the importance of frequency stability for robust forecasting."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed comparison with existing frequency-based methods (e.g., Fourier-domain approaches) that could contextualize FredNormer's novelty. The stability metric (Coefficient of Variation) is not thoroughly justified in the context of time series forecasting. The experimental section could include ablation studies to isolate the impact of frequency weighting versus normalization. Additionally, the claim of 'plug-and-play' efficiency needs more concrete evidence (e.g., parameter counts, runtime comparisons)."
          },
          "questions": {
            "value": [
              "How does FredNormer specifically handle different types of non-stationarity (e.g., trend vs. seasonality) compared to time-domain methods?",
              "What is the theoretical basis for using the Coefficient of Variation (CV) as a stability metric in the frequency domain? Are there alternative metrics that could be more effective?",
              "Are there cases where frequency-domain normalization might degrade performance compared to time-domain methods, and how does FredNormer mitigate this?",
              "How does the linear projection layer in FredNormer capture data-specific properties? Could this be replaced with a more complex architecture without sacrificing efficiency?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "5WtovCb1ZE": {
    "paper_id": "5WtovCb1ZE",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces 'Self-Proving models' that generate outputs accompanied by interactive proofs verifiable by a algorithm $V$, ensuring worst-case soundness guarantees. The authors propose Transcript Learning (TL) and Reinforcement Learning from Verifier Feedback (RLVF) to train such models, demonstrating their effectiveness in teaching a transformer to compute GCDs with high verifiability."
          },
          "strengths": {
            "value": "Originality lies in applying interactive proof systems to model verification, a novel approach for ensuring trust in individual predictions. Theoretical contributions include convergence bounds under specific assumptions, while empirical results show significant improvements in verifiability (96% with Annotated-TL). The paper is clearly structured with illustrative figures and tables, and the problem of input-specific trustworthiness is highly significant for safety-critical applications."
          },
          "weaknesses": {
            "value": "The scope is limited to GCD computation and simple verification algorithms, raising questions about scalability to complex tasks. Theoretical guarantees rely on strong assumptions (convexity, Lipschitzness) that may not hold in practice. The empirical evaluation focuses narrowly on one domain, and the paper does not address how to design verification algorithms for new tasks. Additionally, the connection to prior work on formal proof generation could be more explicitly discussed."
          },
          "questions": {
            "value": "How do the proposed methods scale to more complex decision problems beyond GCD? What are the limitations of the verification algorithm $V$ in this framework, and how might they affect real-world applicability? Could the theoretical assumptions (e.g., convexity) be relaxed while maintaining convergence guarantees? How does the model handle adversarial inputs that intentionally mislead the verification process?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 5
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces 'Self-Proving models' that generate outputs accompanied by interactive proofs verifiable by a algorithm $V$, ensuring worst-case soundness guarantees. The authors propose two training methods (Transcript Learning and Reinforcement Learning from Verifier Feedback) and demonstrate their effectiveness in training a transformer to compute GCD with high verifiability."
          },
          "strengths": {
            "value": "Originality: The concept of Self-Proving models with formal interactive proofs represents a novel intersection of machine learning and proof systems. Quality: The paper provides theoretical convergence guarantees under specific assumptions and includes empirical results on a non-trivial arithmetic task. Clarity: The problem formulation and methodology are well-structured, with clear definitions of guarantees. Significance: The work addresses a critical gap in model reliability, offering a framework for verifiable outputs with strong theoretical foundations."
          },
          "weaknesses": {
            "value": "Theoretical assumptions (convexity, Lipschitzness) may not hold for complex models like transformers. Experimental validation is limited to GCD computation, with insufficient analysis of generalization to other tasks. The verification algorithm $V$'s design and its interaction with the model are under-specified. The 'Annotated Transcript Learning' method lacks detailed implementation guidance, making reproducibility challenging. The paper does not address potential limitations of interactive proofs (e.g., computational overhead) or compare with alternative verifiability approaches."
          },
          "questions": {
            "value": [
              "How is the verification algorithm $V$ designed for GCD computation? What guarantees does it provide?",
              "Are the theoretical convergence bounds applicable to non-convex models like transformers? How were the assumptions validated?",
              "What specific annotations are used in Annotated Transcript Learning, and how are they generated?",
              "How does the performance of Self-Proving models compare to baselines that generate natural language explanations?",
              "What are the computational costs of the interactive proof process, and how do they scale with input size?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces 'Self-Proving models' that generate outputs accompanied by interactive proofs verifiable by a separate algorithm, ensuring worst-case soundness guarantees. The authors propose two training methods—Transcript Learning and Reinforcement Learning from Verifier Feedback—and demonstrate their effectiveness in training a transformer to compute GCDs with formal verification."
          },
          "strengths": {
            "value": "Originality lies in applying interactive proof systems to machine learning for per-input correctness verification, a novel approach compared to average-case accuracy metrics. The paper provides rigorous convergence bounds under specific assumptions and connects to existing work on Prover-Verifier Games and formal proof systems. Empirical results on GCD computation are concrete, and the methodology is clearly explained with well-structured figures and tables. The significance is high, addressing trust in model outputs, a critical challenge in AI."
          },
          "weaknesses": {
            "value": "The empirical evaluation is limited to GCD computation, a narrow domain, with no discussion of scalability to more complex tasks. The theoretical analysis relies on restrictive assumptions (convexity, Lipschitzness) that may not hold in practice. The verification algorithm's design is not detailed, leaving unclear how it generalizes to other problems. Additionally, the paper does not address cases where the model cannot generate a valid proof, nor does it compare against alternative verification methods."
          },
          "questions": {
            "value": "How is the verification algorithm constructed for tasks beyond GCD? Are the theoretical assumptions (e.g., convexity) feasible in real-world scenarios? What are the computational costs of interactive proofs during inference? How does the model handle inputs where no valid proof exists? Are there ablation studies comparing TL, RLVF, and ATL under varying conditions?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "5YRw1m6GSz": {
    "paper_id": "5YRw1m6GSz",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces FlatLand, a novel personalized federated learning (PFL) method that leverages tailored Lorentz spaces to address data heterogeneity. By embedding clients' data in hyperbolic Lorentz spaces with personalized curvatures, the approach aims to capture complex geometric properties of heterogeneous data, avoiding the limitations of Euclidean spaces. The method employs a parameter decoupling strategy for server aggregation without client similarity estimation, demonstrating superior performance on federated graph learning tasks."
          },
          "strengths": {
            "value": "Originality: The paper proposes the first integration of Lorentz space (a hyperbolic geometry) into PFL, addressing a critical gap in handling heterogeneous data. Quality: The methodology is theoretically grounded in hyperbolic geometry and includes a parameter decoupling strategy to reduce heterogeneity interference. Clarity: The abstract and introduction clearly articulate the problem, motivation, and contributions. Significance: The work advances PFL by offering a new geometric framework for personalized modeling, with potential implications for real-world applications involving complex data structures."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, as key sections (e.g., methodology, results) are cut off. The parameter decoupling strategy is not thoroughly explained, and its effectiveness remains unverified. The claim of being 'the first' is not contextualized against related work, which is partially incomplete. Additionally, the paper does not address how curvature adjustments are optimized for each client or validate performance on non-graph tasks."
          },
          "questions": {
            "value": [
              "What is the exact mechanism of the parameter decoupling strategy, and how does it avoid client similarity estimation while aggregating shared parameters?",
              "How are the curvatures of Lorentz spaces determined for each client, and what optimization techniques are used?",
              "What specific datasets and metrics were used to validate the empirical results, and how do they compare to baselines?",
              "Are there ablation studies demonstrating the contribution of Lorentz space versus Euclidean space in the proposed framework?",
              "How does the method handle computational overhead and scalability in large-scale federated settings?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces FlatLand, a novel personalized federated learning (PFL) method that leverages tailored Lorentz spaces (hyperbolic geometry) to address data heterogeneity. By embedding clients' data in personalized Lorentz spaces with unique curvatures, the approach aims to better capture hierarchical and scale-free structures in heterogeneous data. The method includes a parameter decoupling strategy for server aggregation without client similarity estimation, demonstrating superior performance on federated graph learning tasks."
          },
          "strengths": {
            "value": "The paper presents a highly original approach by applying hyperbolic geometry (specifically Lorentz spaces) to PFL, a novel direction in the field. The theoretical motivation is strong, linking data heterogeneity to hyperbolic curvature properties. The parameter decoupling strategy is creative and addresses a key challenge in PFL. The empirical results on graph tasks show clear improvements over Euclidean baselines, particularly in low-dimensional settings. The writing is clear, with good contextualization of the problem and prior work."
          },
          "weaknesses": {
            "value": "The paper lacks thorough experimental validation across diverse data types beyond graph learning. The parameter decoupling strategy's mechanism is not well-explained, and there is no ablation study to isolate its contribution. The theoretical analysis is minimal, with only brief mentions of Ricci curvature without deeper mathematical justification. The claim of being 'the first' to use Lorentz geometry in PFL may be overstated, as related hyperbolic FL work (e.g., FedHGCN) exists but is not adequately addressed. The paper is cut off mid-section, leaving critical details incomplete."
          },
          "questions": {
            "value": "1. How does the parameter decoupling strategy explicitly mitigate heterogeneity interference during aggregation? 2. Are there existing hyperbolic FL methods (e.g., FedHGCN) that the authors failed to cite or compare against? 3. What is the impact of varying curvature parameters on performance, and how are they optimized? 4. How is the time-like dimension in Lorentz space practically implemented in neural networks? 5. Why are the empirical results limited to graph learning tasks, and what evidence supports generalizability to other data modalities?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces FlatLand, a novel personalized federated learning (PFL) method that leverages tailored Lorentz spaces to address data heterogeneity. By embedding clients' data in hyperbolic geometries with personalized curvatures, the approach aims to better capture hierarchical and scale-free structures in real-world data, while a parameter decoupling strategy enables direct aggregation of shared information without client similarity estimation."
          },
          "strengths": {
            "value": "The paper presents a highly original idea by integrating hyperbolic geometry into PFL, addressing a critical gap in modeling heterogeneous data. The theoretical foundation is well-reasoned, connecting graph data heterogeneity to hyperbolic curvature. The method's simplicity and interpretability are notable strengths, with clear motivation for using Lorentz spaces. The empirical results on graph learning tasks suggest potential superiority in low-dimensional settings, and the paper provides meaningful analysis of Ricci curvature in real-world datasets."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive experimental validation, including comparisons with existing hyperbolic or Euclidean PFL methods. The parameter decoupling strategy is not sufficiently detailed, and the theoretical guarantees for its effectiveness remain unclear. The claims about 'superior performance' are based on limited results, with no ablation studies or analysis of scalability. The incomplete nature of the paper (e.g., truncated references and figures) raises concerns about the rigor of the evaluation and reproducibility."
          },
          "questions": {
            "value": [
              "What specific baselines were compared against FlatLand, and why were they chosen? How does it perform against other hyperbolic PFL approaches?",
              "Can the authors provide a detailed explanation of the parameter decoupling strategy and its mathematical formulation?",
              "Are there ablation studies demonstrating the impact of Lorentz space embeddings versus Euclidean alternatives?",
              "How does the method handle computational overhead, especially with the claimed 'tailored' Lorentz spaces for each client?",
              "What is the theoretical basis for preferring Lorentz space over other hyperbolic geometries (e.g., Poincaré balls)?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "5ZkuWAbxzT": {
    "paper_id": "5ZkuWAbxzT",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper provides a theoretical analysis of KL-regularized contextual bandits and reinforcement learning from human feedback (RLHF), demonstrating improved sample complexity bounds compared to prior work. The authors propose a two-stage mixed sampling strategy that reduces dependence on data coverage coefficients and establishes sharp convergence rates under specific assumptions."
          },
          "strengths": {
            "value": "The paper introduces novel theoretical analysis of KL-regularization in RLHF, addressing a gap in understanding its fundamental benefits. The sharp sample complexity bounds (O(1/ε)) represent a significant improvement over existing O(1/ε²) results. The work systematically explores the interplay between KL-regularization and data coverage, offering a structured approach to online RLHF. The clarity of problem formulation and theoretical derivation is strong, with explicit connections to practical RLHF applications."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation to support its theoretical claims, which is critical for a machine learning conference. The assumptions about reward function classes and coverage coefficients are not thoroughly discussed, leaving questions about practical applicability. The two-stage sampling strategy's implementation details and computational efficiency remain unclear. The paper does not compare its bounds to existing works in depth, making it difficult to assess the magnitude of improvement."
          },
          "questions": {
            "value": [
              "How do the authors handle the interplay between the KL-regularization coefficient η and the coverage coefficient D in their analysis? Are there explicit trade-offs between these parameters?",
              "What are the practical implications of the O(1/ε) sample complexity? How does this translate to real-world RLHF systems with finite resources?",
              "The paper mentions 'sufficient coverage from the reference policy' but does not define or quantify this concept. How is coverage measured, and what guarantees does it provide?",
              "Are the theoretical bounds tight, and what are the limitations of the assumptions (e.g., constant reward scale, specific structure of the reward function class)?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper provides a theoretical analysis of KL-regularized contextual bandits and reinforcement learning from human feedback (RLHF), demonstrating that KL-regularization can achieve an improved sample complexity of O(1/ε) compared to the standard O(1/ε²) in unregularized settings. The authors propose a two-stage mixed sampling strategy that reduces dependence on data coverage coefficients and analyze the role of coverage in online RLHF."
          },
          "strengths": {
            "value": "The paper makes original contributions by providing the first sharp analysis of KL-regularized contextual bandits and RLHF, revealing improved sample complexity bounds. The theoretical framework adapts to the optimization landscape of reverse-KL regularization, addressing a gap in prior work. The two-stage sampling strategy is a novel approach to mitigate coverage dependencies. The paper clearly contextualizes its work within existing RLHF literature and highlights the significance of KL-regularization in policy optimization."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation to support its theoretical claims, which limits the ability to assess practical relevance. The analysis assumes specific structural properties (e.g., constant reward scale) that may not hold in real-world scenarios. The comparison to existing methods is limited, and the paper does not address how the proposed bounds scale with problem complexity or hyperparameters like the KL-regularization coefficient η. The discussion of data coverage's role is abstract and lacks concrete examples."
          },
          "questions": {
            "value": "1. How do the theoretical bounds scale with the KL-regularization coefficient η in practice? 2. Are the assumptions about the reward function class (e.g., constant reward scale) realistic for real-world RLHF applications? 3. What are the practical implications of the two-stage sampling strategy, and how does it compare to existing exploration-exploitation methods? 4. How sensitive are the results to violations of the coverage assumption, and what guarantees hold in partially covered scenarios?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper provides a theoretical analysis of KL-regularized contextual bandits and RLHF, claiming to achieve a sharper sample complexity bound of O(1/ε) compared to the standard O(1/ε²) in unregularized settings. The authors propose a two-stage mixed sampling strategy that reduces dependence on the coverage coefficient and analyze the role of data coverage in online RLHF."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel fine-grained analysis of KL-regularization in contextual bandits and RLHF, addressing a gap in theoretical understanding. Quality: The theoretical framework is well-structured, with rigorous lower/upper bounds for sample complexity. Clarity: The paper is clearly organized, with explicit sectioning of contributions and problem formulations. Significance: Improving sample complexity bounds has direct implications for the efficiency of RLHF algorithms, which is a critical area in alignment research."
          },
          "weaknesses": {
            "value": "Theoretical claims lack empirical validation, making it difficult to assess practical relevance. The paper does not compare its bounds to existing work in detail (e.g., specific constants or dependencies). The two-stage sampling strategy's practical implementation and limitations are not discussed. The analysis assumes a constant reward scale, which may not hold in real-world scenarios. The role of the KL-regularization coefficient η is not thoroughly explored in the context of algorithm design."
          },
          "questions": {
            "value": "How do the theoretical sample complexity bounds translate to practical performance on real-world RLHF tasks? What are the exact assumptions about the reward function class and coverage coefficient that enable the O(1/ε) bound? How does the two-stage sampling strategy handle non-ideal coverage scenarios? Are the derived bounds tight, or are there gaps between the lower and upper bounds? How does the proposed method compare to existing RLHF algorithms in terms of computational efficiency?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "5btqauRdz0": {
    "paper_id": "5btqauRdz0",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces STAGE, a method for zero-shot generalization in graph neural networks (GNNs) by learning statistical dependencies between node attributes rather than the attributes themselves. This approach enables GNNs to generalize across unseen attribute domains by leveraging invariant statistical relationships, demonstrated through improved performance on link prediction and node classification tasks."
          },
          "strengths": {
            "value": "Originality: STAGE's focus on statistical dependencies instead of raw attributes addresses a critical gap in zero-shot generalization for GNNs, offering a novel framework for domain-invariant representation learning. Quality: The theoretical foundation connecting maximal invariants to statistical dependencies is rigorous, with clear claims about provable generalization. Clarity: The paper effectively explains the method through diagrams and logical steps, though some details are truncated. Significance: Zero-shot generalization across attribute domains is a major challenge in graph ML, and STAGE's empirical improvements (40-103% relative gain in Hits@1) highlight its practical relevance."
          },
          "weaknesses": {
            "value": "The paper lacks specific details about the datasets used for experiments, making it difficult to assess the generality of results. The comparison with baselines is vague (e.g., 'state-of-the-art' without naming methods). Theoretical proofs are summarized briefly, with limited discussion of assumptions or edge cases. The method's handling of heterogeneous features (continuous vs. categorical) is not explicitly addressed. Additionally, the computational efficiency and scalability of STAGE remain unclear."
          },
          "questions": {
            "value": "1. Could the authors specify the datasets used in experiments and their attribute domain diversity? 2. What are the exact baselines compared (e.g., textification approaches, domain adaptation methods)? 3. How does STAGE handle categorical vs. continuous features, and are there any feature-specific limitations? 4. Are there ablation studies demonstrating the contribution of statistical dependency learning versus other components? 5. What is the computational complexity of STAGE compared to standard GNNs?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 5
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces STAGE, a method for zero-shot generalization in graph neural networks (GNNs) across distinct attribute domains. STAGE focuses on learning statistical dependencies between node attributes rather than raw attribute values, enabling generalization to unseen feature spaces by leveraging invariant properties of these dependencies."
          },
          "strengths": {
            "value": "Originality: STAGE's focus on statistical dependencies instead of raw attributes is a novel approach to zero-shot generalization in GNNs. The theoretical connection to maximal invariants and statistical dependencies provides a strong foundation. Quality: The method's design to handle heterogeneous features and domain shifts is well-structured. Clarity: The paper clearly outlines the motivation, problem, and technical approach. Significance: Addressing zero-shot generalization across attribute domains is a critical challenge in graph ML, and STAGE's potential to improve this area is highly relevant."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-experiment, leaving key details about baselines, experimental setup, and results incomplete. The theoretical proofs are not fully elaborated, and the claims about generalization to arbitrary graphs may be overstated. The practical implementation details of STAGE-edge-graph construction and GNN training are not sufficiently explained. The empirical validation lacks ablation studies or analysis of failure cases."
          },
          "questions": {
            "value": "1. How does the STAGE-edge-graph construction handle high-dimensional or sparse attributes? 2. What specific baselines were used for comparison, and how do they differ from STAGE in design? 3. Are there limitations to the types of statistical dependencies STAGE can capture? 4. How does the method scale to large graphs with millions of nodes/edges? 5. What is the computational cost of constructing and processing STAGE-edge-graphs compared to standard GNNs?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces STAGE, a method for zero-shot generalization of Graph Neural Networks (GNNs) across distinct attribute domains. STAGE focuses on learning statistical dependencies between node attributes rather than raw attribute values, enabling generalization to unseen feature domains by capturing invariant relationships. The approach constructs STAGE-edge-graphs to model these dependencies and leverages GNNs to process them, achieving significant improvements in link prediction and node classification tasks."
          },
          "strengths": {
            "value": "The paper presents a novel approach to a critical problem in GNNs: zero-shot generalization across disparate attribute domains. The theoretical foundation in maximal invariants and statistical dependencies is robust, providing a clear rationale for the method's design. The methodology is well-explained, with clear figures illustrating the STAGE-edge-graph construction and processing pipeline. The empirical results, though partially cut off, claim substantial improvements over baselines, highlighting the method's practical significance. The paper also addresses key challenges in heterogeneous feature handling and context-dependent attribute interpretation, demonstrating strong originality in problem formulation."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-experiment, leaving critical details about the datasets, baseline comparisons, and quantitative results incomplete. This makes it difficult to fully assess the empirical validity of the claims. Additionally, the theoretical analysis of provable generalization is brief, with limited discussion of edge cases or limitations. The computational complexity of constructing STAGE-edge-graphs for large graphs is not addressed, raising questions about scalability. The paper also lacks ablation studies to isolate the contribution of specific components (e.g., statistical dependency modeling vs. GNN architecture)."
          },
          "questions": {
            "value": [
              "What specific datasets were used in the experiments, and how do they differ in terms of attribute domains and feature types?",
              "How do the results compare to state-of-the-art methods in detail (e.g., exact metrics, statistical significance)?",
              "Are there any limitations or failure cases where STAGE does not generalize well, and how does the theoretical analysis address these?",
              "How does STAGE handle high-dimensional or sparse feature spaces, and what is the computational cost of constructing STAGE-edge-graphs?",
              "What ablation studies were performed to validate the importance of statistical dependency modeling versus other components of the method?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "5cPEkoHHyG": {
    "paper_id": "5cPEkoHHyG",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces MetaInv, a hybrid framework that dynamically combines iterative methods (like i-ResNet) and analytical approaches (like DipDNN) to address limitations in invertible neural networks (INNs). The framework adapts in real-time based on task-specific signals, aiming to balance flexibility, invertibility, and performance across diverse applications."
          },
          "strengths": {
            "value": "The paper identifies a critical problem in INNs—trade-offs between flexibility and invertibility—and proposes a novel solution with clear practical motivation. The hybrid approach is theoretically grounded, and the framework's adaptability is a strong contribution. The paper is well-structured, with a clear problem statement, related work, and a logical flow. The emphasis on real-world applications (e.g., control systems, scientific computing) highlights its significance. The dynamic switching mechanism between i-ResNet and DipDNN is a creative and potentially impactful idea."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, as the content is cut off, leaving key results and comparisons unexplored. The novelty of the hybrid approach is not fully justified, as similar ideas (e.g., adaptive architectures like AdaNet) are mentioned but not critically compared. The theoretical analysis of why the meta-inverse framework improves performance is minimal. Additionally, the task-driven signal for model selection is not elaborated, raising questions about its feasibility and generalizability. The paper also does not address potential limitations, such as increased computational overhead from dynamic switching."
          },
          "questions": {
            "value": "1. What specific benchmarks and datasets were used to evaluate MetaInv, and how does it compare to state-of-the-art methods in terms of metrics like inverse consistency and computational efficiency? 2. How is the task-specific signal for model selection determined—through explicit engineering, learned parameters, or another mechanism? 3. Are there ablation studies demonstrating the contribution of the dynamic switching component versus static hybrid models? 4. How does MetaInv handle scenarios where the task requirements are ambiguous or evolve over time? 5. What are the theoretical guarantees (if any) for the stability and invertibility of the combined framework?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes MetaInv, a hybrid framework that dynamically combines iterative (i-ResNet) and analytical (DipDNN) invertible neural network approaches. The method adapts to task-specific requirements by switching between architectures to balance flexibility, invertibility, and performance. The authors claim improved forward accuracy, inverse consistency, and computational efficiency across diverse domains."
          },
          "strengths": {
            "value": "The paper identifies a meaningful problem gap between iterative and analytical invertible networks, which is well-motivated by real-world applications. The hybrid framework concept is novel and addresses complementary strengths of existing methods. The introduction of task-driven switching mechanisms shows potential for practical adaptability. The paper is well-structured with clear connections to prior work and a logical flow from problem analysis to solution design."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of the meta-inverse mechanism, particularly how task-specific signals are derived or how the switching logic is implemented. Experimental validation is superficial - while the abstract mentions 'extensive experiments,' the provided content only includes a figure without quantitative results. The claims about 'significant outperformance' over baselines are not supported by concrete metrics or comparisons. The analysis of failure modes is limited to qualitative descriptions without empirical evidence."
          },
          "questions": {
            "value": [
              "How is the task-specific signal for model switching determined? Is it handcrafted or learned?",
              "What are the exact metrics used to evaluate forward accuracy, inverse consistency, and computational efficiency?",
              "Are there ablation studies demonstrating the contribution of the hybrid framework vs. individual components?",
              "How does MetaInv handle cases where neither i-ResNet nor DipDNN is optimal?",
              "What theoretical guarantees exist for the stability of the hybrid approach under dynamic switching?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces MetaInv, a hybrid framework that dynamically combines iterative (i-ResNet) and analytical (DipDNN) invertible neural network (INN) methods to address their respective limitations. The framework adapts in real-time based on task-specific signals, aiming to balance flexibility, invertibility, and performance across diverse inverse learning tasks."
          },
          "strengths": {
            "value": "The paper identifies a critical gap in INN design, offering a novel solution to reconcile the trade-offs between iterative and analytical methods. The proposed framework's adaptability addresses practical challenges in real-world applications requiring both precision and flexibility. The work is well-motivated, with clear connections to prior art and a structured problem formulation. The abstract mentions 'extensive experiments,' suggesting potential significance for domains like control systems and scientific computing."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with key sections (e.g., methodology details, experiments, ablation studies) missing. The dynamic switching mechanism between i-ResNet and DipDNN is not elaborated, leaving unclear how task-specific signals are defined or implemented. The claims of 'outperformance' lack empirical validation, and the paper does not address potential limitations of hybrid approaches (e.g., increased complexity, computational overhead). The novelty of combining these methods is not sufficiently distinguished from prior work like AdaNet or hybrid control systems."
          },
          "questions": {
            "value": "1. How is the task-specific signal for model switching defined and extracted? 2. What metrics are used to evaluate forward accuracy, inverse consistency, and computational efficiency? 3. Are there ablation studies demonstrating the contribution of each component (i-ResNet vs. DipDNN)? 4. How does MetaInv handle ambiguous or overlapping task requirements? 5. What are the specific computational costs of the dynamic switching mechanism compared to standalone methods?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "5f0n5yi8qK": {
    "paper_id": "5f0n5yi8qK",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes using video instructions instead of text to enhance generalization in online reinforcement learning (RL) for open-ended tasks, particularly in environments like Minecraft. The authors introduce a dual-path encoder-decoder architecture with a novel attention layer to integrate semantic and visual features, enabling better zero-shot generalization. The approach is evaluated on a Minecraft benchmark, showing superior performance compared to existing methods."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel modality (video instructions) for RL, addressing limitations of text-based methods. The dual-path attention mechanism is a creative solution to bridge semantic and visual information. Significance: The work tackles the critical challenge of building generalist agents in open-ended environments, which is highly relevant to RL research. Clarity: The problem formulation and motivation are well-articulated, with a logical flow from problem statement to method and experiments. Quality: The paper highlights a promising direction for improving RL generalization, with clear claims about zero-shot performance."
          },
          "weaknesses": {
            "value": "The paper is truncated, leaving critical details about the experimental setup, ablation studies, and comparisons with baselines incomplete. The dual-path architecture's implementation and training process are not sufficiently explained, making it hard to assess its effectiveness. The claim of outperforming 'almost all other models' lacks specific metrics or benchmarks. The paper also does not address potential limitations of video instructions (e.g., data requirements, computational costs)."
          },
          "questions": {
            "value": [
              "How exactly does the dual-path attention layer integrate semantic and visual features? What are the specific design choices (e.g., attention mechanisms, feature fusion strategies)?",
              "What tasks were used in the Minecraft benchmark, and how were they evaluated? Are there quantitative results comparing the proposed method to baselines like VPT, STEVE1, or GROOT?",
              "How does the model handle the complexity of Minecraft's environment, such as sparse rewards or dynamic changes? Are there ablation studies showing the contribution of the dual-path architecture?",
              "What are the computational and data requirements for training with video instructions? How does this compare to text-based approaches in terms of efficiency?",
              "The paper mentions overfitting in latent spaces. How does the dual-path architecture specifically mitigate this issue? Are there analyses of the latent representations?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes using video instructions instead of text instructions to improve the generalization of online reinforcement learning (RL) agents in open-ended environments like Minecraft. The authors introduce a dual-path encoder-decoder architecture that processes both semantic and visual features, aiming to enhance the agent's ability to follow instructions and generalize to unseen tasks. They demonstrate zero-shot performance improvements over existing methods in their Minecraft benchmark."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in RL: generalization to untrained tasks. The idea of using video instructions to align with the environment's modality is novel and well-motivated. The dual-path architecture introduces a creative solution to integrate semantic and visual information, which could significantly improve instruction-following capabilities. The focus on open-ended environments like Minecraft is timely and relevant. The paper also highlights the limitations of text-based instruction methods, providing a clear justification for their approach."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about the experimental setup, baseline comparisons, and ablation studies incomplete. The claims about 'excellent zero-shot generalization' lack supporting data, making it impossible to assess the validity of the results. The dual-path architecture's design and training process are not sufficiently explained, leaving questions about how it avoids overfitting. Additionally, the paper does not address potential scalability issues when applying the method to more complex tasks or environments."
          },
          "questions": {
            "value": "1. What specific video instruction format and preprocessing pipeline were used? How were video sequences aligned with environment states?\n2. Which baselines were compared against (e.g., VPT, STEVE1, GROOT)? What metrics were used to evaluate zero-shot performance?\n3. How was the dual-path attention layer implemented? Were there ablation studies to validate its necessity?\n4. How was overfitting mitigated during training on a small number of tasks? What hyperparameters were used for the PPO component?\n5. What is the exact definition of 'generalization' in the Minecraft benchmark? How were unseen tasks generated and evaluated?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes using video instructions instead of text to enhance the generalization of online reinforcement learning (RL) agents in open-world environments like Minecraft. The authors introduce a dual-path encoder-decoder architecture with a new attention layer to enable semantic and visual feature interaction, demonstrating strong zero-shot generalization on new tasks."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in online RL: generalization to untrained tasks in open-ended environments. The use of video instructions aligns modalities (video input and environment) to avoid complex text-video alignment, a novel approach. The dual-path architecture introduces a structured way to combine semantic and visual information, which is theoretically sound. The experimental results on Minecraft show strong zero-shot performance, suggesting practical relevance. The paper also clearly motivates the problem and contextualizes its contribution within existing RL and multi-task learning literature."
          },
          "weaknesses": {
            "value": "The methodology section is incomplete (cut off mid-sentence), limiting the ability to assess technical details like how video instructions are processed, how the dual-path architecture is implemented, or how the attention layer operates. The experiments lack sufficient comparison with state-of-the-art methods (e.g., GROOT or STEVE1) and ablation studies on the dual-path component. The paper does not thoroughly analyze why video instructions outperform text, nor does it address potential limitations of video-based approaches (e.g., data requirements or computational costs)."
          },
          "questions": {
            "value": "1. How are video instructions preprocessed and encoded into the model? What is the role of the new attention layer in the dual-path architecture? 2. Are there ablation studies demonstrating the necessity of the dual-path design compared to a single-path baseline? 3. How does the model handle variations in video quality or frame rate, which could affect generalization? 4. What specific metrics were used to evaluate zero-shot performance, and how do they compare to existing benchmarks? 5. How does the approach scale to more complex tasks or larger environments beyond Minecraft?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "5iUUorHeM3": {
    "paper_id": "5iUUorHeM3",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces the CIRCUIT dataset, a benchmark for evaluating Large Language Models' (LLMs) reasoning capabilities in analog circuit design. The dataset contains 510 question-answer pairs focusing on circuit topology understanding, with a unique unit-test-like evaluation framework. Experiments show that even advanced models like GPT-4o struggle with multi-level reasoning tasks, achieving 48.04% accuracy on numerical answers but only 27.45% on unit tests."
          },
          "strengths": {
            "value": "The paper's originality lies in creating a domain-specific benchmark for analog circuit reasoning, addressing a critical gap in LLM evaluation. The dataset design with template-based questions and unit-test grouping is innovative, enabling nuanced analysis of model performance. The clarity of the problem statement, methodology, and examples (e.g., Figure 2) is strong, and the significance of highlighting LLM limitations in analog design is substantial for advancing AI applications in semiconductor engineering."
          },
          "weaknesses": {
            "value": "The dataset's scope appears limited to basic circuit topology understanding, with no indication of complex tasks like optimization or non-linear analysis. The evaluation only tests three models (GPT-4o, GPT Turbo, Gemini 1.5 Pro), restricting generalizability. The unit-test framework's implementation details are sparse, and the paper lacks ablation studies to isolate the impact of netlists or prompt variations. The numerical setups seem simplistic, potentially underestimating model capabilities."
          },
          "questions": {
            "value": "1. How were the unit tests structured to measure multi-level reasoning specifically? 2. Are there plans to expand the dataset to include advanced analog design tasks (e.g., op-amp analysis, non-linear components)? 3. What ablation studies were conducted to determine the effectiveness of netlists vs. diagrams in improving model performance? 4. How were the numerical setups generated to ensure diversity while maintaining consistency with template questions?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces the CIRCUIT dataset, a benchmark for evaluating Large Language Models (LLMs) on analog circuit interpretation and reasoning tasks. It includes 510 question-answer pairs with templates, numerical setups, and netlists, designed to assess models' ability to reason about circuit topologies. The study evaluates GPT-4o, GPT Turbo, and Gemini 1.5 Pro, highlighting that even state-of-the-art models struggle with circuit reasoning, achieving 48.04% accuracy on numerical answers and 27.45% on unit tests."
          },
          "strengths": {
            "value": "The paper's primary strength lies in its novel contribution of a specialized benchmark for analog circuit reasoning, a domain underexplored in LLM research. The dataset's design, including template-based questions, numerical setups, and unit-test-like evaluations, offers a structured approach to assess model capabilities. The scalability of the dataset and its focus on multi-level reasoning align with significant challenges in analog design. The paper also addresses a critical gap in the literature by emphasizing the need for reasoning-based approaches in circuit design, which is both timely and impactful."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient experimental depth, testing only three LLMs without comparative analysis against specialized circuit design tools or prior methods. The dataset's scope is limited to simple circuit topologies, raising questions about its applicability to complex real-world designs. The unit-test evaluation metric, while innovative, is not thoroughly validated, and the paper does not address how it captures multi-level reasoning. Additionally, the error analysis is superficial, and the dataset statistics are incomplete, hindering a full assessment of its representativeness and diversity."
          },
          "questions": {
            "value": "1. How were the templates and numerical setups designed to ensure diversity and coverage of analog circuit concepts? 2. What specific challenges in circuit reasoning does the unit-test approach address, and how does it compare to traditional evaluation metrics? 3. Are there plans to expand the dataset to include more complex circuits or multi-component topologies in future work? 4. How does the inclusion of netlists affect model performance, and were there ablation studies to validate this? 5. What are the limitations of using LLMs for circuit reasoning, and how might the dataset be adapted to mitigate these?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces the CIRCUIT dataset, a benchmark for evaluating Large Language Models (LLMs) on analog circuit interpretation and reasoning tasks. The dataset includes 510 question-answer pairs with templates, numerical setups, and netlists, designed to assess LLMs' ability to perform circuit analysis and unit-test-like evaluations. The authors highlight that even advanced models like GPT-4o struggle with multi-level reasoning in analog circuits."
          },
          "strengths": {
            "value": "The paper presents a novel and timely contribution by addressing a critical gap in evaluating LLMs for analog circuit design, a domain traditionally reliant on human expertise. The dataset's design, including unit-test-like evaluation and scalable templates, is innovative and well-structured. The methodology for creating diverse numerical setups and incorporating netlists demonstrates rigor. The paper also emphasizes the significance of reasoning capabilities in analog design, which is crucial for advancing AI applications in semiconductor engineering. The clarity of the problem formulation and the systematic approach to dataset curation are commendable."
          },
          "weaknesses": {
            "value": "The dataset appears to focus on basic circuit problems (e.g., Ohm's law) rather than complex analog designs, limiting its scope for evaluating advanced reasoning. The evaluation metrics prioritize numerical accuracy over the reasoning process, which may not fully capture LLMs' understanding of circuit topologies. The unit-test approach, while innovative, lacks depth in testing multi-level reasoning for complex circuits. The experiments are limited to three models, and the error analysis is insufficiently detailed. Additionally, the paper does not discuss how the dataset can evolve to include more advanced tasks, which could hinder its long-term utility."
          },
          "questions": {
            "value": [
              "How were the templates selected to ensure coverage of diverse analog circuit topics beyond simple resistor networks?",
              "What specific challenges do the unit tests address that standard benchmarks do not, and how do they differ from existing evaluation methods?",
              "How does the dataset handle more complex circuit topologies (e.g., op-amps, filters) that require advanced reasoning?",
              "Are there plans to expand the dataset to include tasks like circuit synthesis or optimization, which are critical in analog design?",
              "How was the human evaluation conducted, and what were the key insights from the error analysis?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "5m43PEd3sz": {
    "paper_id": "5m43PEd3sz",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes ETGL-DDPG, an enhanced version of DDPG for sparse reward continuous control. The method introduces three key innovations: (1) εt-greedy, a temporally extended exploration strategy using tree search for directional exploration, (2) a goal-conditioned dual replay buffer (GDRB) to differentiate goal-reached and goal-not-reached experiences, and (3) longest n-step returns for more efficient Q-value updates. The authors demonstrate improved performance over DDPG and state-of-the-art methods on sparse-reward benchmarks."
          },
          "strengths": {
            "value": "The paper presents a novel combination of techniques addressing critical limitations of DDPG in sparse reward settings. The theoretical analysis of εt-greedy's polynomial sample complexity under mild MDP assumptions adds rigor. The GDRB framework introduces a fresh approach to replay buffer design, and the use of longest n-step returns shows creative adaptation of existing methods. The experimental validation on standard benchmarks, along with ablation studies, provides strong empirical support. The paper is well-structured with clear motivation and context."
          },
          "weaknesses": {
            "value": "The novelty of εt-greedy appears closely related to prior work like εz-greedy, and the paper could clarify how it differs in mechanism and performance. The theoretical analysis lacks detailed proofs, relying instead on vague references to polynomial complexity. The GDRB implementation details (e.g., buffer size, sampling strategies) are under-specified, making reproducibility challenging. The experiments compare against older methods (e.g., DDPG) but omit recent state-of-the-art approaches like HER or SAC. The ablation studies focus on individual components but do not analyze interactions between techniques."
          },
          "questions": {
            "value": [
              "How does εt-greedy differ mechanistically from εz-greedy, and what specific advantages does the tree-based search provide over temporal abstraction?",
              "Can the authors clarify the design choices for GDRB, such as buffer size, retention policies, and how they adapt to different environments?",
              "Why was the longest n-step return chosen over other n-step methods (e.g., multi-step TD learning), and how does it interact with the GDRB framework?",
              "Are there quantitative results comparing ETGL-DDPG to recent methods like SAC or HER on the tested benchmarks?",
              "How sensitive is the algorithm to hyperparameters (e.g., ε, n-step length, buffer sizes), and what guidelines are provided for tuning?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes ETGL-DDPG, an enhanced version of DDPG for sparse reward continuous control. It introduces three techniques: εt-greedy for directional exploration, a goal-conditioned dual replay buffer (GDRB), and longest n-step returns to improve reward propagation. The method is evaluated on standard benchmarks, claiming superior performance over DDPG and state-of-the-art methods."
          },
          "strengths": {
            "value": "Originality is evident in combining directional exploration (εt-greedy), goal-aware replay (GDRB), and n-step returns for sparse rewards. Theoretical claims about polynomial sample complexity for εt-greedy add rigor. The paper addresses three key limitations of DDPG in sparse reward settings. Ablation studies suggest individual contributions of the techniques, and the integration of these components into DDPG is novel. The figure provides a clear visual summary of innovations."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation. It does not specify which benchmarks were used, how baselines were selected, or whether statistical significance was tested. The GDRB's adaptive sampling strategy and the εt-greedy tree search mechanism are under-described. The theoretical analysis of sample complexity is not provided, making it hard to assess its validity. The longest n-step return implementation details (e.g., how n is chosen) are missing. Ablation studies do not quantify the individual impact of each technique."
          },
          "questions": {
            "value": "1. What is the exact mechanism of the εt-greedy tree search? How is the hash function φ defined, and how does it estimate state visit counts? 2. How is the adaptive sampling strategy in GDRB implemented? What criteria determine which transitions are stored in Dβ vs. De? 3. How does the longest n-step return differ from standard n-step TD methods? What value of n was used, and how was it selected? 4. Which specific benchmarks were tested, and what baselines were compared against? 5. How were hyperparameters tuned, and what ablation studies were conducted to validate the necessity of each component?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes ETGL-DDPG, an improved version of DDPG for sparse reward continuous control. It introduces three key components: (1) an epsilon t-greedy exploration strategy using tree search for directional exploration, (2) a goal-conditioned dual replay buffer (GDRB) to differentiate goal-reached and goal-not-reached experiences, and (3) longest n-step returns for more effective reward propagation. The method is evaluated on standard sparse-reward benchmarks, showing superior performance over DDPG and state-of-the-art methods."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in reinforcement learning—sparse reward environments—with clear, actionable contributions. The proposed techniques (epsilon t-greedy, GDRB, and longest n-step) are well-motivated and build on existing ideas (e.g., temporal abstraction, prioritized replay). The theoretical analysis of polynomial sample complexity for epsilon t-greedy adds rigor. The experiments are comprehensive, with ablation studies demonstrating the individual impact of each component. The paper is well-structured, with clear figures and a logical flow from problem statement to solution."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with recent sparse-reward methods like HER (Hindsight Experience Replay) or DDPG variants with reward shaping. The description of GDRB's adaptive sampling strategy is vague, and it's unclear how the two buffers differ in implementation. The epsilon t-greedy method's novelty over existing approaches (e.g., epsilon z-greedy) is not sufficiently justified. The theoretical guarantees for sample complexity rely on 'mild MDP assumptions' without explicit definitions. Additionally, the paper does not discuss computational overhead or scalability of the proposed methods."
          },
          "questions": {
            "value": [
              "How does epsilon t-greedy differ from the existing epsilon z-greedy method in terms of exploration efficiency and implementation? Are there quantitative comparisons?",
              "What is the exact mechanism of GDRB's adaptive sampling strategy? How is the 'goal-conditioned' aspect implemented in practice?",
              "The paper claims 'longest n-step returns' but does not clarify whether this refers to a novel variant or a standard n-step TD method. How does this compare to other n-step approaches like n-step DDPG?",
              "Are there any limitations or failure cases where ETGL-DDPG underperforms compared to baselines? How does the algorithm handle environments with very sparse rewards or complex state spaces?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "5o9JJJPPm6": {
    "paper_id": "5o9JJJPPm6",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces ComaDICE, an offline cooperative multi-agent reinforcement learning (MARL) algorithm that addresses distributional shift by incorporating stationary distribution regularization. The method combines DICE-based distribution correction with a value decomposition strategy that decomposes global value functions and advantage functions, rather than using Q-functions. The authors theoretically analyze the convexity of the global objective under specific conditions and demonstrate superior performance on multi-agent benchmarks."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel approach by extending DICE to MARL through stationary distribution regularization and a unique value decomposition strategy. The theoretical analysis of convexity and policy equivalence is a significant contribution. Quality: The experiments on standard benchmarks (MuJoCo, StarCraft II) suggest practical effectiveness. Clarity: The paper is well-structured with clear problem formulation and methodology. Significance: Addressing distributional shift in offline MARL is a critical challenge, and the method offers a principled framework for cooperative settings."
          },
          "weaknesses": {
            "value": "Theoretical assumptions (non-negative weights, convex activations) may not hold in practice, limiting generalizability. The paper lacks detailed ablation studies to validate the decomposition strategy's components. Experimental comparisons are not fully described—e.g., which baselines were tested, and on what subsets of tasks? The computational efficiency and scalability of ComaDICE with more agents are not discussed. The related work section is incomplete, potentially missing key prior art."
          },
          "questions": {
            "value": [
              "How do the authors ensure non-negative weights and convex activation functions in the mixing network during training? Are there practical constraints or approximations?",
              "What ablation studies were conducted to isolate the impact of stationary distribution regularization versus value decomposition?",
              "Are the experiments evaluated on all tasks in the benchmarks, or only a subset? How do results vary across different task complexities?",
              "How does ComaDICE handle non-cooperative or adversarial multi-agent settings, which are common in MARL?",
              "What is the computational overhead of the proposed method compared to existing offline MARL approaches?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces ComaDICE, an offline cooperative multi-agent reinforcement learning (MARL) algorithm that addresses distributional shift by incorporating stationary distribution regularization. The method combines the DICE framework with a novel value decomposition strategy, decomposing global value functions and advantage functions to ensure convexity and stability in multi-agent training. Experiments on MuJoCo and StarCraft II benchmarks demonstrate superior performance compared to existing offline MARL methods."
          },
          "strengths": {
            "value": "The paper presents a novel approach to offline MARL by focusing on stationary distribution regularization, an underexplored area compared to prior Q-function or policy-based regularizers. The theoretical analysis of convexity in the global objective under specific value decomposition conditions is a significant contribution. The experiments are comprehensive, covering standard MARL benchmarks, and the paper clearly articulates the motivation and challenges of offline MARL. The structured presentation of contributions and related work enhances readability."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the necessity of key components, such as the stationary distribution regularizer or the specific value decomposition design. The theoretical analysis assumes non-negative weights and convex activation functions, which may limit practical applicability. The comparison with state-of-the-art methods is primarily qualitative, with limited quantitative metrics on specific tasks. Additionally, the paper is cut off mid-sentence in the related work section, raising concerns about completeness in addressing prior art."
          },
          "questions": {
            "value": [
              "How does the algorithm handle non-convex activation functions or non-negative weight constraints in practical implementations?",
              "What ablation studies were conducted to isolate the impact of stationary distribution regularization versus value decomposition?",
              "Are the theoretical guarantees (e.g., convexity) dependent on specific network architectures, and how generalizable are these results?",
              "How does ComaDICE compare to other DICE-based methods in terms of computational efficiency or sample complexity?",
              "Were any experiments conducted on tasks with varying numbers of agents to test scalability?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces ComaDICE, an offline multi-agent reinforcement learning (MARL) algorithm that addresses distributional shift by regularizing the stationary distribution of joint policies. The method combines DICE-based distribution correction with a value decomposition strategy under the CTDE framework, enabling efficient training through convexity guarantees in policy decomposition."
          },
          "strengths": {
            "value": "The paper presents a novel approach to offline MARL by extending DICE to stationary distribution regularization, which is theoretically grounded and addresses a critical challenge in multi-agent settings. The value decomposition strategy is creatively designed to ensure convexity in the global objective, offering stability benefits. The experiments on standard benchmarks (MuJoCo, StarCraft II) demonstrate competitive performance against state-of-the-art methods, and the work builds on a strong theoretical foundation from prior single-agent DICE literature."
          },
          "weaknesses": {
            "value": "The paper lacks detailed theoretical analysis of the proposed decomposition strategy (e.g., proofs of convexity, assumptions about mixing networks). Experimental results are summarized but not thoroughly analyzed—key metrics like sample efficiency, ablation studies, or comparisons with more baselines (e.g., recent MARL methods) are missing. The interaction between stationary distribution regularization and CTDE is not clearly explained, and the practical implementation details (e.g., hyperparameter sensitivity) are underdeveloped."
          },
          "questions": {
            "value": "1. What specific value decomposition architecture is used, and how do non-negative weights and convex activations ensure convexity in the global objective? 2. How is the stationary distribution regularization integrated into the CTDE framework without compromising the decentralized execution aspect? 3. Are there ablation studies showing the contribution of each component (e.g., DICE regularization vs. decomposition strategy)? 4. How does ComaDICE handle non-cooperative or adversarial settings, which are not mentioned in the paper?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "5pd46nlxc6": {
    "paper_id": "5pd46nlxc6",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces value factorization for asynchronous multi-agent reinforcement learning (MARL) with macro-actions. The authors formalize Macro-IGM, a generalization of the individual global max (IGM) principle for macro-action-based value functions, and propose asynchronous value factorization (AVF) algorithms with novel update schemes and a macro-state buffer. Experiments show AVF outperforms synchronous methods on complex coordination tasks involving macro-actions."
          },
          "strengths": {
            "value": "The paper makes a novel theoretical contribution by formalizing Macro-IGM, which generalizes the IGM principle to macro-actions. The proposed AVF algorithms introduce practical mechanisms like the macro-state buffer and adaptive update strategies, addressing a critical gap in MARL for asynchronous systems. The experiments on standard macro-action benchmarks demonstrate scalability and effectiveness, with clear comparisons to baseline methods. The work is well-structured, with clear connections to prior value factorization literature."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of the macro-state buffer and update strategies. The theoretical analysis of Macro-IGM's properties (e.g., uniqueness, optimality) is limited, and the connection to existing macro-action frameworks (e.g., HAM-DQN) is not explicitly discussed. Experimental results are promising but focus on a narrow set of benchmarks; broader validation on diverse tasks would strengthen claims. The practical update schemes (e.g., gradient detachment/masking) lack rigorous justification for their effectiveness in different scenarios."
          },
          "questions": {
            "value": "1. How does the macro-state buffer specifically enable learning in asynchronous settings, and what alternative designs were considered? 2. Are the proposed update strategies (gradient detachment/masking) task-dependent, and how should practitioners choose between them? 3. How do AVF methods compare to non-factorization approaches for macro-actions (e.g., HAM-DQN)? 4. What are the computational costs of the macro-state buffer and additional updates compared to synchronous factorization? 5. How does the paper address the challenge of variable macro-action durations in the theoretical framework?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces value factorization for asynchronous multi-agent reinforcement learning (MARL) with macro-actions. It formalizes Macro-IGM, a generalization of the individual global max (IGM) principle for macro-action-based value functions, and proposes asynchronous value factorization (AVF) algorithms. The approach uses a macro-state buffer and novel update schemes to handle asynchronous execution, demonstrating effectiveness on macro-action benchmarks."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in MARL by extending value factorization to asynchronous settings with macro-actions, which is both original and timely. The theoretical foundation of Macro-IGM is well-justified, and the practical AVF algorithms show promise through ablation studies. The paper highlights the importance of macro-state buffers and proposes novel update strategies. The significance lies in enabling scalable, decentralized policies for complex coordination tasks where synchronous methods fail."
          },
          "weaknesses": {
            "value": "The paper lacks specific details about the benchmarks used (e.g., exact tasks, baselines, or metrics), making it hard to assess the novelty and impact of the results. The experimental evaluation is insufficient: it does not compare AVF with existing macro-action methods (e.g., [Xiao et al. 2020, 2022]) or analyze failure cases. The theoretical analysis of Macro-IGM is brief, and the generalization to broader function classes is not rigorously demonstrated. Additionally, the practical update schemes (e.g., gradient detachment, value masking) lack ablation studies on their effectiveness across tasks."
          },
          "questions": {
            "value": [
              "What specific macro-action benchmarks were used, and how do they compare to existing ones in the literature?",
              "How do AVF methods compare to state-of-the-art macro-action baselines (e.g., [Xiao et al. 2020, 2022]) in terms of performance and scalability?",
              "What are the limitations of the proposed update schemes (e.g., gradient detachment, value masking) in different task settings?",
              "How does the macro-state buffer design affect training stability and sample efficiency?",
              "Are the theoretical claims about Macro-IGM's generalization to broader functions supported by rigorous proofs or empirical validation?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces value factorization for asynchronous multi-agent reinforcement learning (MARL) with macro-actions. The authors formalize Macro-IGM, an extension of the individual global max (IGM) principle for macro-action-based value functions, and propose asynchronous value factorization (AVF) algorithms that handle asynchronous execution and variable-duration actions. They evaluate these methods on macro-action benchmarks, demonstrating improved performance over synchronous baselines in complex coordination tasks."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in MARL by generalizing value factorization to asynchronous settings, which is highly relevant for real-world applications. The theoretical contribution of Macro-IGM provides a principled foundation for extending IGM to macro-actions. The practical algorithms introduce novel update schemes (e.g., partially centralized updates) and a macro-state buffer, which are well-motivated. The experiments on macro-action benchmarks highlight the importance of handling asynchrony, and the ablation study demonstrates the necessity of the proposed mechanisms."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art macro-action methods, which limits the assessment of its novelty and effectiveness. The theoretical analysis of Macro-IGM is brief, and the proofs of its generalization over primitive IGM are not sufficiently elaborated. The experimental evaluation focuses on older benchmarks (e.g., Xiao et al. 2020, 2022), which may not fully reflect modern challenges. Additionally, the paper does not discuss computational overhead or scalability of AVF in large-scale settings."
          },
          "questions": {
            "value": "How do the AVF algorithms specifically handle the varying durations of macro-actions during training? What are the exact implementation details of the macro-state buffer, and how does it differ from existing buffer mechanisms in MARL? Are the partially centralized updates (e.g., gradient detachment or masking) theoretically justified, and how do they interact with the Macro-IGM principle? What are the limitations of the current approach in scenarios with extreme asynchrony or partial observability?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "5pd78GmXC6": {
    "paper_id": "5pd78GmXC6",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper presents a systematic exploration of the design space for neural graph representations in subgraph matching, identifying key axes such as interaction type (early/late), alignment strategy (set alignment vs. aggregated embeddings), and interaction structure (injective vs. non-injective). The authors propose design principles derived from extensive experiments, demonstrating that combining specific choices leads to improved performance over existing methods."
          },
          "strengths": {
            "value": "The paper's originality lies in its comprehensive analysis of the design space for subgraph matching, which has not been systematically explored before. The quality of the experiments is strong, with careful comparisons of different design choices. The clarity of the problem formulation and the organization of the analysis are commendable. The significance is high, as subgraph matching is critical for applications like knowledge graphs and molecular design, and the proposed design principles could guide future research."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient comparison with state-of-the-art methods, particularly newer approaches that might have addressed some of the design challenges discussed. The experiments focus on specific datasets, but the generalizability of the findings to other domains or graph structures is unclear. The paper does not adequately address computational trade-offs, such as the cost of late vs. early interaction. Additionally, the theoretical justification for the proposed design principles is limited, relying heavily on empirical results without deeper analysis."
          },
          "questions": {
            "value": [
              "How do the proposed design principles perform on datasets beyond those tested in the experiments?",
              "What are the computational costs of the late interaction approach compared to early interaction, and how does this impact scalability?",
              "Are the edge-based alignment techniques generalizable to different types of graphs (e.g., directed vs. undirected)?",
              "How do the authors address the potential overfitting of their model to specific datasets given the lack of ablation studies on hyperparameters?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper systematically explores the design space of neural graph representations for subgraph matching, identifying key axes such as interaction types (early vs. late), alignment methods (injective vs. non-injective), and scoring mechanisms. The authors propose design principles derived from extensive experiments, demonstrating that combining specific choices leads to improved performance over existing methods."
          },
          "strengths": {
            "value": "The paper's strengths lie in its comprehensive exploration of the design space, which addresses a critical gap in understanding neural graph matching models. The identification of key axes (e.g., interaction type, alignment granularity) provides a structured framework for future work. The experimental analysis is thorough, offering actionable insights into the interplay between design choices. The clarity of the problem formulation and the practical relevance of subgraph matching in domains like knowledge graphs and chemistry enhance the paper's significance. The paper also highlights novel combinations of design elements that outperform prior methods, showcasing its originality."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as specific datasets, metrics, or ablation studies to quantify the impact of individual design choices. The reference to 'extensive experiments' is vague, making it difficult to assess the robustness of the findings. The absence of the figure (figure1.png) and limited discussion of implementation details (e.g., hyperparameters) hinders reproducibility. Additionally, the paper does not fully address how the proposed design principles generalize across different graph types or scales, which limits their practical applicability."
          },
          "questions": {
            "value": "1. What specific datasets and metrics were used to evaluate the performance of the proposed design combinations? 2. How were the experiments designed to isolate the effect of individual axes (e.g., interaction type vs. alignment method)? 3. Are the proposed design principles applicable to graphs with varying sizes or structures, and how were these scenarios tested? 4. How do the computational costs of the new design combinations compare to existing methods, especially for large-scale graphs? 5. What are the limitations of the current approach, and how might they be addressed in future work?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper explores the design space of neural graph representations for subgraph matching by systematically analyzing key axes such as interaction strategies (early vs. late), alignment types (set alignment vs. aggregated embeddings), and interaction structures (injective vs. non-injective). The authors identify combinations of design choices that significantly improve performance and propose generalizable design principles for future methods."
          },
          "strengths": {
            "value": "The paper's originality lies in its systematic exploration of a previously uncharted design space for subgraph matching, breaking down the problem into distinct axes that have not been comprehensively studied. The experiments are extensive, providing empirical validation of the proposed design principles. The clarity of the structure and the detailed analysis of prior work strengthen the paper's readability. The significance is high, as subgraph matching is critical in domains like knowledge graphs and molecular design, and the insights could guide future research."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with all prior methods, particularly in terms of quantitative metrics and ablation studies. The experiments may not cover diverse datasets or edge cases, limiting generalizability. The figure illustrating design axes is cut off, which could hinder understanding of key concepts. Additionally, the derivation of design principles is not fully explained, leaving questions about their robustness across different scenarios."
          },
          "questions": {
            "value": [
              "How were the five design principles derived, and what specific experiments validate their effectiveness?",
              "What datasets were used for the experiments, and how do they compare to standard benchmarks for subgraph matching?",
              "Are there cases where the proposed combinations of design choices fail, and how were these addressed?",
              "How does the computational cost of early vs. late interaction models compare in practice, and what trade-offs exist?",
              "What is the role of the 'asymmetric ordering' introduced in the interaction non-linearity axis, and how does it differ from prior approaches?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "5qg6JPSgCj": {
    "paper_id": "5qg6JPSgCj",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces Fokker-Planck Neural Network (FPNN), a novel framework for solving high-dimensional steady-state Fokker-Planck (SFP) equations. FPNN decouples score learning and density normalization into two stages using a score PDE loss, allowing free-form network architectures and post-processing to strictly satisfy normalization constraints. The method claims superior accuracy and 20× speedup over state-of-the-art methods on 4D-20D problems without labeled data."
          },
          "strengths": {
            "value": "Originality lies in decoupling score learning and normalization via a score PDE loss, enabling flexible architectures. The method's efficiency (e.g., 256 parameters for 4D Ring) and scalability to 20D are notable. The paper addresses key challenges in SFP solvers, such as normalization constraints and high-dimensional complexity, with a theoretically grounded approach. The experimental results on MAPE and speedup suggest practical relevance."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with specific baselines (e.g., PINNs, normalizing flows) and does not provide ablation studies to validate components like the score PDE loss. The claim of 20× speedup is unsubstantiated without runtime comparisons. The normalization post-processing method is vaguely described, and the theoretical justification for decoupling stages is underdeveloped. Experiments are limited to 4D-6D problems, with no evidence of 20D scalability. The figure and table lack sufficient detail to assess results."
          },
          "questions": {
            "value": [
              "What specific baselines were used for comparison (e.g., PINNs, GMM, KRnet)? How does FPNN outperform them quantitatively?",
              "How is the score PDE loss mathematically formulated? What is the exact post-processing method for normalization?",
              "Are there ablation studies showing the impact of the score PDE loss vs. traditional PDE losses?",
              "How does FPNN handle different types of SFP equations (e.g., with varying drift/diffusion terms)?",
              "What is the computational cost of post-processing normalization compared to training? How is scalability to 20D validated?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces Fokker-Planck Neural Networks (FPNN), a novel framework for solving high-dimensional steady-state Fokker-Planck (SFP) equations. FPNN decouples score learning and density normalization into two stages using a score PDE loss, enabling free-form network architectures and strict normalization constraints via post-processing. The method demonstrates superior accuracy and speedup over existing approaches on benchmark problems."
          },
          "strengths": {
            "value": "Originality is evident in decoupling score learning and normalization, addressing a critical limitation in prior methods. The experimental results show strong performance metrics (e.g., MAPE of ~12% on 6D problems with minimal parameters), suggesting practical utility. The paper provides clear problem motivation, contextualizes prior work, and includes illustrative figures. The method's scalability to 20+ dimensions highlights its significance for high-dimensional PDEs."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contribution of the score PDE loss versus other components. The claim of a 20× speedup is not substantiated with runtime comparisons against baselines. The normalization post-processing step is not fully explained—how is the normalizing constant computed? The paper also does not address potential limitations (e.g., sensitivity to hyperparameters, applicability to non-steady-state equations). The comparison with state-of-the-art methods is superficial, with no statistical significance tests for the reported MAPE improvements."
          },
          "questions": {
            "value": "1. How is the normalizing constant calculated during post-processing? Is this computationally feasible for high-dimensional problems? 2. What specific architectural choices enable 'free-form' networks, and are there implicit constraints? 3. How does FPNN handle cases where the unnormalized density has multiple modes or sharp gradients? 4. Are the reported speedups due to the score PDE loss, the architecture, or other factors? 5. What is the theoretical justification for the score PDE loss's effectiveness in decoupling stages?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces the Fokker-Planck Neural Network (FPNN), a novel framework for solving high-dimensional steady-state Fokker-Planck (SFP) equations. FPNN decouples score learning and density normalization into two stages, enabling free-form network architectures and strict enforcement of normalization constraints via post-processing. The method achieves high accuracy and efficiency, with reported MAPEs of 11.36-13.87% on 4D-6D problems and a 20× speedup over state-of-the-art methods."
          },
          "strengths": {
            "value": "The paper presents a novel approach to decouple score learning and normalization, addressing a critical challenge in solving SFP equations. The methodology is theoretically grounded, with clear motivation for avoiding trivial solutions. Experimental results demonstrate impressive accuracy and efficiency on high-dimensional problems, with minimal parameters. The paper's structure is logical, and the problem formulation is well-contextualized within existing literature. The claims about scalability and computational efficiency are significant for high-dimensional PDE solving."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical exposition of the score PDE loss formulation and how it relates to the Fokker-Planck equation. The experimental section omits comparisons with key baselines like normalizing flows or GMMs, which are mentioned in the introduction. The post-processing normalization step is not explained, raising questions about its numerical stability. The methodology section is truncated, leaving critical details about the stochastic Runge-Kutta method and domain adaptation unresolved. The figure references are incomplete, and the claimed 20× speedup lacks quantitative justification."
          },
          "questions": {
            "value": "1. How is the score PDE loss derived mathematically from the Fokker-Planck equation? 2. What specific baselines were compared against in the experiments (e.g., KRnet, TN, or other PINN variants)? 3. Can the authors elaborate on the post-processing normalization procedure and its numerical implementation? 4. What are the limitations of the stochastic Runge-Kutta method in generating training data for high-dimensional cases? 5. How does the framework handle the curse of dimensionality beyond 20 dimensions?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "5sRnsubyAK": {
    "paper_id": "5sRnsubyAK",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "本文提出使用Constant Q Cepstral Coefficients (CQCC)作为神经退行性疾病的语音分类特征，通过几何间隔频率分箱提升时频分辨率。实验表明CQCC在随机森林和SVM分类器上分别提升5.6%和7.7%，优于传统MFCC和声学指标。研究验证了CQCC的form-invariance特性，并在帕金森和ALS数据库上进行了测试。"
          },
          "strengths": {
            "value": "1. 原创性：首次将CQCC应用于神经退行性疾病的多疾病分类，填补了持续元音声音分析的空白。2. 有效性：实验结果明确显示CQCC的性能优势，且通过LDA图验证了鲁棒性。3. 实用性：针对临床诊断需求，提出了具有潜在应用价值的特征提取方法。4. 理论深度：结合CQT的form-invariance特性解释了性能提升的原理。"
          },
          "weaknesses": {
            "value": "1. 方法细节不足：CQT公式推导不完整（第3页公式被截断），未说明如何处理不同长度语音信号。2. 实验对比有限：未与最新深度学习方法对比，缺乏消融实验验证CQCC各组件贡献。3. 数据集局限性：仅在两个特定数据库测试，未讨论跨数据库泛化能力。4. 分析深度不足：未深入探讨CQCC在不同疾病类型（如PD vs ALS）中的差异表现。"
          },
          "questions": {
            "value": "1. CQCC的具体实现细节（如滤波器组设计、倒谱转换步骤）是否在补充材料中？2. 如何处理语音信号长度不一致的问题？是否采用固定长度截断或动态调整？3. 是否进行了超参数调优（如CQT的Q值选择）？4. LDA图的具体可视化结果如何？是否与MFCC结果有定量对比？5. 是否测试了CQCC在噪声环境下的鲁棒性？"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes Constant Q Cepstral Coefficients (CQCC) as a novel feature extraction method for classifying neurodegenerative disorders using speech signals. The authors claim that CQCC outperforms traditional features like MFCC and acoustic measures (Jitter, Shimmer) by leveraging the form-invariance property of the Constant Q Transform (CQT), which provides better spectrotemporal resolution for capturing fundamental frequencies and harmonics in speech. The method is validated on two datasets (Italian Parkinson’s and Minsk2019 ALS) with classifiers like Random Forest and SVM."
          },
          "strengths": {
            "value": "The paper introduces a novel feature extraction approach (CQCC) tailored for neurodegenerative disorder classification, addressing limitations of traditional spectral features. The methodology is grounded in signal processing principles, particularly the form-invariance of CQT, which is a strong theoretical justification. The experimental validation on real-world datasets and comparison with established baselines (MFCC, acoustic measures) demonstrate practical relevance. The paper also highlights the importance of spectrotemporal resolution in capturing disease-specific speech patterns, which is a significant contribution to the field."
          },
          "weaknesses": {
            "value": "The methodology section is incomplete, with equations and descriptions cut off, making it impossible to assess the technical details of CQCC computation. The paper lacks critical experiments, such as comparisons with state-of-the-art deep learning models, ablation studies on CQT parameters, or analysis of class imbalance in datasets. The statistical significance of results (e.g., 5.6% and 7.7% improvements) is not rigorously validated. The form-invariance claim is not sufficiently explained or supported by empirical evidence. The paper also omits details on preprocessing, hyperparameter tuning, and cross-validation strategies."
          },
          "questions": {
            "value": "1. How exactly is the Constant Q Transform implemented in this work, and what are the specific parameters (e.g., number of bins, frequency range) used? 2. Why does CQCC outperform MFCC in this context, and are there quantitative analyses (e.g., feature importance, error distribution) to support this? 3. What baseline models (e.g., deep learning) were excluded, and how does CQCC compare to them? 4. Are the datasets (Italian Parkinson’s, Minsk2019) publicly available, and were they preprocessed in a standard way? 5. How robust is CQCC to variations in speech data (e.g., background noise, speaker demographics)? 6. What is the computational cost of CQCC compared to MFCC, and is it suitable for real-time applications?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Constant Q Cepstral Coefficients (CQCC) as a novel feature extraction method for classifying neurodegenerative disorders (NDS) using speech signals. The approach leverages the Constant Q Transform (CQT) to achieve superior spectrotemporal resolution, particularly for capturing fundamental frequency and harmonics. The authors demonstrate that CQCC outperforms traditional features like MFCC and acoustic measures (Jitter, Shimmer) on Parkinson’s and ALS datasets, with improvements of 5.6% and 7.7% respectively when combined with Random Forest and SVM classifiers."
          },
          "strengths": {
            "value": "Originality: The paper proposes CQCC for neurodegenerative disorder classification, addressing limitations of traditional spectral features like MFCC. The form-invariance property of CQT is highlighted as a novel advantage for robust feature representation. Quality: Experiments are conducted on two well-known datasets (Italian Parkinson’s and Minsk2019 ALS), with clear comparative results against established baselines. Clarity: The methodology is described with mathematical formulations, and the paper emphasizes the technical rationale for CQT over STFT. Significance: Early detection of NDs via speech analysis has critical clinical implications, and the paper contributes a promising feature engineering approach for this domain."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, limiting evaluation of key details like hyperparameter choices, ablation studies, and comprehensive comparisons with state-of-the-art methods. The experimental validation of CQCC's robustness (e.g., LDA plots) is mentioned but not elaborated. The generalizability of results to other NDs or speech modalities (e.g., sustained vowels vs. connected speech) remains unexplored. Additionally, the lack of statistical significance testing for the reported performance gains raises questions about the reliability of the findings."
          },
          "questions": {
            "value": [
              "What specific implementation details of CQCC (e.g., filter bank design, cepstral coefficients' order) were used, and how do they differ from prior work on CQCC in antispoofing or infant cry classification?",
              "How does CQCC handle variations in pitch and tonal conditions, given the claim of form-invariance? Are there quantitative experiments validating this property?",
              "What is the sample size and demographic breakdown of the Italian Parkinson’s and Minsk2019 datasets? Are there class imbalance issues that could affect classifier performance?",
              "Were the improvements over MFCC and traditional features statistically significant, or were they based on single-trial results?",
              "How do the results compare to recent deep learning-based approaches for ND classification, which might outperform traditional machine learning models?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "5swfKRkCx7": {
    "paper_id": "5swfKRkCx7",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a novel retrieval-augmented generation (RAG) approach for question answering (QA) by integrating external knowledge through an 'external knowledge attention' mechanism. The method dynamically fuses retrieved knowledge with LLMs using a memory-based module that adjusts knowledge integration based on the relationship between the question and retrieved information, demonstrating effectiveness in general and domain-specific QA tasks."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel integration of external knowledge via a dedicated attention head within LLM layers, combined with a dynamic memory module for weighted fusion. Quality: The approach addresses limitations of prior RAG methods by emphasizing structured knowledge integration rather than simple concatenation. Clarity: The architecture is well-explained with a figure, and the problem statement is clearly contextualized. Significance: The method has potential to improve LLM performance in knowledge-intensive QA tasks, particularly in specialized domains."
          },
          "weaknesses": {
            "value": "The paper lacks concrete experimental results (e.g., specific metrics, comparisons with state-of-the-art methods) to validate claims of effectiveness. The memory module's implementation details are insufficient, making it hard to assess its contribution. The novelty of the external attention mechanism is not sufficiently differentiated from existing multi-head attention or RAG approaches. Theoretical justification for the dynamic fusion strategy is underdeveloped."
          },
          "questions": {
            "value": "1. How does the external knowledge attention mechanism differ from existing RAG methods like those using simple concatenation or fine-tuning? 2. What specific QA datasets and metrics were used in experiments, and how do the results compare to baselines? 3. Can the authors clarify the training objective for the memory module and its interaction with LLM layers? 4. Are there ablation studies demonstrating the individual contributions of the attention head and memory module? 5. How does the method handle noisy or conflicting retrieved knowledge?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes a novel Retrieval-Augmented Generation (RAG) approach for question answering (QA) by introducing an external knowledge attention mechanism integrated into the multi-head attention framework of large language models (LLMs). The method dynamically controls knowledge integration via a memory module, which assigns weights to encoded knowledge based on its relevance to the input question, aiming to enhance answer accuracy by combining external knowledge with LLM capabilities."
          },
          "strengths": {
            "value": "Originality: The paper introduces a creative combination of multi-head attention with external knowledge integration, proposing a memory-based mechanism for dynamic knowledge fusion. Quality: The methodology is well-structured, with clear descriptions of the knowledge encoder, attention mechanism, and memory module. Clarity: The abstract, introduction, and architecture diagram are well-written, though the technical details are incomplete. Significance: RAG is a critical area for improving LLM reliability, and the proposed approach addresses limitations in existing methods by enabling layer-specific knowledge integration."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about the memory module's implementation, experimental results, and comparisons with baselines incomplete. The technical description of the attention mechanism and memory module lacks specificity (e.g., projection matrices are not fully defined). The experiments on domain-specific tasks (e.g., medical QA) are mentioned but not elaborated, making it impossible to assess effectiveness. The related work and baseline comparisons are insufficiently detailed, reducing confidence in the novelty claims."
          },
          "questions": {
            "value": [
              "How is the memory module implemented? What are the exact mechanisms for dynamically controlling knowledge fusion based on question-knowledge relationships?",
              "What specific datasets and baselines were used in the experiments? How does the proposed method compare to existing RAG approaches in terms of metrics like accuracy and robustness?",
              "Are there ablation studies to validate the contribution of the external knowledge attention head versus the memory module?",
              "How is the knowledge encoder adapted for different input formats (e.g., full-text vs. entities)? What pre-trained models are used for encoding?",
              "What are the computational and memory costs of the proposed method compared to standard RAG frameworks?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a novel retrieval-augmented generation (RAG) approach for question answering (QA) by integrating external knowledge through an attention mechanism and a memory module. The method dynamically controls knowledge integration based on the relationship between the question and retrieved knowledge, aiming to improve LLM performance in both general and domain-specific QA tasks."
          },
          "strengths": {
            "value": "The paper presents a clear problem statement addressing the limitations of existing RAG methods in handling dynamic or specialized knowledge. The proposed approach introduces a novel integration of external knowledge attention as an additional head in LLMs, which could offer a fresh perspective on knowledge fusion. The methodological framework is structured, with a focus on dynamic knowledge weighting via a memory module. The potential significance lies in improving LLM reliability for QA tasks, particularly in scenarios requiring up-to-date or domain-specific knowledge."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient experimental validation, as the provided content is truncated and does not include detailed results, comparisons with state-of-the-art baselines, or ablation studies. The description of the memory module's design and how it dynamically controls knowledge fusion is vague, with no concrete examples or mathematical formulations. Additionally, the paper does not address potential limitations, such as computational overhead or scalability to large knowledge bases. The claims about the method's effectiveness remain unverified without comprehensive experiments."
          },
          "questions": {
            "value": [
              "How is the memory module's weight calculation explicitly defined? What criteria determine the significance of knowledge instances at different LLM layers?",
              "What specific baselines were used for comparison, and how do the results on benchmark datasets (e.g., SQuAD, MedQA) demonstrate the method's superiority?",
              "Are there any ablation studies to validate the contribution of the knowledge attention mechanism versus the memory module?",
              "How does the method handle noisy or conflicting knowledge from retrieval, and what is its robustness to such cases?",
              "What are the computational costs of the proposed approach compared to existing RAG methods?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "5w51I0XlOP": {
    "paper_id": "5w51I0XlOP",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes Self-Choose, a method for self-correcting multimodal large language models (MLLMs) by leveraging diverse reasoning solutions. The approach generates candidate answers using different reasoning methods, evaluates them through comparative analysis, and selects the optimal solution. The method is evaluated on vision reasoning benchmarks, showing improvements over existing self-correction techniques like Self-Refine and Self-Review."
          },
          "strengths": {
            "value": "The paper addresses a relevant problem in MLLMs—self-correction in complex reasoning tasks—by introducing a novel strategy that leverages diverse reasoning methods. The concept of mitigating 'mental set' bias through multi-perspective reasoning is original and aligns with psychological insights. The method's plug-and-play nature and reliance on the model itself without external tools are practical advantages. The paper also contextualizes its work within existing self-correction literature, demonstrating awareness of prior art."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results, such as specific metrics (e.g., accuracy improvements) or comparisons with baseline methods. The evaluation claims are vague, and the described benchmarks (e.g., ScienceQA, Whoops, MM-Vet) are not sufficiently explained. The methodology section is incomplete, with truncated diagrams and insufficient description of how candidate answers are evaluated or how 'diverse reasoning solutions' are defined. The connection between the 'mental set' phenomenon and the proposed solution is not thoroughly justified."
          },
          "questions": {
            "value": "1. How are the diverse reasoning methods selected, and what criteria define their 'diversity'? 2. What specific evaluation metrics were used to compare Self-Choose with baselines like Self-Refine? 3. How does the method handle cases where all candidate answers are incorrect? 4. Are there ablation studies to isolate the contribution of each component (e.g., candidate generation vs. evaluation)? 5. How does Self-Choose scale in terms of computational cost compared to existing methods?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes Self-Choose, a method for self-correcting multimodal large language models (MLLMs) by leveraging diverse reasoning solutions. It generates candidate answers using different reasoning methods, evaluates them through comparison, and selects the optimal solution. The approach is tested on vision reasoning benchmarks, showing improvements over existing methods."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in MLLMs—self-correction in complex reasoning tasks—which is underexplored. The methodology is novel, drawing inspiration from psychological concepts like 'mental set' to overcome fixed thinking patterns. The approach is practical, as it is plug-and-play and does not require external tools. The experiments span multiple datasets and models, indicating broad applicability."
          },
          "weaknesses": {
            "value": "The paper is cut off, leaving critical details about the implementation of diverse reasoning methods, evaluation metrics, and quantitative results missing. The description of Self-Choose is vague—how are the 'diverse reasoning methods' defined? Are they different prompts, models, or strategies? The lack of ablation studies and analysis of failure cases weakens the claims. Additionally, the comparison with existing methods like Self-Refine lacks concrete metrics, making it difficult to assess the magnitude of improvements."
          },
          "questions": {
            "value": "1. How are the diverse reasoning methods implemented (e.g., different prompts, models, or architectures)? 2. What specific metrics were used to evaluate improvements (e.g., accuracy, F1, or task-specific benchmarks)? 3. Are there quantitative results comparing Self-Choose to baselines like Self-Refine and Self-Review? 4. What are the limitations of the method, and under what conditions does it fail? 5. How is the 'mental set' phenomenon modeled in the approach, and why is it effective?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes Self-Choose, a method for self-correcting multimodal large language models (MLLMs) by leveraging diverse reasoning solutions. The approach generates candidate answers using different reasoning methods, evaluates them through process comparison, and selects the optimal solution. Experiments on benchmarks like ScienceQA and MM-Vet show improvements over existing methods like Self-Refine and Self-Review."
          },
          "strengths": {
            "value": "The paper introduces a novel self-correction framework for MLLMs, addressing a critical gap in handling complex vision reasoning tasks. The methodology is well-structured, combining diverse reasoning strategies with evaluation mechanisms. The experiments demonstrate consistent improvements across multiple benchmarks, highlighting the practical value of the approach. The paper also provides clear contextualization within related work, emphasizing the limitations of prior self-correction methods."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contribution of individual components (e.g., diversity of reasoning methods vs. evaluation strategies). The comparison with existing methods like Self-Refine and Self-Review is superficial, with limited quantitative analysis of why Self-Choose outperforms them. The evaluation focuses on a narrow set of models (LLaVA, Gemini) and datasets, leaving gaps in generalizability. Additionally, the diagrams and technical details of the proposed framework are incomplete, reducing clarity."
          },
          "questions": {
            "value": "1. What specific reasoning methods are used to generate candidate answers, and how do they differ in their focus (e.g., image vs. text understanding)? 2. How is the 'diversity' of reasoning solutions quantified, and what metrics are used to evaluate their effectiveness? 3. Are there ablation studies showing the impact of individual components (e.g., evaluation step, multiple reasoning paths)? 4. How does Self-Choose handle cases where all candidate answers are incorrect, and what guarantees exist for the final output? 5. What are the computational costs of the proposed method compared to baseline approaches?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "5wxCQDtbMo": {
    "paper_id": "5wxCQDtbMo",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces GotenNet, a novel Geometric Tensor Network for 3D equivariant graph neural networks. It aims to address the trade-off between expressiveness and computational efficiency in modeling 3D molecular systems by avoiding irreducible representations and Clebsch-Gordan transforms, instead using geometric tensor representations and hierarchical refinement mechanisms. The method claims superior performance on benchmark datasets like QM9, rMD17, MD22, and Molecule3D."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel approach to 3D equivariance by eliminating irreducible representations and Clebsch-Gordan transforms, focusing on inner product operations for efficiency. Quality: The experiments on multiple benchmark datasets suggest strong empirical performance. Clarity: The abstract and introduction are well-structured, though the paper is cut off. Significance: 3D equivariance in molecular modeling is a critical challenge with broad applications in drug discovery and materials science."
          },
          "weaknesses": {
            "value": "The paper is cut off, preventing full evaluation of the method's technical details. Key claims (e.g., computational efficiency, ablation studies) lack supporting evidence. The comparison to baselines is vague, and the paper does not address how the approach scales to larger or more complex molecular systems. The theoretical justification for avoiding irreps and CG transforms is underdeveloped."
          },
          "questions": {
            "value": [
              "What is the exact architecture of GotenNet, and how does it ensure E(3) equivariance without irreducible representations?",
              "How does the paper quantify computational efficiency gains compared to existing methods like SchNet or E(3)Transformer?",
              "Are there ablation studies demonstrating the contribution of tensor attention and hierarchical refinement mechanisms?",
              "How does the method handle varying molecular sizes or complex geometries beyond the tested datasets?",
              "What specific optimizations enable the 'unified structural embedding' and 'hierarchical tensor refinement'?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces GotenNet, a novel Geometric Tensor Network designed for 3D equivariant graph neural networks. The approach emphasizes computational efficiency by avoiding irreducible representations and Clebsch-Gordan transforms, instead using spherical-scalarization, geometry-aware tensor attention, and hierarchical tensor refinement. The method is evaluated on benchmark datasets like QM9 and MD22, where it claims superior performance over existing state-of-the-art models."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel framework that avoids traditional irreducible representations and Clebsch-Gordan transforms, offering a fresh perspective on 3D equivariance. Quality: The experiments are conducted on well-established molecular datasets, and the results are presented with clear metrics. Clarity: The abstract and introduction are well-structured, though the technical details in later sections may lack depth. Significance: 3D equivariant models are critical for molecular property prediction, and the efficiency gains could have broad implications for scalability in large-scale applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with specific baseline models, such as e3nn or SchNet, making it hard to assess the magnitude of improvements. The theoretical justification for avoiding CG transforms is not thoroughly explained. The ablation studies are not explicitly described, leaving questions about the contribution of individual components. Additionally, the paper is cut off mid-section, potentially missing critical details about the method or experiments."
          },
          "questions": {
            "value": [
              "How does GotenNet compare to specific state-of-the-art models like e3nn or SchNet in terms of performance and computational efficiency?",
              "What are the exact mechanisms by which the spherical-scalarization approach captures geometric relationships without irreducible representations?",
              "Are there ablation studies demonstrating the contribution of geometry-aware tensor attention versus hierarchical refinement?",
              "How does the model handle force prediction tasks, which are critical for molecular dynamics simulations?",
              "What are the limitations of the method in terms of scalability or generalization across different molecular properties?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces GotenNet, a novel Geometric Tensor Network designed for 3D equivariant graph neural networks. The approach aims to address the expressiveness-efficiency trade-off by avoiding irreducible representations and Clebsch-Gordan transforms, instead using inner product operations and hierarchical tensor refinement. It demonstrates superior performance on multiple molecular property prediction tasks compared to state-of-the-art methods."
          },
          "strengths": {
            "value": "The paper tackles a critical challenge in 3D molecular modeling by proposing a computationally efficient framework that maintains geometric expressiveness. The focus on inner product operations instead of complex tensor transforms is a novel direction. The experimental results on benchmark datasets (QM9, rMD17, MD22, Molecule3D) suggest consistent improvements over existing methods. The paper also introduces geometry-aware tensor attention and hierarchical refinement mechanisms, which could enhance model interpretability and scalability."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical explanations of how inner product operations capture geometric relationships without relying on irreps or CG transforms. The related work section is incomplete, making it difficult to contextualize the contribution. The experiments do not include ablation studies or comparisons with specific baselines (e.g., scalarization vs. high-degree steerable models). The paper also does not address potential limitations, such as performance on non-molecular 3D data or scalability to larger systems."
          },
          "questions": {
            "value": "How does the model handle varying molecular geometries or topologies? What are the computational costs of the hierarchical tensor refinement compared to existing methods? Are there specific cases where the absence of irreps/clebsch-gordan transforms leads to suboptimal performance? How does the model generalize to tasks beyond molecular property prediction?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "5x1Gklb3mf": {
    "paper_id": "5x1Gklb3mf",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a novel approach for microstructural segmentation in metallographic images by integrating phase ratio information as domain knowledge into a deep learning model. The proposed Phase Learning Module (PLM) combines phase ratios with image encodings to generate ratio-aware features, improving segmentation performance. The method leverages expert-estimated phase ratios during inference and trains with accurate ratios from ground truth masks, achieving improvements in Dice scores on private and public datasets."
          },
          "strengths": {
            "value": "Originality is demonstrated by incorporating phase ratios as input constraints, a novel approach in material segmentation. The methodology is well-structured, with clear integration of domain knowledge into the model architecture. Experiments show measurable improvements in segmentation performance, and visualizations suggest better phase representation. The paper addresses a specific but important challenge in materials science, with potential broader applicability."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how expert-estimated phase ratios are obtained and their accuracy, which could affect generalizability. The private dataset is not publicly available, limiting reproducibility. The ablation studies are insufficient to isolate the contribution of the PLM. The comparison with existing methods that use domain knowledge (e.g., prior work on semantic segmentation) is incomplete. The theoretical justification for why phase ratios improve representations is underdeveloped."
          },
          "questions": {
            "value": "1. How are expert-estimated phase ratios quantified, and what is their accuracy compared to ground truth? 2. What is the source and composition of the private dataset, and why is it not publicly available? 3. Are there ablation studies demonstrating the PLM's impact on performance when phase ratios are omitted? 4. How does the model handle cases where phase ratios are inaccurate or unavailable? 5. What is the computational overhead of the PLM, and how does it scale to larger datasets?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes a novel approach for microstructural segmentation in metallographic images by integrating phase ratio information as domain knowledge into a deep learning model. The Phase Learning Module (PLM) combines expert-estimated phase ratios with image encodings to generate ratio-aware features, improving segmentation accuracy while maintaining spatial details. The method achieves significant performance gains on both private and public datasets with minimal parameter increases."
          },
          "strengths": {
            "value": "Originality is demonstrated through the integration of phase ratios as explicit inputs to guide segmentation, a novel approach in the field. The methodology is well-structured, combining domain knowledge with deep learning, and the experiments show measurable improvements in Dice scores. The clarity of the paper is strong, with detailed explanations of the PLM and visualizations that highlight the effectiveness of the approach. The significance lies in addressing a critical challenge in materials science segmentation, with potential broader applications in domain-specific image analysis."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with existing methods that incorporate domain knowledge, making it difficult to assess the novelty and relative effectiveness of the PLM. The integration of phase ratios into the model is not thoroughly explained, particularly how the ratio encoder interacts with the image encodings. The experiments focus on quantitative metrics but do not analyze qualitative failures or edge cases. Additionally, the use of LoRA on SAM is mentioned but not elaborated, and the private dataset's characteristics are not fully described."
          },
          "questions": {
            "value": "1. How does the Phase Learning Module specifically integrate phase ratios with image encodings? Are there ablation studies to validate the contribution of the ratio encoder? 2. What are the baselines used for comparison, and how do they handle domain knowledge? 3. How does the model perform when expert-estimated phase ratios are inaccurate or absent? 4. Are there limitations to the private dataset's generalizability, and how were the ground truth phase ratios calculated? 5. Can the method be adapted to other segmentation tasks beyond metallographic images?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a novel approach for microstructural segmentation in metallographic images by integrating phase ratio information as domain knowledge into a deep learning model. The proposed Phase Learning Module (PLM) combines expert-estimated phase ratios with image encodings to generate ratio-aware features, improving segmentation accuracy while maintaining spatial details. The method achieves significant performance gains on both private and public datasets with minimal parameter increases."
          },
          "strengths": {
            "value": "The paper demonstrates originality by introducing phase ratios as a novel input constraint for segmentation models, which is claimed to be first-of-its-kind. The experimental results show measurable improvements in Dice scores, and visualizations suggest better phase representation. The work addresses a critical challenge in materials science and leverages domain expertise in a creative way. The clarity of the problem statement and methodology is strong, and the significance of the contribution to materials analysis is clear."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the PLM's contribution to performance gains. The handling of phase ratio estimation errors (e.g., if expert estimates are inaccurate) is not discussed. The integration of LoRA with SAM is mentioned but not thoroughly explained in the main text. The paper also does not compare against state-of-the-art methods beyond a few baselines, leaving the relative impact of the PLM unclear. The theoretical justification for how phase ratios improve representations is limited."
          },
          "questions": {
            "value": "1. How were phase ratios estimated during inference, and what is the robustness of the model to errors in these estimates? 2. Are there ablation studies demonstrating the PLM's impact on segmentation performance? 3. How does the model perform when phase ratio inputs are absent or incorrect? 4. What specific modifications were made to SAM for LoRA adaptation, and why was this approach chosen over others? 5. How does the proposed method compare to recent segmentation frameworks like Swin-UNet or other domain-specific architectures?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "5x88lQ2MsH": {
    "paper_id": "5x88lQ2MsH",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces BONSAI, a gradient-free graph condensation method for node classification that addresses key limitations of existing techniques. BONSAI leverages the structure of computation trees in message-passing GNNs to create a condensed graph by selecting representative exemplar trees, enabling model-agnostic, efficient condensation without requiring full-dataset training."
          },
          "strengths": {
            "value": "Originality: BONSAI introduces a novel approach by framing graph condensation through computation trees, which is a fresh perspective compared to gradient-based methods. Quality: The method is theoretically grounded with mathematical guarantees, and the experiments on 7 real-world datasets demonstrate superior accuracy and speed. Clarity: The paper is well-structured, with clear problem formulation, comparisons, and a motivating analogy to bonsai art. Significance: Addressing the critical issues of gradient dependency and model-specific condensation has broad implications for scalable GNN training."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contribution of computation tree selection versus other components. The claim of being 'model-agnostic' is not thoroughly validated across diverse GNN architectures. The experimental comparison with baselines is limited to a subset of existing methods, omitting some relevant works (e.g., DosCOND, MIRAGE). The speedup of 22x is not contextualized against alternative gradient-free approaches. Additionally, the theoretical guarantees are mentioned but not elaborated, making it difficult to assess their rigor."
          },
          "questions": {
            "value": "How does BONSAI explicitly select 'dense' computation trees for exemplars? What metrics are used to quantify 'representation' of the full tree set? Are there cases where the computation tree decomposition might fail (e.g., highly irregular graphs)? How does the method handle dynamic graphs or changes in node/edge features post-condensation? The paper mentions robustness to GNN architectures—can the authors provide specific examples where BONSAI outperforms baselines across different architectures?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces BONSAI, a gradient-free graph condensation method for node classification that addresses limitations of existing techniques. BONSAI leverages the observation that message-passing GNNs decompose graphs into computation trees, selecting representative exemplar trees to form a condensed graph. It claims to be model-agnostic, faster than baselines, and achieves higher accuracy across 7 real-world datasets."
          },
          "strengths": {
            "value": "Originality: BONSAI introduces a novel approach by framing graph condensation through computation trees, a departure from gradient-based methods. Quality: The paper provides rigorous empirical evaluation on diverse datasets and claims mathematical guarantees. Clarity: The problem formulation and contributions are well-structured, with clear comparisons to existing work. Significance: Solving the need for full-dataset training and model-specific condensation addresses critical scalability issues in GNNs."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the effectiveness of individual components (e.g., exemplar selection criteria). The mathematical guarantees are not explicitly described, making it hard to assess their robustness. The comparison with baselines focuses on accuracy and speed but omits analysis of edge reduction efficiency, a key metric for computational cost. Additionally, the claim of 'model-agnosticism' requires further justification, as the method's dependency on message-passing frameworks may limit its applicability."
          },
          "questions": {
            "value": "1. How does BONSAI ensure the selected exemplar trees capture the diversity of computation trees across different GNN architectures? 2. What are the specific mathematical guarantees provided, and under what assumptions do they hold? 3. Are there cases where the computation tree decomposition might fail or lead to suboptimal condensation? 4. How does BONSAI handle dynamic graphs or graphs with non-uniform node distributions?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces BONSAI, a gradient-free graph condensation method for node classification that avoids full GNN training by leveraging computation trees in message-passing GNNs. It claims to be model-agnostic, faster, and more robust than existing methods, with empirical validation on 7 real-world datasets."
          },
          "strengths": {
            "value": "Originality is demonstrated through the novel use of computation trees for condensation, addressing key limitations of gradient-dependent methods. The paper provides rigorous empirical evaluation across diverse datasets, showing superior accuracy and speed. The problem formulation and mathematical guarantees add credibility. Clarity is strong, with well-structured sections and a helpful comparison table. The significance lies in enabling scalable GNN training for resource-constrained applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the effectiveness of key components like exemplar tree selection. The theoretical guarantees are mentioned but not elaborated, making it hard to assess their rigor. The speed claims (22x faster) need clarification on benchmarking metrics (e.g., CPU vs. GPU). The model-agnosticity claim requires stronger evidence, as the experiments focus on specific GNNs. The paper also omits comparisons with recent methods like MIRAGE and KiDD, which may be relevant."
          },
          "questions": {
            "value": [
              "How are exemplar trees selected? What clustering or similarity metrics are used to identify dense regions?",
              "Are the mathematical guarantees for approximation strategies explicitly derived or referenced?",
              "What is the exact computational complexity of BONSAI, and how does it compare to baselines?",
              "How does BONSAI handle GNN architectures that deviate from standard message-passing frameworks?",
              "Why are MIRAGE and KiDD excluded from the experiments despite their relevance to graph condensation?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "5xSRg3eYZz": {
    "paper_id": "5xSRg3eYZz",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces VVC-Gym, a novel reinforcement learning environment for multi-goal long-horizon problems based on fixed-wing UAV velocity vector control. It addresses challenges in exploration complexity by providing environment ablation studies, multi-quality demonstration sets, and baselines for evaluating RL algorithms."
          },
          "strengths": {
            "value": "The paper's strengths include a well-motivated contribution to a critical but underexplored area (multi-goal long-horizon RL) with a realistic UAV-based environment. The ablation studies on environment design and demonstration quality provide actionable insights. The inclusion of multiple demonstration sets and baselines for diverse RL algorithms enhances its utility. The paper's structure and clarity are strong, with clear technical explanations of the environment's design and objectives."
          },
          "weaknesses": {
            "value": "The experiments lack depth in comparing VVC-Gym to existing environments (e.g., Mujoco, AntMaze) and do not thoroughly analyze how specific environment design choices (e.g., reward shaping, termination conditions) directly impact RL performance. The demonstration set generation methodology is under-specified, and the paper does not address scalability or generalizability beyond UAV tasks. The theoretical analysis of the environment's benefits is limited."
          },
          "questions": {
            "value": "How does VVC-Gym differ from existing multi-goal environments in terms of task complexity and realism? What specific metrics were used to evaluate demonstration quality, and how were they validated? Are there plans to expand VVC-Gym to other domains or tasks beyond UAV control? How do the ablation studies isolate the impact of environment design from algorithmic choices?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces VVC-Gym, a novel Reinforcement Learning (RL) environment for multi-goal long-horizon problems using fixed-wing UAV velocity vector control. The environment addresses challenges in spatial and temporal exploration by incorporating ablation studies on environment design, multi-quality demonstration sets, and baselines for evaluating RL algorithms. It aims to advance research in Goal-Conditioned RL (GCRL) by providing a realistic, complex task framework."
          },
          "strengths": {
            "value": "The paper presents a highly original environment (VVC-Gym) tailored to multi-goal long-horizon problems, which is critical for advancing GCRL research. The inclusion of ablation studies on environment design and diverse demonstration sets demonstrates a thorough approach to understanding their impact on training. The work addresses a significant gap in existing RL environments, which often lack realism and proper termination mechanisms. The potential to study the interplay between environment design, demonstrations, and algorithm efficiency is a strong contribution to the field."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing multi-goal environments (e.g., Mujoco, AntMaze) to quantify VVC-Gym's improvements. The experiments are limited in scope, with no clear analysis of how specific environment design choices (e.g., reward shaping, termination conditions) directly affect algorithm performance. The demonstration sets' quality and diversity are not thoroughly characterized, and the paper does not address potential biases in their generation. Additionally, the absence of results for state-of-the-art algorithms (e.g., HER, MEGA) weakens the evaluation of VVC-Gym's utility as a benchmark."
          },
          "questions": {
            "value": [
              "How were the multi-quality demonstration sets generated, and what metrics were used to ensure their diversity and representativeness?",
              "What specific environment design choices (e.g., reward functions, termination conditions) were ablated, and how do they impact training efficiency and effectiveness?",
              "Why were certain RL algorithms prioritized over others in the baselines, and how do the results generalize across different algorithmic paradigms?",
              "Are there plans to open-source the environment and demonstrations in a more accessible format (e.g., standardized APIs) to encourage broader adoption?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces VVC-Gym, a novel reinforcement learning (RL) environment for multi-goal long-horizon problems using fixed-wing UAV velocity vector control. It includes ablation studies on environment design, multi-quality demonstration sets, and baselines for evaluating RL algorithms. The goal is to address challenges in exploration complexity by providing a realistic testbed for studying environment design, demonstration utility, and algorithm robustness."
          },
          "strengths": {
            "value": "Originality is evident in creating a domain-specific environment for fixed-wing UAVs, a less-explored application in RL. The quality of the work is supported by ablation studies and demonstration sets, though more detailed analysis is needed. Clarity is generally good, with clear problem formulations and structure. The significance lies in addressing critical challenges in multi-goal long-horizon RL, which has broad applicability in real-world control tasks."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient experimental validation. Key details about the baselines, algorithms tested, and quantitative comparisons are missing, making it hard to assess the environment's impact. The demonstration generation methodology and quality metrics are under-specified. The related work section is incomplete, limiting the context for novelty claims. The paper also does not thoroughly address how VVC-Gym differs from existing environments like Mujoco or PointMaze in solving the cited issues."
          },
          "questions": {
            "value": "1. What specific RL algorithms were tested, and how do they compare to state-of-the-art methods? 2. How were the demonstration sets generated, and what metrics define their 'quality'? 3. Are there quantitative results showing VVC-Gym's superiority over existing environments in addressing exploration challenges? 4. How were the environment design ablations conducted, and what metrics were used to evaluate their impact? 5. What is the exact definition of 'realistic' in the UAV control task, and how was it validated?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "5yDS32hKJc": {
    "paper_id": "5yDS32hKJc",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces EDQ (Earliest Disagreement Q-Evaluation), a novel deep-Q algorithm for off-policy evaluation in sequential decision-making problems with irregularly spaced time intervals. EDQ estimates the causal effect of both treatment timing and type, addressing limitations of existing methods that discretize time or ignore timing policies. The approach leverages recursion and dynamic programming, compatible with sequence models like transformers, and is validated on survival time and tumor growth tasks."
          },
          "strengths": {
            "value": "Originality: EDQ addresses a critical gap in causal effect estimation for irregularly timed interventions, combining timing and action evaluation in a novel framework. Quality: The theoretical foundation includes a theorem proving EDQ's validity under standard assumptions, and the method is designed for flexibility with sequence models. Clarity: The problem formulation and algorithm description are well-structured, though some sections are cut off. Significance: The work has high relevance for healthcare, finance, and robotics, where timing and action decisions are interdependent."
          },
          "weaknesses": {
            "value": "The paper is truncated, leaving key details about experiments, ablation studies, and comparisons with baselines incomplete. The theorem's assumptions (e.g., causal validity based on Røysland) lack explicit discussion of their practical validity. The integration of transformers into EDQ is not elaborated, and the paper does not address scalability or robustness to misspecified intensity functions. The experimental validation on tumor growth and survival tasks lacks quantitative metrics or comparisons to state-of-the-art methods."
          },
          "questions": {
            "value": [
              "How does EDQ handle varying time intervals between events, and what assumptions are made about the underlying intensity function?",
              "What specific modifications are required to adapt transformers for EDQ, and how do they interact with the recursion-based Q-function?",
              "Are the assumptions in Theorem 1 (e.g., causal validity) practical in real-world scenarios, and how sensitive is EDQ to violations of these assumptions?",
              "What baseline methods were compared against, and what metrics (e.g., mean squared error, causal effect accuracy) were used to evaluate EDQ's performance?",
              "How does EDQ scale to high-dimensional state spaces or large datasets, given its reliance on dynamic programming and regression?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces EDQ, a deep-Q algorithm for off-policy evaluation in sequential decision-making scenarios with irregularly spaced time intervals. EDQ estimates the causal effect of both treatment timing and type, leveraging recursion and sequence models like transformers. The approach is validated on survival time and tumor growth tasks, demonstrating advantages over discretization-based baselines."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in causal effect estimation for sequential decision-making under irregular time intervals, which is underexplored in existing literature. The EDQ method is theoretically grounded, combining direct regression with dynamic programming to handle flexible architectures. The use of recursion and compatibility with transformers is a novel contribution. The experiments on domain-specific tasks (e.g., survival analysis) highlight practical relevance, and the paper provides clear formal definitions of the problem setup."
          },
          "weaknesses": {
            "value": "The experimental validation is limited in scope, focusing on two synthetic tasks without real-world datasets. The comparison to baselines lacks depth, as it is unclear how EDQ outperforms existing methods beyond discretization. Theoretical guarantees (e.g., Theorem 1) rely on assumptions (e.g., causal validity from Røysland) that are not thoroughly discussed or justified. The paper also does not address computational scalability or robustness to misspecified intensity functions, which are critical in real-world applications."
          },
          "questions": {
            "value": "1. How does EDQ handle cases where the target policy's intensity function is not well-approximated by the observed data? 2. What are the specific limitations of EDQ in high-dimensional state spaces, and how does it scale to larger datasets? 3. Could the recursion-based approach introduce bias in long-horizon tasks, and how is this mitigated? 4. Are the assumptions in Theorem 1 (e.g., causal validity) practically feasible in real-world scenarios like healthcare?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces EDQ, a deep-Q algorithm for off-policy evaluation in sequential decision-making tasks with irregularly spaced time intervals. EDQ estimates the causal effect of both treatment timing and type, leveraging recursion and sequence models like transformers. The method is validated on survival time and tumor growth tasks, showing advantages over discretization-based baselines."
          },
          "strengths": {
            "value": "Originality: EDQ addresses a novel problem of jointly modeling treatment timing and type in irregularly sampled processes, which is underexplored in prior work. The use of recursion and compatibility with flexible models like transformers is creative. Quality: The paper provides a theoretical guarantee (Theorem 1) for EDQ's correctness under standard causal assumptions. Clarity: The abstract and problem definition are well-structured, though the technical depth may limit accessibility. Significance: Solving irregular time challenges in healthcare and finance applications is highly relevant, and the method's flexibility could enable broader adoption."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-sentence, leaving critical details about experiments, baselines, and implementation (e.g., how EDQ integrates with transformers) unaddressed. The theoretical analysis relies on prior work (Røysland et al.) without sufficient elaboration on how EDQ improves upon or differs from existing continuous-time methods. The validation on survival/tumor tasks lacks quantitative results, ablation studies, or comparisons to non-discretization baselines. The definition of 'earliest disagreement time' and its estimation mechanism are unclear."
          },
          "questions": {
            "value": [
              "What specific baselines were compared against EDQ? How do they differ from discretization approaches?",
              "How is the transformer model adapted to incorporate EDQ's recursion? Are there architectural details provided?",
              "What is the exact formulation of the causal assumptions in Theorem 1, and how do they differ from prior work?",
              "How is the 'earliest disagreement time' (δ) computed in practice? Is it derived from the observed trajectory, target policy, or another criterion?",
              "Are there ablation studies demonstrating the contribution of recursion vs. sequence modeling in EDQ's performance?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "5z9GjHgerY": {
    "paper_id": "5z9GjHgerY",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces DPLM-2, a multimodal diffusion protein language model that simultaneously generates protein sequences and 3D structures. It addresses the challenge of integrating sequence and structural modalities by using a lookup-free quantization-based tokenizer to convert 3D coordinates into discrete tokens and leveraging pre-trained sequence models for structural inductive biases. The model demonstrates capabilities in joint sequence-structure generation, folding, and conditional tasks."
          },
          "strengths": {
            "value": "Originality is evident in combining diffusion models with multimodal protein generation, particularly through the lookup-free quantization approach. The paper's methodology is technically sound, with clear explanations of the structure tokenizer and warm-up strategy. Clarity is enhanced by figures illustrating the model's architecture and applications. The significance lies in addressing the gap of joint sequence-structure modeling, which could advance protein engineering tasks."
          },
          "weaknesses": {
            "value": "The experimental evaluation is incomplete, with the abstract cut off mid-sentence, leaving critical results and comparisons unverified. The paper lacks detailed ablation studies to validate the warm-up strategy or self-mixup training. Comparisons to state-of-the-art models like Multiflow are superficial, and the paper does not address computational efficiency or scalability. The structure tokenizer's performance on diverse protein families is not discussed, and the impact of the lookup-free quantization on generation quality remains unclear."
          },
          "questions": {
            "value": "1. How does DPLM-2 compare quantitatively to Multiflow and other multimodal models in terms of structure-sequence consistency and generation quality? 2. What specific metrics (e.g., RMSD, sequence identity) were used to evaluate folding and scaffolding tasks? 3. Are there ablation studies demonstrating the necessity of the warm-up strategy and self-mixup training? 4. How does the lookup-free quantization handle structural variations in different protein families? 5. What are the computational costs and training requirements for DPLM-2 compared to sequence-only models?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces DPLM-2, a multimodal diffusion protein language model that jointly models protein sequences and 3D structures. The approach converts 3D coordinates into discrete tokens via a lookup-free quantization tokenizer and trains on both experimental and synthetic structures. It addresses challenges in multimodal learning through a warm-up strategy leveraging evolutionary data and self-mixup training to improve generation quality."
          },
          "strengths": {
            "value": "Originality is evident in combining diffusion models with multimodal protein generation, particularly through the lookup-free quantization tokenizer. The methodology is technically rigorous, with clear strategies for handling structural data and integrating sequence-based priors. The paper's structure and figures (e.g., Fig. 1) effectively convey the model's architecture and applications. The significance lies in addressing the critical gap of joint sequence-structure modeling, which has broad implications for protein engineering tasks."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-explanation, leaving key details about experimental results, ablation studies, and comparisons with baselines incomplete. The effectiveness of the lookup-free quantization tokenizer is not thoroughly validated, and the warm-up strategy's implementation details are vague. The claim about 'better alignment with the character' in generated proteins lacks quantitative support. Additionally, the paper does not address potential limitations of using synthetic structures for training."
          },
          "questions": {
            "value": "1. How does the lookup-free quantization tokenizer compare to other structure tokenization methods in terms of fidelity and computational efficiency? 2. What specific metrics were used to evaluate structure-sequence consistency in Fig. 1C(1)? 3. How does the warm-up strategy explicitly leverage evolutionary data to improve structural inductive biases? 4. Are there quantitative results demonstrating the superiority of DPLM-2 over Multiflow in protein folding tasks? 5. How were synthetic structures curated, and what is their distribution relative to experimental data?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces DPLM-2, a multimodal diffusion protein language model that integrates both amino acid sequences and 3D structural information. The model addresses challenges in multimodal protein generation by using a lookup-free quantization tokenizer to convert 3D coordinates into discrete tokens, training on experimental and synthetic structures, and incorporating a warm-up strategy to leverage pre-trained sequence models. It demonstrates capabilities in joint sequence-structure generation, folding, and conditional generation tasks."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel multimodal approach to protein modeling by combining sequence and structure generation within a diffusion framework, with a focus on lookup-free quantization for structural tokens. Quality: The methodology includes specific techniques like warm-up strategies and self-mixup training to address challenges in data scarcity and exposure bias. Clarity: The abstract and introduction are well-structured, with clear problem statements and contributions. Significance: Protein structure-sequence modeling is a critical area, and the paper addresses a key gap by enabling joint generation without separate models, which could impact applications like antibody design and protein engineering."
          },
          "weaknesses": {
            "value": "The paper is truncated, limiting the ability to assess full experimental results and comparisons with baselines. The novelty compared to existing methods (e.g., Multiflow) is not thoroughly discussed, and the effectiveness of the warm-up strategy or self-mixup training is not empirically validated. The paper lacks detailed ablation studies on key components like the tokenizer or training strategies. Additionally, the evaluation of structure-sequence consistency and downstream tasks (e.g., folding accuracy) is incomplete."
          },
          "questions": {
            "value": "1. How does DPLM-2 compare to Multiflow in terms of structure-sequence compatibility and generation quality? 2. What specific metrics (e.g., RMSD, sequence identity) are used to evaluate folding and inverse folding tasks? 3. How does the lookup-free quantization tokenizer outperform existing methods like structure-based tokenizers? 4. Are there ablation studies demonstrating the contribution of the warm-up strategy and self-mixup training? 5. What are the limitations of DPLM-2 in handling rare or complex structural motifs?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "5zjsZiYEnr": {
    "paper_id": "5zjsZiYEnr",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces M-LongDoc, a benchmark for evaluating multimodal long document understanding, along with a retrieval-aware tuning framework. The benchmark includes 851 samples with lengthy, multimodal documents requiring open-ended answers, and the framework improves model performance by 4.6% relative to baselines."
          },
          "strengths": {
            "value": "Originality: M-LongDoc addresses a critical gap by focusing on long, multimodal documents, which are underrepresented in existing benchmarks. The automated evaluation framework eliminates reliance on human annotation, enabling scalable assessment. Clarity: The paper is well-structured with clear descriptions of the benchmark, methodology, and experiments. Significance: The work tackles practical challenges in document understanding, with potential applications in business, legal, and academic domains."
          },
          "weaknesses": {
            "value": "The paper lacks detailed justification for the choice of document sources (e.g., are they real-world or synthetic?). The comparison to existing benchmarks (e.g., DocVQA, MMLongBench) is qualitative, with no quantitative analysis of task difficulty or model performance gaps. The 4.6% improvement is a relative metric without absolute numbers or statistical significance. The retrieval-aware tuning framework is vaguely described, with no ablation studies to validate its components. The training corpus construction method is under-specified, raising concerns about quality and representativeness."
          },
          "questions": {
            "value": [
              "How are the documents in M-LongDoc sourced? Are they real-world documents or synthetic? What criteria define their 'recentness' and 'length'?",
              "What specific metrics (e.g., BLEU, ROUGE, task-specific scores) were used to quantify the 4.6% improvement? How does this compare to human performance?",
              "How is the retrieval-aware tuning framework implemented? Does it integrate with existing models like LLaVA or Qwen-VL, and what are the computational costs?",
              "What are the limitations of the automated evaluation framework? Could it fail to capture nuanced aspects of open-ended answers?",
              "Are there any ablation studies demonstrating the effectiveness of individual components (e.g., distractor inclusion, retrieval augmentation) in the tuning framework?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces M-LongDoc, a benchmark for evaluating multimodal long document understanding, along with a retrieval-aware tuning framework to improve model performance on open-ended question-answering tasks. The benchmark includes 851 samples with lengthy, diverse documents requiring in-depth analysis, and the proposed method achieves a 4.6% relative improvement in answer correctness compared to baseline models."
          },
          "strengths": {
            "value": "The paper's originality lies in addressing the critical gap of evaluating multimodal models on long, complex documents with open-ended answers, which prior benchmarks lacked. The significance is high, as it tackles real-world challenges in document understanding. The clarity is strong, with clear problem formulation, examples, and comparisons. The contribution of a scalable evaluation framework and a novel retrieval-aware tuning approach demonstrates practical value for the community."
          },
          "weaknesses": {
            "value": "The automatic construction of the training corpus lacks detailed methodology, raising concerns about data quality and relevance. The evaluation framework's reliance on judge models needs justification of their reliability. The paper does not provide ablation studies to isolate the impact of the retrieval-aware tuning components. Additionally, the generalizability of the benchmark is unclear, as examples focus on specific domains (e.g., technical manuals). The comparison to baselines is limited to preliminary experiments without rigorous baseline selection."
          },
          "questions": {
            "value": [
              "How is the automatic training corpus constructed? What criteria ensure its relevance and quality for multimodal document understanding?",
              "How are the judge models in the evaluation framework validated? Are they trained on human-labeled data or other benchmarks?",
              "What specific components of the retrieval-aware tuning framework contribute to the 4.6% improvement? Are there ablation studies to confirm this?",
              "How diverse is the M-LongDoc dataset in terms of document types and domains? Are there plans to expand it to cover more real-world scenarios?",
              "How does the paper address potential biases in the automatic evaluation framework, such as overfitting to specific answer patterns?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces M-LongDoc, a benchmark for evaluating multimodal models on long documents with 851 samples, and a retrieval-aware tuning framework to improve performance. The benchmark includes lengthy documents (up to 210 pages) with open-ended questions requiring in-depth understanding, while the framework addresses multimodal bias and irrelevant content through retrieval-aware training."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in document understanding benchmarks by focusing on long, multimodal documents with open-ended questions, which is more representative of real-world scenarios. The automated evaluation framework is innovative, avoiding reliance on human annotations. The retrieval-aware tuning approach introduces a novel method for handling multimodal long documents, and the benchmark's scale (210-page documents) is a significant contribution. The work also provides a training corpus of 10,070 samples, enabling reproducibility."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons to state-of-the-art models beyond baseline open-source models, making it difficult to assess the significance of the 4.6% improvement. The training corpus construction methodology is under-described, raising questions about its representativeness and quality. The evaluation framework's judge models and scoring mechanism are not thoroughly validated. The paper also does not address potential limitations of the benchmark, such as domain specificity or diversity of document types. Experimental results lack ablation studies to isolate the impact of the retrieval-aware tuning components."
          },
          "questions": {
            "value": [
              "Which specific open-source models were used as baselines, and how do they compare to recent state-of-the-art multimodal models?",
              "Can the authors provide details on how the training corpus was automatically constructed and validated for quality?",
              "How were the judge models in the evaluation framework trained, and what metrics were used to ensure their reliability?",
              "Are there ablation studies demonstrating the effectiveness of individual components of the retrieval-aware tuning framework?",
              "How does the benchmark handle domain-specific challenges (e.g., legal, scientific documents) that might affect model performance?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "60rQpnbgmE": {
    "paper_id": "60rQpnbgmE",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of confidence estimation in large language models (LLMs) for reasoning tasks, particularly under resource constraints. The authors propose two methods, Perplexity Consistency (PC) and Reasoning-pruning Perplexity Consistency (RPC), which leverage accurate LLM probability estimates and pruning of low-probability reasoning paths to reduce variance and improve error reduction rates. They introduce a new problem setting, Resource-Constrained Confidence Estimation, and provide theoretical analysis and empirical validation across seven benchmark datasets."
          },
          "strengths": {
            "value": "The paper's originality lies in its novel problem formulation of resource-constrained confidence estimation, which addresses practical limitations like limited samples and no calibration data. The integration of LLM probability estimates with self-consistency confidence is creative, and the theoretical analysis of quadratic error reduction ($\\mathcal{O}(1/n^2)$) is a strong contribution. The empirical results across diverse tasks (mathematical reasoning, code generation) demonstrate practical significance. The paper also clarifies the trade-offs between variance reduction and reasoning capability, which is valuable for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with existing methods like temperature scaling or other calibration techniques, which are critical for validating the superiority of the proposed approach. The theoretical analysis of error decomposition and exponential convergence is abstract and not sufficiently grounded in empirical evidence. The reasoning pruning technique is vaguely described, with no justification for how pruning thresholds are determined or how it avoids discarding valid reasoning paths. Additionally, the experiments do not include ablation studies to isolate the contributions of PC and RPC, and the supplementary material's code availability is not thoroughly explained."
          },
          "questions": {
            "value": "1. How does the proposed method compare to temperature scaling or other calibration techniques in terms of performance without requiring a calibration set? 2. What is the exact mechanism of reasoning pruning, and how are the thresholds for pruning determined? 3. Are there scenarios where pruning might remove correct reasoning paths, and how is this mitigated? 4. How does the theoretical error decomposition translate to practical improvements in real-world tasks? 5. Are the empirical results reproducible, and what specific baselines were used for comparison?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a novel problem setting called Resource-Constrained Confidence Estimation for Large Language Models (LLMs), where the goal is to accurately estimate self-consistency confidence with limited samples and no calibration set. The authors propose two methods, Perplexity Consistency (PC) and Reasoning-pruning Perplexity Consistency (RPC), which integrate LLM probability estimates and prune low-probability reasoning paths. Theoretical analysis shows PC achieves a quadratic error decay rate, and experiments on seven benchmark datasets demonstrate improved accuracy and calibration."
          },
          "strengths": {
            "value": "The paper presents a novel problem formulation (resource-constrained confidence estimation) that addresses practical limitations in LLM reasoning. The integration of LLM probability estimation with self-consistency is creative, and the theoretical analysis of error decay rates (O(1/n²)) is a strong contribution. The experimental results across multiple domains (math reasoning, code generation) highlight the method's generalizability. The clarity of the motivation and structure is commendable, with well-defined contributions."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies or comparisons with baseline methods beyond what is briefly mentioned. The theoretical analysis assumes idealized conditions (e.g., perfect probability estimates) that may not hold in practice. The reasoning pruning mechanism is described conceptually but lacks implementation details or computational cost analysis. The experiments section is not fully provided, limiting the ability to assess the method's scalability or robustness."
          },
          "questions": {
            "value": [
              "How does the PC method handle cases where the LLM's probability estimates are inaccurate? What are the assumptions required for the theoretical error bounds?",
              "Can the authors provide more details on the reasoning pruning algorithm? How is the threshold for pruning determined, and what is its impact on computational efficiency?",
              "The paper mentions experiments on seven datasets but does not include results. Could the authors share additional metrics (e.g., calibration curves, per-dataset performance) to validate the method's effectiveness?",
              "How does the RPC approach balance the trade-off between error reduction and the potential loss of valid reasoning paths during pruning?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of confidence estimation in large language models (LLMs) under resource constraints. The authors propose two methods, Perplexity Consistency (PC) and Reasoning Pruning (RPC), which integrate LLM prediction probabilities and prune low-probability reasoning paths to reduce variance and improve error reduction rates. Theoretical analysis and experiments on seven benchmark datasets demonstrate superior performance in accuracy and calibration compared to existing methods."
          },
          "strengths": {
            "value": "The paper introduces a novel problem setting—Resource-Constrained Confidence Estimation—which addresses practical limitations in LLM reasoning tasks. The integration of LLM probability estimation with self-consistency confidence is original and theoretically grounded, with claims of quadratic error reduction. The experiments cover diverse tasks (mathematical reasoning, code generation), and the paper provides clear methodological contributions. Theoretical analysis and empirical results collectively strengthen the validity of the approach."
          },
          "weaknesses": {
            "value": "The theoretical analysis lacks detailed justification for assumptions, such as the validity of using zero-variance LLM probabilities. The error decomposition analysis is not thoroughly explained, leaving questions about how pruning achieves exponential convergence. The experiments, while extensive, do not include ablation studies or comparisons with state-of-the-art methods beyond self-consistency. The claim of 'data-free' methods is ambiguous, as it is unclear how the approach avoids all external data dependencies."
          },
          "questions": {
            "value": [
              "How does the PC method ensure the accuracy of LLM prediction probabilities when sampling is limited? Are there scenarios where this assumption fails?",
              "What specific conditions or constraints are required for the theoretical quadratic error reduction rate to hold? Are these assumptions validated empirically?",
              "How is the 'reasoning pruning' step implemented in practice? What metrics are used to determine which paths to prune, and how does this affect computational efficiency?",
              "The paper mentions 'data-free' methods, but it is unclear whether the approach relies on pre-trained LLM probabilities or requires additional data for calibration. Clarification is needed.",
              "How does the RPC method compare to other pruning techniques in terms of effectiveness and computational cost? Are there cases where pruning could harm performance?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "61ss5RA1MM": {
    "paper_id": "61ss5RA1MM",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes OC-Flow, a training-free framework for guided flow matching using optimal control theory. It addresses both Euclidean and SO(3) manifolds, providing theoretical guarantees and demonstrating superior performance on tasks like text-guided image generation and molecular design."
          },
          "strengths": {
            "value": "The paper offers a theoretically grounded approach to guided flow matching, extending prior work to complex geometries like SO(3), which is critical for scientific applications. The connection to existing methods (e.g., D-Flow, FlowGrad) as special cases provides a unified perspective. The experiments span multiple domains, showcasing practical utility. The optimal control formulation introduces a novel way to balance prior constraints and target rewards."
          },
          "weaknesses": {
            "value": "The theoretical analysis for SO(3) is superficial, with limited details on convergence guarantees or how the method handles Riemannian geometry. The experiments on SO(3) lack direct comparisons to specialized methods for that manifold. The approximation techniques for computational efficiency are not thoroughly evaluated. The paper also does not address potential limitations of the optimal control formulation, such as sensitivity to hyperparameters or scalability."
          },
          "questions": {
            "value": [
              "How is the convergence analysis for SO(3) rigorously derived, given the complexities of Riemannian manifolds?",
              "What specific benchmarks or baselines were used for SO(3) experiments, and how do they compare to prior work on that manifold?",
              "Are the approximation techniques for SO(3) validated in terms of both computational efficiency and accuracy?",
              "How does OC-Flow handle the non-Euclidean geometry in the co-state flow and control term updates?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces OC-Flow, a training-free framework for guided flow matching using optimal control theory. It formulates controlled generation as an optimal control problem with a running cost to regularize trajectory proximity to prior distributions while optimizing target rewards. The method is generalized to both Euclidean and SO(3) manifolds, with theoretical convergence guarantees and practical algorithms for complex geometries like those in protein design."
          },
          "strengths": {
            "value": "The paper presents a novel theoretical framework (OC-Flow) that bridges optimal control theory with flow matching, addressing a key gap in guided generation methods. The extension to SO(3) manifolds is significant for scientific applications, and the theoretical analysis of convergence in both Euclidean and non-Euclidean spaces is rigorous. The practical implementation includes efficient techniques like vector-Jacobian products and asynchronous updates, demonstrated through diverse experiments on image manipulation, molecule generation, and peptide design."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing backprop-through-ODE methods (e.g., D-Flow, Flow-Grad) in terms of computational efficiency and performance on SO(3) tasks. The convergence analysis assumes continuous-time formulations, but the discrete implementation details and their impact on theoretical guarantees are not thoroughly discussed. Additionally, the practical benefits of OC-Flow's running cost formulation over simpler baselines (e.g., direct gradient weighting) are not empirically validated."
          },
          "questions": {
            "value": [
              "How does OC-Flow's running cost formulation specifically improve upon existing gradient-based guidance methods in terms of stability or sample quality?",
              "What are the computational trade-offs between OC-Flow and Euclidean-specific methods like D-Flow when applied to SO(3) manifolds?",
              "Can the convergence guarantees for SO(3) be extended to other non-Euclidean manifolds, and what assumptions are critical for this generalization?",
              "Are there limitations to the asynchronous update scheme in OC-Flow for high-dimensional or highly curved manifolds?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces OC-Flow, a training-free framework for guided flow matching using optimal control theory. It addresses limitations in existing methods by providing theoretical guarantees for convergence in both Euclidean and SO(3) manifolds, unifying prior approaches like D-Flow and Flow-Grad as special cases, and demonstrating effectiveness on diverse tasks including text-guided image manipulation and molecular generation."
          },
          "strengths": {
            "value": "The paper's originality lies in applying optimal control theory to flow matching, offering a novel theoretical foundation for guided generation. The systematic analysis of convergence guarantees and extension to SO(3) manifolds (critical for scientific applications) is significant. The experiments across multiple domains (images, molecules, peptides) highlight practical relevance. The paper's clarity is strong, with well-structured sections and clear connections to prior work."
          },
          "weaknesses": {
            "value": "The theoretical analysis for SO(3) lacks detailed mathematical rigor, particularly in how the Extended Method of Successive Approximations (E-MSA) is adapted. The experiments on SO(3) tasks (e.g., protein design) are not sufficiently detailed, and the computational efficiency of the proposed methods on complex manifolds is not thoroughly evaluated. Additionally, the paper does not address potential limitations of the optimal control formulation, such as sensitivity to hyperparameters or scalability."
          },
          "questions": {
            "value": "1. How does OC-Flow specifically handle the non-Euclidean geometry of SO(3) in its optimal control formulation? 2. What are the computational bottlenecks in applying OC-Flow to high-dimensional manifolds like SO(3), and how does the proposed approximation technique mitigate them? 3. Are there scenarios where the theoretical convergence guarantees might fail, and how are these addressed in practice? 4. How does the choice of the reward-weighting factor α impact performance, and what guidelines are provided for its selection?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "63Pq7q7ybl": {
    "paper_id": "63Pq7q7ybl",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper addresses domain adaptation in neural machine translation (NMT) by leveraging only monolingual data from the target domain. The authors propose using energy-based models (EBMs) and Conditional Distributional Policy Gradients (CDPG) to fine-tune pre-trained NMT models, avoiding catastrophic forgetting by approximating EBMs based on unigram distributions of the target domain."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to domain adaptation by focusing on monolingual data, which is more accessible than parallel data. The use of EBMs and CDPG to model domain-specific constraints is creative and addresses the challenge of catastrophic forgetting. The method's scalability to specific domains via unigram distributions and its demonstrated effectiveness on benchmarks highlight its practical relevance. The work also extends prior CDPG frameworks by incorporating a larger number of EBMs, showing promise for robust domain shifts."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of key components, such as the number of EBMs or the role of unigram distributions. The dynamic CDPG approach is mentioned but not thoroughly explained or compared to static CDPG. The theoretical justification for why the method avoids catastrophic forgetting is underdeveloped. Additionally, the experiments focus on limited domains and benchmarks, leaving gaps in understanding the method's generalizability. The novelty of combining EBMs with CDPG for domain adaptation is not clearly contextualized against prior work."
          },
          "questions": {
            "value": [
              "How are the EBMs constructed from unigram distributions? What specific features or constraints are encoded in the energy functions?",
              "What is the exact mechanism for dynamically adjusting parameters using bilingual validation data in DYNAMIC CDPG? How does this compare to static CDPG in terms of performance and complexity?",
              "Are there limitations to the types of domain shifts this method can handle (e.g., syntactic vs. lexical shifts)? How does the approach scale to more complex or larger domains?",
              "What ablation studies have been conducted to validate the contribution of unigram distributions versus other potential features (e.g., n-grams, contextual embeddings)?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes a method for domain adaptation in neural machine translation (NMT) using only monolingual data from the target domain. It leverages energy-based models (EBMs) and Conditional Distributional Policy Gradients (CDPG) to fine-tune pre-trained NMT models without catastrophic forgetting, aiming to shift the model's distribution to match the target domain's unigram statistics."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in NMT: domain adaptation with limited parallel data. The use of EBMs and CDPG for distributional control is novel, particularly in extending prior work to handle large-scale domain shifts. The method's theoretical foundation is sound, and the focus on monolingual data aligns with real-world constraints. The paper also introduces dynamic CDPG, which adds flexibility for parameter selection. The clarity of the problem formulation and technical details is strong, with clear connections to prior work."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive experimental validation, as the provided content cuts off before the results section. Key details about the implementation of the large-scale EBMs, the specific monolingual data used, and the evaluation metrics are missing. The claim that CDPG avoids catastrophic forgetting requires stronger empirical support. Additionally, the paper does not compare against alternative domain adaptation methods (e.g., unsupervised or semi-supervised approaches), limiting the assessment of its novelty and effectiveness."
          },
          "questions": {
            "value": [
              "What specific domain adaptation benchmarks and datasets were used to evaluate the method? How do the results compare to existing unsupervised or semi-supervised approaches?",
              "How were the monolingual target domain data processed to construct the EBMs? Were there any preprocessing steps or statistical features extracted beyond unigram distributions?",
              "What is the computational cost of training with a large number of EBMs, and how does it scale to real-world applications?",
              "How was the dynamic CDPG implemented? What criteria were used to select the best parameters from the bilingual validation data?",
              "Are there ablation studies demonstrating the necessity of using a large number of EBMs versus a smaller set? How does the method handle overlapping constraints or conflicting domain features?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes a method for domain adaptation in neural machine translation (NMT) using only monolingual data from the target domain. The approach leverages energy-based models (EBMs) and Conditional Distributional Policy Gradients (CDPG) to fine-tune a pre-trained NMT model, avoiding catastrophic forgetting by incorporating domain-specific unigram distributions as constraints. The method is evaluated on domain adaptation benchmarks, demonstrating effectiveness with minimal parallel data."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel framework for domain adaptation without relying on parallel data, instead utilizing monolingual resources and distributional constraints. The extension of CDPG to handle large-scale EBMs with unigram distributions is a creative application. Quality: The method is theoretically grounded, with clear formulations of EBMs and CDPG. Clarity: The paper provides detailed explanations of the mathematical framework and its connection to domain adaptation. Significance: Domain adaptation is critical for real-world NMT systems, and the focus on monolingual data addresses a practical challenge where parallel data is scarce."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with existing methods that use monolingual data for domain adaptation, making it difficult to assess the novelty and effectiveness of the proposed approach. The experiments focus on a limited set of benchmarks without detailed analysis of the impact of hyperparameters or the trade-offs between different constraints. The claim of 'unsupervised domain adaptation' is weakened by the use of a small amount of bilingual validation data in the DYNAMIC CDPG component. The theoretical justification for why the method avoids catastrophic forgetting is underdeveloped, relying primarily on empirical results."
          },
          "questions": {
            "value": [
              "How does the proposed method compare to other monolingual-based domain adaptation approaches (e.g., style transfer or data augmentation) in terms of performance and efficiency?",
              "What is the exact mechanism for dynamically selecting parameters in DYNAMIC CDPG, and how does it ensure robustness without explicit parallel data?",
              "Are the unigram distribution constraints sufficient to capture complex domain-specific characteristics, or are there cases where they might fail?",
              "What ablation studies were conducted to validate the contribution of individual components (e.g., the number of EBMs, the use of distributional constraints vs. binary constraints)?",
              "How does the computational cost scale with the number of EBMs, and what are the practical limitations of the approach for large-scale domains?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "67X93aZHII": {
    "paper_id": "67X93aZHII",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of merging LoRA-finetuned models by proposing KnOTS, a method that uses singular value decomposition (SVD) to align task-updates across models. The authors demonstrate that LoRA models exhibit lower alignment compared to fully-finetuned models and show that KnOTS improves merging performance by up to 4.3% on vision and language benchmarks. They also introduce a new joint-evaluation benchmark to assess the generalizability of merged models."
          },
          "strengths": {
            "value": "The paper introduces a novel approach (KnOTS) for aligning LoRA model updates via SVD, addressing a critical gap in model merging literature. The method is simple yet effective, with clear experimental validation across multiple benchmarks. The introduction of a new joint-evaluation benchmark adds value by explicitly testing the generalizability of merged models. The writing is clear, and the problem statement is well-motivated, particularly given the popularity of LoRA in parameter-efficient fine-tuning."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough comparison with alternative alignment methods (e.g., gradient-based or task-specific alignment techniques). The theoretical justification for using SVD to align task-updates is underdeveloped, and the paper does not analyze why SVD is effective in this context. The experiments focus on aggregate performance gains but lack ablation studies to isolate the contribution of SVD versus other components. Additionally, the new benchmark is not described in sufficient detail to assess its validity or how it differs from existing ones."
          },
          "questions": {
            "value": "1. How does KnOTS compare to other SVD-based methods for LoRA alignment (e.g., Meng et al. 2024)? 2. What is the computational cost of applying SVD to large models, and does it scale to real-world scenarios? 3. Are there cases where KnOTS fails, and what factors determine its effectiveness? 4. How is the joint-evaluation benchmark constructed, and what metrics are used to assess 'general' models? 5. Can the SVD alignment be applied to non-LoRA fine-tuning methods, or is it specific to LoRA?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes KnOTS, a method for improving model merging of LoRA-finetuned models by using SVD to align their task-updates. It introduces a new benchmark to evaluate the generalization of merged models and demonstrates improvements over existing merging techniques."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in model merging for parameter-efficient finetuning methods like LoRA, which are widely used but poorly supported by existing merging techniques. The SVD-based alignment approach is novel and well-motivated by empirical analysis of alignment discrepancies. The introduction of a new benchmark for evaluating general models adds significant value. The methodology is clearly explained with illustrative figures, and the experiments show consistent improvements across multiple tasks."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough comparison with existing SVD-based approaches for LoRA alignment, which limits the understanding of KnOTS's novelty. The new benchmark's design is not fully detailed, and its validity as a generalization metric requires further justification. The theoretical analysis of why SVD alignment improves merging is limited, and the experiments could include ablation studies on SVD parameters or scalability analysis. The paper is cut off mid-section, leaving some claims (e.g., SVD-based LoRA approaches) underdeveloped."
          },
          "questions": {
            "value": [
              "How does KnOTS compare to other SVD-based alignment methods for LoRA models? Are there existing works that use SVD for similar purposes?",
              "What specific limitations of the new benchmark could affect its reliability as a generalization metric?",
              "Are there cases where KnOTS fails to improve merging performance, and what factors contribute to this?",
              "How sensitive is KnOTS to hyperparameters like the rank of SVD or the choice of merging method applied to V matrices?",
              "What is the computational cost of KnOTS compared to baseline merging methods, and how does it scale with model size?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of merging LoRA-finetuned models, which are less compatible with existing model merging techniques compared to fully-finetuned models. The authors propose KnOTS, a method that uses singular value decomposition (SVD) to align task-updates across LoRA models before applying standard merging strategies. They also introduce a new benchmark to evaluate the generalization capabilities of merged models."
          },
          "strengths": {
            "value": "The paper presents a novel solution to a specific problem in model merging, leveraging SVD for alignment—a creative approach that addresses a gap in the literature. The experimental results demonstrate consistent improvements (up to 4.3%) across vision and language benchmarks, highlighting the practical value of the method. The introduction of a new benchmark for evaluating general models adds significant theoretical and practical contributions. The writing is clear, and the motivation for the problem is well-justified, emphasizing the importance of LoRA in efficient fine-tuning."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed comparison with existing SVD-based methods for LoRA alignment, which limits the understanding of KnOTS's novelty. The theoretical justification for using SVD to align task-updates is superficial, and the paper does not thoroughly analyze how the choice of SVD rank affects performance. The new benchmark's design is not fully described, making it difficult to assess its validity. Additionally, the experiments focus on a limited set of tasks, and the paper does not explore the method's scalability to larger models or more diverse datasets."
          },
          "questions": {
            "value": [
              "How does KnOTS handle varying layer structures or non-uniform LoRA rank configurations across models?",
              "What is the computational overhead of applying SVD to large-scale models, and how does it compare to existing merging methods?",
              "Can the new benchmark be adapted to evaluate other merging techniques, and what metrics does it use to assess generalization?",
              "Are there ablation studies demonstrating the importance of specific components of KnOTS (e.g., SVD vs. other alignment methods)?",
              "How does the merged V matrix in KnOTS influence task-specific performance versus generalization?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "689MfSyeNz": {
    "paper_id": "689MfSyeNz",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes ZoomVLM, a tuning-free framework for efficient video understanding that mimics human 'zoom-then-focus' perception by adaptively allocating tokens between video overviews and detailed segments. The method combines a Video Overview Augmenter and Adaptive Token Adjustment to reduce token counts while maintaining accuracy."
          },
          "strengths": {
            "value": "Originality: The human-perception-inspired 'zoom-then-focus' strategy offers a novel approach to video token efficiency. Quality: The paper presents clear methodological components with specific implementation details. Clarity: Technical concepts are well-explained with illustrative examples. Significance: Addressing the critical efficiency bottleneck in video VLMs has practical importance for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies on the two core components (Overview Augmenter and Token Adjustment). Experimental validation is incomplete - the results section is cut off, and it's unclear if the 30% improvement is statistically significant or generalizable across datasets. The 'tuning-free' claim needs clarification - does it require any parameter adjustments during inference? The method's adaptability to different video content types (e.g., dynamic vs static scenes) is not discussed."
          },
          "questions": {
            "value": [
              "How exactly is the Video Overview Augmenter implemented? Does it use pre-trained models or custom architectures?",
              "What specific metrics are used to determine 'importance' in the Adaptive Token Adjustment component?",
              "Are the results consistent across different video lengths and complexities?",
              "How does ZoomVLM handle videos where the 'important' regions change dynamically during generation?",
              "What is the exact implementation of the 'tuning-free' aspect - are there any hyperparameters that need adjustment?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes ZoomVLM, a tuning-free framework for efficient video understanding in vision-language models (VLMs). Inspired by human perception, ZoomVLM generates a video overview and adaptively allocates tokens to critical segments during inference, reducing token count while maintaining accuracy. Key components include a Video Overview Augmenter and Adaptive Token Adjustment, validated through experiments showing improved inference efficiency and throughput."
          },
          "strengths": {
            "value": "The paper introduces a novel, human-inspired approach to video VLM efficiency, addressing a critical bottleneck in token-heavy architectures. The tuning-free, plug-and-play design is practical for real-world deployment. Experiments demonstrate significant improvements in token generation rate and accuracy, with clear ablation studies. The work bridges human perception insights with technical innovation, offering a meaningful contribution to video understanding research."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how LLM attention distributions are leveraged for token allocation, leaving the mechanism somewhat abstract. The experiments focus on specific models (e.g., Llava-Next-Video-7B-DPO) without broader validation across diverse architectures. The Adaptive Token Adjustment's implementation details and generalizability are under-specified. Additionally, the comparison to alternative efficiency methods (e.g., Q-Former) is limited, and the paper does not address potential trade-offs in edge cases."
          },
          "questions": {
            "value": [
              "How exactly does the attention distribution from the LLM guide token allocation? Are there quantitative metrics to validate this relationship?",
              "What is the exact mechanism of the Adaptive Token Adjustment? How is 'importance' of video segments defined and computed?",
              "How does the Video Overview Augmenter handle videos with varying temporal dynamics or content complexity?",
              "Are there scenarios where ZoomVLM's token reduction leads to information loss, and how is this mitigated?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces ZoomVLM, a tuning-free framework for efficient video understanding in vision-language models (VLMs). Inspired by human perception, ZoomVLM reduces the number of tokens required for video processing by first generating a high-level overview and then adaptively allocating tokens to critical video segments. The framework combines a Video Overview Augmenter and Adaptive Token Adjustment to improve inference efficiency without sacrificing accuracy."
          },
          "strengths": {
            "value": "The paper presents a novel approach inspired by human perception, addressing a critical efficiency bottleneck in video VLMs. The tuning-free and plug-and-play design is practical for real-world applications. The experiments demonstrate significant improvements in throughput and accuracy, with clear ablation studies validating the components. The work is well-structured, with a strong emphasis on both theoretical insights and empirical results."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical explanations of how attention distributions in LLMs guide token allocation, which is central to the method. The ablation studies focus on a single model (Llava-Next-Video-7B-DPO) and may not generalize to other architectures. The comparison with prior work is limited, particularly in terms of computational efficiency metrics. Additionally, the paper does not address potential limitations in handling complex or long videos where token reduction might lead to information loss."
          },
          "questions": {
            "value": [
              "How exactly does the attention distribution from the LLM determine where to allocate or discard tokens? What specific mechanisms or thresholds are used?",
              "The experiments focus on one model (Llava-Next-Video-7B-DPO). How do the results generalize to other VLMs, such as those with different architectures or training paradigms?",
              "What is the impact of the Video Overview Augmenter on videos with varying lengths or content complexity? Are there scenarios where the overview might miss critical details?",
              "The paper mentions a 30% increase in token generation rate but does not provide baseline metrics for computational resources (e.g., GPU memory, inference time). How does ZoomVLM compare in terms of resource efficiency?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "6ADnEk90R2": {
    "paper_id": "6ADnEk90R2",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes CoMMIT, a method for coordinated instruction tuning in multimodal large language models (MLLMs), addressing learning imbalance between the feature encoder and the backbone LLM. It introduces a Multimodal Balance Coefficient, a dynamic learning scheduler, and gradient regularization to mitigate issues like oscillation and insufficient training. The approach is architecture-agnostic and demonstrated on multiple downstream tasks."
          },
          "strengths": {
            "value": "The paper presents a novel theoretical framework to analyze learning imbalance in MLLMs, which is a significant contribution. The dynamic scheduler and gradient regularization are well-motivated and theoretically grounded. The method's generality across architectures and the convergence analysis add to its value. The empirical results, though limited in detail, suggest practical effectiveness."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental comparisons with state-of-the-art methods, such as specific baselines from the related work section (e.g., Li et al. 2023, Liu et al. 2024). The theoretical analysis of the balance coefficient and convergence is abstract, with limited concrete examples or validation. The gradient regularization's mechanism and its impact on training are not thoroughly explained. The truncated content (e.g., Section 3) prevents a full assessment of the methodology."
          },
          "questions": {
            "value": [
              "How does CoMMIT compare to existing MLLM instruction tuning methods in terms of quantitative metrics (e.g., accuracy, efficiency)?",
              "What specific implementations of the Multimodal Balance Coefficient are used, and how is it calculated during training?",
              "Are the theoretical convergence guarantees validated with empirical results, and if so, on which datasets or tasks?",
              "How does the gradient regularization interact with different optimizers, as claimed in the paper?",
              "What are the limitations of the proposed method in terms of computational cost or scalability?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes CoMMIT, a method for coordinated instruction tuning in multimodal large language models (MLLMs), addressing imbalanced learning between feature encoders and LLMs. It introduces a Multimodal Balance Coefficient to quantify learning balance, a dynamic learning rate scheduler, and gradient regularization to mitigate oscillation and insufficient training issues. The approach is architecture-agnostic and demonstrates effectiveness across multiple downstream tasks."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel theoretical framework and practical techniques (balance coefficient, dynamic scheduler, gradient regularization) to address imbalanced learning in MLLMs. Quality: The work combines theoretical analysis with empirical validation, including convergence rate analysis. Clarity: The problem statement and methodology are well-structured, with clear motivation for the proposed solutions. Significance: MLLMs are critical for real-world applications, and addressing their training inefficiencies has broad implications for multimodal AI systems."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing methods, such as how CoMMIT differs from prior scheduling techniques. The experiments are described as 'multiple downstream tasks' but do not specify which benchmarks or MLLMs were used, limiting reproducibility. The theoretical analysis of convergence rate is abstract and lacks concrete examples. The gradient regularization's implementation details and hyperparameter sensitivity are not discussed, which could affect practical adoption."
          },
          "questions": {
            "value": [
              "How is the Multimodal Balance Coefficient calculated in practice? Are there specific metrics or formulas provided?",
              "What ablation studies were conducted to validate the individual components (scheduler vs. regularization)?",
              "How does CoMMIT compare to existing methods like unified training or modality-specific fine-tuning? Are there quantitative results on this?",
              "Which specific MLLMs and benchmarks were used in the experiments? How generalizable is the approach across different architectures?",
              "What is the computational overhead of the dynamic scheduler and regularization? Are there trade-offs in training speed or resource usage?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes CoMMIT, a method for coordinated instruction tuning in multimodal large language models (MLLMs) by addressing unbalanced learning between the backbone LLM and feature encoder. It introduces a Multimodal Balance Coefficient, a dynamic learning rate scheduler, and gradient regularization to mitigate oscillation and insufficient training issues."
          },
          "strengths": {
            "value": "The paper's originality lies in its theoretical analysis of learning imbalance in MLLMs and the introduction of a quantifiable balance coefficient. The dynamic scheduler and gradient regularization are practical solutions with clear motivation. The empirical results demonstrate effectiveness across multiple tasks and architectures, and the problem statement is well-contextualized. The work's generality across MLLM designs adds to its significance."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of the balance coefficient, scheduler, and regularization. The theoretical convergence analysis is superficial, with unclear proofs and limited discussion of assumptions. Experimental validation is sparse, with no comparison to state-of-the-art MLLM instruction tuning methods. The balance coefficient's computation and hyperparameter sensitivity are not thoroughly explained, limiting reproducibility."
          },
          "questions": {
            "value": [
              "How is the Multimodal Balance Coefficient mathematically defined, and what are its key components?",
              "What ablation studies were conducted to validate the individual contributions of the scheduler and gradient regularization?",
              "Are there specific scenarios where CoMMIT fails to address oscillation or insufficient learning?",
              "How does the gradient regularization interact with different optimizers (e.g., Adam vs. SGD)?",
              "Which specific MLLMs and downstream tasks were used in experiments, and why were they selected?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "6HcnC3pPkp": {
    "paper_id": "6HcnC3pPkp",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces Token-Supervised Value Models (TVMs), a novel class of verifiers for tree-search-based inference in mathematical problem-solving. TVMs assign probability scores to individual tokens based on their likelihood of leading to the correct final answer, addressing limitations of existing outcome-supervised (ORMs) and process-supervised (PRMs) verifiers that struggle with indirect evaluation of partial solutions during tree search."
          },
          "strengths": {
            "value": "The paper presents a creative solution to a critical problem in LLM verification for mathematical tasks. The token-level supervision approach is original and addresses a clear gap in existing methods. The experimental examples (e.g., Figure 1) demonstrate intuitive improvements over ORMs and PRMs. The paper's structure and problem formulation are clear, and the significance of improving tree-search-based inference for mathematical reasoning is well justified."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with key sections (e.g., full experiments, contribution details) cut off, making it difficult to assess the rigor of claims. The method's implementation details (e.g., how token-level labels are generated, training procedure) are not fully described. The comparison with baselines lacks quantitative metrics, and the paper does not address potential limitations of TVMs (e.g., scalability, generalization to non-math tasks)."
          },
          "questions": {
            "value": [
              "How are the token-level labels for TVMs generated? Are they based on human annotations, programmatic checks, or another method?",
              "What is the exact architecture of TVMs, and how do they differ from ORMs/PRMs in terms of training objectives?",
              "Are there quantitative results comparing TVMs to state-of-the-art verifiers on standard benchmarks like GSM8K or MathQA?",
              "How does the token-level supervision reduce false negative errors compared to PRMs, and what metrics were used to measure this?",
              "What is the computational cost of TVMs compared to ORMs/PRMs, and how does this impact their suitability for real-time tree search?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Token-Supervised Value Models (TVMs), a novel class of verifiers that assign probabilities to individual tokens in mathematical problem-solving solutions to directly evaluate partial solutions during tree search. This approach aims to address limitations of existing verifiers (ORMs and PRMs) by reducing false negatives and improving the accuracy of tree-search-based inference strategies."
          },
          "strengths": {
            "value": "The paper presents a novel idea of token-level supervision for verifiers, which is a significant departure from existing outcome- or process-level supervision methods. The motivation is well-articulated, highlighting the limitations of prior verifiers in tree search scenarios. The experimental comparison (e.g., Figure 1) visually demonstrates TVMs' advantages over ORMs and PRMs. The problem statement and technical contributions are clearly defined, and the paper addresses an important gap in mathematical reasoning for LLMs."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-sentence, leaving critical details about the methodology, training data, and experimental results incomplete. The proposed TVMs' token-level supervision mechanism lacks explanation of how labels are generated (e.g., whether they rely on final answer correctness or a more nuanced process). The experimental validation appears limited to a single example (GSM8K), with no comprehensive comparison against state-of-the-art verifiers or ablation studies. The paper also fails to address potential limitations of TVMs, such as scalability or generalization to non-math tasks."
          },
          "questions": {
            "value": "1. How are token-level labels generated for training TVMs? Are they derived solely from final answer correctness (like ORMs) or through a more granular process? 2. What is the architecture of TVMs, and how do they process sequential token probabilities? 3. Are there quantitative metrics (e.g., F1 scores, pruning rates) to support the claim that TVMs reduce false negatives compared to PRMs? 4. How do TVMs handle ambiguous or multi-step reasoning where intermediate tokens may have varying degrees of correctness? 5. What is the computational cost of TVMs compared to ORMs/PRMs, and how does this impact their practicality for large-scale tree search?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Token-Supervised Value Models (TVMs), a novel class of verifiers for mathematical problem-solving tasks. TVMs assign probabilities to individual tokens to evaluate partial solutions during tree search, addressing limitations of existing outcome-supervised reward models (ORMs) and process-supervised reward models (PRMs). The approach aims to improve the accuracy of large language models (LLMs) by enabling more explicit and direct evaluation of intermediate steps."
          },
          "strengths": {
            "value": "The paper presents a novel verification framework (TVMs) that addresses critical gaps in existing methods for mathematical problem-solving. The methodology is theoretically grounded, with clear motivation for token-level supervision to avoid the limitations of ORMs and PRMs. The example in Figure 1 effectively illustrates TVMs' advantages over prior approaches. The significance of improving tree-search-based inference for LLMs is well justified, and the paper's focus on practical applications in mathematical reasoning aligns with important research directions."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-sentence, leaving critical details about the TVM training process, evaluation metrics, and comparative experiments incomplete. The claims about TVMs' superiority over ORMs/PRMs lack empirical validation due to the absence of full experimental results. The paper does not address how TVMs handle diverse mathematical problem types or their scalability. Additionally, the theoretical justification for token-level supervision is not sufficiently elaborated, and the paper omits ablation studies to isolate the impact of key design choices."
          },
          "questions": {
            "value": [
              "How are TVMs trained? What dataset and supervision signals are used to assign token-level probabilities?",
              "What specific mathematical problem types were used to evaluate TVMs, and how do they generalize across domains?",
              "Are there ablation studies demonstrating the contribution of token-level supervision versus alternative approaches?",
              "How does the computational cost of TVMs compare to ORMs/PRMs, and what is their inference speed?",
              "What is the exact mechanism for distinguishing 'promising' from 'incorrect' steps using token probabilities?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "6HfNB34x9I": {
    "paper_id": "6HfNB34x9I",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces the Decoupling Optimistic Online Mirror Descent (DOOMD) algorithm for online episodic Markov Decision Processes (MDPs) with real-time improving predictions. The approach allows continuous policy updates within episodes by decomposing decision-making across states and incorporating cumulative costs. The authors theoretically establish a sub-linear regret bound for DOOMD, addressing the challenge of leveraging dynamic, improving predictions during interaction with the environment."
          },
          "strengths": {
            "value": "The paper presents a novel framework for online MDPs with dynamic policy updates, addressing a gap in existing literature where predictions improve over time. The decomposition of policies into state-specific sub-algorithms and the use of cumulative costs for long-term impact are original and well-justified. The theoretical analysis is rigorous, with a clear sub-linear regret bound. The paper is well-structured, with a thorough review of related work and a logical flow from problem formulation to algorithm design and analysis. The significance lies in its potential to improve decision-making in real-world scenarios with evolving predictions, such as routing or resource allocation."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation, which is critical for assessing the practical effectiveness of DOOMD. While the theoretical regret bound is promising, the absence of experiments on real or synthetic datasets limits the ability to evaluate the algorithm's performance against existing methods. Additionally, the problem formulation assumes that predictions improve over time, but the paper does not specify how this improvement is modeled or how DOOMD adapts to varying prediction accuracy. The connection between the cumulative cost concept and practical implementation details (e.g., computational complexity) is underdeveloped."
          },
          "questions": {
            "value": "1. How are the 'real-time improving predictions' formalized? Are they deterministic, probabilistic, or based on a specific model? 2. What empirical benchmarks or datasets were used to evaluate DOOMD, and how does it compare to existing methods in terms of regret and computational efficiency? 3. The decomposition of policies into state-specific sub-algorithms is abstract—what are the practical challenges in implementing this, and how does DOOMD handle them? 4. Are there scenarios where the sub-linear regret guarantee might not hold, and how does the algorithm address such cases?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces the Decoupling Optimistic Online Mirror Descent (DOOMD) algorithm for online episodic Markov Decision Processes (MDPs) with real-time improving predictions. Unlike traditional methods that fix policies per episode, DOOMD allows continuous updates of both predictions and policies within an episode by decomposing decision-making across states. The authors theoretically establish a sub-linear regret bound of $O(\\sqrt{T})$ for their approach."
          },
          "strengths": {
            "value": "The paper's originality lies in addressing dynamic policy updates within episodes, a gap in existing literature that typically assumes fixed policies. The theoretical analysis is rigorous, with a clear derivation of the regret bound. The presentation is well-structured, with a logical flow from problem formulation to algorithm design and analysis. The significance is strong, as the framework addresses real-world scenarios like routing where predictions improve over time, offering potential practical impact."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation, which is critical for establishing the practical relevance of the theoretical results. The decomposition of policies into states and the concept of 'cumulative cost' are not sufficiently detailed, leaving ambiguity about their implementation. The comparison with existing methods is limited, and the paper does not address potential limitations of the DOOMD algorithm in specific scenarios. Additionally, the theoretical analysis assumes idealized conditions (e.g., perfect knowledge of prediction quality), which may not hold in practice."
          },
          "questions": {
            "value": [
              "How does the DOOMD algorithm handle the decomposition of policies across states in practice? Are there specific constraints or assumptions required for this decomposition?",
              "What is the exact mechanism for updating policies during an episode, and how does it integrate with the 'cumulative cost' framework?",
              "The paper claims sub-linear regret but does not provide empirical results. Are there plans to evaluate DOOMD on benchmark tasks or real-world scenarios?",
              "How does the algorithm account for the uncertainty in real-time improving predictions, especially when predictions are noisy or inconsistent?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces the Decoupling Optimistic Online Mirror Descent (DOOMD) algorithm for online episodic MDPs with real-time improving predictions. It proposes a framework that allows dynamic policy updates within episodes by decomposing decision-making across states and using a cumulative cost formulation. The authors theoretically establish a sub-linear regret bound of O(√T)."
          },
          "strengths": {
            "value": "Originality: The paper presents a novel framework for online MDPs with dynamic policy updates, addressing a gap in existing literature that typically fixes policies per episode. The decomposition of decisions into state-specific sub-algorithms and the use of cumulative costs are creative approaches. Quality: The theoretical analysis of the regret bound is rigorous, though incomplete due to the paper's truncation. Clarity: The abstract and introduction are well-structured, with a clear motivation and problem statement. Significance: The problem of real-time improving predictions is relevant to applications like routing and self-driving cars, and the framework could impact online learning in dynamic environments."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation, making it difficult to assess the practical effectiveness of DOOMD. The theoretical analysis is incomplete (the paper is cut off), and key details about the algorithm's implementation and assumptions are missing. The comparison with existing works that allow within-episode policy updates (e.g., Cai et al., 2020; Neu & Olkhovskaya, 2021) is absent, weakening the claim of novelty. The definition of 'real-time improving predictions' and their integration into the algorithm is not sufficiently elaborated. The regret bound of O(√T) is standard for online learning, and the paper does not demonstrate how this improves upon prior results."
          },
          "questions": {
            "value": "1. How are real-time improving predictions formally modeled, and what assumptions are made about their accuracy over time? 2. What is the exact mechanism for decomposing policies into state-specific sub-algorithms, and how does this interact with the cumulative cost formulation? 3. How does DOOMD handle the trade-off between trusting early predictions and adapting to later improvements? 4. Are there specific scenarios where DOOMD outperforms existing methods, and what are the limitations of the theoretical guarantees? 5. Why are the experiments and detailed analysis (e.g., proof of regret bound) omitted, and how does this affect the paper's credibility?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 2
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "6LtdZCyuZR": {
    "paper_id": "6LtdZCyuZR",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces NUTRIBENCH, the first publicly available benchmark for evaluating large language models (LLMs) on nutrition estimation from natural language meal descriptions. It presents a dataset of 11,857 human-verified meal descriptions with macronutrient annotations, benchmarks 12 LLMs using diverse prompting strategies, compares their performance to professional nutritionists, and demonstrates real-world implications for diabetes management."
          },
          "strengths": {
            "value": "Originality: NUTRIBENCH addresses a critical gap by providing the first natural language-based benchmark for nutrition estimation, expanding beyond traditional tabular or image-based datasets. Quality: The dataset is rigorously constructed from global dietary data, with human verification and diverse serving size representations. Clarity: The paper is well-structured, with clear examples (e.g., Figure 1) and systematic evaluation protocols. Significance: The work highlights LLMs' potential to revolutionize dietary monitoring, with direct implications for healthcare accessibility and chronic disease management."
          },
          "weaknesses": {
            "value": "The dataset's reliance on GPT-4o-mini for generating meal descriptions may introduce biases or inconsistencies, requiring further validation. The real-world risk assessment focuses narrowly on Type 1 diabetes, limiting generalizability to other populations. The comparison to nutritionists is limited to a small sample size (three experts), and the study does not address potential biases in human annotations. Additionally, the paper lacks analysis of LLM performance on macronutrients beyond carbohydrates."
          },
          "questions": {
            "value": "1. How was the human verification process conducted for the dataset? Were there inter-rater reliability checks? 2. Could the authors elaborate on the limitations of GPT-4o-mini's role in dataset generation and its impact on downstream results? 3. How generalizable are the diabetes risk assessment findings to other conditions or dietary contexts? 4. What metrics were used to evaluate 'accuracy' in the nutritionist comparison, and how do they align with clinical standards? 5. Are there plans to expand NUTRIBENCH to include micronutrients or other dietary factors in future work?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces NUTRIBENCH, a novel dataset of 11,857 human-verified natural language meal descriptions annotated with macronutrient labels. It evaluates 12 LLMs on carbohydrate estimation tasks using various prompting strategies, compares results with professional nutritionists, and simulates the impact of LLM-generated nutrition estimates on diabetes management. The work highlights the potential of LLMs for nutrition estimation while addressing gaps in existing datasets."
          },
          "strengths": {
            "value": "Originality: NUTRIBENCH fills a critical gap by providing the first publicly available natural language meal description dataset for LLM evaluation in nutrition estimation. Quality: The comprehensive benchmarking of 12 diverse LLMs (open-source, closed-source, and domain-specific) with multiple prompting strategies demonstrates rigorous experimentation. Clarity: The paper is well-structured, with clear motivation, methodology, and results. Significance: The real-world risk assessment for diabetes patients underscores the practical implications of LLM-driven nutrition guidance, addressing a pressing healthcare challenge."
          },
          "weaknesses": {
            "value": "The dataset's quality relies on GPT-4o-mini-generated text, which may introduce biases or inaccuracies not addressed in the paper. The human verification process is not detailed, leaving uncertainty about annotation reliability. The evaluation focuses narrowly on carbohydrates, limiting insights into other macronutrients. The nutritionist study involves only three participants, raising questions about generalizability. The diabetes risk simulation lacks methodological details (e.g., glucose dynamics models, patient variability), weakening its conclusive power."
          },
          "questions": {
            "value": [
              "How was the human verification process conducted? What criteria were used to ensure the accuracy of macronutrient annotations?",
              "Why was the evaluation limited to carbohydrates? Could the methodology be extended to other macronutrients like proteins or fats?",
              "What specific criteria were used to select the three nutritionists? How were their estimates validated against gold-standard references?",
              "Can the authors provide details about the diabetes risk simulation, such as the glucose-insulin model used and assumptions about patient variability?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces NUTRIBENCH, a novel public dataset of 11,857 human-verified natural language meal descriptions annotated with macronutrient labels. It evaluates 12 LLMs on carbohydrate estimation tasks using various prompting strategies, compares results with professional nutritionists, and simulates real-world health impacts for diabetes patients. The work highlights LLMs' potential for nutrition estimation but lacks depth in addressing dataset limitations and methodological rigor."
          },
          "strengths": {
            "value": "Originality: NUTRIBENCH fills a critical gap by providing the first public benchmark for LLM nutrition estimation from text. Quality: Comprehensive experiments with diverse models (open-source/closed-source) and prompting strategies (CoT/RAG) demonstrate robust evaluation. Clarity: The paper clearly outlines the dataset construction, evaluation protocols, and real-world risk assessment. Significance: The potential to improve dietary health monitoring and reduce user burden is impactful, particularly for chronic disease management."
          },
          "weaknesses": {
            "value": "Dataset reliability is questionable: The meal descriptions are generated by GPT-4o-mini, which may inherit biases or inaccuracies. No details on human verification processes (e.g., inter-rater reliability, validation criteria) weaken credibility. Limited scope: Focus on carbohydrates alone neglects other macronutrients. The real-world risk assessment relies on simulations with unclear assumptions about glucose dynamics. The nutritionist study involves only three participants, limiting generalizability. No ablation studies on prompting strategies or analysis of model-specific strengths/weaknesses. Ethical considerations (e.g., data privacy, health misinformation risks) are unaddressed."
          },
          "questions": {
            "value": "1. How was the human verification process structured? Were annotations cross-checked by multiple experts? 2. What metrics were used to validate the accuracy of GPT-4o-mini-generated meal descriptions? 3. How representative are the three nutritionists in terms of expertise and regional background? 4. What assumptions underlie the glucose simulation, and how were they validated? 5. Could the paper address potential biases in the dataset (e.g., overrepresentation of certain cuisines)? 6. How do the results generalize to other languages or cultural dietary contexts?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "6MiOlatqMV": {
    "paper_id": "6MiOlatqMV",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces MathCAMPS, a framework for generating high-quality mathematical word problems grounded in K-8 Common Core standards. It uses formal grammars to create symbolic problems, converts them into natural language with LLMs, validates fidelity via cycle-consistency, and introduces novel 'mathematical dialogue' tasks with follow-up questions. The approach enables scalable, reproducible problem synthesis and reveals gaps in LLM capabilities."
          },
          "strengths": {
            "value": "Originality lies in systematically aligning problem generation with structured educational standards and introducing mathematical dialogue tasks. The methodology combines symbolic reasoning (SymPy) with LLMs, demonstrating rigorous validation through cycle-consistency. Experiments on 29 models and Pythia 12B checkpoints provide actionable insights. The pipeline's extensibility and open release of 9,607 problems showcase significant practical value. Clarity is strong, with illustrative figures and structured problem taxonomies."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparison to established benchmarks like MATH/GSM8K, making it unclear how MathCAMPS' difficulty/coverage differs. The cycle-consistency validation method is under-described (e.g., exact back-translation process, handling of ambiguous problems). Follow-up question generation details (e.g., criteria for counterfactual/incremental variations) are missing. The evaluation focuses on aggregate failures but doesn't analyze which Common Core standards are most challenging. The paper also doesn't address potential biases in Common Core standards or their alignment with broader mathematical competencies."
          },
          "questions": {
            "value": "1. How does MathCAMPS' problem difficulty compare to existing benchmarks? 2. What are the exact steps for cycle-consistency validation (e.g., how is symbolic back-translation performed)? 3. How are follow-up questions generated and validated for mathematical validity? 4. Are there specific Common Core standards where LLMs consistently fail, and why? 5. How does the framework handle edge cases (e.g., non-unique solutions, ambiguous problem formulations)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces MathCAMPS, a framework for generating high-quality mathematical word problems aligned with K-8 Common Core Standards. It uses formal grammars to create symbolic problems, leverages LLMs for natural language realization, and employs a cycle-consistency method to validate problem faithfulness. The work also introduces novel 'mathematical dialogue' tasks with incremental and counterfactual follow-up questions, evaluated across 29 LLMs."
          },
          "strengths": {
            "value": "The paper addresses critical issues in LLM benchmarking, such as data contamination and lack of fine-grained analysis, through a structured, scalable approach. The use of formal grammars for Common Core standards enables precise alignment with educational curricula. The cycle-consistency validation method is a novel and practical solution for ensuring generated problems' accuracy. The introduction of follow-up questions as a task to probe deeper understanding is innovative. The release of a large dataset (9,607 problems) and an extensible pipeline strengthens the paper's contribution."
          },
          "weaknesses": {
            "value": "The paper lacks detailed descriptions of how grammars are constructed for each of the 44 standards, which limits reproducibility. The evaluation of LLMs focuses on aggregate performance without analyzing specific model failures or error patterns. The follow-up question tasks (incremental/counterfactual) are not thoroughly justified, and their effectiveness in probing robustness remains unproven. The comparison to existing benchmarks like GSM8K or MATH is superficial, and the paper does not address how MathCAMPS complements or improves upon them. The cycle-consistency method's limitations (e.g., handling ambiguous problems) are not discussed."
          },
          "questions": {
            "value": "1. How are the formal grammars for each Common Core standard developed? Are they manually curated or automatically generated? 2. What is the exact process for the cycle-consistency validation, and how are discrepancies resolved? 3. How do the incremental and counterfactual follow-up questions ensure deeper understanding compared to standard benchmarks? 4. Are there any limitations in the types of problems generated (e.g., complexity, domain coverage)? 5. How does the framework handle edge cases or ambiguous problem formulations?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces MathCAMPS, a framework for generating high-quality mathematical word problems grounded in K-8 Common Core Standards. It uses formal grammars to create symbolic problems, converts them into natural language with LLMs, and validates fidelity via cycle-consistency. The work also proposes novel 'mathematical dialogue' tasks with follow-up questions to probe model understanding, and evaluates 29 LLMs, revealing unexpected failures in strong models."
          },
          "strengths": {
            "value": "Originality is strong in structuring problem generation around standardized curricula and introducing mathematical dialogue tasks. The methodology is rigorous, with a clear pipeline for scalable problem synthesis. The dataset of 9,607 problems is substantial, and the cycle-consistency validation addresses a critical challenge in LLM evaluation. The work's significance lies in enabling reproducible, fine-grained analysis of mathematical reasoning in LLMs, addressing data contamination concerns."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing benchmarks (e.g., MATH, GSM8K) to contextualize MathCAMPS' novelty. Experimental results are sparse: it's unclear how many problems passed cycle-consistency validation or the exact failure rates of LLMs. The analysis of Pythia 12B's training dynamics is superficial, with no concrete insights into skill development. The follow-up tasks (IFUP/CFUP) are not sufficiently differentiated from existing reasoning benchmarks."
          },
          "questions": {
            "value": "1. How does MathCAMPS' difficulty/coverage compare to established benchmarks like MATH/GSM8K? 2. What is the success rate of the cycle-consistency validation, and how many problems fail it? 3. Are the IFUP/CFUP tasks novel compared to existing work on reasoning or dialogue in math? 4. What specific mathematical skills does the Pythia 12B analysis reveal, and how do they evolve during training?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "6NNA0MxhCH": {
    "paper_id": "6NNA0MxhCH",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates how transformer language models (LMs) answer formatted multiple-choice questions (MCQA) by analyzing hidden states through vocabulary projection and activation patching. The authors identify key middle layers and attention mechanisms responsible for answer selection, demonstrate how later layers amplify correct answer probabilities, and introduce a synthetic task to disentangle model errors. They also observe that models adjust to alternative answer symbols in later layers and show that logit differences between answer choices grow during training."
          },
          "strengths": {
            "value": "The paper presents a novel combination of vocabulary projection and activation patching to study MCQA mechanisms, offering fresh insights into how models bind answers to symbols. The experimental scope is broad, covering multiple model families (Llama 3.1, Olmo 0724, Qwen 2.5) and datasets (MMLU, HellaSwag). The synthetic task for error disentanglement is a significant contribution. The findings on sparse attention head roles and training dynamics highlight important aspects of model behavior. The work addresses real-world robustness issues in MCQA, which is critical for LM evaluation."
          },
          "weaknesses": {
            "value": "The synthetic task's design and effectiveness are not sufficiently detailed, leaving unclear how it isolates specific error sources. The analysis of model differences (e.g., Olmo vs. Qwen) lacks depth, and the role of specific attention heads in different layers is not fully explained. The copying task mentioned in the abstract is not described, creating ambiguity. The paper references a figure (Figure 1) without providing its content, making it harder to grasp the summarized findings. Additionally, the experiments on training dynamics (logit differences) lack concrete results or visualizations."
          },
          "questions": {
            "value": [
              "Could the authors elaborate on the design of the synthetic task and how it effectively disentangles formatted MCQA performance from dataset-specific factors?",
              "How do the observed adjustments to alternative answer symbols (e.g., Q/Z/R/X) vary across the three model families studied? Are there consistent patterns or model-specific behaviors?",
              "What is the exact mechanism by which specific attention heads influence answer symbol probabilities, and how do these roles differ across layers?",
              "Could the authors provide more details about the copying task and its relevance to the study of symbol binding?",
              "Why is Figure 1 not included in the provided content, and how does it visually represent the key findings (e.g., attention head roles, training dynamics)?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates how language models (LMs) answer formatted multiple-choice questions (MCQA) by analyzing hidden states through vocabulary projection and activation patching. It identifies key middle layers and attention mechanisms responsible for answer selection, highlights differences in model behavior with alternative answer symbols, and introduces a synthetic task to disentangle model errors."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by applying interpretability techniques to a critical NLP task (MCQA) and proposing a novel synthetic task for error analysis. The experimental design is comprehensive, covering multiple models (Llama 3.1, Olmo, Qwen) and datasets (MMLU, HellaSwag). The findings on attention head roles and layer dynamics provide actionable insights. The paper is well-structured with clear motivation and technical depth."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons to state-of-the-art models beyond the three chosen families, limiting generalizability. The synthetic task's validity and practical utility are underexplored. The analysis of attention head roles is descriptive but lacks quantitative justification for why specific heads are critical. The study does not address how these findings could directly improve model robustness or training."
          },
          "questions": {
            "value": "1. How do the results generalize across different LM architectures (e.g., GPT vs. LLaMA)? 2. What are the limitations of the synthetic task in capturing real-world MCQA challenges? 3. Can the identified attention heads be targeted for model fine-tuning to improve robustness? 4. How does the paper reconcile its findings with prior work on label bias (e.g., Li & Gao, 2024)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper investigates how language models (LMs) answer multiple-choice questions (MCQAs) using vocabulary projection and activation patching to identify key hidden states and attention mechanisms involved. It reveals that specific middle layers and sparse attention heads drive answer selection, with later layers refining probabilities. A synthetic task is introduced to disentangle model errors and track training progression."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by applying activation patching and vocabulary projection to analyze MCQA mechanisms, a novel approach in this context. The methodology is rigorous, with experiments across diverse models (Llama 3.1, Olmo 0724, Qwen 2.5) and datasets (MMLU, HellaSwag, a copying task). The findings on attention head roles and model adaptation to non-standard symbols are significant. The synthetic task for isolating formatted MCQA performance is a valuable contribution. The paper is well-structured, with clear explanations of technical methods and results."
          },
          "weaknesses": {
            "value": "The synthetic task's design and implementation are under-described, leaving gaps in understanding how it disentangles formatted MCQA from dataset-specific performance. The identification of 'sparse attention heads' lacks quantitative justification (e.g., statistical significance, head importance metrics). The analysis of models adjusting to non-standard symbols (e.g., Q/Z/R/X) is qualitative; more data on frequency, conditions, or model characteristics influencing this behavior would strengthen the claims. The copying task is mentioned but not elaborated, limiting its perceived value."
          },
          "questions": {
            "value": [
              "Could the authors elaborate on the synthetic task's design and validation? How does it effectively isolate formatted MCQA performance from other factors?",
              "How were the 'sparse attention heads' identified? Were there statistical measures (e.g., ablation studies, attention head importance scores) to confirm their critical role?",
              "What factors (e.g., model size, training data) influence a model's ability to adapt to non-standard symbols like Q/Z/R/X? Are there quantitative trends in the data?",
              "What distinguishes the copying task from MMLU and HellaSwag? How does it provide unique insights into MCQA mechanisms?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "6PEbll1C0M": {
    "paper_id": "6PEbll1C0M",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces PLAID, a method for generating all-atom protein structures and sequences by leveraging the latent space of a sequence-based model (ESMFold) without requiring structural data during training. It enables conditioning on biological annotations like Gene Ontology terms and organism classes, using sequence databases to expand data coverage and annotations."
          },
          "strengths": {
            "value": "Originality: The approach of using sequence-only latent spaces for multimodal generation is novel, addressing data scarcity in structural biology. Quality: The method integrates established techniques (diffusion models, DiT architecture) with creative combinations like latent space compression. Clarity: The paper provides a clear overview of the framework and motivating examples. Significance: It addresses critical challenges in protein design, such as data bias and controllability, with potential applications in bioengineering."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the necessity of key components (e.g., latent space compression, conditioning mechanisms). The comparison to structure-based methods is limited, making it hard to assess relative performance. The analysis of function-conditioned samples is superficial, with insufficient evidence for claims about catalytic residue preservation. The experimental setup for structure quality metrics is not thoroughly described, raising questions about reproducibility."
          },
          "questions": {
            "value": "How does the model ensure physical plausibility of generated structures without explicit structure training? What is the exact role of ESMFold's latent space in capturing structural information? How are function/organism conditions integrated into the diffusion process? Are there ablation studies demonstrating the impact of different conditioning variables? How does PLAID compare to ESM3 in terms of all-atom generation capabilities?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces PLAID, a method for generating all-atom protein structures using only sequence data during training. By leveraging the latent space of a sequence predictor (e.g., ESMFold), PLAID enables simultaneous generation of 1D sequences and 3D structures, conditioning on biological annotations like Gene Ontology terms and organism classes. The approach avoids reliance on experimental structure databases, expanding data coverage and enabling richer controllability."
          },
          "strengths": {
            "value": "Originality is strong, as PLAID redefines multimodal protein generation by using sequence-only training to infer structure, addressing limitations of prior methods. The quality of experiments is notable, with evaluations on structure quality, diversity, and cross-modal consistency. Clarity is generally good, though some technical details (e.g., conditioning mechanisms) could be more explicit. Significance is high, as the method opens new possibilities for protein design by democratizing access to structure generation without experimental data."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons to related methods like ESM3, which also operates in sequence-structure space. The conditioning mechanism (e.g., how GO terms and organism labels are integrated) is under-specified, making it hard to assess its effectiveness. Some figures and explanations (e.g., Figure 4) are incomplete, reducing confidence in the technical analysis. The paper also does not address potential limitations of using ESMFold's latent space, such as biases in its training data."
          },
          "questions": {
            "value": "1. How does PLAID compare to ESM3 in terms of structure generation quality and controllability? 2. Can the authors clarify how function/organism conditioning is implemented in the DiT architecture? 3. What metrics are used to evaluate 'cross-modal consistency' between generated sequences and structures? 4. Are there ablation studies showing the impact of sequence-only training vs. joint sequence-structure training? 5. How are generated structures validated for physical plausibility (e.g., energy minimization, steric clashes)?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces PLAID, a method for generating all-atom protein structures from sequence-only data by leveraging the latent space of a sequence predictor (e.g., ESMFold). It enables simultaneous sequence and structure generation, conditioned on functional annotations and organism labels, using sequence databases to expand data coverage and annotations."
          },
          "strengths": {
            "value": "Originality is demonstrated through the novel use of sequence-based latent spaces for multimodal generation, addressing data scarcity in structure databases. The method creatively combines existing sequence models (ESMFold) with diffusion techniques. Experiments show strong performance on structure quality, diversity, and cross-modal consistency, with clear motivation for leveraging sequence annotations. The paper's clarity is aided by figures and structured explanations of the diffusion framework."
          },
          "weaknesses": {
            "value": "Key weaknesses include lack of comparison with structure-conditioned baselines (e.g., ESM3), no ablation studies on critical components (e.g., CHEAP encoder), and insufficient quantitative analysis of sample diversity and novelty. The paper also omits computational efficiency details and fails to address potential limitations in the conditioning variables (e.g., GO terms). Experimental validation of functional preservation (e.g., catalytic residues) lacks rigorous metrics."
          },
          "questions": {
            "value": "How does PLAID compare to ESM3 in terms of structure generation quality and data efficiency? What specific metrics are used to evaluate 'structure quality' and 'novelty'? Are there quantitative results demonstrating the preservation of non-adjacent catalytic residues? How does the method handle out-of-distribution generalization to non-annotated functions/organisms? What are the computational costs of training/inference compared to prior work?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "6R4TGPd74N": {
    "paper_id": "6R4TGPd74N",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces Ladder Residual, an architectural modification for residual-based models that decouples communication and computation in tensor parallelism (TP) to reduce inference latency. By rerouting residual connections to use 'stale' inputs from earlier layers, the approach enables overlapping of communication and computation. The method achieves significant speedups (up to 29% for 8B models) while maintaining performance comparable to standard Transformers. A secondary modification, Desynced Residual, further reduces communication by restricting modules to local activations."
          },
          "strengths": {
            "value": "The paper's core contribution, Ladder Residual, is highly original and addresses a critical bottleneck in TP for large models. The architectural insight of decoupling communication from computation through residual reconfiguration is creative and broadly applicable. The experiments demonstrate consistent speedups across model sizes, with practical validation via training smaller models and adapting Llama-3.1. The approach is compatible with existing frameworks like PyTorch, enhancing its accessibility. The paper also explores a secondary modification (Desynced Residual) to further reduce communication, showcasing the versatility of the idea."
          },
          "weaknesses": {
            "value": "The theoretical justification for using 'stale' inputs (e.g., why two layers back is optimal) is underdeveloped. The paper lacks ablation studies or analysis of how residual reconfiguration affects model expressiveness or training dynamics. The Desynced Residual section is brief and lacks quantitative results. The 70B model's lower speedup (1.17x with NVLink) compared to smaller models is not explained. Additionally, the trade-offs between speed and accuracy (e.g., in coding benchmarks) are not thoroughly discussed."
          },
          "questions": {
            "value": "1. What is the theoretical basis for choosing a two-layer-back residual connection? How does this affect the model's ability to learn long-range dependencies? 2. How does Ladder Residual impact the training stability or convergence of large models? 3. What are the specific limitations of Desynced Residual, and how do they vary across tasks (e.g., code vs. language tasks)? 4. Why does the speedup decrease for larger models (e.g., 70B)? Is this due to increased communication overhead or other factors? 5. How sensitive is the performance of the adapted Llama-3.1 model to the number of retrained layers or tokens?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces Ladder Residual, an architectural modification for residual-based models that decouples communication and computation in tensor parallelism (TP) to reduce inference latency. By rerouting residual connections to use stale inputs, the method enables overlapping of communication and computation, achieving significant speedups. The approach is tested on various Transformer sizes, showing 29% speedup for 8B models, and is compatible with Sequence Parallelism. A secondary modification, Desynced Residual, further reduces communication by restricting modules to local activations."
          },
          "strengths": {
            "value": "Originality: The Ladder Residual architecture offers a novel approach to decoupling communication and computation in TP, addressing a critical bottleneck. Quality: The experiments demonstrate measurable speedups across multiple model sizes, and the method is compatible with existing frameworks like PyTorch. Clarity: The problem statement and methodology are well-explained, with clear figures and tables. Significance: The work addresses a practical challenge in scaling large language models, with potential impact on inference efficiency and hardware utilization."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with existing communication-overlapping techniques (e.g., Chang et al. [2024], Jangda et al. [2022]) that also aim to reduce TP latency. The Desynced Residual approach is underexplored, with no detailed analysis of performance trade-offs or scalability. The pretraining experiments on 1B/3B models only mention 'comparable performance' without specific metrics. The post-training adaptation of Llama-3.1-8B lacks details on layer selection and retraining protocols. The 70B model's lower speedup (1.17x with NVLink) raises questions about the method's scalability."
          },
          "questions": {
            "value": "1. How does Ladder Residual compare to existing communication-overlapping techniques in terms of speedup and generality? 2. What are the specific metrics used to evaluate 'comparable performance' in the pretraining experiments? 3. How were the 'upper half layers' selected for adaptation in the Llama-3.1-8B experiment? 4. What is the theoretical justification for why stale inputs do not degrade model performance? 5. How does Desynced Residual perform in scenarios with cross-node or wireless communication, where overlapping is impractical?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Ladder Residual, an architectural modification for residual-based models that enables overlapping of communication and computation in tensor parallelism (TP) for transformers. By rerouting residual connections to use 'stale' inputs, the method reduces communication latency during inference, achieving up to 29% speedup on 8B models with TP. The approach is compatible with existing frameworks and demonstrates comparable performance to standard transformers on pretraining and adaptation tasks."
          },
          "strengths": {
            "value": "Originality: Ladder Residual presents a novel architectural approach to decouple communication and computation in TP, differing from prior systems-level optimizations. The idea of using 'stale' inputs for overlapping is creative and addresses a critical bottleneck in distributed inference. Quality: Experiments demonstrate consistent speedups across model sizes (1B-70B) with both NVLink and non-NVLink setups. The paper also shows practical applicability by adapting Llama-3.1 8B with minimal accuracy loss. Clarity: The method is well-explained with clear diagrams and equations, and the paper is structured logically. Significance: Reducing communication latency in TP has broad implications for scaling large language models, making this work relevant to both industry and academia."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to quantify the contribution of Ladder Residual versus other factors (e.g., framework optimizations). The pretraining experiments on 1B/3B models are described but lack quantitative metrics (e.g., perplexity, accuracy) for comparison with standard transformers. The adaptation experiments on Llama-3.1 8B are limited to 3B tokens of retraining, with no analysis of how the 'stale input' mechanism affects different layers. The Desynced Residual approach is mentioned as a proof of concept but lacks concrete results or analysis of its trade-offs. The theoretical justification for why stale inputs do not degrade performance is superficial, leaving room for skepticism about generalizability."
          },
          "questions": {
            "value": "1. How does the Ladder Residual affect the model's ability to capture long-range dependencies, especially in tasks requiring global context? 2. What specific hyperparameters (e.g., learning rate, batch size) were used for training the 1B/3B Ladder Transformers, and how do they compare to standard training setups? 3. How is the 'stale input' mechanism implemented in practice, and what guarantees are there that it does not introduce significant bias in the residual stream? 4. The paper mentions compatibility with Sequence Parallelism but does not elaborate on how the two techniques interact—can they be combined without additional overhead?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "6RmZ0V8Vwk": {
    "paper_id": "6RmZ0V8Vwk",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper explores how language can guide representation learning in vision models, proposing two approaches: explicit and implicit language guidance. It claims that training on text alone can improve vision encoders' generalization, robustness, and adaptability in continual learning, leveraging language's role in human abstraction."
          },
          "strengths": {
            "value": "The paper addresses a significant and timely problem in deep learning—improving generalization and robustness through language-guided learning. Its theoretical framework draws on cognitive science (e.g., System 1/2 processing, Global Workspace Theory) to justify the role of language in abstraction, which is a novel and interdisciplinary angle. The problem formulation is clear, and the motivation for bridging the gap between human and machine learning is compelling. The paper also highlights important challenges in DNNs (e.g., shortcut learning, catastrophic forgetting), demonstrating awareness of the field's limitations."
          },
          "weaknesses": {
            "value": "The paper lacks critical experimental details and results, as the content is truncated. The methods for explicit/implicit language guidance are not fully described, and there is no comparison to strong baselines or prior work on language-vision alignment (e.g., CLIP, ViLT). Claims about 'improvements in generalization' and 'task adaptability' are unsupported without empirical validation. The paper also fails to address key questions, such as how language supervision is applied to vision models, what specific tasks were evaluated, and whether the gains are statistically significant. The truncation severely limits the ability to assess the work's validity and contribution."
          },
          "questions": {
            "value": "1. What specific datasets and tasks were used to evaluate the proposed methods? 2. How exactly is language guidance implemented (e.g., loss functions, alignment techniques)? 3. What baselines were compared against (e.g., vision-only models, existing language-vision models)? 4. Are the results reproducible, and what hyperparameters were used? 5. How does the paper address the challenge of training vision models exclusively on text data? 6. What ablation studies were conducted to isolate the effect of language guidance?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates how language can guide representation learning in vision models, proposing two approaches: explicit language guidance (direct verbalizable insights) and implicit language guidance (indirect cues). The authors demonstrate that training on text-only data can improve vision encoders' generalization, robustness, and adaptability in continual learning tasks."
          },
          "strengths": {
            "value": "The paper addresses critical limitations of DNNs (shortcut learning, catastrophic forgetting) with a novel approach leveraging language as an inductive bias. It connects to cognitive theories (System 1/2, Global Workspace Theory) to justify the methodology. The motivation is strong, and the problem statement is clear. The empirical analysis shows promising results, and the visualizations (e.g., Figure 1) effectively illustrate the shared semantic space between modalities."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with key sections (methods, experiments) cut off, making it impossible to assess technical details or validate claims. The hypothesis that language models can generalize to visual tasks without seeing images lacks concrete evidence. The paper does not address potential limitations (e.g., dependency on pre-trained language models, scalability). The comparison to existing methods and ablation studies are unclear."
          },
          "questions": {
            "value": "How exactly was the vision encoder trained using only text? What datasets and baselines were used? How were generalization and robustness measured? Are there ablation studies showing the contribution of explicit vs. implicit guidance? How does the method handle catastrophic forgetting in continual learning? What are the computational costs and scalability limitations?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper explores how language can guide representation learning in vision models, proposing two approaches: explicit language guidance (direct verbalizable insights) and implicit language guidance (indirect cues). The authors claim that training on text alone can improve vision models' generalization, robustness, and continual learning capabilities by leveraging shared semantic concepts between text and vision."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel framework for integrating language as an inductive bias in vision models, distinguishing between explicit and implicit language guidance. Quality: The work addresses critical limitations of DNNs (e.g., catastrophic forgetting, texture bias) with a clear theoretical foundation. Clarity: The problem statement and motivation are well-articulated, with references to cognitive science and prior work. Significance: Improving generalization and robustness in vision models has broad implications for real-world AI systems."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with critical sections (e.g., experiments, methodology details) missing. The claims about text-only training improving vision models lack empirical validation in the provided content. The distinction between explicit and implicit guidance is not sufficiently explained or justified. The paper fails to address how language models without visual training can generalize to visual tasks. Missing baselines and comparisons with existing methods (e.g., vision-language pretraining) weaken the evaluation. The theoretical connection to cognitive science (e.g., System 1/2 processing) is superficial and not directly tied to the proposed methods."
          },
          "questions": {
            "value": "What specific vision tasks were used to evaluate generalization and robustness? How were pre-trained language models integrated with vision models (e.g., architecture details)? What metrics were used to quantify improvements in continual learning? How does the paper address domain shifts between text and visual modalities? Are there ablation studies to validate the contribution of language guidance? How do the results compare to vision-language pretraining approaches that use aligned text-image pairs?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 2
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "6VuTXirQIv": {
    "paper_id": "6VuTXirQIv",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces a novel framework called Coarsened Graph Learning (CGL) that directly learns a coarsened graph from feature data without requiring an explicit initial graph structure. The method aims to improve scalability, robustness against adversarial attacks, and semi-supervised learning performance by integrating label information into the graph learning process."
          },
          "strengths": {
            "value": "The paper presents a novel approach to graph coarsening by eliminating the need for pre-existing graph structures, addressing a critical limitation in existing methods. The integration of label information for semi-supervised learning is a valuable contribution. The problem formulation is well-structured, and the method's potential to enhance robustness against adversarial attacks is promising. The paper also provides a clear optimization framework using BSUM, which suggests computational efficiency."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient experimental details to validate its claims. Key aspects such as the exact formulation of the optimization problem, convergence guarantees for the non-convex BSUM approach, and thorough comparisons with recent state-of-the-art methods (e.g., GNN-based coarsening techniques) are underdeveloped. The adversarial robustness claims are not substantiated with concrete attack scenarios or metrics. Additionally, the paper does not address how the framework handles directed graphs or dynamic graph structures, which limits its applicability."
          },
          "questions": {
            "value": [
              "What specific adversarial attack scenarios were used to evaluate robustness, and how do the results compare to baseline methods?",
              "Are there ablation studies demonstrating the contribution of each component (e.g., label integration, optimization objectives) to the overall performance?",
              "How does the proposed method handle directed graphs or graphs with heterogeneous node/edge types?",
              "What is the exact mathematical formulation of the optimization problem, and how is non-convexity addressed in the BSUM framework?",
              "Are there comparisons with recent graph coarsening methods that explicitly handle feature-based graph learning (e.g., [cite relevant works])?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Coarsened Graph Learning (CGL), a novel framework for directly learning a coarsened graph from feature data without relying on pre-existing graph structures. The method addresses scalability challenges in graph neural networks (GNNs) by reducing computational complexity and enhancing robustness against adversarial attacks. It also incorporates label information for semi-supervised learning, improving downstream task performance."
          },
          "strengths": {
            "value": "The paper presents a novel approach to graph coarsening by eliminating the need for explicit graph structures, which is a significant contribution for domains like healthcare where graph data is scarce. The integration of label information for semi-supervised learning and the use of BSUM optimization for scalability demonstrate strong methodological rigor. The work addresses a critical gap in existing methods that assume pre-existing graphs, offering potential practical value. The problem formulation and theoretical foundations are well-structured."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation due to truncated content, making it difficult to assess the empirical claims. Key details about the adversarial attack scenarios tested, baseline comparisons, and ablation studies are missing. The theoretical guarantees of the BSUM-based optimization are not thoroughly discussed. Additionally, the paper does not address potential limitations of the framework, such as its performance on extremely large graphs or sensitivity to hyperparameters."
          },
          "questions": {
            "value": "1. How does the framework handle graphs with highly imbalanced node features or label distributions? 2. What specific adversarial attack scenarios were tested to validate robustness claims? 3. Can the authors provide detailed comparisons with baseline methods on standard datasets (e.g., Cora, Citeseer)? 4. How does the computational complexity of CGL scale with graph size compared to existing methods? 5. What are the theoretical convergence guarantees for the BSUM optimization approach used?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a novel framework called Coarsened Graph Learning (CGL) that directly learns a coarsened graph from feature data, aiming to improve scalability and robustness for graph neural networks (GNNs). The method bypasses the need for pre-existing graph structures by integrating label information for semi-supervised learning and employs optimization techniques to address computational challenges."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in GNNs: scalability and the lack of explicit graph structures in domains like healthcare. The integration of label information for semi-supervised learning enhances downstream task performance. The proposed optimization framework (CGL) is theoretically grounded, leveraging BSUM for efficient computation. The focus on adversarial robustness is timely, given the vulnerabilities of GNNs. The paper also highlights practical benefits through experiments on homophilic and heterophilic datasets."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough comparison with existing graph learning methods that also learn from features (e.g., [6, 32]). The experimental evaluation is incomplete (e.g., the final section is cut off), making it difficult to assess the significance of claimed improvements. The paper does not discuss limitations, such as handling high-dimensional data or computational costs of BSUM. The semi-supervised variant (SCGL) is mentioned but not elaborated on in terms of its formulation or impact."
          },
          "questions": {
            "value": "1. How does CGL differ from existing methods like [6, 32] that also learn graphs from features? 2. What is the exact formulation of the optimization problem, and how does it ensure graph coarsening? 3. How is label information integrated into the objective function for SCGL? 4. What specific adversarial scenarios were tested to validate robustness? 5. Are there ablation studies to isolate the contributions of different components (e.g., coarsening vs. semi-supervision)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "6bKEWevgSd": {
    "paper_id": "6bKEWevgSd",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces MS-HAB, a benchmark for low-level manipulation in home-scale rearrangement tasks. It presents a GPU-accelerated implementation of the Home Assistant Benchmark (HAB) with realistic physics and control, extensive RL/IL baselines, automated trajectory filtering, and scalable dataset generation capabilities. The key innovations include a 3x speedup over prior implementations and a rule-based system for generating safe, behavior-specific demonstrations."
          },
          "strengths": {
            "value": "Originality is demonstrated through the combination of GPU acceleration with realistic low-level control in a home-scale environment, addressing a gap in existing benchmarks. The paper's technical quality is strong, with detailed implementation details and performance metrics. Clarity is good, with clear sectioning and explanations of the benchmark's components. The significance is high, as benchmarks are critical for advancing embodied AI research, and the paper addresses key challenges in simulation speed, dataset generation, and safety constraints."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with alternative simulators beyond Habitat 2.0, making it hard to assess the uniqueness of the speed improvements. The RL/IL baselines are extensive but not compared to state-of-the-art methods, limiting their evaluative value. The automated event labeling system's validation and coverage of failure modes are under-specified. The paper also does not address potential limitations of the trajectory filtering approach, such as overfitting to specific criteria."
          },
          "questions": {
            "value": [
              "How do the RL/IL baselines compare to existing methods in terms of task success rates and efficiency?",
              "What specific metrics were used to validate the automated event labeling system's effectiveness?",
              "Are there any cases where the trajectory filtering system might fail to capture critical failure modes?",
              "How was the 3x speedup benchmarked against other simulators (e.g., RoboCasa) in similar tasks?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces MS-HAB, a benchmark for low-level manipulation in home-scale rearrangement tasks. It features a GPU-accelerated simulation with realistic physics, RL/IL baselines, automated trajectory filtering, and scalable dataset generation. The key contributions include a 3x faster simulation, extensive baselines, and a rule-based system for generating safe, behavior-controlled demonstrations."
          },
          "strengths": {
            "value": "Originality is evident in the holistic integration of fast simulation, realistic control, and automated trajectory filtering for home-scale tasks. The paper demonstrates significant speed improvements (4300 SPS) over prior work, which is critical for large-scale training. The extensive RL/IL baselines and trajectory categorization provide valuable resources for future research. Clarity is strong, with detailed technical descriptions of the simulation and filtering framework. The significance lies in addressing gaps in existing benchmarks for embodied AI."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons to other simulators (e.g., PyBullet, MuJoCo) and does not thoroughly evaluate the effectiveness of the trajectory filtering system. The RL/IL baselines are not compared against state-of-the-art methods, and performance metrics (e.g., success rates, reward values) are not quantified. The automated event labeling is described but not validated for accuracy or impact on downstream tasks. The paper also omits ablation studies on the trajectory filtering constraints and their trade-offs with data diversity."
          },
          "questions": {
            "value": [
              "How does the GPU-accelerated simulation compare to other simulators like PyBullet or MuJoCo in terms of realism and scalability?",
              "What specific RL/IL algorithms were used for the baselines, and how do they compare to prior work in terms of performance?",
              "Are the trajectory filtering criteria (e.g., collision thresholds) validated against real-world safety constraints or human demonstrations?",
              "What metrics are used to evaluate the quality of the generated datasets, and how do they correlate with downstream task performance?",
              "How are the success/failure modes defined, and are they robust to variations in object geometry or environmental conditions?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces MS-HAB, a benchmark for low-level manipulation in home rearrangement tasks. It features a GPU-accelerated implementation of the Home Assistant Benchmark (HAB) with realistic physics, extensive RL/IL baselines, automated trajectory categorization, and scalable dataset generation. The benchmark aims to address the need for faster simulation, controlled data generation, and standardized evaluation in home-scale robotic manipulation."
          },
          "strengths": {
            "value": "Originality is demonstrated through the integration of GPU acceleration, realistic low-level control, and automated trajectory filtering into a unified benchmark. The quality of the work is strong, with detailed technical specifications (e.g., 4300 SPS, 3x speedup over prior work) and extensive baselines (150 policies trained with 1.83B samples). Clarity is maintained through structured sections and clear explanations of technical components. The significance lies in addressing critical gaps in embodied AI research, particularly for home-scale manipulation tasks."
          },
          "weaknesses": {
            "value": "The paper lacks comparative analysis with alternative simulators (e.g., ManiSkill3 vs. other GPU-based platforms) and does not fully justify the choice of specific trajectory filtering criteria. The RL/IL baselines are extensive but lack detailed ablation studies or comparisons to prior work. The claim of 'whole-body control' is not elaborated, and the paper does not address limitations in generalizability to non-Fetch robots or unstructured environments. Additionally, the automated event labeling system's robustness to noisy or ambiguous scenarios is not discussed."
          },
          "questions": {
            "value": [
              "How does the GPU-accelerated HAB implementation compare to other simulators like ManiSkill3 in terms of accuracy and flexibility? Are there trade-offs in realism for speed?",
              "What specific criteria are used for trajectory filtering, and how are these criteria validated to ensure they align with safety and behavioral requirements?",
              "The paper mentions 'whole-body control' but does not explain how this differs from traditional end-effector control. Can the authors elaborate on the technical implementation?",
              "Are the RL/IL baselines evaluated against existing benchmarks or prior work in home-scale manipulation, and if not, what metrics are used to assess their performance?",
              "How does the benchmark handle domain shifts (e.g., new objects, environments) not present in the training data? Are there plans for future extensions?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "6e3hoDZKuO": {
    "paper_id": "6e3hoDZKuO",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a method for training goal-directed dialogue agents using reinforcement learning (RL) by leveraging large language models (LLMs) to generate synthetic dialogue data. The key idea is to use LLMs to simulate human-like conversations, which are then used for offline RL to optimize multi-step conversational objectives. The approach achieves state-of-the-art results in tasks like teaching and preference elicitation, even with smaller models compared to prompt-based baselines."
          },
          "strengths": {
            "value": "The paper introduces a novel framework combining LLMs' ability to simulate diverse conversations with offline RL for multi-step optimization. The method addresses a critical gap in goal-directed dialogue by enabling zero-shot learning without relying on human-labeled data. The empirical evaluation includes a user study demonstrating significant improvements over prompt-based LLMs, and the proposed 'imagination engine' (IE) provides a structured way to generate task-relevant dialogues. The work is well-motivated and tackles an important problem in interactive language generation."
          },
          "weaknesses": {
            "value": "The experimental validation is limited in scope, with unclear details about the synthetic dialogue generation process and its quality. The comparison to baselines lacks thorough ablation studies (e.g., how much does the IE contribute vs. RL training alone?). The user study's methodology is under-described (e.g., sample size, metrics used). The paper does not address potential biases in LLM-generated data or how the approach generalizes to unseen tasks. Additionally, the claim of 'zero-shot' performance is ambiguous without clear definitions of the training/testing setup."
          },
          "questions": {
            "value": [
              "How is the quality of the LLM-generated dialogue data evaluated? Are there quantitative metrics (e.g., diversity, task relevance) or human evaluations?",
              "What specific RL algorithm is used for offline training, and how are hyperparameters tuned? Are there comparisons to other offline RL methods?",
              "The paper mentions smaller models outperforming prompt-based baselines—what are the exact model sizes and training resources? Could this be due to architectural differences rather than the proposed method?",
              "How is the 'zero-shot' capability defined? Does it refer to no task-specific fine-tuning, or no human data at all?",
              "What are the limitations of the imagination engine (IE) in generating diverse or challenging dialogues? Are there cases where the synthetic data fails to capture necessary interaction patterns?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes a method for training goal-directed dialogue agents using reinforcement learning (RL) by leveraging large language models (LLMs) to generate synthetic human-like conversations. The key idea is to use LLMs to simulate hypothetical dialogues for offline RL training, enabling agents to optimize multi-step conversational goals without direct human interaction. The approach is evaluated on tasks like teaching, persuasion, and preference elicitation, showing improved performance over prompt-based LLMs."
          },
          "strengths": {
            "value": "The paper addresses a significant gap in LLMs' ability to perform goal-directed dialogue, which is critical for interactive applications. The method's integration of LLMs for data synthesis and offline RL is novel and practical, as it avoids the need for costly human interaction data. The experimental results, including a user study, demonstrate the approach's effectiveness. The work also provides a clear framework for generating task-relevant dialogues and training agents, showing potential for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of key components (e.g., the imagination engine vs. RL training). The synthetic dialogue generation process is not thoroughly explained, leaving questions about the quality and diversity of the generated data. The experiments rely on small-scale user studies without statistical significance testing, and the comparison to baselines (e.g., prompt-based LLMs) is not sufficiently detailed. Additionally, the paper does not address potential biases or limitations in the LLM-generated data, such as overfitting to specific patterns."
          },
          "questions": {
            "value": "1. How is the 'imagination engine' (IE) trained, and what criteria are used to ensure the generated dialogues are task-relevant and diverse? 2. What specific metrics were used in the user study to evaluate conversational performance, and were they validated for reliability? 3. How does the method handle out-of-distribution scenarios or tasks outside the training domain? 4. Are there any analyses of the RL agent's policy stability or convergence during training?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a method for training zero-shot goal-directed dialogue agents using reinforcement learning (RL) on data generated by large language models (LLMs). The key idea is to leverage LLMs to simulate human-like, task-specific dialogues (referred to as 'imagined conversations') and then apply offline RL to optimize multi-step conversational objectives. The approach is evaluated on tasks like teaching, persuasion, and preference elicitation, showing state-of-the-art performance compared to prompt-based LLMs."
          },
          "strengths": {
            "value": "Originality: The paper presents a novel framework combining LLMs' ability to generate diverse dialogues with RL's capacity to optimize multi-step goals, addressing a critical gap in goal-directed dialogue systems. Quality: The methodology is well-structured, with a clear separation of data generation (LLM-based) and RL training. Clarity: The problem statement and contributions are clearly articulated, with illustrative examples (e.g., travel agent scenario). Significance: The work addresses a practical challenge in interactive AI systems, with potential applications in education, customer service, and personalized assistance."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of the quality and diversity of the 'imagined conversations' generated by LLMs, which could introduce biases or suboptimal data. Experimental validation is limited to a user study with unspecified metrics and sample size, making it difficult to assess the robustness of results. The comparison with existing methods (e.g., online RL, other zero-shot approaches) is superficial, and ablation studies on key components (e.g., imagination engine design) are missing."
          },
          "questions": {
            "value": [
              "How is the quality of the LLM-generated dialogue data evaluated? Are there metrics to quantify the realism or task-relevance of the 'imagined conversations'?",
              "What specific metrics were used in the user study to compare the proposed method with prompt-based LLMs? How were participants recruited and randomized?",
              "Are there ablation studies demonstrating the necessity of the 'imagination engine' versus simpler data generation strategies (e.g., direct LLM prompting)?",
              "How does the method handle tasks requiring explicit knowledge beyond the LLM's training data? Is there a mechanism for incorporating external knowledge sources?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "6fDjUoEQvm": {
    "paper_id": "6fDjUoEQvm",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces HyperDAS, a transformer-based hypernetwork architecture that automates the process of identifying concept-mediated features in neural network hidden states. By encoding concepts with a transformer and using cross-attention to locate token positions in the residual stream, HyperDAS constructs feature subspaces for causal interventions. The method achieves state-of-the-art results on the RAVEL benchmark for disentangling entity attributes."
          },
          "strengths": {
            "value": "Originality: HyperDAS introduces a novel hypernetwork approach to automate mechanistic interpretability, addressing a key bottleneck in brute-force searches. The integration of cross-attention mechanisms for concept encoding and token localization is creative. Quality: The experiments on RAVEL demonstrate strong performance, and the paper acknowledges limitations of prior methods. Clarity: The problem statement and high-level framework are well-articulated, with a focus on scalability. Significance: Automating feature localization could greatly enhance interpretability workflows for large models, making this a valuable contribution to the field."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the necessity of key components (e.g., cross-attention blocks, Householder transformations). The mechanism for selecting token positions and constructing subspaces is described at a high level without quantitative justification. The claim of 'faithfulness' to the target model is not thoroughly supported with empirical evidence. The RAVEL benchmark details are incomplete, making it difficult to assess the exact metrics used. The computational cost of training separate HyperDAS models per entity type is not discussed."
          },
          "questions": {
            "value": [
              "How does the hypernetwork's architecture specifically enable automatic token localization? What are the key design choices that differentiate this approach from prior work?",
              "The paper mentions a 'fixed orthogonal matrix' with Householder transformations. Can the authors clarify how this component is trained or derived, and what assumptions it relies on?",
              "What are the limitations of HyperDAS in terms of concept types or model architectures? Are there scenarios where the method might fail to locate relevant features?",
              "The faithfulness analysis is mentioned but not elaborated. Could the authors provide specific metrics or experiments to demonstrate that HyperDAS does not inject spurious information into the target model?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces HyperDAS, a transformer-based hypernetwork architecture designed to automate the identification of concept-mediated features in neural network hidden states. It addresses the limitation of brute-force search in existing mechanistic interpretability methods by localizing token positions and constructing subspaces for concepts, achieving state-of-the-art results on the RAVEL benchmark."
          },
          "strengths": {
            "value": "The paper tackles a critical problem in mechanistic interpretability by proposing an automated method to replace laborious manual searches. The integration of hypernetworks with transformer architectures demonstrates creative innovation. The experimental results on RAVEL show practical utility, and the discussion on methodological faithfulness to the target model addresses important concerns in interpretability research. The work bridges gaps between concept localization and causal intervention techniques."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate key components of HyperDAS, such as the role of the Householder transformation or cross-attention blocks. The technical description of how the hypernetwork 'automatically locates token-positions' remains vague, with insufficient explanation of the underlying optimization process. Experimental comparisons to baseline methods other than DAS are missing, and the RAVEL benchmark results lack granularity (e.g., per-concept performance metrics). The paper also does not address potential limitations in scalability or generalizability across different model architectures."
          },
          "questions": {
            "value": "1. Can the authors clarify how the Householder transformation is applied in step 3 of the framework? 2. What specific metrics were used to evaluate 'state-of-the-art performance' on RAVEL, and how does HyperDAS compare to other SOTA methods? 3. How does the token-position selection mechanism avoid overfitting to prompt-specific patterns? 4. Are there cases where HyperDAS fails to localize concepts accurately, and what are the failure modes? 5. What are the computational costs of training and deploying HyperDAS compared to existing methods?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces HyperDAS, a transformer-based hypernetwork architecture designed to automate the identification of concept-mediating features in neural network hidden states. It addresses the limitation of brute-force search in distributed alignment search (DAS) by learning to locate token positions and construct feature subspaces for concepts, achieving state-of-the-art results on the RAVEL benchmark."
          },
          "strengths": {
            "value": "The paper's originality lies in applying hypernetworks to automate mechanistic interpretability, a novel approach that could significantly reduce manual effort in feature discovery. The experiments on RAVEL demonstrate practical effectiveness, and the method's ability to generalize across entity types is promising. The discussion on faithfulness to the target model addresses a critical concern in interpretability research. The paper is well-structured with clear technical descriptions of the framework and its components."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficient: the paper claims SOTA performance on RAVEL but does not compare with all relevant baselines (e.g., DAS, circuit discovery methods). The RAVEL benchmark details are vague, making it hard to assess the significance of the results. The hypernetwork's design choices (e.g., Householder transformations, cross-attention mechanisms) lack justification. The faithfulness analysis is superficial, with no quantitative metrics to support the claims. The paper also omits ablation studies to isolate the contribution of key components."
          },
          "questions": {
            "value": [
              "How does HyperDAS compare to DAS in terms of computational efficiency and scalability? Are there quantitative results to support the claim of automation?",
              "What specific metrics are used in the RAVEL benchmark, and how do the results compare to existing methods? Are the results statistically significant?",
              "Why was the Householder transformation chosen for feature selection? What alternatives were considered, and how do they differ in performance?",
              "How is the faithfulness of HyperDAS quantitatively measured? Are there experiments showing that it does not inject spurious information into the target model?",
              "What are the limitations of the hypernetwork architecture, and how might they affect generalization to other models or tasks?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "6ifeGfWxtX": {
    "paper_id": "6ifeGfWxtX",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces Slashed Normal, a novel parameterization for the posterior distribution in variational inference, using a new stdplus activation function to compute standard deviation. It links the squared L2-norm of neural network outputs (KL amplitude) to exact KL divergence, enabling direct control over KL values to address issues like posterior collapse."
          },
          "strengths": {
            "value": "Originality lies in the stdplus activation and the KL amplitude concept, which connects L2-norm to KL divergence. The method is simple, aligns with conventional VAE practices, and unifies existing KL control techniques. Theoretical insights into posterior collapse and new capabilities like fixed-rate VIB are highlighted. The paper addresses critical challenges in VAEs with potential practical impact."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experiments and comprehensive comparisons with existing methods. The stdplus activation function is not thoroughly explained, and its advantages over softplus/exp are unclear. The theoretical derivation of the KL divergence from the L2-norm is underdeveloped. The connection between KL amplitude and KL divergence remains ambiguous. The motivational example in Figure 1 is incomplete, hindering evaluation of its claims."
          },
          "questions": {
            "value": [
              "What is the exact mathematical form of the stdplus activation function, and how does it differ from softplus or exponential functions?",
              "Can the authors provide a rigorous derivation of how the squared L2-norm of the raw output directly corresponds to the exact KL divergence?",
              "Are there specific scenarios or architectures where Slashed Normal might fail, and how does the method handle non-Gaussian priors?",
              "What are the computational costs and scalability of Slashed Normal compared to existing approaches?",
              "How does the method address cases where the KL amplitude might not align with desired divergence values?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces Slashed Normal, a novel parameterization for normal posterior distributions in variational inference, using a new 'stdplus' activation function to directly link the L2-norm of neural network outputs (KL amplitude) to the exact KL divergence. This enables explicit control over the KL term, addressing issues like posterior collapse and numerical instability in VAEs."
          },
          "strengths": {
            "value": "Originality: The paper presents a fresh approach to KL divergence control via a novel activation function and theoretical connection to L2-norm. Quality: The method is mathematically grounded with clear motivation for addressing posterior collapse. Clarity: The structure is logical, with well-defined contributions and connections to prior work. Significance: Direct KL control has broad applications in representation learning, compression, and information bottleneck scenarios."
          },
          "weaknesses": {
            "value": "Theoretical depth: The connection between KL amplitude and KL divergence lacks rigorous derivation or empirical validation. Experimental scope: Experiments are limited, with no comparisons to state-of-the-art KL control methods like GECO or adaptive scheduling. Practical details: The 'stdplus' activation function is not thoroughly analyzed, and its advantages over softplus/exp are unclear. Generalization: The claim that results apply to conventional parameterizations (e.g., softplus) is not empirically supported."
          },
          "questions": {
            "value": "1. What is the mathematical basis for the KL amplitude-L2-norm relationship? 2. How does 'stdplus' differ from existing activations, and what are its theoretical guarantees? 3. Are there ablation studies demonstrating the necessity of the KL amplitude mechanism? 4. How does Slashed Normal compare to Residual Normal Distribution in terms of performance and stability? 5. What are the limitations of this approach in non-Gaussian settings or complex architectures?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces 'Slashed Normal,' a novel parameterization for the normal posterior distribution in variational inference models. It replaces conventional standard deviation computation (e.g., softplus/exp) with a new 'stdplus' activation function, linking the squared L2-norm of neural network outputs (KL amplitude) directly to the exact KL divergence. This enables explicit control over the KL divergence, addressing issues like posterior collapse and offering theoretical insights."
          },
          "strengths": {
            "value": "Originality is evident in the stdplus activation and KL amplitude formulation, which provide a novel link between neural network outputs and KL divergence. The methodology is theoretically grounded, with clear connections to existing work. Clarity is strong, with well-structured sections and explanations. The significance lies in addressing critical VAE challenges (posterior collapse, numerical instability) and enabling direct KL control, which has broad applications in representation learning and compression."
          },
          "weaknesses": {
            "value": "The paper lacks extensive empirical validation, relying on initial experiments that do not compare directly with established methods like GECO or β-tuning. The theoretical analysis of the stdplus function's properties (e.g., differentiability, numerical stability) is incomplete. The connection between KL amplitude and exact KL divergence requires deeper justification, especially in non-Gaussian settings. The paper also does not address potential limitations of the parameterization in complex architectures."
          },
          "questions": {
            "value": "1. How does the stdplus activation function compare to softplus/exp in terms of training stability and gradient propagation? 2. Are there specific scenarios where Slashed Normal outperforms existing KL control methods (e.g., GECO)? 3. What theoretical guarantees exist for the KL amplitude's correspondence to the exact KL divergence? 4. How does the parameterization generalize to non-Gaussian priors or more complex posterior distributions?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "6j0GH40mFt": {
    "paper_id": "6j0GH40mFt",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes a novel framework called WDA-LIC that integrates dynamic attention mechanisms into learned image compression (LIC). The key contributions include a window-based dynamic attention (WDA) module to sparsify attention patterns and a dynamic-reference entropy model (DREM) to select relevant contextual information. The authors claim significant BD-rate improvements over existing methods like MLIC++ and VTM-17.0."
          },
          "strengths": {
            "value": "Originality: The paper introduces dynamic attention into LIC, addressing overfitting by sparsifying attention patterns, which is a novel approach. Quality: The methodology is theoretically grounded in addressing overfitting and redundancy in ViT-based compression. Clarity: The paper is well-structured, with clear problem formulation and motivation. Significance: If validated, the approach could advance LIC by improving rate-distortion performance while reducing overfitting risks."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of the WDA and DREM modules, making it difficult to assess their novelty and implementation. Experimental validation is incomplete—BD-rate gains are claimed but not contextualized against other state-of-the-art methods. The paper does not provide ablation studies or analysis of how dynamic sparsification reduces overfitting. Additionally, the methodology's reliance on ViT-based architectures is not sufficiently justified compared to CNNs."
          },
          "questions": {
            "value": [
              "How exactly does the WDA module dynamically sparsify attention patterns? What criteria are used to determine which references to ignore?",
              "What is the relationship between the entropy distribution and the attention sparsification mechanism? How is this linked to reducing overfitting?",
              "Are the BD-rate improvements statistically significant? How do the results compare to other recent LIC methods beyond MLIC++ and VTM-17.0?",
              "What ablation studies were conducted to validate the contributions of WDA and DREM individually?",
              "Why is the window-based approach preferred over global attention in this context? Are there quantitative comparisons of computational efficiency or model complexity?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a novel framework called WDA-LIC for learned image compression (LIC) that integrates dynamic attention mechanisms. The key contributions include a window-based dynamic attention (WDA) module for sparsifying attention patterns based on entropy distribution and a dynamic-reference entropy model (DREM) to adaptively select contextual information. The method achieves significant BD-rate improvements over existing LIC approaches."
          },
          "strengths": {
            "value": "The paper presents a novel application of dynamic attention in LIC, addressing overfitting issues through sparsification. The WDA module's hierarchical, scale-adaptive design and DREM's entropy-driven context selection show creative problem-solving. The experimental results demonstrate competitive performance, and the work fills a gap by focusing on overfitting in transformer-based LIC. The clarity of the problem statement and technical exposition is strong, with a logical flow from motivation to methodology."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the necessity of key components like WDA and DREM. The sparsification mechanism's implementation details (e.g., how attention matrices are pruned) are under-specified, making reproducibility challenging. The experiments focus on BD-rate improvements but omit comparisons with recent state-of-the-art methods like SymGen or Flow-IC. The theoretical analysis of why dynamic sparsification reduces overfitting is limited, and the computational efficiency gains (if any) are not quantified."
          },
          "questions": {
            "value": [
              "How is the attention sparsification threshold determined? Is it learned or handcrafted?",
              "What is the exact difference between DREM and existing entropy models like those in SymGen or Flow-IC?",
              "Are the BD-rate improvements consistent across different bitrates and image types?",
              "How does the WDA module handle varying image resolutions or aspect ratios?",
              "What is the computational overhead of the dynamic attention mechanism compared to baseline methods?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a novel framework for learned image compression (LIC) called Window-based Dynamic Attention (WDA-LIC), which integrates dynamic attention mechanisms to address overfitting and improve entropy estimation. The key contributions include the WDA module for sparsifying attention based on entropy distribution and the dynamic-reference entropy model (DREM) for adaptive context selection. The method achieves superior BD-rate gains over existing state-of-the-art approaches."
          },
          "strengths": {
            "value": "Originality: The paper presents the first integration of dynamic attention in LIC, addressing overfitting through sparsification of attention patterns. Quality: The experiments demonstrate significant BD-rate improvements on benchmark datasets. Clarity: The structure is logical, with clear sections on methodology and related work. Significance: Improving entropy modeling and reducing redundancy in LIC has broad implications for compression efficiency."
          },
          "weaknesses": {
            "value": "The paper lacks a thorough analysis of why dynamic sparsification reduces overfitting, with minimal theoretical justification. Experimental comparisons are limited to VTM-17.0 and MLIC++, omitting recent state-of-the-art methods. The DREM mechanism is described briefly without ablation studies or quantitative validation of its contribution. The computational efficiency of the proposed method is not discussed, which is critical for real-world applications."
          },
          "questions": {
            "value": [
              "How does the WDA module specifically sparsify the attention matrix? What metrics were used to determine the optimal sparsity level?",
              "What is the exact mechanism of DREM, and how does it differ from existing entropy models in LIC?",
              "Are there comparisons to other dynamic attention methods (e.g., those in NLP) adapted to LIC? How does WDA-LIC handle varying image resolutions or content types?",
              "What ablation studies were conducted to isolate the contributions of WDA and DREM to the overall performance?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "6jA1R0Z1G2": {
    "paper_id": "6jA1R0Z1G2",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper revisits generalized entropy indices (GEI) as a unified measure of group and individual fairness in machine learning. The authors provide a theoretical analysis of GEI, arguing that by decoupling fairness metrics from model accuracy, they can reconcile the trade-off between fairness and accuracy. They propose new representations of the metric and use hypothetical examples to illustrate its behavior under different parameter settings."
          },
          "strengths": {
            "value": "The paper's originality lies in its theoretical re-examination of GEI as a fairness metric, offering a novel perspective on aligning utility and fairness. The clarity of the mathematical derivations and the structured presentation of the problem are commendable. The significance of addressing the fairness-accuracy trade-off is high, given its relevance to real-world applications. The work also contributes to the understanding of parameter sensitivity in fairness metrics, which is a critical gap in the literature."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation of its theoretical claims, relying solely on hypothetical examples. The proposed parameter selection guidelines are not grounded in experiments, making it unclear how to apply them in practice. The analysis of GEI's behavior under different error distributions is abstract and does not address real-world data complexities. Additionally, the paper does not compare its approach to existing fairness mitigation techniques, weakening its contribution to the field."
          },
          "questions": {
            "value": "How do the authors validate the theoretical claims about GEI's alignment with fairness and utility? What empirical evidence supports the assertion that GEI can decouple fairness from accuracy? Are there specific parameter ranges or selection strategies proposed, and how are they justified? How does the paper address the computational feasibility of implementing GEI in practical scenarios? What are the limitations of the hypothetical examples used to illustrate the metric's behavior?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper revisits generalized entropy indices (GEI) as a unified fairness metric for machine learning, proposing a data-agnostic theoretical analysis to clarify their behavior. It argues that GEI can align utility with fairness through parameter selection, demonstrating consistency between cost-sensitive learning and individual fairness. The work also critiques existing implementations for lacking justification in parameter choices and highlights the need for principled parameter tuning."
          },
          "strengths": {
            "value": "Originality lies in the theoretical framework for understanding GEI's behavior, particularly its relationship with fairness and accuracy. The paper's clarity is strong in mathematical formulations, though limited by truncation. Significance is high due to the practical implications for parameter selection in fairness-aware ML. The quality of analysis is rigorous, with novel insights into the orthogonality of fairness metrics and model accuracy."
          },
          "weaknesses": {
            "value": "The paper is severely truncated, omitting critical sections like full experimental validation, detailed proofs, and results. The claim that GEI reconciles fairness and accuracy lacks empirical support due to missing experiments. The hypothetical examples are underdeveloped, and the paper does not address how to practically enforce the proposed orthogonality constraint. The critique of existing parameter choices is superficial without comparative analysis."
          },
          "questions": {
            "value": "1. What are the missing experimental results that validate the theoretical claims about GEI's alignment with fairness and utility? 2. How does the paper address the computational feasibility of enforcing orthogonality between GEI and model accuracy in real-world scenarios? 3. Are the hypothetical examples (punitive vs. assistive systems) sufficiently grounded in empirical data or case studies? 4. How does the proposed parameter selection framework compare to existing methods in terms of practical implementation and scalability?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 2
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper revisits generalized entropy indices (GEI) as a unified metric for quantifying both group and individual fairness in machine learning. The authors provide a theoretical analysis of GEI's behavior under different parameter choices, argue that the original parameter settings conflated model accuracy with fairness, and demonstrate how careful parameter selection can align utility with fairness. They also propose a framework for reconciling fairness and accuracy by viewing classification decisions as transactions between individuals and decision-makers."
          },
          "strengths": {
            "value": "The paper's originality lies in its theoretical analysis of GEI's properties and its argument for parameter-driven alignment of fairness and utility. The methodological approach is rigorous, with clear connections to existing fairness literature. The paper's focus on data-agnostic reasoning and its emphasis on practical implications for algorithmic governance are significant contributions. The clarity of the problem formulation and the structured presentation of theoretical claims enhance its academic value."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation of its theoretical claims, which weakens the evidence for its arguments about parameter selection and fairness-utility alignment. The analysis of the original Speicher et al. (2018) work's limitations is based on speculative reasoning rather than concrete experiments. The truncated conclusion raises concerns about the completeness of the analysis, and the paper does not address how its framework would scale to complex real-world scenarios. Additionally, the relationship between GEI and individual fairness remains underexplored."
          },
          "questions": {
            "value": [
              "How does the paper's argument that GEI should be orthogonal to model accuracy hold under different parameter settings? What evidence supports this claim?",
              "The paper critiques the original Speicher et al. (2018) parameter choices but provides no empirical analysis of their impact. How were these parameter choices evaluated in the authors' experiments?",
              "The truncated conclusion suggests a reconciliation of fairness and accuracy via GEI, but the mechanisms for this are not fully explained. Can the authors clarify how their framework addresses computational challenges in individual fairness (e.g., similarity metric learning)?",
              "How does the proposed framework handle scenarios where error costs vary across subgroups, and what guarantees does it provide for group fairness?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "6jxUsDAdAu": {
    "paper_id": "6jxUsDAdAu",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates benign overfitting in the context of Out-of-Distribution (OOD) generalization for over-parameterized linear models under covariate shift. The authors provide non-asymptotic risk bounds for ridge regression and Principal Component Regression (PCR), identifying key covariance-related quantities that determine OOD performance. They show that ridge regression achieves 'benign overfitting' under specific structural conditions on the target covariance, while PCR outperforms ridge regression in scenarios where the target distribution lies outside the source's low-dimensional manifold."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by extending the theory of benign overfitting to OOD settings, a less-explored area. The methodology is rigorous, with non-asymptotic guarantees that connect to prior work on in-distribution and under-parameterized models. The clarity of the problem formulation and theoretical analysis is excellent, and the significance of addressing OOD generalization in over-parameterized models is high. The paper also provides a clear comparison between ridge regression and PCR, highlighting their respective strengths in different scenarios."
          },
          "weaknesses": {
            "value": "The experiments are limited to synthetic multivariate Gaussian data, which may not fully capture real-world distribution shifts. The theoretical assumptions (e.g., dominance of major eigenvalues in source covariance) could be restrictive, and the paper does not thoroughly discuss how these assumptions hold in practical settings. Additionally, the analysis of PCR's advantages is somewhat abstract, with limited discussion of its applicability to non-linear models or real-world datasets."
          },
          "questions": {
            "value": [
              "How do the authors' assumptions about the source covariance structure (e.g., dominance of major eigenvalues) hold in practical scenarios where data may not conform to such idealized conditions?",
              "The paper mentions simulations in the appendix but does not discuss them in the main text. Could the authors elaborate on the experimental setup and results to strengthen the empirical validation?",
              "What are the limitations of PCR in scenarios where the true signal does not align with the major directions of the source covariance? How might this affect its performance in real-world applications?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates benign overfitting in the context of Out-of-Distribution (OOD) generalization for over-parameterized linear models under covariate shift. The authors provide non-asymptotic risk bounds for ridge regression and Principal Component Regression (PCR), showing that benign overfitting occurs when the target covariance aligns with the low-dimensional structure of the source distribution. They also demonstrate that PCR achieves faster convergence rates than ridge regression in certain OOD scenarios."
          },
          "strengths": {
            "value": "The paper makes a novel contribution by extending the understanding of benign overfitting to the OOD regime, which is a critical gap in current literature. The theoretical analysis is rigorous, with sharp bounds that recover prior in-distribution and under-parameterized OOD results as special cases. The work creatively compares ridge regression and PCR, highlighting their differing behaviors under varying covariance structures. The clarity of the problem formulation and the logical flow of the arguments are strong, and the paper effectively contextualizes its contributions within existing research."
          },
          "weaknesses": {
            "value": "The focus on linear models limits the applicability of the findings to more complex, non-linear settings. The experimental validation is minimal, with only a brief mention of simulations in the appendix, which could be expanded for stronger evidence. The analysis assumes specific structural conditions on the source and target covariances, which may not hold in practical scenarios. Additionally, the paper does not discuss computational trade-offs between ridge regression and PCR, which could be important for real-world deployment."
          },
          "questions": {
            "value": "1. How sensitive are the theoretical guarantees to deviations from the assumed covariance structure (e.g., if the source covariance has more than a few dominant eigenvalues)? 2. What are the computational costs of PCR compared to ridge regression, and how do these scale with high-dimensional data? 3. Are there practical strategies to detect whether the target distribution falls into 'case 1' (benign overfitting) or 'case 2' (poor generalization) without prior knowledge of the covariance structure?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper investigates benign overfitting in over-parameterized linear models under out-of-distribution (OOD) generalization, focusing on covariate shift. It provides non-asymptotic risk bounds for ridge regression and Principal Component Regression (PCR), showing that benign overfitting occurs when the target distribution aligns with the source's low-dimensional manifold. The work bridges in-distribution and under-parameterized OOD guarantees, demonstrating PCR's superiority in scenarios where ridge regression fails."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in understanding OOD generalization for over-parameterized models, which is increasingly relevant in practical ML. The theoretical contributions are sharp, with non-asymptotic bounds that recover prior results under special cases, showcasing the work's generality. The distinction between 'case 1' (target on source manifold) and 'case 2' (target outside the manifold) provides a clear framework for analyzing OOD performance. The presentation is structured, and the experiments on synthetic data validate the theoretical claims."
          },
          "weaknesses": {
            "value": "The focus on linear models limits the paper's applicability to real-world non-linear systems. The assumptions about the source covariance (dominated by a few eigenvalues) may be restrictive, and the paper does not address more complex distribution shifts beyond covariate shift. The practical implications of PCR's superiority are not fully explored, such as how to select between ridge regression and PCR in practice. Additionally, the experiments are limited to Gaussian data, raising questions about real-world generalization. The paper also lacks comparisons to alternative methods like kernel-based approaches or neural networks."
          },
          "questions": {
            "value": "1. How can practitioners verify whether the target distribution satisfies the 'case 1' conditions (low-dimensional alignment with the source)? 2. Are the theoretical guarantees robust to deviations from the assumed covariance structure, such as noisy or misspecified eigenvalues? 3. How does the performance of PCR compare to other dimensionality reduction techniques (e.g., PCA with different truncation levels) in case 2? 4. What are the computational costs of PCR versus ridge regression in high-dimensional settings, and how do they scale with $ n $? 5. Can the analysis be extended to other types of distribution shifts, such as label shift or concept drift?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "6o9QUqUq9f": {
    "paper_id": "6o9QUqUq9f",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates causal relationships among candidate output tokens in large language models (LLMs), proposing a method to analyze these relationships and improve decoding through two heuristics: Critical Layer Ablation (CLA) and Causally-Informed Decoding (CID). The authors hypothesize that certain 'cause tokens' influence 'effect tokens' during generation and demonstrate that adjusting probabilities based on these relationships enhances model performance on benchmarks."
          },
          "strengths": {
            "value": "The paper presents a novel perspective by focusing on causal dynamics among output tokens rather than internal activations, which is a significant shift in interpretability research. The proposed heuristics (CLA and CID) are theoretically grounded and show measurable improvements in accuracy. The clarity of the problem statement, methodology, and experimental setup is strong, with a clear connection to practical applications. The significance lies in addressing both interpretability and controllability of LLMs, which are critical challenges in the field."
          },
          "weaknesses": {
            "value": "The experimental validation is limited in scope, with insufficient details on benchmark comparisons, statistical significance, and ablation studies to isolate the impact of CLA/CID. The causal graph construction is simplified, potentially oversimplifying complex interactions. The example in Figure 1 is specific to a single case, and generalization to other scenarios is not thoroughly discussed. Additionally, the paper lacks analysis of computational overhead or scalability of CID for large models."
          },
          "questions": {
            "value": "1. How is the causal graph constructed from final-layer outputs? What assumptions underlie this simplification? 2. Are there cases where CID might degrade performance, and how are these handled? 3. How does CID compare to existing decoding strategies (e.g., top-k, nucleus sampling) in terms of effectiveness and efficiency? 4. What is the theoretical justification for the heuristic of increasing effect token probabilities while decreasing cause tokens? 5. Are the reported accuracy improvements statistically significant across multiple trials?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates causal relationships among candidate output tokens in large language models (LLMs), proposing a method called Critical Layer Ablation (CLA) to approximate causal dependencies and a decoding algorithm (Causally-Informed Decoding, CID) that leverages these relationships to improve generation quality. The approach focuses on token-level causality rather than internal layer activations, demonstrating performance gains on benchmark tasks."
          },
          "strengths": {
            "value": "The paper introduces a novel perspective on LLM interpretability by focusing on token-level causal relationships rather than internal activations. The methodology is well-structured, with clear definitions of cause and effect tokens. The experiments show measurable improvements in accuracy, and the proposed heuristics (CLA and CID) are practical for real-time decoding. The work bridges theoretical understanding with practical control, addressing a critical gap in LLM controllability. The clarity of explanations and figures (e.g., Figure 1) enhances readability."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with existing causal analysis methods (e.g., activation patching or causal mediation analysis) that target internal representations. The causal analysis relies on residual connections, but the exact mechanism by which CLA approximates causality is underexplained. The experiments focus on specific benchmarks (e.g., arithmetic tasks), leaving questions about generalizability. The claim that cause tokens degrade performance is not thoroughly validated across diverse tasks or models. Additionally, the theoretical foundation for CID's probability adjustments is not rigorously justified."
          },
          "questions": {
            "value": "How does CLA account for the complex interactions of residual connections in approximating causality? What is the theoretical basis for CID's strategy of increasing effect token probabilities while decreasing cause tokens? Are the observed improvements consistent across different LLM architectures (e.g., GPT vs. Llama)? How does the method handle cases where cause tokens are necessary for generating correct effect tokens? Could the removal of cause tokens introduce unintended side effects in downstream tasks?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates causal relationships among candidate output tokens in large language models (LLMs), proposing a method to analyze these relationships and a decoding algorithm (CID) that leverages them to improve generation quality. The approach involves Critical Layer Ablation (CLA) to approximate causal dependencies and adjust token probabilities during decoding."
          },
          "strengths": {
            "value": "Originality lies in focusing on causal dynamics between output tokens rather than internal activations, offering a novel perspective on LLM interpretability. The methodology is structured, with clear heuristics (CLA and CID) that address practical constraints of full-scale causal analysis. The experiments demonstrate measurable improvements on benchmarks, and the paper provides a thorough comparison with related work. Clarity is strong, with well-defined contributions and intuitive explanations of concepts like 'cause tokens' and 'effect tokens'."
          },
          "weaknesses": {
            "value": "The causal graph construction is not rigorously justified—how are cause-effect relationships quantified? The claim that 'including cause tokens degrades performance' lacks empirical validation. The experiments focus on arithmetic benchmarks but do not test diverse tasks or model architectures. The heuristics (CLA and CID) are not thoroughly analyzed for limitations or edge cases. The paper also does not address how residual connections interact with the proposed method, which is critical to the hypothesis."
          },
          "questions": {
            "value": [
              "How is the causal graph constructed? Are there specific statistical tests or metrics to validate the cause-effect relationships between tokens?",
              "What ablation studies were conducted to verify the necessity of CLA for CID's performance improvements?",
              "How do the results generalize across different LLM architectures (e.g., GPT vs. LLaMA) or tasks beyond arithmetic benchmarks?",
              "The paper mentions residual connections allow early activations to influence final outputs, but how does CID specifically account for this in its probability adjustments?",
              "Are there cases where CID's modifications to logit values might introduce unintended biases or reduce diversity in generated text?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "6ozaf7VRIP": {
    "paper_id": "6ozaf7VRIP",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces LogicVista, a comprehensive benchmark for evaluating multimodal large language models (MLLMs) on logical reasoning tasks within visual contexts. It addresses gaps in existing benchmarks by focusing on explicit visual-logical reasoning, incorporating 448 multiple-choice questions across five reasoning categories, and providing human-written reasoning annotations. The work also proposes a crowdsourced annotation tool for scalability and evaluates 11 MLLMs."
          },
          "strengths": {
            "value": "Originality is strong, as LogicVista fills a critical gap in systematically assessing visual-logical reasoning, which is underexplored in existing benchmarks. The quality of the benchmark is high, with rigorous manual annotation and a structured evaluation framework. Clarity is evident in the detailed problem formulation and task categorization. Significance is underscored by the relevance of logical reasoning for real-world applications like robotics and medical diagnosis, with the potential to drive advancements in MLLM capabilities."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing benchmarks like GLoRE or MathVista, making it harder to quantify LogicVista's novelty. The methodology for mitigating data leakage (e.g., ensuring benchmark data isn't in training sets) is not thoroughly explained. The evaluation metrics for open-ended reasoning are vague, and the results section is truncated, limiting insight into model performance. Additionally, the crowdsourced annotation tool's design and validation are not elaborated."
          },
          "questions": {
            "value": "1. How does LogicVista specifically address data leakage compared to prior benchmarks? 2. What criteria were used to define the 11 specific capabilities within the 3 broad categories? 3. Can the authors clarify the evaluation metrics for open-ended reasoning tasks? 4. How was the crowdsourced annotation tool validated to ensure consistency and quality? 5. Are there plans to release the annotation tool's code and guidelines for community use?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces LogicVista, a benchmark for evaluating multimodal large language models (MLLMs) on their logical reasoning abilities in visual contexts. The benchmark includes 448 multiple-choice questions across five logical reasoning categories, with human-annotated reasoning steps and a crowdsourced annotation tool for scalability."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in evaluating explicit visual-logical reasoning in MLLMs, which is underexplored compared to text-based benchmarks. The benchmark's design covers diverse reasoning types (inductive, deductive, spatial, etc.) and incorporates both multiple-choice and open-ended evaluation methods. The manual annotation pipeline and crowdsourcing tool demonstrate a commitment to quality and scalability. The focus on systematic evaluation of logical reasoning in visual contexts is significant for advancing MLLMs in real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as statistical significance of results, comparisons with stronger baselines, or ablation studies. The claims about model performance (e.g., 65% in deductive reasoning) are not supported by sufficient quantitative analysis or error analysis. The data leakage concern is mentioned but not empirically validated. The paper also does not thoroughly compare LogicVista to existing benchmarks like GLoRE or MathVista, leaving unclear how it improves upon them. The evaluation framework's scalability and community tool's effectiveness remain unproven without additional evidence."
          },
          "questions": {
            "value": "1. How were the 448 questions selected to ensure coverage of all 11 specific capabilities? 2. What is the exact methodology for the LLM answer evaluator, and how is its reliability validated? 3. Are there ablation studies showing the impact of different reasoning categories on model performance? 4. How does LogicVista explicitly mitigate data leakage compared to existing benchmarks? 5. What quality control measures are in place for the crowdsourced annotations? 6. Why do models score significantly lower in non-deductive categories, and what specific challenges do these tasks pose?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces LogicVista, a benchmark for evaluating multimodal large language models (MLLMs) on their logical reasoning abilities in visual contexts. It addresses gaps in existing benchmarks by focusing on explicit visual-logical reasoning, covering five task categories (inductive, deductive, numerical, spatial, mechanical) with 448 multiple-choice questions. The benchmark includes human-written reasoning explanations and a crowdsourced annotation tool for scalability."
          },
          "strengths": {
            "value": "The paper identifies a critical gap in evaluating MLLMs' logical reasoning in visual contexts, which is essential for applications like robotics and medical diagnosis. The benchmark's structure is comprehensive, covering diverse reasoning types (deductive, spatial, mechanical) and formats (OCR, graphs). The inclusion of both multiple-choice and open-ended evaluations, along with human-written explanations, enhances its utility. The crowdsourced annotation tool demonstrates a commitment to scalability and community involvement, which is a significant strength."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing benchmarks like GLoRE or MathVista, making it unclear how LogicVista differs in scope or methodology. The evaluation of 11 MLLMs is mentioned, but without specific results or analysis, the benchmark's effectiveness remains unverified. The sample size of 448 questions may be insufficient for a robust assessment, and the paper does not address potential biases in the dataset. Additionally, the claim about data leakage is mentioned but not thoroughly explored, leaving questions about the benchmark's fairness."
          },
          "questions": {
            "value": "1. How does LogicVista specifically address the limitations of existing benchmarks (e.g., GLoRE, MathVista) in terms of task diversity and evaluation methodology? 2. What measures were taken to prevent data leakage, and how was the benchmark's fairness validated? 3. Can the authors elaborate on the 11 specific capabilities and their alignment with real-world applications? 4. How was the annotation pipeline validated to ensure consistency and quality? 5. What are the exact results for the 11 MLLMs, and how do they compare to state-of-the-art performance in other benchmarks?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "6w9qffvXkq": {
    "paper_id": "6w9qffvXkq",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes a novel approach to improve CNN training by combining Riemannian optimization on a generalized Stiefel manifold (with constraint $X^T S X = I$) with gradient-based optimization of the overlap matrix $S$. The method aims to expand the solution space compared to traditional Stiefel manifolds while retaining the benefits of compact constraints, claiming faster convergence and better test performance on benchmark datasets."
          },
          "strengths": {
            "value": "The paper addresses an important problem in deep learning: enforcing orthonormality constraints for stable training. The generalization of the Stiefel manifold via the overlap matrix $S$ introduces a novel framework for flexible optimization. The method's theoretical foundation on Riemannian geometry and its potential to improve convergence rates are promising. The paper also contextualizes its work within prior research on Stiefel manifolds and Riemannian optimization, showing awareness of the literature."
          },
          "weaknesses": {
            "value": "The paper lacks critical details about how $S$ is optimized, with incomplete equations (e.g., the projection mapping formula is cut off). The experimental section is underdeveloped, with no baseline comparisons to standard optimization methods or other generalized Stiefel approaches. The claimed improvements in convergence and accuracy are not substantiated with sufficient quantitative results. The theoretical analysis of the generalized manifold's properties is superficial, and the paper does not address potential computational costs or scalability."
          },
          "questions": {
            "value": "1. How is the overlap matrix $S$ optimized? What is the exact gradient-based procedure, and how is the gradient of $S$ computed during training? 2. Are there theoretical guarantees for the convergence of the combined optimization of $S$ and CNN parameters? 3. What baselines were used to validate the claimed improvements (e.g., standard SGD, Adam, or other manifold methods)? 4. How does the computational overhead of optimizing $S$ compare to traditional methods, and is it feasible for large-scale CNNs?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a novel approach for CNN training by leveraging Riemannian optimization on a generalized Stiefel manifold, where the overlap matrix S is dynamically optimized via gradient-based methods. This expands the solution space compared to traditional Stiefel manifolds while retaining orthonormality benefits, leading to faster convergence and improved test performance on standard datasets."
          },
          "strengths": {
            "value": "The paper introduces a theoretically grounded method for generalizing the Stiefel manifold, which could offer broader applicability than prior work. The motivation for expanding the solution space is well-reasoned, and the connection to prior Riemannian optimization literature is clear. The experimental scope covers multiple benchmark datasets, which adds practical relevance. The paper also addresses the gap in applying generalized Stiefel manifolds to CNNs, showing potential for innovation."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficiently detailed. Key claims about 'faster convergence' and 'improved test performance' lack quantitative comparisons against strong baselines (e.g., standard CNNs, prior Stiefel manifold methods). The optimization of the overlap matrix S is not clearly explained—how is S updated? What are the hyperparameters? The paper also omits computational efficiency analyses (e.g., GPU memory usage, training time). The theoretical analysis of the generalized manifold's impact on training dynamics is underdeveloped, and the projection mapping equations are presented without sufficient context or justification."
          },
          "questions": {
            "value": "1. How is the overlap matrix S optimized during training? Is it updated via gradient descent on S itself, and if so, how is the gradient computed? 2. What specific hyperparameters are used for optimizing S, and how were they tuned? 3. How does the proposed method compare to baseline approaches (e.g., standard CNNs, Li et al. 2020) in terms of metrics like accuracy, convergence speed, and computational cost? 4. Are there cases where the generalized Stiefel manifold might fail compared to the standard Stiefel manifold? 5. How does the interaction between Riemannian optimization and standard optimizers like Adam work in practice?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes a novel approach for improving CNN training by leveraging Riemannian optimization on a generalized Stiefel manifold, where parameter matrices are constrained by $X^T S X = I$ with a dynamically optimized overlap matrix $S$. The method combines Riemannian optimization on the generalized manifold with gradient-based optimization of $S$, aiming to balance the benefits of compact constraints with increased flexibility."
          },
          "strengths": {
            "value": "The paper addresses a relevant problem in deep learning by extending Riemannian optimization techniques to a generalized Stiefel manifold, which offers a more flexible solution space than traditional orthonormal constraints. The approach is theoretically grounded, with clear connections to prior work on Stiefel manifolds and Riemannian optimization. The experiments on multiple benchmark datasets (CIFAR10, CIFAR100, SVHN, Tiny ImageNet32) demonstrate practical relevance and suggest improvements in convergence and accuracy. The paper also provides a structured presentation of the generalized Stiefel manifold's properties and optimization procedures."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of the generalized Stiefel manifold versus other components (e.g., $S$-optimization). The theoretical analysis of how the generalized manifold improves training dynamics is limited, and the paper does not thoroughly compare against state-of-the-art methods that enforce orthogonality (e.g., orthogonal initialization or regularization). Additionally, the computational cost of optimizing $S$ is not quantified, and the paper does not address potential instability in $S$-updates during training. The experimental results are promising but lack statistical significance testing or detailed hyperparameter analysis."
          },
          "questions": {
            "value": "1. How is the overlap matrix $S$ initialized, and what are the effects of different initialization strategies on training? 2. What is the computational overhead of optimizing $S$ compared to standard Riemannian optimization on the Stiefel manifold? 3. Are there specific scenarios where the generalized manifold fails to outperform the standard Stiefel manifold, and why? 4. How sensitive is the method to the choice of optimizer (e.g., Adam vs. SGD) for $S$-updates? 5. Can the approach be extended to other architectures beyond CNNs (e.g., RNNs or transformers)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "6xCgMOm9oM": {
    "paper_id": "6xCgMOm9oM",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces Learned Farthest Point Sampling (LFPS), a novel method for point cloud downsampling that combines the uniformity of algorithmic sampling (like FPS) with the adaptability of learned sampling. LFPS uses a custom loss function to enforce uniform distribution while allowing the network to prioritize informative regions, with experiments showing improved efficiency and performance over existing methods."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel loss function inspired by FPS, bridging the gap between algorithmic and learned sampling. Quality: Theoretical analysis and ablation studies demonstrate the method's validity. Clarity: The problem statement and approach are well-articulated. Significance: LFPS addresses a critical bottleneck in point cloud processing, with potential impact on large-scale applications."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with state-of-the-art learned sampling methods (e.g., APES, ADS) beyond FPS. The theoretical proof of uniformity is not detailed enough to assess its rigor. Experimental validation is incomplete, with no analysis of LFPS's performance on diverse datasets or architectures. The integration of LFPS into existing networks is not thoroughly explained, and the claimed 'plug-in' versatility remains unverified."
          },
          "questions": {
            "value": "1. How does LFPS handle varying point cloud densities or non-uniform distributions? 2. What is the computational overhead of the proposed loss function compared to FPS? 3. Are there specific scenarios where LFPS underperforms compared to other methods? 4. How were the ablation studies designed to isolate the impact of the loss function vs. network architecture? 5. What datasets and metrics were used to evaluate LFPS's performance in large-scale applications?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces Learned Farthest Point Sampling (LFPS), a novel method that combines the uniform distribution properties of traditional Farthest Point Sampling (FPS) with the adaptability of learned sampling techniques. LFPS uses a novel loss function to enforce uniformity while allowing the network to prioritize informative regions, demonstrating improved efficiency and performance in point cloud processing tasks."
          },
          "strengths": {
            "value": "The paper's originality lies in its hybrid approach, bridging algorithmic and learned sampling methods. The theoretical proof of the loss function's minima ensuring FPS-like uniformity is a strong technical contribution. The clarity of the methodology and experimental setup is excellent, with well-structured ablation studies and clear comparisons to existing methods. The significance is high, as point cloud downsampling is critical for real-world applications like LiDAR processing, and the paper addresses a key gap in the literature."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with recent learned sampling methods (e.g., APES, LightTN) beyond the claims in the abstract. The theoretical proof is briefly described but lacks depth, making it difficult to verify the guarantees of uniformity. The experiments focus on specific tasks (e.g., classification) but do not thoroughly evaluate LFPS in diverse applications like segmentation or 3D reconstruction. Additionally, the ablation studies could explore the impact of hyperparameters or different network architectures more extensively."
          },
          "questions": {
            "value": "1. How does LFPS compare to state-of-the-art learned sampling methods like APES or LightTN in terms of runtime efficiency and accuracy on large-scale point clouds? 2. What are the specific limitations of the theoretical proof, and how does the loss function handle edge cases (e.g., sparse or noisy point clouds)? 3. Are there scenarios where LFPS might fail to preserve critical information, and how could this be mitigated? 4. Could the framework be extended to handle dynamic point clouds or streaming data?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes Learned Farthest Point Sampling (LFPS), a novel method that combines algorithmic and learned sampling for point cloud processing. LFPS introduces a loss function to enforce uniform point distribution while allowing adaptive selection of informative regions. The approach is theoretically analyzed, validated through experiments, and demonstrated to improve efficiency and performance compared to existing methods like FPS and learned sampling techniques."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel loss function inspired by FPS properties, combining uniformity with adaptability. Theoretical analysis proves that the loss's minima achieve FPS-like uniformity, addressing a key gap between algorithmic and learned methods. Quality: The experiments are comprehensive, including ablation studies and evaluations across diverse tasks. Clarity: The paper is well-structured, with clear explanations of LFPS's design and theoretical contributions. Significance: LFPS offers a practical, efficient alternative for large-scale point cloud applications, with potential to improve existing network architectures."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with other learned sampling methods (e.g., APES, ADS) beyond the claims in the abstract. The theoretical proof of uniformity assumes idealized conditions; real-world scenarios may require further validation. The ablation studies focus on specific tasks but do not explore how LFPS generalizes across architectures. The scalability of LFPS for extremely large point clouds (e.g., billions of points) is not addressed, which is critical for applications like autonomous driving."
          },
          "questions": {
            "value": [
              "How does the proposed loss function handle non-Euclidean point cloud structures or varying densities? Are there specific scenarios where it underperforms?",
              "The paper claims LFPS outperforms FPS in runtime efficiency. What is the exact computational complexity of LFPS compared to FPS, and how does it scale with point cloud size?",
              "Are there cases where the adaptive prioritization of informative regions leads to overfitting or bias in downstream tasks? How is this mitigated?",
              "The theoretical analysis assumes a continuous space. How does the discrete nature of point cloud sampling affect the loss function's effectiveness in practice?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 5
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "6ycX677p2l": {
    "paper_id": "6ycX677p2l",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces a framework for modeling and evaluating episodic memory capabilities in Large Language Models (LLMs), proposing a structured approach to represent episodic events with temporal and spatial contexts. The authors synthesize a novel benchmark dataset with synthetic narratives and question-answer pairs, and evaluate state-of-the-art LLMs (e.g., GPT-4, Llama 3.1) on their ability to handle complex spatio-temporal reasoning tasks. The study highlights significant limitations in current LLMs' episodic memory performance."
          },
          "strengths": {
            "value": "Originality: The paper presents a novel benchmark focused on episodic memory, which is distinct from existing long-context benchmarks. The integration of cognitive science principles into the modeling approach demonstrates creative problem formulation. Quality: The methodology for generating synthetic datasets and defining spatio-temporal reasoning tasks is well-structured. Clarity: The paper provides a comprehensive overview of episodic memory's role in human cognition and its relevance to AI. Significance: The work addresses a critical gap in LLM capabilities, with potential implications for improving factual accuracy and consistency in AI systems."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient comparison with existing memory-augmentation techniques (e.g., retrieval-augmented generation) that could address the limitations identified. The evaluation of LLMs focuses on a narrow set of models (GPT-4, Claude, Llama 3.1, o1-mini) without including other prominent models like Qwen or BERT-based architectures. The benchmark's synthetic nature raises concerns about real-world applicability and potential biases in generated narratives. The paper does not thoroughly analyze how context window size affects performance, despite mentioning 10k-100k token limits. The claims about 'confabulation' and 'episodic grounding' lack quantitative validation beyond anecdotal examples."
          },
          "questions": {
            "value": "1. How was the synthetic benchmark dataset validated for realism and absence of contamination? Were human evaluators involved in quality control? 2. What specific metrics were used to quantify the 'struggle with multiple related events' phenomenon? 3. How do the authors address the potential bias in synthetic narratives that might favor certain models over others? 4. Are there plans to evaluate the benchmark on open-source models beyond the ones tested? 5. How does the paper's definition of episodic memory differ from existing cognitive science frameworks, and what evidence supports its suitability for LLM evaluation?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a framework for modeling and evaluating episodic memory in Large Language Models (LLMs), inspired by cognitive science. It proposes a structured approach to represent episodic events with temporal and spatial contexts, along with a novel benchmark dataset for assessing LLMs' performance on episodic memory tasks. The study evaluates state-of-the-art models (e.g., GPT-4, Llama 3.1) and highlights their limitations in handling complex spatio-temporal relationships."
          },
          "strengths": {
            "value": "The paper addresses a novel and important problem: the lack of episodic memory in LLMs, which is critical for human-like reasoning and avoiding hallucinations. The proposed benchmark is structured and grounded in cognitive science, offering a systematic way to evaluate episodic memory. The work is well-motivated, with clear connections to prior research in neuroscience and memory theory. The release of code and datasets enhances reproducibility and community impact."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as quantitative results on the proposed benchmark or comparisons with existing methods. The claims about LLMs struggling with episodic tasks are not supported by sufficient empirical evidence. The benchmark's construction methodology is not fully described (e.g., how narratives are generated, how ground truth is ensured). The paper also does not clarify how the proposed framework could be integrated into LLMs, beyond theoretical discussion."
          },
          "questions": {
            "value": "1. How was the episodic memory benchmark constructed to ensure it is 'free from contamination'? What measures were taken to validate the ground truth of the generated narratives and QA pairs? 2. Are there quantitative results demonstrating the limitations of existing LLMs on the proposed benchmark, or are the claims based on qualitative observations? 3. How does the proposed framework differ from existing retrieval-augmented or in-context learning methods? 4. What specific challenges arise when integrating episodic memory into LLMs, and how does the paper address them? 5. How scalable is the benchmark to larger contexts or more complex spatio-temporal scenarios?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces a framework for modeling and evaluating episodic memory in Large Language Models (LLMs), inspired by cognitive science principles. It proposes a synthetic episodic memory benchmark with 11 datasets to assess tasks involving temporal, spatial, and contextual reasoning. The work highlights the limitations of state-of-the-art LLMs in handling complex episodic tasks and advocates for integrating episodic memory to improve consistency and factual accuracy."
          },
          "strengths": {
            "value": "The paper's originality lies in its focus on episodic memory, a largely unexplored area in LLM research, and its structured approach to modeling events with temporal, spatial, and entity-based contexts. The benchmark is novel and well-organized, with synthetic datasets designed to control for contamination and ensure coherence. The evaluation of multiple LLMs (e.g., GPT-4, Llama 3.1) demonstrates the feasibility of the tasks and identifies gaps in current models. The clarity of the problem statement, methodology, and contributions is strong, and the significance of addressing hallucinations and memory limitations in LLMs is compelling."
          },
          "weaknesses": {
            "value": "The synthetic benchmark, while controlled, may lack real-world complexity and diversity. The paper does not fully address how the proposed framework can be integrated into existing LLM architectures or adapted for dynamic, long-term memory retention. The evaluation focuses on a limited set of models and configurations, and the experiments do not explore the impact of varying context lengths or the role of specific memory mechanisms (e.g., retrieval-augmented generation). Additionally, the paper could provide more details on the synthetic data generation process and its alignment with real-world episodic scenarios."
          },
          "questions": {
            "value": [
              "How was the synthetic episodic memory benchmark generated to ensure it reflects real-world spatio-temporal complexities while maintaining control over ground truth?",
              "What specific challenges did the evaluated LLMs face in handling multiple related events or complex spatio-temporal relationships, and how do these relate to their architectural limitations?",
              "Are there plans to expand the benchmark to include more diverse or real-world data sources, and how would this affect the evaluation of episodic memory capabilities?",
              "How does the proposed framework address the dynamic updating of episodic memories, which is critical for long-term retention and retrieval beyond the context window?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "6z4YKr0GK6": {
    "paper_id": "6z4YKr0GK6",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces ScienceAgentBench, a benchmark for evaluating language agents in data-driven scientific discovery. It extracts 102 tasks from peer-reviewed publications across four disciplines, validates them with subject matter experts, and evaluates five LLMs using multiple frameworks. The results show limited agent performance, highlighting the need for rigorous task-specific evaluation before end-to-end automation claims."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in evaluating language agents for scientific tasks with a well-designed benchmark. Key strengths include scientific authenticity through expert collaboration, rigorous graded evaluation with multiple metrics, and multi-stage quality control. The focus on real-world tasks from published research enhances relevance. The comparison of frameworks (direct prompting, OpenHands, self-debug) provides actionable insights into agent performance trade-offs."
          },
          "weaknesses": {
            "value": "The experimental scope is limited, with only five LLMs evaluated, which may not represent the broader landscape. The tasks, while diverse, are confined to four disciplines, potentially limiting generalizability. The paper lacks detailed analysis of why certain tasks are particularly challenging for agents. The data contamination mitigation strategies are mentioned but not thoroughly explained. Additionally, the computational cost analysis is superficial beyond API fees."
          },
          "questions": {
            "value": "How were tasks selected from the 44 publications? Were there criteria to ensure diversity or representativeness? What specific challenges do the tasks pose that make them difficult for current agents? How does ScienceAgentBench differ from existing benchmarks like MMLU or ARC? What are the limitations of the evaluation metrics used, and how were they validated? Are there plans to expand the benchmark to additional disciplines or task types?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces ScienceAgentBench, a benchmark for evaluating language agents in data-driven scientific discovery. The authors extract 102 tasks from peer-reviewed publications across four disciplines, validate them with subject matter experts, and unify outputs as self-contained Python programs. They evaluate five LLMs with multiple frameworks, revealing limited agent performance (32.4% task completion) despite significant effort."
          },
          "strengths": {
            "value": "Originality is strong through the creation of a domain-specific benchmark validated by experts, addressing a critical gap in rigorous scientific agent evaluation. The quality of curation (102 tasks from 44 papers) and multi-faceted evaluation (code, execution, costs) demonstrates thoroughness. Clarity is achieved through structured organization and illustrative examples like Figure 2. The significance lies in challenging overblown claims about end-to-end automation while providing a foundation for future research."
          },
          "weaknesses": {
            "value": "The benchmark's scope is limited to four disciplines, potentially restricting generalizability. The data contamination mitigation strategies are described but lack quantitative analysis of their effectiveness. The paper does not compare ScienceAgentBench to existing benchmarks, making it harder to assess its novelty. Additionally, the evaluation focuses on code generation but does not explicitly address other critical scientific skills like hypothesis formulation or experimental design."
          },
          "questions": {
            "value": [
              "How were the 102 tasks selected from the 44 papers? Were there specific criteria to ensure diversity or representativeness?",
              "What is the detailed process for mitigating data contamination? How were these strategies validated?",
              "How do the evaluation metrics handle domain-specific nuances (e.g., differing conventions in Bioinformatics vs. Psychology)?",
              "Are there plans to expand the benchmark to additional scientific disciplines or task types?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 5
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces ScienceAgentBench, a benchmark for evaluating language agents in data-driven scientific discovery. It proposes a rigorous framework to assess agents on individual tasks across four disciplines, emphasizing scientific authenticity through expert validation and standardized Python program outputs. The evaluation of five LLMs reveals limited capabilities, with the best agent solving only 32.4% of tasks independently."
          },
          "strengths": {
            "value": "Originality is evident in the benchmark's design, combining expert validation with task diversity across disciplines. The quality of the work is strong, with meticulous manual validation, multi-stage quality control, and comprehensive evaluation metrics. Clarity is achieved through detailed task examples (e.g., Figure 2) and structured methodology. Significance lies in addressing a critical gap in evaluating agents for scientific workflows, providing a foundation for future research."
          },
          "weaknesses": {
            "value": "The benchmark's scope is limited to four disciplines, potentially restricting its generalizability. The paper lacks comparative analysis with existing benchmarks for scientific code generation. Data contamination mitigation strategies are not thoroughly explained, and the experimental setup does not explore the impact of task difficulty or agent-specific limitations. The results highlight limitations but do not delve into why agents fail, such as knowledge gaps or suboptimal prompting strategies."
          },
          "questions": {
            "value": "1. How were the two data contamination mitigation strategies implemented, and what evidence supports their effectiveness? 2. Were tasks selected to represent a balanced distribution of difficulty, or does the benchmark favor specific types of problems? 3. How do the evaluation metrics (e.g., execution results vs. costs) weigh against each other in determining overall performance? 4. Could the low success rates be attributed to task complexity, agent capabilities, or limitations in the frameworks (e.g., OpenHands CodeAct)? 5. What criteria were used to select the five LLMs, and are there plans to evaluate more diverse models in the future?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "70ul28Zwwp": {
    "paper_id": "70ul28Zwwp",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of efficient annotation in label-scarce settings by proposing a sparse linear bandit framework with a blocking constraint. The authors introduce BSLB, an Explore-Then-Commit algorithm for selecting hard samples based on expert feedback about annotation difficulty, and a meta-algorithm C-BSLB that adapts to unknown sparsity parameters. They provide theoretical regret guarantees and validate the approach on the PASCAL-VOC dataset using real-world difficulty scores."
          },
          "strengths": {
            "value": "The paper presents a novel problem formulation for annotation efficiency under label scarcity, addressing a gap in active learning and coreset selection literature. The theoretical analysis is rigorous, with a regret bound that accounts for sparsity and noise, and the computational efficiency of BSLB is well-justified. The practical application to image classification demonstrates relevance, and the meta-algorithm C-BSLB adds flexibility by eliminating the need for prior knowledge of sparsity parameters. The paper also contributes offline statistical guarantees for the Lasso estimator under mild conditions."
          },
          "weaknesses": {
            "value": "The experimental evaluation is limited to a single dataset (PASCAL-VOC), and comparisons with existing methods in the same domain (e.g., active learning, coreset selection) are not provided. The theoretical assumptions, such as the sparse linear model for difficulty feedback, may not generalize to real-world scenarios. The paper lacks discussion on practical challenges, such as how expert feedback is elicited or integrated into the algorithm. Additionally, the analysis assumes a fixed sparsity level, which may not align with dynamic annotation settings."
          },
          "questions": {
            "value": "1. How does the performance of BSLB compare to baseline methods like random sampling or existing active learning approaches on the PASCAL-VOC dataset? 2. Are the theoretical guarantees robust to violations of the sparse linear model assumption, such as non-linear difficulty patterns? 3. What are the practical considerations for eliciting expert feedback on annotation difficulty, and how does this affect the algorithm's performance? 4. How does C-BSLB adapt to varying sparsity levels in practice, and what are its limitations compared to BSLB?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of efficient annotation in label-scarce settings by proposing a blocked sparse linear bandits framework (BSLB) to sequentially select hard-to-annotate datapoints. The authors introduce a regret analysis for their algorithm under sparsity assumptions, a meta-algorithm (C-BSLB) for adaptive sparsity, and demonstrate efficacy on PASCAL-VOC with real-world difficulty feedback."
          },
          "strengths": {
            "value": "Originality is strong, as the paper combines sparse linear bandits with a blocking constraint to model annotation efficiency, a novel approach for label-scarce regimes. Theoretical contributions include a tight regret bound under mild assumptions, and the analysis of Lasso estimators with robustness to sparsity. Clarity is maintained through formal problem definitions and notation. Significance is evident in addressing practical challenges of expert annotation with limited budgets, with potential applications beyond NLP (e.g., edge devices for personalized recommendations)."
          },
          "weaknesses": {
            "value": "The experimental validation is limited, with only a single dataset (PASCAL-VOC) and no comparison against strong baselines like active learning or coreset methods. The sparsity assumptions (e.g., tail magnitude β_k) may be restrictive in practice. The regret bound's complexity and dependence on unknown parameters (e.g., β_k) could hinder interpretability. The paper lacks ablation studies on hyperparameters or sensitivity analysis for the blocking constraint."
          },
          "questions": {
            "value": "How does BSLB handle cases where the true sparsity level k is unknown? What ablation studies were conducted to validate the impact of difficulty feedback vs. heuristics? Are there comparisons with non-bandit-based active learning methods on PASCAL-VOC? How generalizable are the theoretical guarantees to non-image tasks (e.g., text or edge devices)?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of efficient annotation in label-scarce settings by proposing a novel algorithm, BSLB (Blocked Sparse Linear Bandits), which leverages expert feedback on annotation difficulty to identify hard samples. The approach models the sequential decision-making process as a sparse linear bandit problem with a blocking constraint, ensuring each datapoint is annotated only once. Theoretical guarantees for regret and robustness to sparsity are provided, along with a meta-algorithm (C-BSLB) that adapts to unknown sparsity parameters. The method is validated on the PASCAL-VOC dataset for image classification."
          },
          "strengths": {
            "value": "The paper introduces a novel framework for annotation efficiency by combining sparse linear bandits with blocking constraints, addressing a gap in label-scarce regimes. The theoretical analysis includes a tight regret bound under mild assumptions, and the use of expert feedback for difficulty assessment is a practical and innovative approach. The meta-algorithm C-BSLB demonstrates adaptability without prior knowledge of sparsity parameters. The application to real-world annotation tasks and edge-device recommendations adds significance."
          },
          "weaknesses": {
            "value": "The experimental validation is limited to a single dataset (PASCAL-VOC) and lacks comparisons with alternative methods in similar settings. The real-world annotation difficulty scores are not thoroughly explained, raising questions about their reliability. The theoretical assumptions, such as the Restricted Eigenvalue condition, are not empirically justified. The regret bound's complexity and dependence on unknown parameters (e.g., β_k) may limit practical applicability. The blocking constraint's implementation and its impact on performance are not explicitly analyzed."
          },
          "questions": {
            "value": "1. How were the real-world annotation difficulty scores collected? Are they derived from expert annotations or synthetic data? 2. What baseline methods were compared against in the experiments, and why were they not included? 3. How does the blocking constraint affect the algorithm's performance in practice, and are there scenarios where it could fail? 4. Are the theoretical assumptions (e.g., RE condition) reasonable for real-world data, and how robust are the guarantees to violations? 5. What is the computational overhead of C-BSLB compared to simpler heuristics?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "70xsq3EO2M": {
    "paper_id": "70xsq3EO2M",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes EAGER, an ante-hoc graph explainer for molecular graphs that integrates the Information Bottleneck (IB) principle with bilevel optimization to learn explanations alongside GNNs. The method aims to identify compact, reproducible substructures for predictions, with experiments on molecular classification tasks claiming superiority over existing approaches."
          },
          "strengths": {
            "value": "Originality is evident in the bilevel optimization approach for IB, which diverges from prior variational bounds. The method's integration of explanations into GNN training aligns with the ante-hoc paradigm, enabling reproducibility. The introduction of semi-synthetic datasets with ground-truth explanations is a significant contribution. The paper's clarity is strong, with clear figures and structured methodology, though some technical details are sparse."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with baseline methods, such as specific metrics (e.g., faithfulness, sufficiency) and explicit baselines like GIB or GSAT. The experiments are limited to molecular graphs, leaving generalizability to other graph types unaddressed. The computational complexity of bilevel optimization is not discussed. The semi-synthetic datasets are described but not thoroughly validated for realism or diversity. The theoretical justification for the bilevel approach over variational bounds is underdeveloped."
          },
          "questions": {
            "value": [
              "How does EAGER's bilevel optimization compare to variational IB methods in terms of stability, convergence, and performance? What ablation studies support this choice?",
              "What specific metrics (e.g., SHAP, LIME, or domain-specific measures) are used to evaluate explanation quality, and how do they differ from existing benchmarks?",
              "Are the semi-synthetic datasets publicly available, and how were ground-truth explanations generated? What validation ensures their relevance to real-world molecular graphs?",
              "How does EAGER handle graph heterogeneity or varying sizes beyond the synthetic LACTAM dataset?",
              "What is the computational cost of bilevel training compared to standard GNN training, and how does it scale with graph size?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces EAGER, an ante-hoc explainer for graph neural networks (GNNs) that integrates explanation learning with model training using a bilevel optimization framework inspired by the Information Bottleneck (IB) principle. EAGER aims to identify compact, reproducible molecular substructures for prediction tasks, with experiments showing competitive performance against existing methods."
          },
          "strengths": {
            "value": "The paper presents a novel bilevel optimization approach for ante-hoc explanation, leveraging the IB principle to distill relevant substructures. It introduces three semi-synthetic molecular datasets with ground-truth explanations, addressing a critical gap in evaluation. The method's theoretical grounding in IB and claims of improved accuracy via inductive bias are compelling. Experiments on diverse molecular tasks demonstrate practical utility, and the paper provides insights into mutual information dynamics between inputs and explanations, which are underexplored in prior work."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with other ante-hoc methods like GIB or GSAT, making it difficult to assess the novelty of EAGER's bilevel approach. The semi-synthetic datasets are not thoroughly described, leaving questions about their realism and generalizability. The bilevel optimization's implementation details (e.g., convergence guarantees, computational complexity) are underdeveloped. Additionally, the paper does not provide ablation studies to validate the components of EAGER or analyze how its inductive bias affects model accuracy in detail."
          },
          "questions": {
            "value": [
              "How does EAGER handle varying graph sizes and complexities beyond molecular graphs? Are there limitations in scalability?",
              "What specific aspects of the semi-synthetic datasets ensure they capture real-world molecular patterns, and how were ground-truth explanations generated?",
              "Can the bilevel optimization framework be adapted to other domains beyond molecular graphs, and what are the challenges?",
              "How does EAGER's inductive bias compare to explicit regularization techniques in improving GNN accuracy?",
              "What metrics were used to quantify the 'precision' of EAGER's explanations, and how do they differ from existing evaluation criteria?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces EAGER, a novel ante-hoc graph explainer for molecular graphs that leverages bilevel optimization based on the Information Bottleneck (IB) principle. EAGER jointly learns a graph predictive model and explanations, aiming to identify compact, reproducible substructures for accurate predictions. The method is evaluated on molecular classification tasks, with claims of superior performance over existing post-hoc and ante-hoc approaches."
          },
          "strengths": {
            "value": "Originality is evident in the bilevel training approach for IB, which avoids variational bounds and offers a fresh perspective on graph explanation. The paper addresses a critical problem in GNN interpretability, particularly in high-stakes domains like chemistry. The experiments are comprehensive, including new semi-synthetic datasets and comparisons with state-of-the-art baselines. Clarity is strong, with illustrative examples and figures. The significance is high, as the work bridges the gap between model accuracy and explainability in graph-based systems."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of the bilevel training component versus other design choices. The comparison with baselines is not exhaustive (e.g., specific metrics like F1-score or AUC are not explicitly reported). The semi-synthetic datasets are described but not thoroughly validated for realism or diversity. The theoretical analysis of the bilevel optimization's convergence properties is limited, and the paper does not address potential scalability issues for large graphs."
          },
          "questions": {
            "value": "How are the semi-synthetic datasets validated for their relevance to real-world molecular data? What are the specific limitations of EAGER in terms of computational efficiency or generalizability to non-molecular graphs? How does the bilevel training process ensure robustness to hyperparameter choices? Are there cases where EAGER's explanations might fail to capture critical substructures, and how are these addressed?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "72yPbvSx0c": {
    "paper_id": "72yPbvSx0c",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes Koopman Embedded Equivariant Control (KEEC), a method that learns an embedding of both states and vector fields for nonlinear dynamical systems. By leveraging Koopman operator theory, KEEC ensures isometric and equivariant mappings between the original and latent spaces, enabling linearized dynamics in the latent space and simplifying control policy derivation. The approach is evaluated on tasks like image-based Pendulum, Lorenz-63, and wave equations, showing improved control performance and efficiency."
          },
          "strengths": {
            "value": "The paper introduces a novel framework combining Koopman operator theory with equivariant control, addressing gaps in existing embedding methods that neglect vector field preservation. The theoretical formulation of isometry and equivariance as guarantees for control consistency is original. Experiments demonstrate empirical superiority over baselines in diverse domains. The clarity of the problem statement and motivation is strong, with logical connections to prior work on embedding and symmetry in control."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art methods like model-based control or recent embedding approaches (e.g., VAEs with symmetry priors). The theoretical analysis of how isometry and equivariance ensure control consistency is superficial, with limited proof of convergence or error bounds. The experimental section is brief, with insufficient ablation studies to isolate the contribution of vector field embedding versus other components. The figure captions and descriptions are minimal, making it hard to assess the visual results without seeing the actual figures."
          },
          "questions": {
            "value": "1. How does KEEC explicitly enforce the embedding of vector fields (derivatives of states) into the latent space? 2. What specific baselines were compared against, and why do the authors claim superiority over existing methods? 3. Are there theoretical guarantees that the learned Koopman operator approximates the true dynamics within a quantifiable error margin? 4. How does the method handle high-dimensional or non-differentiable systems, given its reliance on derivative information? 5. What ablation studies demonstrate the necessity of equivariance/isometry for performance gains?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces Koopman Embedded Equivariant Control (KEEC), a method for controlling nonlinear dynamical systems by learning an embedding that preserves both the vector fields of the dynamics and control effectiveness between the original and latent spaces. KEEC leverages the Koopman operator to approximate latent dynamics as linear, simplifying control policy derivation while ensuring equivariance and isometry."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel approach by integrating Koopman operator theory with equivariant embeddings, explicitly addressing the gap of embedding vector fields rather than just states. Quality: The method is theoretically grounded, with claims of improved computational efficiency and control consistency. Clarity: The abstract and introduction are well-structured, though the paper is cut off, limiting full assessment. Significance: Solving control for unknown nonlinear systems has broad implications, and the method's potential to simplify control synthesis is impactful."
          },
          "weaknesses": {
            "value": "The paper is truncated, making it impossible to evaluate the completeness of experiments, ablation studies, or comparisons to baselines. The theoretical guarantees for equivariance and isometry are not thoroughly detailed. The claims of superior performance on tasks like the wave equation lack specific metrics or qualitative analysis. The embedding training process and handling of Koopman operator approximation are unclear."
          },
          "questions": {
            "value": "1. What specific baselines were compared against in the experiments? 2. How is the Koopman operator approximated in practice, and what are the assumptions? 3. Are there ablation studies to validate the contribution of equivariance/isometry? 4. How does KEEC handle high-dimensional or complex dynamics beyond the examples mentioned? 5. What is the computational overhead compared to existing methods?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Koopman Embedded Equivariant Control (KEEC), a method for controlling nonlinear dynamical systems by learning an embedding that preserves both the vector fields of the dynamics and control consistency between the original and latent spaces. KEEC leverages Koopman operator theory to linearize latent dynamics, enabling efficient control policy computation while ensuring isometry and equivariance."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in embedding methods for control by formally incorporating vector field preservation and control consistency. The theoretical foundation in Koopman operators and geometric control theory is strong. The proposed approach has clear potential for improving computational efficiency in nonlinear control. The experiments on diverse domains (e.g., Pendulum, Lorenz-63) suggest practical relevance, and the focus on equivariance and isometry aligns with established control principles."
          },
          "weaknesses": {
            "value": "The paper lacks detailed implementation specifics for the embedding function $g$, particularly how vector fields are embedded alongside states. The experimental validation is incomplete (section 4 is cut off), making it impossible to assess performance claims. The novelty over existing Koopman-based methods (e.g., \\cite{chua2018koopman}) is not clearly differentiated. Theoretical guarantees for isometry and equivariance are mentioned but not rigorously proven. The control policy derivation relies on assumptions (e.g., control-affine) without justification for their validity in the latent space."
          },
          "questions": {
            "value": "1. How is the embedding function $g$ learned? What architecture or optimization objective ensures vector field preservation? 2. What metrics are used to quantify control consistency between original and latent spaces? 3. How does KEEC compare to baseline methods like Koopman-based LQR or VAE-based embeddings in terms of computational efficiency and control accuracy? 4. Are there ablation studies demonstrating the necessity of isometry/equivariance constraints? 5. What specific challenges in the wave equation and Lorenz-63 domains does KEEC address that existing methods cannot?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "75PhjtbBdr": {
    "paper_id": "75PhjtbBdr",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces Multi-Label Test-Time Adaptation (ML-TTA) with Bound Entropy Minimization (BEM), addressing the limitation of existing TTA methods that prioritize the top-1 class in multi-label scenarios. BEM binds top-k predicted labels as a single entity using paired captions to create weak/strong label sets, enabling simultaneous confidence maximization for multiple labels. The approach is evaluated on MSCOCO, VOC, and NUSWIDE datasets, showing superior performance over existing TTA methods."
          },
          "strengths": {
            "value": "The paper presents a novel solution for multi-label TTA, a critical but underexplored area. The idea of leveraging paired captions as pseudo-images with known labels is creative and well-motivated. The method addresses a clear problem (distribution shifts in multi-label settings) with a theoretically grounded objective (BEM). Experiments demonstrate consistent improvements across diverse datasets and architectures, highlighting the approach's generalizability. The writing is clear, and the problem statement effectively contextualizes the contribution."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing multi-label TTA methods, relying instead on vague references to 'SOTA methods.' The process for determining the number of labels (k) and the criteria for selecting paired captions are not sufficiently explained. The experiments focus on CLIP-based models but do not explore other VLMs or domain-specific scenarios. Additionally, the theoretical analysis of BEM's properties (e.g., robustness to label noise) is minimal, and ablation studies are not provided to validate key components."
          },
          "questions": {
            "value": [
              "How does the method handle cases where paired captions are unavailable or inaccurate? What is the dependency on the quality of retrieved captions?",
              "The paper mentions 'weak label set' and 'strong label set' but does not clarify how these are derived or validated. Can the authors provide more details on this process?",
              "What is the impact of varying k (number of top labels) on performance? Are there optimal ranges for k in different datasets?",
              "The experiments focus on CLIP-based models. How would BEM generalize to other VLMs or non-CLIP architectures?",
              "Are there any limitations of BEM in scenarios with highly imbalanced label distributions or rare labels?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a Test-Time Adaptation (TTA) framework for multi-label scenarios, termed ML-TTA, with a novel Bound Entropy Minimization (BEM) objective. The method leverages paired captions to generate pseudo-labels for multi-label adaptation, binding top-k predicted labels to simultaneously increase their confidence. Experiments on MSCOCO, VOC, and NUSWIDE demonstrate superior performance over existing TTA methods."
          },
          "strengths": {
            "value": "Originality: The paper addresses multi-label TTA, a largely unexplored area, and proposes BEM to handle varying label counts. The use of captions as pseudo-images with known labels is innovative. Quality: Experiments are comprehensive, covering multiple datasets, architectures, and label scenarios. Clarity: The paper is well-structured, with clear problem formulation and contributions. Significance: Multi-label adaptation is critical for real-world applications, and the method improves CLIP's adaptability without requiring source domain data."
          },
          "weaknesses": {
            "value": "The paper relies on captions to generate strong labels, but the accuracy of caption-derived labels is not validated. How are captions paired with images, and what happens if they are incorrect? The method assumes the number of labels (k) is known, but this is not addressed in real-world scenarios without ground truth. The theoretical justification for BEM is minimal, and the choice of k lacks empirical analysis. Experiments lack comparisons with multi-label-specific TTA methods, focusing only on multi-class baselines."
          },
          "questions": {
            "value": [
              "How are paired captions retrieved, and what ensures their alignment with image labels? What is the error rate of caption-derived labels?",
              "How is the value of k determined without ground truth? Does the method handle cases where the actual label count differs from the caption's label count?",
              "Are there ablation studies showing the contribution of BEM versus entropy minimization alone?",
              "What is the impact of noisy or incorrect captions on performance? How does the method filter out unreliable captions?",
              "How does BEM generalize to datasets with highly variable label counts (e.g., NUSWIDE vs. VOC)?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of multi-label test-time adaptation (TTA) by proposing Bound Entropy Minimization (BEM), which simultaneously increases the confidence of multiple top predicted labels. The method leverages paired captions to create weak and strong label sets, binding top-k predicted labels for joint optimization of view and caption prompts. Experiments on multiple datasets demonstrate superior performance over existing TTA methods."
          },
          "strengths": {
            "value": "The paper introduces a novel BEM objective tailored for multi-label TTA, addressing a critical gap in existing entropy minimization approaches that focus on single-label confidence. The method's integration of visual and textual modalities through paired captions is creative and well-motivated. The experimental validation across diverse datasets and architectures demonstrates robustness. The paper also provides a clear theoretical justification for binding top-k labels to avoid over-optimizing a single class."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contribution of BEM versus other components (e.g., caption retrieval). The mechanism for determining the number of labels (k) per view is under-explained, and the reliability of using captions as 'strong labels' needs further justification. The comparison with multi-label-specific TTA methods is limited, and the paper does not discuss computational efficiency or scalability to large-scale datasets."
          },
          "questions": {
            "value": [
              "How is the value of k (number of labels) determined for each augmented view? Is it fixed or adaptive?",
              "What criteria are used to retrieve paired captions, and how robust is this process to caption quality variations?",
              "Are there cases where the weak label set (from views) and strong label set (from captions) conflict? How is this resolved?",
              "How does BEM handle images with fewer labels than k, or when captions contain irrelevant labels?",
              "What is the computational overhead of incorporating caption prompts compared to standard TTA methods?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "7652tHbbVE": {
    "paper_id": "7652tHbbVE",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "FlexMotion introduces a lightweight, physics-aware framework for human motion generation using a diffusion model in latent space. It combines a multimodal Transformer encoder-decoder with spatial controllability modules to ensure physical plausibility and fine-grained control over motion parameters like joint locations, muscle activations, and contact forces."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in human motion generation by balancing physical realism, efficiency, and controllability. The use of a latent-space diffusion model reduces computational costs compared to physics simulators, while the multimodal Transformer integrates biomechanical constraints. The plug-and-play controllability module offers practical utility for domain-specific applications. The work extends datasets with muscle activations, enabling new research directions in multimodal motion synthesis."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, including quantitative metrics for physical plausibility and comparisons to state-of-the-art methods. The method section is truncated, leaving critical technical details (e.g., physics constraints, latent space design) unexplained. The claim of being 'the first' to ensure physical plausibility requires verification against prior work. The evaluation on existing datasets (e.g., HumanML3D) does not demonstrate superiority over baselines, and ablation studies are missing."
          },
          "questions": {
            "value": [
              "How does the Transformer encoder-decoder explicitly enforce physical constraints? Are there specific biomechanical equations or rules incorporated?",
              "What are the exact metrics used to evaluate physical plausibility and controllability? How do they compare to existing benchmarks?",
              "How is the latent space defined, and what portion of biomechanical information is preserved during encoding?",
              "Are there ablation studies showing the contribution of the controllability module versus the base diffusion model?",
              "How does FlexMotion compare in efficiency (e.g., training time, inference speed) to physics-based simulators or other diffusion models?",
              "What are the limitations of the dataset extensions (e.g., muscle activation annotations)? How generalizable are the results to unseen motions?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "FlexMotion introduces a lightweight, physics-aware framework for human motion generation using a diffusion model in latent space and a multimodal Transformer encoder-decoder. It claims to enable physical plausibility, computational efficiency, and spatial controllability by integrating joint locations, contact forces, and muscle activations without relying on physics simulators."
          },
          "strengths": {
            "value": "The paper's strengths include a novel approach to integrating physics-aware constraints into diffusion models, the introduction of a plug-and-play controllability module, and the extension of datasets with muscle activation data. The method's focus on latent-space operations for efficiency is promising, and the use of a Transformer for multimodal input demonstrates creative problem formulation. The paper also highlights practical applications across domains like animation and robotics."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous experimental validation, such as ablation studies or comparisons with state-of-the-art methods. Claims about physical plausibility and efficiency are not supported by concrete metrics or benchmarks. The methodology section is underdeveloped, with insufficient details on how the Transformer integrates physics constraints or how the latent-space diffusion model is trained. The related work section appears incomplete, omitting key prior works on physics-aware motion generation. The evaluation datasets (e.g., HumanML3D) are not thoroughly analyzed for their suitability or limitations."
          },
          "questions": {
            "value": "1. What specific metrics were used to quantify physical plausibility, and how do FlexMotion's results compare to baselines like MotionDiffusion or PhysGen? 2. How is the pretrained Transformer encoder-decoder trained, and what datasets were used for pre-training? 3. Are the extended datasets (e.g., with muscle activations) publicly available, and how were they curated? 4. What are the computational costs (e.g., FLOPs, inference time) of FlexMotion compared to existing methods? 5. How does the plug-and-play module handle conflicting spatial constraints (e.g., joint locations vs. contact forces)?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "FlexMotion introduces a lightweight, physics-aware framework for generating controllable human motion using a diffusion model in latent space. It integrates a multimodal Transformer encoder-decoder to model biomechanical constraints and includes a plug-and-play module for spatial controllability. The paper claims to extend datasets with muscle activations and contact forces to improve physical plausibility and efficiency."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in balancing physical realism, computational efficiency, and controllability in human motion generation. The use of a diffusion model in latent space for efficiency and the integration of physical constraints via a pre-trained Transformer show originality. The proposed plug-and-play controllability module is a novel approach for fine-grained motion manipulation. The paper also highlights practical applications across animation, robotics, and VR, underscoring its significance."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results and quantitative comparisons to baseline methods, making it hard to validate claims of 'superior performance' in realism, physical plausibility, and controllability. The extension of datasets with muscle activations and contact forces is not described in terms of data collection or validation. The method section is incomplete, leaving critical details about how physical constraints are enforced in the Transformer unclear. The computational efficiency claims are unsupported without training/inference time metrics or model size comparisons."
          },
          "questions": {
            "value": "1. How were the extended datasets (e.g., muscle activations) collected and validated? 2. What specific physical constraints are enforced in the Transformer, and how are they integrated into the model? 3. Are there quantitative results comparing FlexMotion to existing methods in terms of physical plausibility and controllability? 4. What is the training/inference speed and model size compared to prior work? 5. Are ablation studies provided to demonstrate the effectiveness of the plug-and-play module?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "78tc3EiUrN": {
    "paper_id": "78tc3EiUrN",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes MADGEN, a two-stage framework for de novo molecular structure generation from MS/MS spectra. It first retrieves molecular scaffolds using contrastive learning and then generates full molecules via an attention-based model conditioned on the spectrum. The approach aims to reduce search space complexity and improve accuracy by leveraging scaffolds as structural anchors."
          },
          "strengths": {
            "value": "Originality is evident in the scaffold-based decomposition of the molecular generation problem, combined with contrastive learning for scaffold retrieval. The method's structured approach to reducing complexity and improving interpretability is well-articulated. Experiments on multiple datasets demonstrate effectiveness, particularly with oracle scaffolds. The paper clearly contextualizes its contributions within metabolomics and molecular generation literature, highlighting practical relevance."
          },
          "weaknesses": {
            "value": "The reliance on an oracle scaffold retriever limits real-world applicability, as the paper does not thoroughly evaluate performance with predicted scaffolds. Comparisons to state-of-the-art methods are limited to a few baselines (e.g., MSNovelist, Spec2Mol), omitting more recent approaches like RetroBridge or MassGenie. The lack of ablation studies on key components (e.g., attention mechanisms, scaffold types) and detailed analysis of failure cases weakens the evaluation. Theoretical justification for scaffold selection (e.g., Murcko scaffolds) is minimal."
          },
          "questions": {
            "value": "1. How does MADGEN handle scaffold prediction errors in practical scenarios? What metrics quantify the impact of scaffold accuracy on final molecule generation? 2. Why were Murcko scaffolds chosen over alternative scaffold definitions (e.g., Bemis-Murcko vs. scaffold trees)? 3. Are there ablation studies showing the contribution of contrastive learning vs. other scaffold retrieval methods? 4. How does the attention mechanism specifically integrate spectral information during generation? 5. What are the computational costs and scalability of MADGEN compared to end-to-end methods?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces MADGEN, a scaffold-based method for de novo molecular structure generation from MS/MS spectra. It proposes a two-stage approach: scaffold retrieval using contrastive learning and spectra-conditioned molecular generation with an attention-based model. The method aims to reduce the complexity of molecular generation by focusing on scaffolds, which are core structural motifs."
          },
          "strengths": {
            "value": "Originality: The use of scaffolds as a structural anchor for de novo generation is a novel approach, particularly in integrating spectral information through attention mechanisms. Quality: The methodology is well-structured, with clear stages for scaffold retrieval and generation. Clarity: The paper is well-written, though some sections (e.g., experimental details) are truncated. Significance: Addressing the 'dark chemical space' in metabolomics is a critical challenge, and the method offers a promising direction for improving annotation accuracy."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient experimental validation. For example, it does not compare MADGEN to state-of-the-art de novo generation methods like MSNovelist or Spec2Mol. The oracle retriever's implementation is unclear, and the comparison with the predictive retriever does not fully address how scaffold prediction errors impact final results. The paper also does not discuss limitations of scaffold-based approaches (e.g., inability to generate novel scaffolds). Additionally, the evaluation metrics (e.g., Tanimoto similarity, validity) are not explicitly defined, making it hard to assess the results."
          },
          "questions": {
            "value": [
              "How is the oracle retriever implemented? Does it use ground-truth scaffolds from the dataset, and if so, how does this affect the evaluation of the predictive retriever?",
              "What baselines were used for comparison? The paper mentions comparing with 'best reported results on MassSpecGym' but does not specify which methods or metrics were used.",
              "How does the scaffold prediction step influence the final generation quality? Are there ablation studies showing the impact of scaffold accuracy on downstream results?",
              "What are the limitations of using Murcko scaffolds? Could this approach fail for molecules with non-standard or novel scaffolds?",
              "Are the experiments on the three datasets (NIST23, CANOPUS, MassSpecGym) sufficiently detailed? For example, how many spectra were used, and what is the distribution of molecular properties?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces MADGEN, a two-stage framework for de novo molecular structure generation from MS/MS spectra. The method first retrieves molecular scaffolds using contrastive learning and then generates full molecules guided by the spectrum via an attention-based model. The approach aims to reduce search space complexity and improve accuracy by leveraging scaffold-based constraints."
          },
          "strengths": {
            "value": "Originality is evident in the scaffold-based strategy, which offers a novel way to constrain molecular generation. The method's structured two-stage approach (scaffold retrieval + spectra-conditioned generation) demonstrates creative problem formulation. Experimental evaluation on three datasets (NIST23, CANOPUS, MassSpecGym) with oracle/predicted scaffold comparisons shows thoroughness. The focus on interpretability through scaffold-based generation is significant for practical applications. The paper also highlights broad applicability in metabolomics and drug discovery."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art de novo generation methods (e.g., MSNovelist, Spec2Mol) beyond the MassSpecGym benchmark. The contrastive learning implementation for scaffold retrieval is under-specified, with no ablation studies on hyperparameters or backbone architectures. The truncated methods section prevents full assessment of the attention mechanism's design and training procedure. The evaluation of predicted scaffold performance appears limited, with no analysis of failure cases or error propagation. The paper does not address how Murcko scaffolds compare to alternative scaffold definitions."
          },
          "questions": {
            "value": "1. How does MADGEN's performance compare to non-scaffold-based methods like MS2Mol or MassGenie on standard benchmarks? 2. What specific architectural choices (e.g., transformer vs. LSTM) underpin the attention-based generator, and how were they validated? 3. Can the authors provide quantitative analysis of how scaffold prediction errors impact final molecular generation accuracy? 4. How does the contrastive learning objective balance scaffold-spectrum alignment with molecular generation quality? 5. Are there domain-specific limitations to using Murcko scaffolds for certain molecule classes?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "7Ab1Uck1Pq": {
    "paper_id": "7Ab1Uck1Pq",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces PROFILER, a novel black-box method for detecting the origin of AI-generated text by analyzing context-level inference patterns. The approach leverages context-aware loss calculations between a surrogate model's logits and input context to extract both independent and correlated features, improving detection accuracy over existing token-level methods. The authors evaluate PROFILER on a newly extended dataset spanning multiple LLMs and domains, reporting significant performance gains."
          },
          "strengths": {
            "value": "Originality is strong through the focus on context-level inference patterns rather than token-level features, addressing a critical gap in existing LLM origin detection. The methodology demonstrates creativity in combining surrogate models with contextual analysis. Experimental results show clear improvements over 10 baselines, with substantial AUC gains. The paper's significance is reinforced by its practical implications for accountability and misuse mitigation in AI systems. Clarity is generally good, though some technical details appear truncated."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, limiting assessment of full methodology and experiments. Key details about the surrogate model architecture, dataset curation process, and statistical significance of results are missing. The claim of 'more than 45.5% increase' lacks baseline comparison details. The paper does not address potential biases in the newly created dataset or evaluate performance on edge cases (e.g., short texts, non-English content). The theoretical justification for context-level features versus token-level features is underdeveloped."
          },
          "questions": {
            "value": "1. How was the GCJ C++ code dataset collected and validated? 2. What specific surrogate model architecture was used, and how does its size/parameters affect results? 3. Are there ablation studies demonstrating the contribution of independent vs. correlated features? 4. How does PROFILER handle texts with minimal context or varying lengths? 5. What statistical tests were used to confirm the significance of AUC improvements? 6. Are there any known limitations when detecting outputs from open-source LLMs vs. commercial ones?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes PROFILER, a black-box method for detecting the origin of AI-generated text by analyzing context-aware inference patterns. The approach extracts distinct contextual features from surrogate models' logits and adjacent input contexts, aiming to improve accuracy in identifying the source LLM compared to existing methods."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in AI ethics and accountability, with clear practical relevance. The method introduces a novel feature extraction framework that leverages context-level inference patterns, expanding beyond traditional token-level analysis. The experimental setup includes a new dataset with diverse LLM-generated text, and preliminary results show significant improvements over baselines. The paper also provides a detailed comparison of existing detection paradigms, demonstrating strong contextualization of the problem."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the necessity of key components like context losses or correlated features. The evaluation metrics (e.g., AUC scores) are not contextualized against baseline performance distributions, making it hard to assess statistical significance. The dataset description is minimal, with no information on sample size, balancing across LLMs, or domain coverage. The claims about 'more than 45.5% improvement' lack error bars or confidence intervals, raising concerns about overfitting to specific test cases."
          },
          "questions": {
            "value": "1. How were the context losses calculated? What specific surrogate model was used for experiments? 2. Are the reported AUC improvements statistically significant across multiple test splits? 3. What is the exact composition of the GCJ C++ code dataset, and how was it collected? 4. How does PROFILER handle out-of-distribution scenarios when the target LLM is not in the training set? 5. Can the method be adapted to detect origins of text generated by open-source LLMs not mentioned in the paper?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes PROFILER, a black-box method for detecting the origin of AI-generated text by analyzing context-aware inference patterns. The approach leverages novel context losses derived from surrogate model logits and adjacent input tokens to extract both independent and correlated features, aiming to improve accuracy in distinguishing outputs from different LLMs compared to existing methods."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to AI-generated text origin detection by shifting from token-level to context-level inference patterns, addressing a critical gap in existing methods. The methodology is theoretically grounded in capturing representational differences through contextual analysis. The experimental claims (e.g., 25% AUC improvement) suggest potential significance, and the extension of datasets with diverse LLM-generated samples demonstrates effort toward comprehensive evaluation. The paper also clearly outlines contributions, including a new feature extraction algorithm and a novel dataset."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, as the provided content is truncated and does not include results, ablation studies, or comparisons against relevant baselines. The novelty of context-aware inference patterns is not sufficiently contextualized against prior work (e.g., whether prior methods ignored context or if similar ideas exist). The method's technical specifics (e.g., how context losses are calculated, how features are extracted) are not elaborated, making it difficult to assess feasibility. The claims of 'more than 25% increase in AUC' lack statistical rigor (e.g., p-values, confidence intervals) and comparison to state-of-the-art methods beyond the stated 10 baselines."
          },
          "questions": {
            "value": "1. How exactly are context losses calculated, and what is the mathematical formulation? 2. What specific baselines were used for comparison, and why are they representative of state-of-the-art methods? 3. Are the experiments reproducible, and what hyperparameters were used for the surrogate models? 4. How does PROFILER handle variations in text length, domain, or generation settings (e.g., zero-shot vs. supervised training)? 5. What is the computational cost of PROFILER compared to existing methods, and how scalable is it for large datasets?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "7BDUTI6aS7": {
    "paper_id": "7BDUTI6aS7",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces an extended φ-divergence and integrates it into the Fundamental Risk Quadrangle (FRQ) framework, enabling a unified perspective on distributionally robust optimization (DRO) and risk-averse optimization. It derives primal and dual representations for risk, deviation, regret, and error, demonstrating how tasks like regression and SVMs can be interpreted through this framework. The work also provides examples showing that common machine learning methods fall within this extended quadrangle."
          },
          "strengths": {
            "value": "The paper makes a novel contribution by extending φ-divergence to allow negative weights, which broadens its applicability beyond coherent risk measures. The theoretical framework is rigorous, with clear definitions and axiomatic foundations. The connection between DRO and FRQ is well-articulated, and the examples (e.g., least-squares regression, SVMs) demonstrate practical relevance. The dual and primal representations offer both theoretical insights and tractable optimization formulations."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation, such as experiments comparing the extended φ-divergence to existing methods. While the theoretical contributions are solid, the practical implications of allowing negative weights in the divergence are not fully explored. The case study mentioned is brief and does not provide concrete results. Additionally, the paper does not address computational challenges in implementing the proposed framework for large-scale machine learning tasks."
          },
          "questions": {
            "value": [
              "What specific examples of extended φ-divergence are provided beyond the mentioned ones (e.g., mean-standard deviation risk measure)? How do these examples demonstrate improved robustness compared to standard DRO?",
              "How does the extended φ-divergence framework handle scenarios where negative weights in the ambiguity set could lead to instability or overfitting?",
              "Are there any limitations to the proposed approach, such as assumptions about the structure of the divergence function or the nature of the loss functions considered?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces an extended φ-divergence and integrates it into the Fundamental Risk Quadrangle (FRQ) framework, unifying distributionally robust optimization (DRO) with risk-averse optimization. The authors derive primal and dual representations of risk, deviation, regret, and error measures, demonstrating their applicability to tasks like regression, classification, and portfolio optimization. The work aims to generalize existing DRO methods by allowing negative weights in the divergence ambiguity set."
          },
          "strengths": {
            "value": "The paper presents a theoretically rigorous extension of φ-divergence and its integration into the FRQ framework, which offers a novel perspective on DRO. The dual representations provide interpretable connections between robust optimization and classical risk measures. The formalism is well-structured, with clear definitions and axiomatic foundations. The examples (e.g., least-squares regression, SVMs) highlight the framework's broad applicability, and the paper situates its contributions within a comprehensive literature review."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation beyond a vague 'case study' description, making it difficult to assess practical utility. The novelty of the extended φ-divergence is partially obscured by its reliance on prior work (e.g., Ahmadi-Javid 2012, Rockafellar and Uryasev 2013), with insufficient discussion of how the extension addresses limitations in existing methods. The theoretical analysis assumes convexity and regularity conditions without exploring their practical implications. Additionally, the paper does not compare the proposed framework to state-of-the-art DRO methods or demonstrate quantitative improvements."
          },
          "questions": {
            "value": [
              "What specific case study was conducted to visualize the inner maximization in robust optimization? How were the results quantitatively evaluated?",
              "How does the extended φ-divergence address limitations of traditional φ-divergence in machine learning tasks (e.g., non-coherent risk measures like mean-standard deviation)?",
              "Are there theoretical guarantees for the tractability of the primal formulations in high-dimensional settings?",
              "How does the proposed framework differ from existing DRO formulations (e.g., those based on Wasserstein or KL divergence) in terms of optimization complexity and generalization performance?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces an extended φ-divergence and integrates it into the Fundamental Risk Quadrangle (FRQ) framework, unifying distributionally robust optimization (DRO) with risk-averse optimization. It derives primal and dual representations of risk, deviation, regret, and error, demonstrating that common machine learning tasks like regression and SVMs fall within this framework. The work also provides theoretical connections between DRO and FRQ, enabling new interpretations of optimization problems."
          },
          "strengths": {
            "value": "The paper presents a novel extension of φ-divergence that broadens the scope of risk measures, enabling negative worst-case weights. The theoretical framework is rigorous, with clear definitions and derivations of primal/dual representations. The connection between DRO and FRQ is well-motivated, and the examples (e.g., least-squares regression, SVMs) illustrate the practical relevance. The paper also addresses gaps in existing literature by incorporating non-coherent risk measures like mean-standard deviation into the FRQ framework."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation to demonstrate the practical utility of the extended φ-divergence. While theoretical connections are established, the paper does not provide case studies or experiments comparing the proposed framework to existing methods. Additionally, the extension of φ-divergence to negative values raises questions about the stability of the resulting risk measures and their applicability to real-world problems. The truncation of the paper also limits the ability to assess the completeness of the analysis."
          },
          "questions": {
            "value": "1. Are there empirical experiments demonstrating the effectiveness of the extended φ-divergence in real-world optimization tasks? 2. How does the extension handle edge cases, such as distributions with heavy tails or sparse data? 3. What are the computational complexities of solving the primal/dual formulations, and how do they scale with problem size? 4. Can the framework be adapted to non-convex settings, such as deep learning models?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "7El7K1DoyX": {
    "paper_id": "7El7K1DoyX",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces CaselawQA, a new benchmark with 260 legal classification tasks derived from U.S. Supreme Court and Court of Appeals databases. It demonstrates that small, fine-tuned open-source models outperform large commercial models like GPT-4.5 and Claude 3.7 Sonnet for legal annotation, emphasizing the benefits of specialization over generalist approaches."
          },
          "strengths": {
            "value": "The paper's originality lies in creating a novel, domain-specific benchmark (CaselawQA) with tasks grounded in real legal datasets, addressing a critical gap in legal NLP. The experiments are methodologically rigorous, comparing model performance across scales and data efficiency. The findings challenge the assumption that larger models are always better, offering practical insights for legal scholars. The clarity of the writing, figures, and structured presentation of results enhance readability. The significance is high, as it reorients research priorities toward specialized models for legal tasks."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of why small models outperform large ones—whether due to architecture, data quality, or task-specific adaptations. The comparison with commercial models is limited to a few examples, and the computational costs of fine-tuning are not discussed. The benchmark's generalizability to non-U.S. legal systems or other domains is unexplored. The intercoder agreement analysis is mentioned but not elaborated, and ethical considerations (e.g., bias in legal data) are omitted. The paper also does not address potential limitations in the task design or evaluation metrics."
          },
          "questions": {
            "value": "What specific factors (e.g., data quality, model architecture) enable small models to outperform large ones? How do the results generalize to other legal jurisdictions or languages? What are the challenges in deploying fine-tuned models for legal annotation in practice? How does the benchmark's design ensure representativeness of real-world legal tasks? Are there any biases or limitations in the training data that could affect model performance?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces CaselawQA, a benchmark of 260 legal classification tasks, and demonstrates that small, fine-tuned open-source models outperform commercial large language models (LLMs) like GPT-4.5 and Claude 3.7 Sonnet. It argues that specialization through fine-tuning is more effective for legal annotation than relying on generalist commercial models."
          },
          "strengths": {
            "value": "The paper's primary strength lies in its creation of a novel, large-scale benchmark (CaselawQA) for legal classification, addressing a critical gap in legal NLP research. The empirical findings are rigorous, showing that small models (e.g., Lawma 8B) achieve significantly higher accuracy than commercial models with minimal fine-tuning. The work also highlights practical implications for legal scholars, emphasizing cost-effectiveness and data efficiency. The paper's structure is clear, and the results are well-supported by experiments, including ablation studies on model size and data efficiency."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons with other open-source models (e.g., BERT, RoBERTa) or specialized legal models, which limits the claim that 'small, fine-tuned models' are universally superior. The benchmark's tasks are narrowly focused on U.S. Supreme Court and Court of Appeals databases, raising questions about generalizability to other jurisdictions or legal domains. The paper does not detail how tasks were selected or validated for legal relevance, nor does it address potential biases in the dataset. Additionally, the analysis of intercoder agreement and task heterogeneity is superficial, leaving gaps in understanding model performance relative to human annotators."
          },
          "questions": {
            "value": "1. How were the 260 tasks selected to ensure representativeness of broader legal classification challenges? 2. What specific legal expertise was used to validate the tasks, and how was this ensured? 3. How do the Lawma models compare to other open-source models (e.g., Legal-BERT) not mentioned in the paper? 4. Are the benchmark's tasks applicable to legal systems outside the U.S., and if not, how can they be adapted? 5. What are the limitations of the data efficiency claims—do certain task types require more examples than others?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces CaselawQA, a benchmark of 260 legal classification tasks derived from U.S. Supreme Court and Court of Appeals databases, to evaluate large language models (LLMs) for legal annotation. The authors demonstrate that small, fine-tuned open-source models outperform commercial LLMs like GPT-4.5 and Claude 3.7 Sonnet, even with minimal labeled data, advocating for specialized models in legal NLP tasks."
          },
          "strengths": {
            "value": "Originality: The paper creates a novel, domain-specific benchmark (CaselawQA) with tasks not previously explored in ML, addressing a critical gap in legal NLP. Quality: The experiments are rigorous, comparing commercial models with fine-tuned open-source models across multiple metrics, including data efficiency and generalization. Clarity: The paper is well-structured, with clear explanations of methodology, results, and implications. Significance: The findings challenge the reliance on large commercial models, offering a practical alternative for legal scholars and highlighting the value of specialization in NLP."
          },
          "weaknesses": {
            "value": "The benchmark's scope is limited to U.S. federal court data, raising questions about generalizability to other legal systems or domains. The comparison with commercial models is limited to GPT-4.5 and Claude 3.7 Sonnet, omitting other prominent models like GPT-4 or Llama 3. The analysis of why fine-tuned models outperform commercial ones is superficial, lacking ablation studies or theoretical insights. The paper also does not address potential biases in the dataset or the ethical implications of deploying specialized models in legal contexts."
          },
          "questions": {
            "value": "1. How representative are the CaselawQA tasks of broader legal domains beyond U.S. federal courts? 2. Why were specific commercial models (e.g., GPT-4.5, Claude 3.7 Sonnet) selected for comparison, and how do results generalize to other commercial models? 3. What measures were taken to ensure the dataset's fairness and reduce biases in legal annotations? 4. How do the authors envision scaling their approach to multilingual or international legal systems?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "7FQDHv9fD4": {
    "paper_id": "7FQDHv9fD4",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces a graph neural network (GNN) approach to decompose heterogeneous dynamical systems by jointly learning interaction rules and latent heterogeneity from observable dynamics. The method aims to infer governing equations by revealing the structure of latent properties through trained embeddings, tested on diverse simulations like particle systems, vector fields, and signaling networks."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in dynamical systems by proposing a novel GNN framework for decomposing complex, heterogeneous systems. The approach is theoretically grounded, with experiments covering a diverse range of simulations (e.g., attraction-repulsion, gravity-like, signaling networks). The ability to cluster discrete classes or uncover continuous manifolds in latent space is promising for understanding underlying dynamics. The work builds on prior methods (e.g., Lemos et al. 2023) but extends them by explicitly modeling latent properties."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results and quantitative validation, as the content is cut off. Key aspects like comparison with baselines, ablation studies, and analysis of latent embedding quality are missing. The novelty over existing work (e.g., Lemos et al. 2023, Zhao et al. 2023) is unclear, and the paper does not address limitations or failure cases. The symbolic regression component for retrieving interaction functions is mentioned but not thoroughly evaluated."
          },
          "questions": {
            "value": "1. How does the proposed method differ from Lemos et al. (2023) and Zhao et al. (2023) in terms of architecture or objectives? 2. What metrics are used to evaluate the quality of the learned latent heterogeneity and decomposition? 3. How effective is symbolic regression in retrieving interaction functions, and what are its limitations? 4. Are there comparisons with state-of-the-art methods for dynamical system inference? 5. What loss functions and training strategies are used to optimize the latent embeddings?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes a graph neural network (GNN) approach to model and decompose heterogeneous dynamical systems by jointly learning interaction rules and latent heterogeneity from observable dynamics. The method uses latent embeddings to reveal underlying structures, enabling decomposition of complex systems and inference of governing equations, validated through simulations of particle interactions, vector fields, and signaling networks."
          },
          "strengths": {
            "value": "The paper introduces a novel framework for decomposing heterogeneous dynamical systems using GNNs, addressing a gap in existing methods that often ignore or pre-specify heterogeneity. The approach demonstrates versatility across discrete and continuous heterogeneity scenarios, with experiments on diverse simulations. The integration of symbolic regression for retrieving interaction rules in simple cases highlights practical utility. The method's ability to uncover latent structures and its clear connection to prior work on governing equations are significant contributions."
          },
          "weaknesses": {
            "value": "The experiments are limited to simulated data, with no discussion of real-world applicability or robustness to noise. The paper lacks comparisons with alternative methods for heterogeneity modeling, making it difficult to assess relative performance. The mechanism for using latent embeddings to decompose systems is vaguely described, and the symbolic regression component is under-explained. The truncated methodology section raises questions about the completeness of the GNN design and training details."
          },
          "questions": {
            "value": [
              "How does the proposed method compare to existing approaches for heterogeneity modeling in dynamical systems (e.g., Zhao et al. 2023, Lemos et al. 2023)? What are its key advantages/disadvantages?",
              "Are the latent embeddings explicitly used for decomposition, or is this inferred post-hoc? How is the decomposition validated quantitatively?",
              "What are the limitations of the symbolic regression step? How does it handle complex interaction rules beyond simple cases?",
              "How sensitive is the method to hyperparameters (e.g., embedding dimension, network architecture)? Are there ablation studies to support the design choices?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes using graph neural networks (GNNs) to decompose heterogeneous dynamical systems by jointly learning interaction rules and latent heterogeneity from observable dynamics. The method leverages a shared function approximator parameterized by observable properties and low-dimensional latent embeddings, demonstrated on simulations of particle systems, vector fields, and signaling networks."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in dynamical systems by introducing a novel framework for decomposing heterogeneity through GNNs. The approach is theoretically grounded, with clear motivation for combining latent embeddings and message-passing mechanisms. The experiments cover diverse simulation scenarios, showcasing the method's versatility. The paper's clarity is strong, with well-structured figures and explanations of the GNN architecture."
          },
          "weaknesses": {
            "value": "The experiments are limited to simulated data, with no discussion of real-world applicability or robustness to noise. The paper lacks quantitative evaluation of the latent embedding's quality (e.g., clustering metrics for discrete classes or manifold learning metrics for continuous properties). Comparisons with alternative methods (e.g., symbolic regression, physics-informed models) are superficial. The theoretical analysis of the GNN's ability to capture heterogeneous dynamics is incomplete, and the paper does not address potential limitations in scalability or computational efficiency."
          },
          "questions": {
            "value": "1. How does the method compare to existing approaches like symbolic regression or physics-based models in terms of accuracy and interpretability? 2. What quantitative metrics were used to validate the latent embeddings' ability to capture heterogeneity? 3. How does the method handle noisy or incomplete observational data, which is common in real-world systems? 4. Are there theoretical guarantees for the uniqueness or identifiability of the learned latent representations?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "7GKbQ1WT1C": {
    "paper_id": "7GKbQ1WT1C",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a causality-guided debiasing framework for large language models (LLMs) by leveraging causal pathways to mitigate social biases. The approach introduces selection mechanisms through prompting strategies to regulate biased reasoning and encourage fact-based decision-making, validated across multiple domains with real-world datasets."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by integrating causal reasoning into prompting-based debiasing, a novel perspective that bridges causal modeling and LLM fairness. The experimental validation on diverse datasets shows significant improvements over existing methods, and the framework's ability to unify existing techniques while addressing gaps in fact-based reasoning is notable. The clarity of the problem formulation and the systematic approach to prompting strategies are well-structured."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contribution of causal pathways versus other components. The causal modeling section is superficial, with insufficient justification for how the framework identifies and manipulates causal paths in LLMs. Additionally, the experiments focus on specific tasks (e.g., coreference resolution) without broader validation across LLM architectures or more complex bias scenarios. The black-box access assumption is not thoroughly addressed in terms of practical limitations."
          },
          "questions": {
            "value": [
              "How does the framework explicitly identify causal pathways in LLMs, given their opaque internal representations?",
              "What metrics were used to quantify the reduction in biased reasoning, and were statistical significance tests conducted?",
              "Are the proposed prompting strategies applicable to other bias types beyond gender and occupation, such as racial or socioeconomic biases?",
              "How does the framework handle conflicting causal dependencies in real-world data, and what guarantees exist for its robustness?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a causality-guided debiasing framework for large language models (LLMs) that leverages causal pathways to mitigate social biases. The approach uses selection mechanisms through prompting to encourage fact-based reasoning while reducing dependence on biased social cues. The framework unifies existing prompting-based methods and demonstrates effectiveness across multiple domains with real-world datasets."
          },
          "strengths": {
            "value": "The paper presents a novel integration of causal reasoning into prompting-based debiasing, offering a structured framework for analyzing how social information influences LLM decisions. The methodology is well-justified theoretically, with clear connections to existing work. The experimental validation across diverse domains highlights practical relevance, and the paper's clarity in explaining concepts like 'biased reasoning' and 'selection mechanisms' enhances readability. The focus on black-box access aligns with real-world constraints of closed-source LLMs, addressing an important practical challenge."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art debiasing methods, making it difficult to assess the magnitude of improvements. The causal models described are abstract, and the assumptions about DAG structures are not thoroughly justified. While experiments are mentioned, specific metrics, baseline comparisons, and analysis of failure cases are missing. The paper also does not address potential limitations of the causal framework, such as sensitivity to incorrect causal assumptions or scalability to complex real-world scenarios."
          },
          "questions": {
            "value": [
              "How does the framework handle cases where the causal relationships between social dimensions and decisions are not well-defined or dynamically changing?",
              "What specific baselines were used for comparison, and how do the proposed methods quantitatively outperform them?",
              "Are the experiments conducted on standardized benchmark datasets, and how are fairness metrics defined and measured?",
              "How does the framework ensure that 'fact-based reasoning' is prioritized without inadvertently introducing new biases or constraints?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes a causality-guided debiasing framework for large language models (LLMs) that leverages prompting strategies to mitigate social biases. The framework introduces selection mechanisms to regulate causal pathways, aiming to reduce biased reasoning and encourage fact-based decision-making. It validates the approach through experiments on real-world datasets across multiple domains."
          },
          "strengths": {
            "value": "The paper addresses a critical problem of bias in LLMs, which is highly relevant given their growing use in high-stakes applications. The integration of causal reasoning into prompting-based debiasing is a novel perspective, offering a structured approach to understand and mitigate bias through causal pathways. The paper's emphasis on fact-based reasoning as a countermeasure to biased shortcuts is promising. The experimental validation across multiple domains suggests practical relevance, and the framework's ability to unify existing methods is a significant strength."
          },
          "weaknesses": {
            "value": "The paper lacks concrete details on how the causal framework is implemented. For instance, it is unclear how causal graphs (DAGs) are constructed, whether they are manually defined or learned from data, and how the selection mechanisms operationalize causal pathways. The experiments are described as 'extensive' but no specific datasets, metrics, or comparisons to state-of-the-art baselines are provided, making it difficult to assess the empirical rigor. Additionally, the paper does not address potential limitations of the framework, such as its applicability to diverse model architectures or real-world deployment challenges."
          },
          "questions": {
            "value": "1. How are the causal relationships (e.g., DAGs) defined in the framework? Are they manually specified, learned from data, or derived from domain knowledge? 2. What specific datasets and evaluation metrics were used in the experiments? 3. How does the proposed framework compare to existing prompting-based debiasing methods in terms of performance and robustness? 4. Are there any cases where the framework might fail, such as when causal assumptions are violated or when social cues are inherently necessary (e.g., gender in medical contexts)? 5. How is 'black-box access' operationalized, and what constraints does it impose on the prompting strategies?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "7IP7dvswE5": {
    "paper_id": "7IP7dvswE5",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces the Rare-mark-aware Next Event Prediction (RM-NEP) problem to address the issue of rare mark omission in Marked Temporal Point Processes (MTPPs) under imbalanced mark distributions. The authors propose a novel Integral-free Neural MTPP (IFNMTPP) model that unifies improper integrals and approximates them via a neural network, aiming to ensure rare marks are consistently predicted. The approach is evaluated on real-world and synthetic datasets."
          },
          "strengths": {
            "value": "The paper identifies a novel problem (rare mark omission) in MTPPs, which is significant for applications like disaster prediction. The problem formulation is clear, and the proposed IFNMTPP model introduces an original method for handling improper integrals. The experiments suggest the approach outperforms baselines, though more details are needed. The paper's structure and motivation are well-articulated."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficiently detailed. The paper lacks comparisons with state-of-the-art MTPP models and does not provide ablation studies to isolate the contribution of IFNMTPP. The claim that rare marks are 'always included' is overstated without quantitative evidence. The theoretical justification for unifying improper integrals and the neural network's ability to approximate them is underdeveloped. The synthetic datasets and evaluation metrics are not described in depth."
          },
          "questions": {
            "value": [
              "What specific baselines were used for comparison, and how do they relate to existing MTPP literature?",
              "How was the performance of IFNMTPP measured? Were metrics like log-likelihood, coverage of rare marks, or calibration scores used?",
              "What is the exact definition of 'improper integration' in this context, and how does unifying the two functions address this issue?",
              "How does IFNMTPP avoid the computational burden of Monte Carlo integration while maintaining accuracy?",
              "Are there theoretical guarantees for the neural network's ability to approximate the integral, or is this purely empirical?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the rare mark missing issue in Next Event Prediction (NEP) within Marked Temporal Point Processes (MTPPs), where imbalanced mark distributions cause rare marks to be overlooked. The authors propose a novel problem formulation, RM-NEP, which ensures rare marks are consistently included in predictions by answering two questions per mark: probability of occurrence and time prediction. They introduce IFNMTPP, a model that avoids improper integrals through a neural network approach, and validate it on real-world and synthetic datasets."
          },
          "strengths": {
            "value": "The paper identifies a critical, underexplored problem in MTPPs—rare mark bias—which has significant real-world implications (e.g., disaster prediction). The RM-NEP formulation is novel and addresses a clear limitation of existing NEP methods. The methodology combines theoretical insights (unifying improper integrals) with a practical neural model (IFNMTPP). The experiments demonstrate superior performance, and the paper provides a structured theoretical foundation, including detailed derivations in the appendix. The problem's relevance and potential impact on safety-critical applications strengthen its significance."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art methods for rare event prediction, making it hard to assess the magnitude of improvements. The novelty of IFNMTPP is somewhat vague—unifying improper integrals is a technical contribution, but the paper does not clearly explain how this differs from prior work on numerical integration. The experimental section is limited in scope, with no ablation studies or analysis of IFNMTPP's robustness to varying levels of class imbalance. Additionally, the paper does not address how RM-NEP scales to high-dimensional or continuous marks, which could limit its applicability."
          },
          "questions": {
            "value": [
              "How does IFNMTPP specifically avoid improper integrals compared to existing methods? What are the theoretical guarantees for its convergence or accuracy?",
              "What are the exact baselines used for comparison? Are they representative of the latest approaches in rare event prediction or MTPPs?",
              "How is the 'rare mark inclusion' quantitatively evaluated? Are there metrics beyond accuracy (e.g., F1-score for rare classes) to validate the effectiveness of RM-NEP?",
              "The paper mentions synthetic datasets, but it is unclear how they are generated. Are they designed to mimic real-world imbalances, and how do they differ from existing benchmarks?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper addresses the rare mark missing issue in Next Event Prediction (NEP) for Marked Temporal Point Processes (MTPPs), where imbalanced mark distributions cause rare events to be overlooked. The authors propose RM-NEP, a novel problem formulation that ensures rare marks are consistently predicted by answering two questions per mark: probability of occurrence and time prediction. They introduce IFNMTPP, a neural model that avoids improper integrals through a monotonic network architecture."
          },
          "strengths": {
            "value": "The paper identifies a critical problem in MTPP applications (rare event neglect) with clear real-world motivation (e.g., earthquake prediction). The RM-NEP formulation is novel and addresses a gap in existing NEP approaches. The methodological contribution of unifying improper integrals and designing an integral-free neural architecture shows creativity. The experimental scope on real-world and synthetic datasets suggests practical relevance, though details are limited."
          },
          "weaknesses": {
            "value": "The paper lacks technical depth in explaining how IFNMTPP avoids improper integrals, which is central to its claim. The theoretical justification for the integral-free approach is underdeveloped, and the paper does not address potential limitations (e.g., scalability, computational cost). Experimental comparisons are superficial, with no ablation studies or analysis of how RM-NEP balances rare vs. frequent mark predictions. The truncated content prevents full evaluation of mathematical formulations and results."
          },
          "questions": {
            "value": "1. How does IFNMTPP mathematically avoid improper integrals? What guarantees exist for its convergence or accuracy? 2. Are there theoretical bounds on the approximation error introduced by the monotonic neural network? 3. How does RM-NEP handle the trade-off between rare and frequent mark prediction accuracy? 4. What baselines were excluded from experiments, and why? 5. How does the method scale to high-dimensional or continuous marks?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "7J2C4QnQrl": {
    "paper_id": "7J2C4QnQrl",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces RL2Grid, a benchmark for evaluating reinforcement learning (RL) methods in power grid operations. Building on Grid2Op, it standardizes tasks, state/action spaces, and rewards to address challenges like complex dynamics, uncertainty, and physical constraints in real-world power grids. The work evaluates existing RL algorithms and highlights the need for novel methods through empirical comparisons."
          },
          "strengths": {
            "value": "Originality lies in creating a standardized benchmark for RL in power grids, extending Grid2Op with flexible configurations and heuristic integration. The clarity of the problem statement and the structured evaluation of RL algorithms are strong. The significance is high, as power grids are critical for decarbonization, and the benchmark addresses key RL challenges. The paper also connects grid-specific issues to broader RL research, fostering interdisciplinary insights."
          },
          "weaknesses": {
            "value": "The experimental section lacks specificity: it does not name the RL algorithms tested, nor does it provide detailed ablation studies on the heuristic module's impact. The paper claims 'drastic improvements' in performance but does not quantify these gains or compare against strong baselines. The realism of the benchmark is not thoroughly validated against real-world grid data or diverse scenarios (e.g., varying renewable integration). Additionally, the paper does not address how RL2Grid differs from existing benchmarks like Grid2Op, limiting its perceived novelty."
          },
          "questions": {
            "value": [
              "Which specific RL algorithms were evaluated, and what are their hyperparameters? How do they compare to state-of-the-art methods?",
              "What are the exact metrics used to quantify the heuristic module's impact on performance and sample efficiency?",
              "How does RL2Grid handle varying renewable energy inputs or grid topologies? Are there plans to incorporate real-world data?",
              "What limitations of Grid2Op does RL2Grid address, and how does it improve upon them?",
              "Are there comparisons to prior benchmarks or baselines in power grid RL? How does RL2Grid advance the state of the art?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces RL2Grid, a benchmark for evaluating reinforcement learning (RL) methods in power grid operations. Built upon the Grid2Op framework, RL2Grid provides standardized tasks, state/action spaces, and rewards to address challenges like complex dynamics, aleatoric uncertainty, and physical constraints. The authors evaluate existing RL algorithms, demonstrate the benefits of integrating heuristic modules, and highlight open research problems in RL for power grids."
          },
          "strengths": {
            "value": "The paper addresses a critical real-world application of RL with clear relevance to decarbonization and grid resilience. The benchmark leverages Grid2Op, a well-established simulation framework, ensuring realism and reproducibility. The integration of a heuristic module to improve sample efficiency and performance is a practical innovation. The empirical analysis of multiple RL algorithms and the discussion of hybrid approaches provide actionable insights. The paper is well-structured, with clear problem formulation and contextualization within the broader RL and power systems literature."
          },
          "weaknesses": {
            "value": "The paper does not clearly articulate how RL2Grid differs from existing benchmarks like Grid2Op, particularly in terms of novel tasks or evaluation metrics. The scope of algorithmic evaluation appears limited, with no mention of state-of-the-art RL methods or comparative analysis against domain-specific solvers. The heuristic module's implementation details and generalization potential are under-specified, raising questions about its applicability to other scenarios. The paper also lacks a detailed discussion of how the benchmark could evolve to address emerging challenges in power grid operations."
          },
          "questions": {
            "value": "1. How does RL2Grid extend or differ from Grid2Op in terms of tasks, constraints, or evaluation metrics? 2. What specific open problems in RL for power grids does this benchmark aim to address, and how are they operationalized? 3. Are there limitations to the heuristic module's design that could hinder its effectiveness in more complex grid scenarios? 4. How do the results compare to domain-specific optimization techniques or hybrid methods that combine RL with physics-based models?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces RL2Grid, a benchmark for evaluating reinforcement learning (RL) methods in power grid operations. Built upon Grid2Op and the CleanRL codebase, RL2Grid provides standardized tasks, state/action spaces, and rewards to address challenges like complex dynamics, aleatoric uncertainty, and physical constraints in power grids. The authors compare RL algorithms and highlight the need for novel methods through empirical analysis."
          },
          "strengths": {
            "value": "The paper addresses a critical real-world application of RL in power grid management, which has significant societal and technical relevance. The benchmarking approach is well-structured, leveraging existing frameworks (Grid2Op and CleanRL) to establish a standardized platform. The inclusion of a heuristic module to improve performance and sample efficiency demonstrates practical innovation. The problem formulation is clear, and the paper effectively contextualizes the challenges of power grids within RL research, emphasizing their importance for decarbonization and grid resilience."
          },
          "weaknesses": {
            "value": "The paper is incomplete, lacking full experimental details, results, and analysis, which limits the ability to assess the rigor of the evaluation. The heuristic module's impact is not thoroughly validated through ablation studies or comparisons with baseline methods. The paper does not sufficiently discuss how RL2Grid differs from existing benchmarks or its specific contributions to overcoming prior limitations in power grid RL. Additionally, the connection between the benchmark's design and real-world deployment challenges remains underexplored."
          },
          "questions": {
            "value": "1. What specific RL algorithms were evaluated, and how do they compare to each other in terms of performance metrics? 2. How was the heuristic module integrated into the training loop, and what ablation studies were conducted to validate its effectiveness? 3. Are there limitations in Grid2Op that RL2Grid addresses, and how does the benchmark capture the complexity of real-world power grid dynamics? 4. What metrics were used to quantify the challenges (e.g., long-horizon goals, physical constraints) in the experiments? 5. How does RL2Grid ensure scalability and generalizability to diverse power grid configurations?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "7L8sZYMlya": {
    "paper_id": "7L8sZYMlya",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes incorporating an intra-class contrastive loss during teacher model training to enrich the diversity of soft labels in knowledge distillation (KD). By leveraging contrastive learning principles, the method aims to prevent soft labels from collapsing to ground-truth labels, while introducing margin loss to improve training stability. Theoretical analysis and experiments on benchmark datasets validate the approach."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel approach to enhance intra-class diversity in KD by combining contrastive learning with teacher training, addressing a gap in existing methods. Quality: The theoretical analysis provides insights into how intra-class and inter-class distances are affected by the proposed loss. Clarity: The paper is well-structured, with clear explanations of the methodology and contributions. Significance: The work advances KD by improving soft label quality, which could benefit model compression and transfer learning tasks."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing methods that also address intra-class diversity (e.g., self-distillation or ensemble approaches). The theoretical proofs are brief and require more rigorous derivation. Experimental validation is limited to a few benchmark datasets, with no ablation studies on hyperparameters or the margin loss. The implementation details of the caching mechanism are unclear, and the paper truncates critical sections of the related work and experiments."
          },
          "questions": {
            "value": "1. How does the margin loss specifically mitigate training instability compared to baseline contrastive loss? 2. Are there ablation studies demonstrating the individual contributions of the intra-class contrastive loss and margin loss? 3. What are the computational costs of the proposed method, and how does it scale to larger datasets? 4. Could the intra-class contrastive loss lead to overfitting on certain datasets, and how is this addressed? 5. How does the caching mechanism reduce memory usage without sacrificing training effectiveness?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces an intra-class contrastive learning approach to enhance knowledge distillation (KD) by enriching the diversity of soft labels generated by teacher models. The method incorporates an intra-class contrastive loss during teacher training, combined with margin loss to address stability and convergence issues. Theoretical analysis and experiments on benchmark datasets demonstrate the effectiveness of the proposed approach in improving intra-class diversity while maintaining inter-class separation."
          },
          "strengths": {
            "value": "The paper presents a novel integration of contrastive learning with knowledge distillation, addressing a key gap in existing methods that neglect intra-class diversity. The theoretical analysis of how intra-class contrastive loss affects class distances is a significant contribution. The proposed margin loss mechanism to stabilize training and the caching strategy for memory efficiency show practical innovation. The experiments validate the method's effectiveness across benchmark datasets, and the paper is well-structured with clear motivation and technical depth."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing methods that also aim to enhance intra-class diversity in KD (e.g., self-distillation or ensemble approaches). The theoretical proofs require more rigor, particularly in establishing the relationship between margin loss and mode collapse mitigation. The experiments focus on a limited set of datasets and architectures, and ablation studies on hyperparameters (e.g., margin loss weight) are insufficient. The figure description is vague, and the caching mechanism's implementation details are not thoroughly explained."
          },
          "questions": {
            "value": "1. How does the proposed intra-class contrastive loss compare to existing methods like self-distillation or ensemble distillation in terms of intra-class diversity? 2. What ablation studies were conducted to validate the necessity of margin loss and the caching mechanism? 3. Are there specific scenarios (e.g., imbalanced datasets) where the method underperforms? 4. How are the parameters of the margin loss (e.g., margin value) determined, and what is their impact on training? 5. Can the theoretical analysis be extended to non-linear models or more complex architectures?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes incorporating intra-class contrastive learning with margin loss during teacher model training to enhance the diversity of intra-class representations in soft labels for knowledge distillation. The method aims to address the limitation of existing distillation approaches that neglect intra-class variability by explicitly encouraging the teacher model to learn diverse representations through contrastive objectives."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to knowledge distillation by combining contrastive learning with teacher-student frameworks, addressing a clear gap in existing methods. The theoretical analysis of intra-class and inter-class distance dynamics provides a rigorous foundation for the proposed method. The paper demonstrates a structured approach to mitigating training instability through margin loss, which is a practical contribution. The problem formulation is relevant to ongoing research in model compression and representation learning."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete due to the paper being cut off, making it impossible to assess the empirical effectiveness of the proposed method. The theoretical analysis lacks concrete mathematical formulations and detailed proofs. The integration of margin loss into intra-class contrastive learning is not sufficiently explained, leaving questions about its implementation and hyperparameter sensitivity. The paper does not compare against relevant works that combine contrastive learning with knowledge distillation (e.g., recent methods like [insert relevant citations]). The ablation studies and消融分析 are missing, which weakens the claims about the method's efficacy."
          },
          "questions": {
            "value": "1. What specific datasets and baseline methods were used in the experiments? 2. How is the margin loss mathematically incorporated into the intra-class contrastive objective? 3. Are there comparisons with existing contrastive-based knowledge distillation approaches (e.g., [insert relevant works])? 4. What is the computational overhead of the proposed method compared to standard KD? 5. How does the method handle class imbalance or imbalanced intra-class variations?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "7NlGsjrEd8": {
    "paper_id": "7NlGsjrEd8",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper addresses alignment quality issues in automatic speech recognition (ASR) models, particularly focusing on the peaky behavior of CTC and HMM training criteria. The authors propose a synthetic framework for studying alignment dynamics, a gradient normalization technique to counteract class imbalance, a hierarchical softmax CTC variant, and a novel gradient-based alignment method. While the methods show improved alignment quality, the WER improvements are minimal."
          },
          "strengths": {
            "value": "The paper introduces a novel synthetic framework for analyzing alignment behavior, which provides a structured way to study training dynamics. The proposed gradient normalization and hierarchical softmax techniques address class imbalance in CTC, showing creative problem-solving. The gradient-based alignment method is generalizable across models (CTC and AED), demonstrating practical utility. The work is well-motivated, with clear connections to prior research and a thorough discussion of limitations."
          },
          "weaknesses": {
            "value": "The paper reports only marginal WER improvements despite significant methodological changes, raising questions about the practical impact of the proposed techniques. The synthetic framework's validity and generalizability to real-world data are not thoroughly justified. The gradient-based alignment method's robustness across diverse scenarios (e.g., noisy inputs, different languages) is not evaluated. The comparison with HMMs and GMMs is superficial, and the theoretical analysis of gradient normalization is limited."
          },
          "questions": {
            "value": [
              "How does the synthetic framework ensure that generated data captures realistic alignment challenges from real-world speech signals?",
              "What is the computational cost of the gradient-based alignment method compared to existing approaches, and how does it scale to large datasets?",
              "Are the proposed methods effective when applied to non-English languages or domains with different acoustic characteristics?",
              "How do the gradient normalization and hierarchical softmax techniques perform when combined with other alignment improvement strategies (e.g., priors or attention mechanisms)?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper addresses alignment quality issues in automatic speech recognition (ASR) models, particularly focusing on CTC and HMM frameworks. The authors propose methods like normalized gradients, hierarchical softmax for blank separation, and gradient-based alignment techniques. They introduce a synthetic framework to study alignment behavior but report only marginal improvements in WER."
          },
          "strengths": {
            "value": "The paper presents multiple novel contributions, including a synthetic framework for alignment analysis, normalized gradient training, and a hierarchical softmax approach. The theoretical analysis of gradient dynamics and alignment mechanisms is thorough. The work bridges gaps between CTC, HMM, and attention-based models, offering insights into training dynamics. The clear problem statement and structured methodology demonstrate strong academic rigor."
          },
          "weaknesses": {
            "value": "The experimental validation is limited: WER improvements are minimal, and the synthetic framework's real-world applicability is unproven. The paper lacks ablation studies to isolate the impact of each proposed method. Comparisons to state-of-the-art alignment techniques (e.g., GMM-HMM) are superficial, and the TSE metric's implementation details are unclear. The theoretical analysis of why normalized gradients improve alignment remains underdeveloped."
          },
          "questions": {
            "value": [
              "How is the time stamp error (TSE) calculated, and what datasets were used for its evaluation?",
              "What specific baselines were compared against for WER and alignment quality? Are the proposed methods tested on diverse speech datasets?",
              "How do the normalized gradients affect training stability and convergence speed compared to prior-based approaches?",
              "What ablation studies were conducted to validate the individual contributions of the proposed methods?",
              "Why do the authors claim gradient-based alignment is 'competitive and more robust' when WER improvements are marginal?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper addresses the challenge of improving alignment quality in automatic speech recognition models, particularly focusing on CTC and HMM frameworks. It introduces a synthetic framework for studying alignment behavior, a gradient normalization technique to counteract class imbalance, a hierarchical softmax variation for CTC, and a novel method for extracting alignments via label log probability gradients. While the proposed methods show some improvements in alignment quality, the gains in WER are modest."
          },
          "strengths": {
            "value": "The paper makes several notable contributions. First, it provides a clear and structured framework for analyzing alignment behavior, which is valuable for understanding training dynamics. Second, the proposed gradient normalization and hierarchical softmax techniques address the class imbalance issue in CTC, which is a well-recognized problem. Third, the novel alignment extraction method via gradients is generalizable across models (e.g., CTC and AED). The work is well-motivated, and the experimental setup is thorough, including comparisons with GMM alignments and analysis of silence rates."
          },
          "weaknesses": {
            "value": "The experimental results show only marginal improvements in WER, which limits the practical impact of the proposed methods. The paper lacks detailed ablation studies to isolate the effect of each contribution. Additionally, the synthetic data framework is not fully described, making it difficult to assess its relevance to real-world scenarios. The comparison with prior work (e.g., HMMs with priors) is superficial, and the paper does not address how the proposed methods could be integrated into end-to-end systems for broader application."
          },
          "questions": {
            "value": [
              "How does the gradient normalization technique specifically differ from prior work on class imbalance (e.g., focal loss or prior-based approaches)?",
              "What are the hyperparameters used for the synthetic data generation, and how do they relate to real speech data?",
              "Why do the proposed methods achieve only small improvements in WER despite better alignment quality? Could there be a trade-off between alignment accuracy and recognition performance?",
              "How does the gradient-based alignment method compare to existing approaches like the forward-backward algorithm or attention mechanisms?",
              "Are the experiments conducted on standard ASR benchmarks (e.g., LibriSpeech, Common Voice), or are the results limited to synthetic data?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "7PGluppo4k": {
    "paper_id": "7PGluppo4k",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces LoCo-LMs, a method to improve the factual consistency and self-consistency of large language models (LLMs) through neuro-symbolic reasoning. The approach involves a novel loss function based on weighted model counting, which enforces logical constraints derived from external knowledge bases (KBs) and rules. The method aims to enhance LLMs' ability to maintain consistency with logical constraints and extrapolate to unseen but semantically similar factual knowledge."
          },
          "strengths": {
            "value": "The paper presents a novel approach by integrating neuro-symbolic reasoning into LLM training, addressing both factual and logical consistency. The method's ability to handle multiple logical constraints simultaneously and its potential for extrapolation to unseen data are significant contributions. The experimental setup is well-structured, and the paper provides clear motivation for the problem of logical consistency in LLMs. The proposed semantic loss is theoretically grounded in probabilistic reasoning and deep learning, offering a principled alternative to existing methods."
          },
          "weaknesses": {
            "value": "The experimental evaluation lacks sufficient detail and comparison with relevant baselines. For example, it is unclear how the proposed method compares to other neuro-symbolic approaches or traditional fine-tuning strategies in terms of scalability and performance on diverse tasks. The paper mentions extrapolation to unseen data but does not provide concrete experiments or metrics to validate this claim. Additionally, the implementation details of compiling logical constraints into circuits and optimizing the semantic loss are not thoroughly explained, which limits the reproducibility of the work."
          },
          "questions": {
            "value": "1. How does the proposed method handle complex logical constraints (e.g., nested or higher-order logic) compared to simpler propositional logic? 2. What specific metrics were used to evaluate extrapolation to unseen data, and how were they validated? 3. Are there ablation studies demonstrating the contribution of individual components (e.g., the semantic loss vs. traditional fine-tuning)? 4. How does the method scale with the size of the knowledge base or the complexity of the logical constraints? 5. What are the computational costs of compiling logical constraints into circuits, and how does this affect training efficiency?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a neuro-symbolic approach to improve the factual consistency and logical self-consistency of large language models (LLMs) by incorporating external knowledge bases (KBs) and logical constraints during fine-tuning. The method uses a semantic loss derived from weighted model counting to enforce consistency with propositional logic constraints, enabling LLMs to maintain coherence even with limited training data and generalize to unseen factual knowledge."
          },
          "strengths": {
            "value": "The paper presents a novel integration of neuro-symbolic reasoning into LLM training, addressing a critical gap in ensuring logical consistency without relying on external tools. The approach is theoretically grounded in probabilistic reasoning and offers a principled way to combine multiple logical constraints. The experimental results demonstrate improvements over baselines in low-data regimes, and the method's ability to extrapolate to unseen but semantically similar knowledge is promising. The paper is well-structured, with clear motivation and a logical flow from problem formulation to evaluation."
          },
          "weaknesses": {
            "value": "The experimental evaluation is limited in scope, with insufficient comparison against state-of-the-art neuro-symbolic methods and a lack of ablation studies to isolate the contribution of key components (e.g., the semantic loss vs. traditional fine-tuning). The paper does not address potential scalability issues when handling large-scale logical constraints or complex non-propositional logic. Additionally, the analysis of generalization to unseen datasets is superficial, with no detailed analysis of failure cases or error distribution."
          },
          "questions": {
            "value": "1. How does the method handle non-propositional logical constraints (e.g., first-order logic) or more complex reasoning tasks beyond propositional formulas? 2. What are the computational costs of compiling logical constraints into circuits, and how does this affect training efficiency? 3. Can the approach be extended to multi-hop reasoning scenarios where consistency depends on multiple interconnected facts? 4. How sensitive is the performance to the choice of KBs or the quality of the provided logical constraints?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces LoCo-LMs, a method for improving the factual consistency and logical self-consistency of large language models (LLMs) through neuro-symbolic integration. The approach uses a semantic loss derived from weighted model counting to enforce compliance with logical constraints, enabling LLMs to maintain consistency even with limited training data and generalize to unseen factual knowledge."
          },
          "strengths": {
            "value": "Originality: The paper presents a novel neuro-symbolic approach that bridges probabilistic reasoning with language model training, offering a principled way to enforce logical constraints without external tools. Quality: The method is theoretically grounded in probabilistic reasoning and semantic loss, with clear connections to existing work in neuro-symbolic AI. Clarity: The paper is well-structured, with a logical flow from problem formulation to experiments. Significance: Addressing logical consistency in LLMs is critical for reliable deployment, and the method's ability to handle multiple constraints simultaneously represents a meaningful advancement."
          },
          "weaknesses": {
            "value": "The experimental evaluation lacks comprehensive comparisons with state-of-the-art methods, particularly those combining neural and symbolic approaches. The paper does not thoroughly analyze the method's performance under complex logical constraints beyond simple negation. The ablation studies are limited, and the paper fails to discuss potential computational bottlenecks when scaling to larger constraint sets. Additionally, the generalization to unseen datasets is not rigorously validated with diverse benchmarks."
          },
          "questions": {
            "value": [
              "How does the method handle conflicting logical constraints during training?",
              "What are the specific limitations of the semantic loss approach when applied to non-propositional logic constraints?",
              "Are there empirical comparisons with other neuro-symbolic methods (e.g., differentiable logic or symbolic reasoning modules)?",
              "How does the performance scale with the size of the constraint set, and what are the computational trade-offs?",
              "What metrics were used to quantify 'logical consistency' beyond negation checks, and how do they correlate with real-world application requirements?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "7PLpiVdnUC": {
    "paper_id": "7PLpiVdnUC",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces LieLAC, a novel framework for achieving equivariance in neural networks under arbitrary Lie groups by leveraging only the action of infinitesimal generators of symmetry groups, bypassing the need for full group structure knowledge. The method enables integration with pre-trained models for tasks like invariant image classification and PDE solving, addressing challenges posed by non-compact symmetry groups."
          },
          "strengths": {
            "value": "Originality: The approach of using infinitesimal generators instead of full group structures for canonicalization is a fresh perspective, particularly for non-compact Lie groups. Quality: The theoretical connections to frame averaging and canonicalization literature are well-articulated, with clear mathematical formulations. Clarity: The paper is structured logically, with illustrative figures (e.g., decision boundary visualization) and clear motivation for addressing non-compact symmetry challenges. Significance: The work addresses a critical gap in equivariant deep learning for scientific computing, with potential impact on physics-informed neural networks and PDE solvers."
          },
          "weaknesses": {
            "value": "The experimental validation is limited to specific cases (e.g., heat equation in Appendix D.3) without comprehensive benchmarks across diverse PDEs or image tasks. The theoretical analysis of how infinitesimal generators capture global symmetry properties remains underdeveloped, with few proofs of equivariance guarantees. The paper lacks ablation studies on hyperparameters of the Lie algebra descent schemes. The comparison with existing canonicalization/frame averaging methods is superficial, missing quantitative performance metrics."
          },
          "questions": {
            "value": "1. How does LieLAC ensure global equivariance when only using local Lie algebra information? 2. What are the computational costs of the proposed Lie algebra descent schemes compared to existing methods? 3. Can the framework handle non-smooth or discrete symmetries not captured by infinitesimal generators? 4. Are there theoretical guarantees for the canonicalization process under non-compact groups? 5. How sensitive is the method to the choice of generator basis in different symmetry scenarios?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces LieLAC, a novel framework for achieving equivariance in neural networks under arbitrary Lie groups by leveraging only the action of infinitesimal generators, bypassing the need for full group structure knowledge. It addresses challenges in applying equivariant architectures to PDE solvers with non-compact symmetry groups, demonstrating efficacy in image classification and PDE solving tasks."
          },
          "strengths": {
            "value": "The work presents a theoretically grounded approach to equivariance under non-compact Lie groups, which is a significant gap in existing literature. The method's integration with pre-trained models is practical and novel. The paper connects Lie algebra canonicalization to frame averaging, offering fresh insights. The motivation for addressing non-compact groups in PDE solvers is compelling, and the framework's flexibility is a strong asset."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation on PDE solvers, with only brief mentions of results. Key theoretical claims (e.g., connections to frame averaging) are not rigorously proven. The ablation studies and comparisons to baseline methods are missing. The practical implementation details of LieLAC (e.g., how infinitesimal generators are discretized) are under-specified. The significance of the contribution is somewhat overstated without concrete benchmarks."
          },
          "questions": {
            "value": "1. How does LieLAC handle the discretization of infinitesimal generators for non-compact groups? 2. What specific baselines were used for comparison in the PDE solver experiments? 3. Can the authors provide theoretical guarantees for the canonicalization process under non-compact groups? 4. How does LieLAC scale to high-dimensional PDEs or complex geometries?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces LieLAC, a method for achieving equivariance in neural networks by leveraging the infinitesimal generators of Lie group symmetries rather than requiring knowledge of the full group structure. The approach enables integration with pre-trained models by canonicalizing inputs according to symmetry properties, particularly for physics-informed neural PDE solvers and invariant image classification tasks."
          },
          "strengths": {
            "value": "The paper presents a novel solution to a critical problem in equivariant deep learning: handling non-compact symmetry groups in PDE solvers, which existing architectures struggle with. By focusing on infinitesimal generators, LieLAC offers flexibility and compatibility with pre-trained models, expanding the scope of applicable symmetries. The theoretical connections to frame averaging and canonicalization are insightful, and the potential impact on scientific machine learning applications is significant. The work also addresses a gap in applying geometric inductive biases beyond classical equivariant architectures."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete due to the paper being cut off, leaving key claims unverified. The theoretical analysis of how LieLAC handles non-compact groups requires deeper exploration, particularly regarding convergence and stability. The paper lacks detailed comparisons with existing canonicalization/frame-averaging methods, making it unclear how LieLAC improves upon them. Additionally, the practical benefits of using infinitesimal generators over full group representations are not sufficiently justified."
          },
          "questions": {
            "value": [
              "How does LieLAC ensure theoretical guarantees (e.g., equivariance properties) when relying solely on infinitesimal generators for non-compact groups?",
              "What specific experiments were conducted to validate LieLAC's efficacy on PDE solvers and image classification? How does it compare to baseline methods?",
              "Can the authors clarify the limitations of using only infinitesimal generators versus full group representations in practical scenarios?",
              "How does LieLAC handle symmetries with non-trivial group actions (e.g., non-linear transformations) that cannot be captured by linear Lie algebraic methods?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "7UKHNQIErp": {
    "paper_id": "7UKHNQIErp",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes a formal framework to characterize direct preference alignment (DPA) algorithms, such as DPO, by translating their loss functions into symbolic expressions using probabilistic logic. The authors aim to systematically analyze the semantics of DPA losses, derive new variants, and understand the structure of the DPA loss landscape."
          },
          "strengths": {
            "value": "The paper introduces an original conceptual framework for analyzing DPA losses through symbolic logic, which could provide valuable insights into their semantics. The idea of treating losses as discrete reasoning problems is novel and aligns with broader trends in neuro-symbolic AI. The paper also highlights the potential for systematic exploration of the DPA loss landscape, which could inspire future research."
          },
          "weaknesses": {
            "value": "The paper lacks concrete examples of how the proposed formalism is applied to specific DPA variants, making it difficult to assess its practical utility. Key claims, such as the doubly exponential size of the DPA loss landscape, are not rigorously justified or supported by evidence. The formalism is described in abstract terms without clear definitions of critical components like 'preference structures' or the probabilistic logic framework. Additionally, the paper provides no experimental validation of its claims or the proposed methods."
          },
          "questions": {
            "value": "1. How exactly is the symbolic expression for DPO derived from the loss function in Equation (1)? Provide a concrete example.\n2. What is the theoretical basis for the claim that the DPA loss landscape has a doubly exponential size? Can this be proven under the assumptions stated?\n3. How does the proposed 'preference structure' primitive address the technical issues mentioned? Provide a formal definition.\n4. Are there empirical results demonstrating that the framework enables the discovery of effective new DPA loss functions?\n5. How does the paper's formalism differ from existing work on symbolic loss functions (e.g., Li et al. 2019)?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes a declarative framework for analyzing direct preference alignment (DPA) algorithms like DPO by formalizing their loss functions as symbolic expressions. The authors introduce a probabilistic logic-based formalism to derive symbolic representations of DPA losses, enabling systematic comparison, modification, and exploration of loss landscapes. They demonstrate how this approach reveals semantic relationships between existing losses and facilitates the design of new variants."
          },
          "strengths": {
            "value": "The paper's originality lies in applying symbolic logic to DPA, offering a novel conceptual framework for understanding alignment algorithms. The formalism addresses a critical gap in the field by providing a structured way to analyze and compare DPA losses. The theoretical contributions are significant, particularly the identification of the doubly exponential size of the DPA loss landscape. The paper's clarity is strong in its conceptual exposition, and the significance of improving LLM alignment through formal methods is well justified."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation of its formalism. While the theoretical analysis is compelling, there are no experiments demonstrating how the derived symbolic expressions translate to practical improvements in model alignment. The section on DPA is cut off, leaving key details about the proposed framework and its implementation unclear. Additionally, the paper does not address potential limitations of the formalism, such as scalability or applicability to non-pairwise preference settings."
          },
          "questions": {
            "value": "How do the symbolic expressions derived in this work relate to the actual training dynamics of DPA algorithms? Are there experiments showing that the proposed framework enables the discovery of superior loss functions? What are the practical challenges in implementing the formalism, and how does it handle complex, real-world preference data? How does the formalism account for the trade-offs between expressiveness and computational feasibility in DPA loss design?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a formal framework to characterize direct preference alignment (DPA) algorithms, such as DPO, using symbolic logic. It aims to derive symbolic expressions for existing DPA losses, analyze their semantic relationships, and explore the structure of the DPA loss landscape. The authors claim their approach enables systematic derivation of new loss functions and provides insights into the complexity of DPA methods."
          },
          "strengths": {
            "value": "The paper introduces a novel formalism for analyzing DPA losses through symbolic logic, which addresses a critical gap in understanding the semantics of these algorithms. The theoretical insights into the doubly exponential size of the DPA loss landscape are significant, offering a new perspective on the complexity of preference learning. The work's conceptual clarity and connection to prior symbolic reasoning literature demonstrate strong originality. The potential practical implications for designing new DPA losses are promising, though not yet validated experimentally."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation of its claims. While it proposes a formal framework, there are no experiments demonstrating how the symbolic expressions improve model performance or how the derived losses compare to existing ones. The theoretical analysis of the loss landscape's size (e.g., $4^{2^n}$) is presented without concrete examples or justification. The paper also does not clearly differentiate its approach from existing formal characterizations of DPA losses, such as those in Azar et al. (2023) or Tang et al. (2024). The absence of case studies or implementation details weakens the practical relevance of the work."
          },
          "questions": {
            "value": [
              "How does the proposed framework address the practical challenges of translating symbolic expressions into implementable loss functions?",
              "What evidence supports the claim that the DPA loss landscape is doubly exponential in size? Are there specific examples or mathematical derivations to substantiate this?",
              "How does the authors' formalism handle the computational intractability of the proposed symbolic reasoning problems, especially given the doubly exponential complexity?",
              "Are there any experiments comparing the performance of DPA losses derived via this framework to existing methods? If not, what are the challenges in doing so?",
              "How does the paper's approach differ from prior work on neuro-symbolic modeling and symbolic compilation (e.g., Li et al. 2019; Manhaeve et al. 2018)?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "7UgQjFEadn": {
    "paper_id": "7UgQjFEadn",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces MOSS, a framework for enhancing Vision-Language Generalists (VLGs) by incorporating modality-specialized adaptation layers (Convolutional LoRA for images and Linear LoRA for text) to address the limitations of unified architectures in interleaved text-image generation. It also presents LEAFINSTRUCT, a large-scale open-source dataset for interleaved instruction tuning."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel design of modality-specialized adaptation layers (Convolutional LoRA and Linear LoRA) within unified VLG architectures, which is a fresh approach to address modality-specific inductive biases. Quality: The method is theoretically grounded, with clear explanations of how the adaptation layers improve modality-specific feature modeling. Clarity: The paper is well-structured, with detailed descriptions of the proposed framework and dataset. Significance: The work addresses a critical gap in interleaved generation, with practical implications for applications like script generation and visual storytelling."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with prior modality-aware expert models that also use specialized architectures (e.g., Akbari et al., 2023; Ye et al., 2024). The experiments do not include ablation studies to isolate the impact of Convolutional LoRA vs. Linear LoRA. The computational cost of adding these layers is not discussed, nor is the scalability of the approach to larger VLGs. The dataset LEAFINSTRUCT is described as 'high-quality' but lacks detailed analysis of domain diversity, annotation quality, or potential biases. The paper also does not address how MOSS generalizes to non-English languages or less common modalities."
          },
          "questions": {
            "value": [
              "How do the proposed Convolutional LoRA and Linear LoRA layers compare to other adaptation techniques (e.g., full fine-tuning, adapter modules) in terms of performance and efficiency?",
              "What are the specific architectures of the two VLG backbones used in the experiments, and how do they differ in their treatment of text and image tokens?",
              "Can the authors provide qualitative examples of how LEAFINSTRUCT improves instruction-following compared to existing datasets?",
              "Are there any limitations in the dataset curation pipeline that could affect the generalizability of the results?",
              "How does the method handle modalities beyond text and images (e.g., audio, video)?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of interleaved text-image generation in Vision-Language Generalists (VLGs) by introducing MODALITY-SPECIALIZED SYNERGIZERS (MOSS), which uses modality-specific adaptation layers (Convolutional LoRA for images and Linear LoRA for text) to better capture domain-specific inductive biases. The authors also present LEAFINSTRUCT, a large-scale dataset for instruction-tuning, and demonstrate improved performance on interleaved generation tasks."
          },
          "strengths": {
            "value": "Originality is strong through the novel application of modality-specific adaptation layers within unified VLG architectures, addressing a critical gap in prior work. The methodology is well-structured, with clear justification for the design choices. The experimental validation is comprehensive, including multiple backbones and datasets. The paper's significance is high, as interleaved generation is a key challenge for VLGs, and the open-sourced dataset and code enhance reproducibility."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of Convolutional LoRA vs. Linear LoRA. The dataset curation process for LEAFINSTRUCT is not sufficiently described, raising questions about its quality and diversity. The experiments focus on specific tasks but do not thoroughly explore edge cases or failure modes. The claims of 'state-of-the-art' performance are not substantiated with comparisons to all relevant baselines."
          },
          "questions": {
            "value": "1. What ablation studies have been conducted to validate the effectiveness of Convolutional LoRA vs. Linear LoRA? 2. How was the LEAFINSTRUCT dataset curated to ensure quality and diversity across domains? 3. Are there specific scenarios where MOSS underperforms compared to baseline VLGs? 4. How does the method scale to very long interleaved sequences or highly complex instructions?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces MOSS, a framework for enhancing Vision-Language Generalists (VLGs) by incorporating modality-specialized adaptation layers (Convolutional LoRA for images and Linear LoRA for text) to address the limitations of unified architectures in interleaved text-image generation. It also presents LEAFINSTRUCT, a large-scale open-sourced dataset for instruction tuning, demonstrating improved performance on interleaved generation tasks."
          },
          "strengths": {
            "value": "Originality is strong, as MOSS introduces modality-specific adaptation layers within unified VLG architectures, a novel approach compared to prior works using identical architectures for both modalities. The quality of experiments is promising, with claims of state-of-the-art results across multiple datasets. Clarity is good, though some technical details are omitted. Significance is high, as interleaved generation is a critical application area for VLGs, and the open-sourced dataset addresses a key resource gap."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with prior modality-aware methods (e.g., Akbari et al., 2023; Ye et al., 2024) to validate the superiority of MOSS. The experimental section is incomplete, with no details on ablation studies or quantitative analysis of modality-specific adaptation layers. The LEAFINSTRUCT dataset's quality control pipeline is described only briefly, raising questions about its reliability. The claim of 'small number of parameters' is vague without explicit quantification."
          },
          "questions": {
            "value": [
              "How does MOSS compare to prior modality-aware approaches in terms of performance and architectural efficiency?",
              "What specific metrics were used to evaluate text quality, image coherence, and text-image consistency in the experiments?",
              "Can the authors provide details on the automatic pipeline used to generate the LEAFINSTRUCT dataset, including quality assurance mechanisms?",
              "What is the exact parameter count and computational cost of MOSS compared to baseline VLGs?",
              "Are the Convolutional LoRA and Linear LoRA layers orthogonal to the pre-trained VLG backbone, or do they require retraining of the base model?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "7X65yoKl3Y": {
    "paper_id": "7X65yoKl3Y",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper identifies three limitations of LoRA (Dropout's ineffectiveness in short training, zero initialization's poor conditioning, and scaling factors causing non-linear interactions) and proposes ALLoRA, an adaptive learning rate method that removes Dropout and scaling factors while using gradient-based normalization. The authors claim improved performance over LoRA and variants like DoRA."
          },
          "strengths": {
            "value": "Originality is demonstrated by systematically analyzing LoRA's limitations and proposing a novel adaptation strategy. The paper provides empirical validation across multiple datasets and models, including Llama3. Clarity is strong, with structured sections and clear problem formulation. Significance is high given LoRA's widespread use in LLM fine-tuning."
          },
          "weaknesses": {
            "value": "The theoretical analysis of why adaptive learning rates address the stated flaws is superficial. The paper lacks rigorous proofs for claims about regularization and optimization dynamics. Empirical results focus on Llama3 but omit comparisons with other architectures (e.g., Mistral, Falcon). The ablation studies do not explore the impact of hyperparameters like the learning rate schedule or normalization coefficient."
          },
          "questions": {
            "value": "1. What is the theoretical justification for using the inverse of the l2 norm as the normalization coefficient? 2. How does ALLoRA handle overfitting without Dropout? 3. Are the improvements consistent across different model sizes (e.g., 7B vs. 70B parameters)? 4. What ablation studies were conducted on the adaptive learning rate mechanism?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper identifies three limitations of LoRA for fine-tuning large language models (LLMs): Dropout's ineffectiveness in short training, zero initialization causing poor optimization conditioning, and scaling factors creating suboptimal interactions between layers. It proposes ALLoRA, an adaptive learning rate method that removes Dropout and scaling while using ℓ2-norm-based gradient scaling to address these issues. Empirical results show improved performance over LoRA and variants like DoRA."
          },
          "strengths": {
            "value": "Originality: The paper provides a principled analysis of LoRA's limitations and introduces a novel adaptive learning rate strategy. Quality: The method is theoretically grounded and simplifies LoRA by removing two hyperparameters. Clarity: The problem formulation and solution are well-structured, with clear motivation. Significance: Addressing fine-tuning inefficiencies in LLMs is highly relevant, especially given the growing use of parameter-efficient methods."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with state-of-the-art PEFT methods beyond DoRA, such as LoRA variants with orthogonal factorization or weight-tied approaches. The theoretical analysis of why ℓ2-norm-based scaling resolves the three flaws is superficial, with minimal mathematical justification. Experimental validation is limited to Llama3 and a few datasets; results on other architectures (e.g., Gemma, OpenELM) or tasks (e.g., code, multilingual) are not discussed. The ablation studies do not explore the sensitivity of ALLoRA to hyperparameters like the learning rate schedule or norm computation method."
          },
          "questions": {
            "value": [
              "How does the ℓ2-norm-based gradient scaling specifically mitigate the three identified flaws? Are there theoretical guarantees for this approach?",
              "What are the computational and memory costs of ALLoRA compared to LoRA and DoRA? Is the method scalable to larger models?",
              "Why were other recent LoRA variants (e.g., those using orthogonal factorization or weight tying) not included in the comparisons?",
              "How does ALLoRA perform on tasks with extremely limited data (e.g., few-shot settings) versus the reported experiments?",
              "What is the impact of different norm computation strategies (e.g., per-layer vs. global) on performance?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper identifies three limitations of LoRA for fine-tuning large language models (LLMs): Dropout's ineffectiveness in short training, zero initialization causing poor optimization conditioning, and scaling factors creating suboptimal interactions between layers. It proposes ALLoRA, an adaptive learning rate approach that removes Dropout and scaling while using ℓ2-norm-based gradient scaling to address these issues. Empirical results show improved performance over LoRA and variants like DoRA."
          },
          "strengths": {
            "value": "Originality is strong in addressing specific, practical limitations of a widely-used method (LoRA) and proposing a simplified, adaptive solution. The paper's clarity is good, with structured sections and clear problem formulation. Significance is high due to LoRA's prevalence in LLM fine-tuning. The quality of empirical validation is promising, with claims of consistent improvements across datasets and models."
          },
          "weaknesses": {
            "value": "The theoretical analysis of why ℓ2-norm-based adaptive learning rates resolve the identified flaws is superficial. The paper lacks detailed ablation studies on the adaptive learning rate mechanism itself. Experimental comparisons are vague (e.g., 'various settings' without specifics) and do not include comprehensive baselines or analysis of hyperparameter sensitivity. The removal of Dropout and scaling factors is presented as a simple fix without deeper exploration of trade-offs."
          },
          "questions": {
            "value": "1. What is the mathematical justification for using ℓ2-norm-based gradient scaling to address the three identified flaws? 2. Which specific datasets, models, and tasks were used to validate ALLoRA's improvements? 3. How sensitive is ALLoRA to the choice of adaptive learning rate coefficient? 4. Are there cases where ALLoRA underperforms compared to LoRA, and if so, why? 5. How does the adaptive learning rate interact with other LoRA variants (e.g., DoRA)?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "7XrVS0K8yr": {
    "paper_id": "7XrVS0K8yr",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes Secure-FLOATING, a scalable framework combining federated learning (FL), secure multi-party computation (SMPC), and blockchain to enable real-time trust verification of mobility data from connected and autonomous vehicles (CAVs), micro-mobility devices, and pedestrians. It aims to address privacy concerns and malicious data threats through lightweight SMPC, decentralized consensus, and verifiable FL."
          },
          "strengths": {
            "value": "Originality lies in integrating FL, SMPC, and blockchain for real-time trust in mobility data, addressing a critical gap in CAV safety. The methodology includes theoretical scalability analysis and practical considerations for lightweight SMPC. Clarity is strong, with logical organization and clear problem motivation. The significance is high, as real-time trust verification is vital for autonomous navigation safety."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental comparisons with existing methods, such as centralized or decentralized FL approaches. The SMPC optimizations (e.g., reduced message exchanges) are mentioned but not elaborated. The theoretical analysis assumes idealized network conditions, which may not reflect real-world dynamics. The blockchain integration's impact on latency and scalability is underexplored. The evaluation section is incomplete due to truncation."
          },
          "questions": {
            "value": "How does Secure-FLOATING handle dynamic network conditions (e.g., varying node participation)? What specific SMPC optimizations reduce message exchanges, and how are they validated? What is the exact mechanism for integrating blockchain with FL to ensure real-time validation? Are there security proofs for the proposed zero-knowledge proof-based trust verification? How do the lightweight trajectory models compare to complex ones in terms of accuracy under adversarial scenarios?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes Secure-FLOATING, a framework combining federated learning (FL), secure multi-party computation (SMPC), and blockchain to enable real-time trust verification for mobility data from connected and autonomous vehicles (CAVs), micro-mobility devices, and pedestrians. It claims to address privacy, scalability, and security challenges through lightweight SMPC, decentralized consensus, and zero-knowledge proofs."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in CAV safety and mobility data trust, which is highly relevant. The integration of FL, SMPC, and blockchain is a novel approach for real-time trust establishment. The theoretical analysis suggests linear scalability with network size, and the use of lightweight models and SMPC optimizations could reduce computational overhead. The application to a realistic NYC scenario adds practical relevance. The paper also highlights the importance of privacy-preserving methods in safety-critical systems."
          },
          "weaknesses": {
            "value": "The experimental evaluation is incomplete and lacks critical details. The paper mentions 8,000 nodes but does not provide metrics like latency, accuracy, or trade-offs between privacy and computational overhead. The SMPC protocol is described as 'lightweight' without concrete implementation details or comparisons to existing methods. The theoretical guarantees are not elaborated, and the role of blockchain in the framework is not clearly justified. The paper is cut off mid-section, leaving key components (e.g., related work, evaluation details) unexplored."
          },
          "questions": {
            "value": "What specific metrics were used to evaluate the 75% successful endorsement rate? How does the SMPC protocol reduce message exchanges without sacrificing security? What are the exact computational and communication overheads compared to baseline methods? How does the blockchain integration affect real-time performance? Are there security proofs for the zero-knowledge proofs or SMPC components? How does the framework handle heterogeneous data distributions and Sybil attacks?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes Secure-FLOATING, a framework combining federated learning (FL), secure multi-party computation (SMPC), and blockchain to enable real-time trust validation of mobility data in CAVs, micro-mobility devices, and pedestrians. It claims to achieve privacy-preserving, scalable, and tamper-proof data validation through lightweight SMPC, zero-knowledge proofs, and blockchain-based consensus."
          },
          "strengths": {
            "value": "Originality: Introduces a novel integration of FL, SMPC, and blockchain for real-time trust in mobility data. Quality: Theoretical analysis suggests linear scalability and privacy guarantees. Clarity: The problem statement and framework overview are well-structured. Significance: Addresses critical safety and privacy challenges in CAV ecosystems, which are highly relevant to real-world deployment."
          },
          "weaknesses": {
            "value": "Lacks detailed experimental validation: The evaluation on 8,000 nodes is described but lacks concrete metrics (e.g., latency, accuracy, attacker resilience). No comparison with existing methods: No baseline or prior work is evaluated against the proposed framework. SMPC protocol specifics are vague: The 'addition-based function' and 'reduced message exchanges' are not elaborated. Theoretical assumptions are unverified: Claims about linear scalability and privacy guarantees lack rigorous proof or empirical support. Blockchain integration is under-specified: How IPFS or blockchain stores trust models is not explained."
          },
          "questions": {
            "value": [
              "What is the exact SMPC protocol used, and how does it ensure security against 50% attacker penetration? How are malicious nodes detected and mitigated?",
              "What are the specific metrics (e.g., communication overhead, latency) used to demonstrate 'lower delays and overhead'? How do these compare to existing FL or SMPC-based systems?",
              "How does the framework handle heterogeneous data distributions and Sybil attacks, which are critical challenges in CAVs?",
              "What is the trade-off between the lightweight models used and the accuracy of trust validation? Are there ablation studies to confirm this?",
              "How is the zero-knowledge proof integrated into the FL process? What is the computational cost of this mechanism?",
              "What are the limitations of the 'addition-based function' in SMPC, and how does the framework handle complex functions when required?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "7ZToWPWUlO": {
    "paper_id": "7ZToWPWUlO",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces Wedge and Ring Transformers (WRT), an RL-based method for minimizing Normalized Cut (NC) in weighted planar graphs with shape constraints (rings/wedges). It combines graph transformations into polar coordinates, multi-head Transformers, and PPO to optimize non-differential NC objectives. Theoretical contributions include new Cheeger inequalities, and experiments claim superior performance over baselines."
          },
          "strengths": {
            "value": "Originality lies in applying Transformers and PPO to shape-constrained graph partitioning, a novel combination. Theoretical contributions (Cheeger inequalities) and practical motivation (road traffic simulation) add value. The method's ability to explicitly measure 'ringness' and 'wedge-ness' metrics demonstrates a clear innovation. Experiments on synthetic and real-world graphs suggest effectiveness, with claims of scalability."
          },
          "weaknesses": {
            "value": "The paper is cut off, leaving critical details (e.g., full experimental results, ablation studies, theoretical proofs) incomplete. The claim of being 'the first' to constrain shape in NC lacks citation of prior work. Theoretical analysis of Cheeger inequalities is unverified. The PPO setup for non-differential objectives is not thoroughly justified. The polar coordinate transformation's generalizability to non-urban graphs is unclear. Baseline comparisons appear superficial, with no analysis of failure cases."
          },
          "questions": {
            "value": "1. What specific Cheeger inequalities were derived, and how do they differ from prior work? 2. How does the polar coordinate transformation handle non-planar or irregular graphs beyond road networks? 3. Are there ablation studies showing the contribution of each component (transformers, PPO, shape constraints)? 4. How does WRT scale to very large graphs (e.g., million-node graphs)? 5. What is the computational cost compared to METIS or NeuroCUT? 6. How are the 'ringness' and 'wedge-ness' metrics formally defined in Section 3?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 2
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Wedge and Ring Transformers (WRT), a novel reinforcement learning (RL) approach for minimizing Normalized Cut (NC) in weighted planar graphs while enforcing shape constraints (rings/wedges). The method leverages graph transformations into polar coordinates, multi-head Transformers, and PPO optimization to explicitly control partition shapes, with theoretical contributions including new Cheeger inequalities."
          },
          "strengths": {
            "value": "Originality: The paper presents a unique approach to shape-constrained graph partitioning, which is not addressed by existing methods. Quality: The combination of polar coordinate transformations, Transformers, and PPO for non-differential optimization shows methodological creativity. Clarity: The paper is well-structured with clear sectioning, and the problem motivation (e.g., road traffic simulation) is relevant. Significance: The work addresses a practical gap in graph partitioning for applications requiring geometric constraints, with potential broader implications for shape-controlled partitioning."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about the two-stage training process, implementation of polar transformations, and full experimental results incomplete. The theoretical Cheeger inequalities lack formal proofs or detailed explanations. The comparison with baselines (e.g., NeuroCUT, METIS) is mentioned but not substantiated with quantitative results. The scalability of the method to large graphs is not thoroughly discussed, and the metrics for 'Ringness' and 'Wedgeness' are not rigorously defined."
          },
          "questions": {
            "value": [
              "How are the polar coordinate transformations implemented for arbitrary planar graphs? Are there cases where this transformation fails?",
              "What is the exact definition of 'Ringness' and 'Wedgeness' metrics, and how are they mathematically connected to the NC objective?",
              "Can the two-stage training process be described in more detail, and how does it improve stability compared to single-stage training?",
              "Are the claimed Cheeger inequalities novel, and do they hold for non-planar graphs or general weighted graphs?",
              "How does WRT compare to GNN-based methods in terms of computational efficiency and scalability?",
              "What ablation studies were conducted to isolate the impact of shape constraints versus other components of the model?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Wedge and Ring Transformers (WRT), a novel reinforcement learning (RL) approach for graph partitioning that explicitly constrains partitions to ring- or wedge-shaped structures. The method transforms graphs into polar coordinates, uses multi-head transformers to model global dependencies, and applies PPO to optimize the non-differential normalized cut (NC) objective. Theoretical contributions include new Cheeger inequalities linking spectral properties to partition shapes, with experiments showing improved performance over baselines on synthetic and real-world graphs."
          },
          "strengths": {
            "value": "Originality lies in combining graph transformations (polar coordinates), transformers, and RL to address shape-constrained NC, a gap in existing methods. Theoretical novelty includes Cheeger inequalities connecting spectral properties to partition shapes. Experiments demonstrate competitive performance on weighted planar graphs, with clear metrics (ringness/wedgeness) to evaluate shape fidelity. The work addresses a specific real-world application (traffic simulation) with practical relevance."
          },
          "weaknesses": {
            "value": "Theoretical analysis of Cheeger inequalities is superficial, lacking detailed proofs or connections to the proposed method. Experiments focus on synthetic data and limited real-world graphs, with insufficient comparison to state-of-the-art methods like NeuroCUT or KaHIP. The two-stage training process is not elaborated, and scalability to very large graphs remains unaddressed. The paper assumes the road network application is representative, but generalizability to other domains is unclear."
          },
          "questions": {
            "value": "1. How do the proposed Cheeger inequalities differ from classical ones, and what guarantees do they provide for shape-constrained partitions? 2. What is the computational complexity of WRT compared to METIS or PPO-based baselines? 3. How are polar coordinates determined for arbitrary graphs, and what happens if the predefined center $ o $ is not optimal? 4. Are ringness/wedgeness metrics validated against human-defined ground truth in real-world scenarios?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "7ZUUNMjM9T": {
    "paper_id": "7ZUUNMjM9T",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes a method to directly minimize the high-order objective in flow matching for maximum likelihood estimation (MLE). The key idea is to use the gradient of the conditional score function instead of the marginal one in the second-order objective, and replace the matrix norm with the trace for efficiency. The authors claim that this ensures the actual objective is minimized, leading to improved likelihood compared to prior works that only minimize upper bounds."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel approach to directly optimize high-order objectives in flow matching, addressing a critical limitation of prior methods that only minimized upper bounds. This represents a significant conceptual advancement in MLE for generative models. Quality: The method is theoretically grounded, with clear connections to existing work on Fisher divergence and KL bounds. The experiments on 2D and high-dimensional datasets demonstrate practical improvements in likelihood. Clarity: The paper is well-structured, with a logical flow from problem formulation to method and experiments. The table summarizing differences with prior work is particularly helpful. Significance: Improving MLE in flow matching has broad implications for generative modeling, especially in high-dimensional domains like images."
          },
          "weaknesses": {
            "value": "The paper lacks detailed theoretical analysis of why the trace-based objective is a valid surrogate for the matrix norm. The experimental evaluation is limited to standard benchmarks (MNIST, CIFAR-10, ImageNet32×32) without ablation studies on hyperparameters or sensitivity analysis. The claim of 'simulation-free' computation is not clearly explained or justified. The paper does not address potential computational costs of the proposed method compared to existing approaches. The comparison with prior work focuses only on likelihood metrics, omitting other important aspects like sampling efficiency or qualitative sample quality."
          },
          "questions": {
            "value": [
              "What is the theoretical justification for replacing the matrix norm with the trace in the second-order objective? How does this affect the optimization landscape?",
              "How does the computational complexity of the proposed method compare to existing flow matching approaches? Are there any practical limitations to its scalability?",
              "The paper mentions 'simulation-free' computation but does not elaborate. Can the authors clarify how this is achieved and whether it applies to all training steps?",
              "Why was the ablation study limited to comparing the second-order objective minimization vs. upper bound minimization? What about other factors like network architecture or training duration?",
              "The experiments show improvements in bits/dim but do not compare to recent state-of-the-art flow matching methods. How does the proposed approach perform relative to these baselines?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes a novel method to directly minimize high-order objectives in flow matching for maximum likelihood estimation (MLE). The key idea is to use the trace of the second-order objective instead of the matrix norm, enabling simulation-free computation by leveraging conditional gradients. The authors demonstrate improved likelihood on synthetic and image datasets compared to prior methods that only minimize upper bounds of high-order objectives."
          },
          "strengths": {
            "value": "The paper introduces a creative technical modification (trace-based second-order objective) that addresses a critical limitation in flow matching for MLE. The approach is theoretically grounded in improving KL divergence bounds by directly minimizing high-order terms, which is a significant step beyond prior work that only optimized upper bounds. The experiments show measurable improvements on standard benchmarks, and the paper provides a clear comparison table highlighting the novelty. The method's efficiency through trace computation is a practical advantage."
          },
          "weaknesses": {
            "value": "The theoretical analysis lacks depth in justifying why the trace substitution preserves the objective's validity. The experiments are limited to 2D synthetic data and standard image datasets without comparisons to recent flow matching variants. The ablation study is not detailed enough to confirm that the second-order objective is the key driver of improvement. The paper also does not address potential computational costs of the trace-based objective or its scalability to larger models."
          },
          "questions": {
            "value": [
              "How does the trace-based second-order objective relate to the original matrix norm formulation? What guarantees exist that this substitution does not compromise the theoretical bounds on KL divergence?",
              "What specific hyperparameters and training protocols were used for the experiments? How do they compare to prior work in terms of computational resources?",
              "Why does the method achieve better likelihood on ImageNet32x32 compared to other flow matching approaches? Are there domain-specific factors that favor this method?",
              "How does the proposed method handle the trade-off between computational efficiency and the accuracy of the second-order objective calculation?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes a method to directly minimize high-order objectives in flow matching, addressing limitations in maximum likelihood estimation (MLE) by avoiding upper bound approximations used in prior work. The approach uses the trace of the matrix norm for efficiency and claims improved likelihood on synthetic and image datasets."
          },
          "strengths": {
            "value": "The paper identifies a critical limitation in existing flow matching methods and proposes a novel approach to directly minimize high-order objectives, which could improve MLE. The experimental results on 2D and image datasets suggest practical improvements, and the table comparing methods provides clarity. The connection to diffusion models and theoretical analysis of Fisher divergence and KL bounds demonstrates thorough understanding of the problem space."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with key methodological details (e.g., exact formulation of the second-order objective, derivation of the trace-based approach) missing. The experiments lack sufficient detail (e.g., baseline comparisons, hyperparameter choices, ablation studies on the trace vs. matrix norm). Theoretical justification for why the trace-based objective is valid and effective is underdeveloped. The claims about 'direct minimization' are not rigorously supported without full method exposition."
          },
          "questions": {
            "value": [
              "What is the exact mathematical formulation of the second-order objective being minimized? How does it differ from prior work's upper bound formulations?",
              "How does the trace-based objective relate to the full matrix norm? What guarantees exist that the trace captures the necessary information for MLE?",
              "What ablation studies were conducted to validate the choice of trace over matrix norm? How does computational efficiency compare to prior methods?",
              "How were the baselines selected? What specific metrics (e.g., bits/dim, FID) were used to evaluate likelihood improvements on image datasets?",
              "What is the theoretical basis for the claim that minimizing the second-order objective directly reduces Fisher divergence and KL bounds?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 2
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "7bAjVh3CG3": {
    "paper_id": "7bAjVh3CG3",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces GRAIN, the first gradient inversion attack targeting graph-structured data in federated learning (FL) settings. The method reconstructs both graph structures and node features from gradients, leveraging the low-rank properties of GNN gradients and a DFS-based algorithm. It evaluates the attack on molecular, citation, and social network datasets, claiming up to 80% exact reconstruction accuracy."
          },
          "strengths": {
            "value": "The paper addresses a novel and important problem: privacy risks in FL for GNNs, which has been underexplored. The methodology introduces a theoretically grounded approach using low-rank gradient analysis and DFS for graph reconstruction, which is creative and specific to graph data. The experiments are on real-world datasets, and the paper provides a new metric for evaluating graph reconstruction. The work highlights critical privacy vulnerabilities in FL with GNNs, which is significant for both security and practical deployment."
          },
          "weaknesses": {
            "value": "The paper lacks detailed baselines for comparison (e.g., what specific methods are used as baselines, and why is GRAIN's 80% accuracy considered significant?). The assumptions (e.g., discrete node features, small graph size relative to embedding dimensions) limit the practical applicability. The theoretical analysis is sparse, and the paper does not thoroughly address how GRAIN scales to larger graphs beyond the tested cases. The evaluation metrics are not fully explained, and the paper does not discuss computational efficiency or robustness to noise. Additionally, the connection to prior work (e.g., DAGER) is not sufficiently clarified, and the extension to GNNs is not rigorously justified."
          },
          "questions": {
            "value": "1. What specific baseline methods are used for comparison, and how do they differ from GRAIN? 2. How are the novel metrics for evaluating graph reconstruction defined, and what do they measure? 3. How does GRAIN handle graphs with continuous node features, which are common in real-world scenarios? 4. What are the computational costs of GRAIN, and how does it scale to graphs with more than 25 nodes? 5. How does the DFS-based algorithm ensure correctness when combining local subgraphs into a global graph? 6. Are the assumptions (e.g., discrete features, small graph size) realistic for typical GNN applications?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces GRAIN, the first exact gradient inversion attack targeting graph-structured data in Federated Learning (FL) settings. GRAIN recovers both graph structures and node features for GNNs (GCN/GAT) by exploiting the low-rank properties of GNN gradients, using subgraph filtering and DFS-based reconstruction. The authors evaluate it on molecular, citation, and social network datasets, claiming up to 80% exact graph reconstruction."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in FL privacy by focusing on GNNs, which have not been studied in gradient inversion attacks. The method's use of low-rank structure and span checks for subgraph identification is novel. The introduction of new metrics for evaluating graph reconstruction quality is valuable. The experimental results on real-world datasets demonstrate significant performance gains over baselines, highlighting the privacy risks of FL with GNNs."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how the low-rank structure of GNN gradients is leveraged for subgraph recovery. The assumptions (e.g., node count < embedding dimension, discrete features) may limit practical applicability. The evaluation metrics are novel but not compared to existing benchmarks. The paper does not discuss countermeasures or practical defenses against GRAIN. The exact mechanisms for DFS-based graph assembly and handling of larger graphs remain under-specified."
          },
          "questions": {
            "value": "1. How does GRAIN handle cases where the number of nodes exceeds the embedding dimension? 2. What are the limitations of the span check method for subgraph identification? 3. How do the proposed metrics differ from existing graph reconstruction metrics, and why are they more effective? 4. Are there scenarios where GRAIN fails, and what causes these failures? 5. How does the discrete feature assumption affect the generality of the approach?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces GRAIN, the first exact gradient inversion attack for graph-structured data in federated learning settings. GRAIN recovers both graph structures and node features for GCN and GAT models by leveraging the low-rank properties of GNN gradients and a DFS-based reconstruction algorithm. The method is evaluated on molecular, citation, and social network datasets, achieving up to 80% exact graph reconstruction accuracy."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in federated learning privacy by proposing the first gradient inversion attack for graph-structured data, which is both novel and timely. The method's theoretical foundation extends prior work on low-rank gradient analysis to GNNs, and the use of span checks and DFS-based reconstruction is creative. The evaluation includes real-world datasets and introduces new metrics for assessing graph reconstruction quality. The work is well-motivated and demonstrates significant practical implications for GNN privacy."
          },
          "weaknesses": {
            "value": "The paper makes strong assumptions about discrete node features and small graph sizes, which may limit its applicability to real-world scenarios. The experimental validation is limited to specific datasets (molecular, citation, social networks) without broader generalization analysis. The comparison to baselines is superficial, and the exactness metric's definition is not clearly explained. The lack of discussion on potential defenses or mitigation strategies weakens the paper's practical impact."
          },
          "questions": {
            "value": [
              "How do the assumptions of discrete node features and small graph sizes affect the practicality of GRAIN in real-world federated learning systems?",
              "What are the limitations of the proposed exactness metric, and how does it compare to existing graph reconstruction evaluation criteria?",
              "How does GRAIN handle dynamic graphs or graphs with continuous node features, which are common in many applications?",
              "What is the computational complexity of GRAIN, and how does it scale to larger graphs beyond the 25-node benchmark?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 5
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "7d2JwGbxhA": {
    "paper_id": "7d2JwGbxhA",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces OCEBO, a method for object-centric pretraining that bootstraps the target encoder using an exponential moving average (EMA) of the object-centric model's encoder. The approach addresses slot collapse through cross-view patch filtering and demonstrates comparable performance to models with frozen non-object-centric target encoders, even when trained from scratch on real-world data."
          },
          "strengths": {
            "value": "The paper presents a novel self-distillation framework for object-centric learning, addressing the critical limitation of frozen target encoders in prior work. The use of EMA to incorporate object-centric inductive biases into the target encoder is a creative solution to the performance upper bound problem. The cross-view patch filtering technique is a promising innovation for mitigating slot collapse. The experimental results on COCO show strong scalability and performance, demonstrating the feasibility of large-scale object-centric pretraining from scratch. The paper also provides a clear theoretical motivation for the proposed method."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the effectiveness of the cross-view patch filtering mechanism. The implementation details of the cross-view correspondences used for filtering are not sufficiently explained. The comparison with existing methods could be more comprehensive, particularly in terms of computational efficiency and training dynamics. The theoretical justification for why EMA with object-centric biases outperforms previous approaches is not thoroughly developed. Additionally, the paper does not analyze how the proposed method scales beyond the tested 241k COCO images."
          },
          "questions": {
            "value": [
              "How are cross-view correspondences determined for the patch filtering approach? What specific augmentations are used to create cross-view pairs?",
              "What is the exact mechanism by which EMA updates enrich the target encoder with object-centric inductive biases? Are there quantitative measures to validate this?",
              "How does the cross-view patch filtering affect the training dynamics of the slot attention module? Are there ablation studies showing its impact on slot collapse mitigation?",
              "What are the computational costs of the proposed method compared to existing object-centric models? How does the training time scale with larger datasets?",
              "Are there qualitative results showing the quality of object discovery with OCEBO compared to baselines? How does the method handle ambiguous or overlapping objects?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces OCEBO, a self-distillation method for object-centric pretraining that updates the target encoder as an exponential moving average (EMA) of the object-centric model. The approach addresses slot collapse through cross-view patch filtering and demonstrates competitive performance on real-world data without relying on frozen non-object-centric encoders."
          },
          "strengths": {
            "value": "The paper presents a novel self-distillation framework for object-centric learning, addressing key limitations of prior work. The cross-view patch filtering mechanism is a creative solution to slot collapse. The experiments show that OCEBO achieves performance comparable to models using massive-scale frozen encoders, demonstrating the effectiveness of their approach. The method's ability to train object-centric models from scratch is a significant contribution to the field."
          },
          "weaknesses": {
            "value": "The paper lacks thorough comparisons with state-of-the-art object-centric models (e.g., Slot-Attention, ODE-Net) and does not provide ablation studies on key components like cross-view patch filtering. The theoretical analysis of why EMA updates work despite prior failures is insufficient. The experiments are limited to COCO, with no evaluation on other datasets. The paper also doesn't address how OCEBO scales to larger datasets beyond 241k images."
          },
          "questions": {
            "value": [
              "How does OCEBO compare to recent object-centric methods like ODE-Net or Slot-Attention in terms of quantitative metrics?",
              "What specific design choices in the cross-view patch filtering make it effective compared to other regularization techniques?",
              "Are the results reproducible? The paper mentions using COCO but doesn't specify which split or evaluation protocol.",
              "How does the method handle video or multi-modal data, which are natural extensions of object-centric learning?",
              "What are the computational costs of the cross-view patch filtering compared to standard training?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes OCEBO, a method for object-centric pretraining by bootstrapping the target encoder using an exponential moving average (EMA) of the object-centric model's encoder. This approach aims to overcome the performance limitations of frozen target encoders by introducing object-centric inductive biases and mitigating slot collapse through cross-view patch filtering."
          },
          "strengths": {
            "value": "The paper introduces a novel self-distillation framework for object-centric pretraining, addressing a critical limitation of prior methods (frozen target encoders). The EMA-based target encoder update mechanism is original and well-motivated, with experiments showing comparable performance to models using much larger pretraining datasets. The cross-view patch filtering approach is a creative solution to slot collapse, and the work represents a significant step toward large-scale pretraining of object-centric models. The paper also provides a clear theoretical analysis of why previous EMA approaches failed."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the effectiveness of individual components (e.g., cross-view patch filtering vs. EMA updates). The experimental comparisons are limited to a single dataset (COCO) and do not include baseline comparisons with other object-centric models or self-distillation methods. The analysis of why the EMA update introduces object-centric biases is superficial, and the paper does not address potential issues with the stability of the EMA process. The claimed scalability is not thoroughly validated beyond the reported numbers."
          },
          "questions": {
            "value": "1. How is the cross-view patch filtering implemented? What specific criteria are used to determine 'sufficiently informative patches'? 2. Are there quantitative metrics demonstrating the effectiveness of the cross-view patch filtering in preventing slot collapse? 3. How does OCEBO compare to other self-distillation methods (e.g., MoCo, BYOL) in terms of performance and stability? 4. What is the computational cost of OCEBO compared to existing object-centric models? 5. Are the results reproducible with different random seeds or hyperparameters?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "7dsC1w4yzP": {
    "paper_id": "7dsC1w4yzP",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces O_SMI-SSM-336M, a Mamba-based foundation model for chemistry that leverages structured state space sequence models (SSMs) to address the limitations of Transformer architectures. The model is pre-trained on a large dataset of 91 million SMILES molecules from PubChem, achieving state-of-the-art results in molecular property prediction, reaction yield prediction, and molecule reconstruction while demonstrating faster inference speeds compared to Transformers."
          },
          "strengths": {
            "value": "The paper presents a novel application of Mamba SSMs to chemical foundation models, addressing key limitations of Transformers (e.g., quadratic scaling). The large-scale dataset curation (91M molecules, 4B tokens) is a significant contribution. The work demonstrates practical benefits of SSMs through speed advantages and diverse task evaluations. The architecture details and theoretical foundation of Mamba are well-explained, and the paper highlights potential for future research in molecular modeling."
          },
          "weaknesses": {
            "value": "The paper is truncated, missing critical experimental details and results. Key claims about SOTA performance lack quantitative comparisons to baseline models. The dataset curation process requires more specifics (e.g., validation criteria, handling of rare molecules). The theoretical justification for SSMs' superiority in chemical tasks is limited. The paper does not address potential limitations of SSMs (e.g., handling long-range dependencies in complex molecules) or compare against other non-Transformer architectures."
          },
          "questions": {
            "value": "1. What specific metrics were used to validate SOTA performance across tasks? How do results compare to state-of-the-art models like MoLFormer or Graph Neural Networks? 2. Can the authors clarify the molecular validation process after canonicalization? 3. Are there ablation studies demonstrating the necessity of Mamba's architecture over simpler alternatives? 4. How does the model handle rare/complex molecular patterns not present in the training data? 5. What is the exact implementation of the SSM encoder-decoder, and how does it differ from standard Mamba configurations?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces a Mamba-based foundation model (O_SMI-SSM-336M) for chemistry, leveraging structured state space models (SSMs) to address limitations of Transformers in molecular modeling. The model is pre-trained on a large SMILES dataset and evaluated on tasks like property prediction and reaction yield estimation, claiming superior speed and performance."
          },
          "strengths": {
            "value": "The paper presents a novel application of Mamba, an SSM architecture, to chemical foundation models, which is technically innovative. The scale of the pre-training dataset (91M molecules, 4B tokens) is impressive, and the experiments cover diverse tasks (property prediction, reconstruction, reaction yield). The work addresses a critical gap in chemical AI by exploring non-Transformer architectures, which is significant for the field."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with critical sections (e.g., model architecture details, experimental results, ablation studies) cut off. Key claims, such as 'twice the speed of Transformers,' lack baseline comparisons against other SSMs or optimized Transformer variants. The dataset curation process is under-described, and the paper omits important details about hyperparameter tuning, validation protocols, and error analysis. The absence of qualitative analysis (e.g., visualization of learned representations) limits the understanding of the model's strengths."
          },
          "questions": {
            "value": "1. What specific Transformer architecture and implementation details were used for speed comparisons? 2. How was the SMILES dataset curated beyond deduplication and canonicalization? 3. Are there any limitations of Mamba in capturing long-range molecular dependencies compared to Transformers? 4. What ablation studies were conducted to validate the necessity of SSM components? 5. How does the model handle rare or complex molecular motifs not present in the training data?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a Mamba-based foundation model (O_SMI-SSM-336M) for chemistry, leveraging structured state space models (SSMs) to address the limitations of Transformer architectures. The model is pre-trained on 91 million SMILES from PubChem and evaluated on tasks like molecular property prediction, reaction yield prediction, and molecule reconstruction, claiming superior speed and performance compared to Transformers."
          },
          "strengths": {
            "value": "The paper presents a novel application of SSMs (specifically Mamba) to chemical modeling, which could address Transformer scalability issues. The dataset curation (91M molecules, 4B tokens) is substantial, and the model's architecture is well-defined. The experiments span multiple benchmark tasks, and the emphasis on inference speed is relevant for practical applications. The work contributes to the growing literature on efficient sequence models in cheminformatics."
          },
          "weaknesses": {
            "value": "The paper is truncated, leaving critical details (e.g., full experimental results, comparison baselines, and ablation studies) missing. Claims of 'state-of-the-art' performance lack specific metrics or comparisons to prior SSM-based models. The speed improvement (twice as fast as Transformers) is not contextualized (e.g., hardware, implementation details). The dataset curation process and validation of molecular quality are under-specified. The paper also fails to discuss potential limitations of SSMs in capturing long-range dependencies compared to Transformers."
          },
          "questions": {
            "value": "1. How was the speed comparison with Transformers conducted (e.g., hardware, batch sizes, implementation details)? 2. What are the exact metrics (e.g., RMSE, accuracy) demonstrating SOTA performance on benchmark tasks? 3. How do the authors validate the quality of the 91M curated molecules (e.g., filtering for validity, diversity)? 4. Are there ablation studies showing the contribution of Mamba's architecture vs. other components? 5. How does the model perform on tasks requiring long-range molecular interactions (e.g., protein-ligand binding)?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "7jDv1RrNQX": {
    "paper_id": "7jDv1RrNQX",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of improving the generative capabilities of BERT-family models by introducing 'path selection' and 'path selection*' methods. These techniques aim to bridge the training-inference mismatch in sequence decomposition by expanding the search space during inference and incorporating path preferences into training. The authors propose a variant called GeBERT and demonstrate significant performance improvements on generation tasks without fine-tuning."
          },
          "strengths": {
            "value": "The paper presents a novel approach to address a critical limitation of BERT-family models in generation tasks, which is a well-defined and important problem. The methodological innovation of path selection and path selection* is creative, particularly in leveraging the inherent flexibility of BERT's bidirectional architecture. The experimental results show competitive performance against autoregressive models, and the paper includes thorough comparisons across multiple tasks. The writing is clear, and the technical details are sufficiently explained."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of path selection vs. path selection*. The comparison with existing non-autoregressive models (e.g., BART, T5) is insufficient, making it difficult to assess the novelty of the proposed methods. The experimental setup for zero-shot tasks is not fully described, and the claims about 'reaching par with AR models' require stronger statistical evidence. Additionally, the theoretical justification for the 2^TN path complexity is underdeveloped."
          },
          "questions": {
            "value": [
              "How are the 'path selection' and 'path selection*' methods implemented in practice? What specific criteria are used to evaluate and select optimal paths?",
              "The paper mentions GeBERT is pre-trained with a 'more widely-used training corpus'—what specific corpus was used, and how does it differ from standard BERT pre-training data?",
              "What are the computational costs of the proposed methods compared to standard BERT inference? Are there any trade-offs in terms of inference speed or memory usage?",
              "The claims about zero-shot performance improvements lack detailed analysis—how do the results compare to task-specific fine-tuned baselines?",
              "The paper references Appendix A.2 for the 2^TN path complexity calculation—can the authors provide a concise explanation of this derivation in the main text?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the limitations of BERT-family models in generation tasks by introducing 'path selection' and 'path selection*', which aim to bridge the training-inference mismatch caused by BERT's random sequence decomposition. The authors propose a new variant called GeBERT and demonstrate significant performance improvements on zero-shot and task-specific generation tasks without requiring additional fine-tuning."
          },
          "strengths": {
            "value": "The paper presents a novel approach to improve BERT-family generation by explicitly addressing the sequence decomposition mismatch between training and inference. The method is theoretically grounded in the analysis of decoding paths, and the experiments show competitive results compared to autoregressive models. The clarity of the problem formulation and the structured presentation of the approach are commendable. The significance of the work lies in demonstrating that BERT-family models can achieve strong generation performance without extensive task-specific fine-tuning."
          },
          "weaknesses": {
            "value": "The experimental validation is limited in scope, focusing on a narrow set of tasks (common sense reasoning, reading comprehension, and two traditional generation tasks). The paper lacks ablation studies to isolate the contribution of 'path selection*' versus 'path selection'. Additionally, the comparison with state-of-the-art autoregressive models is not thoroughly detailed, and the computational efficiency of the proposed methods is not discussed. The theoretical analysis of why path selection improves performance is superficial."
          },
          "questions": {
            "value": [
              "How does 'path selection*' differ from standard training procedures, and what specific modifications are made to the BERT architecture to enable this?",
              "What are the baseline models used for comparison in the zero-shot tasks, and how do they perform relative to GeBERT without path selection?",
              "Are the results robust across different types of generation tasks (e.g., dialogue, summarization), or are they limited to the specific tasks evaluated?",
              "How does the computational cost of path selection compare to standard decoding methods, and what are the practical implications for real-world applications?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper addresses the limitations of BERT-family models in generation tasks by introducing 'path selection' and 'path selection*' methods. The authors propose expanding the decoding search space during inference and integrating path preference learning into training to bridge the training-inference mismatch inherent in BERT's random masking approach. They evaluate their methods on zero-shot reasoning and generation tasks, demonstrating competitive performance with autoregressive models."
          },
          "strengths": {
            "value": "The paper presents a novel perspective on BERT's generation limitations by framing them as a sequence decomposition mismatch. The proposed path selection method offers a fresh approach to decoding for non-autoregressive models, with experiments showing significant improvements over baseline BERT. The work is well-structured, with clear problem formulation and meaningful comparisons to autoregressive models. The focus on untapped generative potential of BERT without fine-tuning is both original and impactful."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of path selection vs. path selection*. The comparison with state-of-the-art non-autoregressive models (e.g., BART, T5) is insufficient, making it harder to assess relative improvements. The theoretical analysis of why path selection works remains underdeveloped, and the paper doesn't address potential computational costs of expanded search spaces. Some claims about 'state-of-the-art' performance require stronger justification through comprehensive baselines."
          },
          "questions": {
            "value": [
              "How does path selection* differ from standard training procedures? What specific modifications were made to integrate path selection into training?",
              "What are the computational trade-offs of expanding the decoding search space? How does this affect inference speed and resource requirements?",
              "Why were specific tasks chosen for evaluation? Could the method generalize to other generation tasks like dialogue or code generation?",
              "How do the authors handle the combinatorial explosion of possible paths (e.g., 2^TN possibilities)? Is there a pruning mechanism or heuristic to manage this?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "7lUdo8Vuqa": {
    "paper_id": "7lUdo8Vuqa",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates the generalization of diffusion models by analyzing how the noisy objective in denoising score matching (DSM) introduces inductive biases. The authors propose a physics-inspired path integral approach (Martin-Siggia-Rose formalism) to theoretically explain how the covariance structure of the noisy proxy score leads to distributions that 'fill in gaps' in the training data, influencing generalization. Key factors like model architecture, training set structure, and forward/reverse process details are identified as critical to this phenomenon."
          },
          "strengths": {
            "value": "The paper presents a novel theoretical framework linking diffusion model generalization to the variance in the DSM objective, leveraging physics-inspired methods (MSR path integrals) that are uncommon in ML. The analysis is mathematically rigorous, with clear connections to known phenomena like inductive biases. The work addresses an important open question in diffusion models and provides a structured breakdown of factors affecting generalization. The clarity of the problem formulation and theoretical derivations is strong, and the significance of understanding generalization in diffusion models is well justified."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation to support its theoretical claims, which limits the ability to assess practical relevance. The analysis is restricted to simplified models (e.g., linear, lazy neural networks), which may not capture the behavior of complex, real-world diffusion models. The connection between the proposed theory and existing empirical observations (e.g., memorization vs. generalization) is not thoroughly discussed. Additionally, the paper is cut off mid-equation, raising concerns about completeness and reproducibility."
          },
          "questions": {
            "value": "How do the theoretical findings translate to non-linear or highly parameterized diffusion models? What experiments could validate the 'gap-filling' hypothesis in practice? How do the identified factors (e.g., model capacity, training set structure) interact in real-world scenarios? The paper mentions 'boundary regions' with high variance but does not elaborate on how these are defined or detected. Further clarification on the role of the forward process's asymmetry in shaping generalization is needed."
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates the generalization properties of diffusion models, attributing them to the 'generalization through variance' phenomenon caused by the noisy target in the denoising score matching (DSM) objective. The authors develop a physics-inspired path integral framework to analyze how the covariance structure of the noisy target shapes inductive biases, influencing the learned distributions. They identify six key factors affecting generalization, with the noisy objective being central."
          },
          "strengths": {
            "value": "The paper presents a novel theoretical framework using path integrals to explain generalization in diffusion models, which is both creative and mathematically rigorous. The analysis of how noise in the DSM objective introduces inductive biases is original and addresses a critical gap in understanding diffusion model behavior. The structured discussion of six factors (noisy objective, forward process, nonlinear dependencies, model capacity, features, and training set structure) provides comprehensive insights. The clarity of definitions and organization enhances readability, and the significance of the problem (generalization in diffusion models) is high."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical parts of the theoretical analysis and experimental validation incomplete. The theoretical contributions are limited to simplified cases (e.g., linear models, lazy neural networks), with no discussion of how results generalize to complex architectures or real-world data. The lack of empirical results or ablation studies makes it difficult to assess practical relevance. The connection to existing literature is superficial, with minimal engagement with prior theoretical work on diffusion models. The truncated content also raises concerns about the paper's completeness and validity."
          },
          "questions": {
            "value": [
              "How do the theoretical findings hold for non-linear or deep diffusion models, which are commonly used in practice?",
              "What empirical evidence supports the claim that the covariance structure of the noisy target drives inductive biases? Are there experiments validating this hypothesis?",
              "How do the six factors interact in practice, and what are the specific conditions under which generalization through variance is most effective?",
              "What are the limitations of the path integral approach in this context, and how does it compare to other theoretical frameworks for diffusion models?",
              "How does the paper address potential contradictions with existing theories that focus on asymptotic recovery of the data distribution?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a theoretical framework to explain how diffusion models generalize by leveraging the variance in the denoising score matching (DSM) objective. It uses a physics-inspired path integral approach to analyze how the noisy, expectation-based target of the DSM objective introduces inductive biases, leading to distributions that interpolate training data while filling in 'gaps'. The study identifies six factors influencing generalization, including the noisy objective, forward process, model capacity, and training set structure."
          },
          "strengths": {
            "value": "The paper's theoretical approach is innovative, combining physics-inspired path integrals with diffusion models to analyze generalization. It provides a comprehensive analysis of multiple factors influencing generalization, demonstrating strong originality through its interdisciplinary methodology. The clarity of the problem statement and theoretical exposition is high, and the significance of understanding diffusion model generalization is well justified. The paper also offers a detailed discussion of how covariance structures in the noisy target shape inductive biases."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation to support its theoretical claims, which is critical for a machine learning conference like ICLR. The experiments described are not detailed, and there is no comparison with existing generalization theories or baselines. The focus on unconditional models limits practical applicability, and the connection between the abstract theory and real-world implications is underdeveloped. Additionally, the paper does not address how its findings apply to latent-variable diffusion models or more complex architectures."
          },
          "questions": {
            "value": "1. What specific experiments were conducted to validate the theoretical analysis of 'generalization through variance'? 2. How does the paper handle the infinite variance of the proxy score at zero noise, as mentioned in the introduction? 3. Are there empirical results demonstrating the interaction between the six factors (e.g., model capacity and training set structure) in practice? 4. How do the authors reconcile the nonlinear dependence of the learned distribution on the score function with their theoretical framework? 5. Can the theory be extended to latent-variable diffusion models or other architectures beyond unconditional models?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "7liN6uHAQZ": {
    "paper_id": "7liN6uHAQZ",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Sketching for Regularized Optimization (SRO) and an iterative variant for solving regularized least squares problems with convex or nonconvex regularization. It claims to establish minimax optimal rates for sparse signal estimation via sketching and provides theoretical guarantees for the iterative algorithm's geometric error reduction."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in large-scale optimization with a novel algorithmic framework. The theoretical contributions include minimax optimal rates for both convex and nonconvex regularization, which is a significant advancement. The iterative SRO algorithm's geometric error reduction is a strong technical result. The work also clarifies distinctions from prior art, such as Yang & Li (2021), and emphasizes the unified theoretical analysis for sparse learning problems."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation to support its claims, as the content is truncated and no empirical results are visible. The theoretical analysis of minimax rates relies on assumptions (e.g., low-rank data) that may not hold in practice. The comparison with existing methods like IHS and Yang & Li (2021) is not fully elaborated, and the practical benefits of the iterative SRO algorithm (e.g., computational efficiency) are not quantified. The paper also does not address potential limitations of sketching techniques for nonconvex problems."
          },
          "questions": {
            "value": "1. What datasets and baselines were used in the experiments, and how do SRO and Iterative SRO compare to existing methods like IHS or standard optimization algorithms? 2. How sensitive are the theoretical guarantees to assumptions about data structure (e.g., low-rank vs. general matrices)? 3. What are the computational costs of Iterative SRO compared to non-iterative sketching methods? 4. How does the algorithm handle nonconvex regularizers with multiple local minima? 5. Are the minimax rates tight, and what is the dependence on problem parameters (e.g., sparsity, regularization strength)?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces Sketching for Regularized Optimization (SRO) and its iterative variant to solve large-scale regularized least squares problems with convex or nonconvex regularization. The authors establish minimax optimal rates for sparse signal estimation using sketching and propose an iterative algorithm that geometrically reduces approximation errors. The work provides a unified theoretical framework for both convex and nonconvex cases, addressing gaps in existing literature."
          },
          "strengths": {
            "value": "Originality is strong, as the paper bridges sketching techniques with minimax optimal guarantees for nonconvex regularized problems, which are less explored. Theoretical contributions include novel analysis of approximation error bounds and a unified framework for convex/nonconvex cases. Clarity is maintained through structured problem formulation and comparisons to prior work. Significance lies in addressing large-scale optimization challenges with efficient randomized algorithms, which are critical for modern machine learning."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation to support the theoretical claims, which is a critical gap for a conference like ICLR. The minimax rate analysis assumes specific structural properties of the data matrix (e.g., low-rank), but the generality of these assumptions is not thoroughly discussed. The iterative SRO algorithm's practical efficiency and scalability compared to existing methods (e.g., IHS) are not empirically evaluated. Additionally, the connection between the sketching process and the nonconvex regularizer's properties requires deeper exploration."
          },
          "questions": {
            "value": [
              "How does the iterative SRO algorithm handle nonconvex regularizers, and what guarantees exist for convergence in such cases?",
              "What are the specific assumptions on the data matrix X (e.g., incoherence, coherence, or restricted isometry properties) required for the minimax optimality results?",
              "Are there empirical comparisons between SRO and state-of-the-art methods like IHS or other sketching-based algorithms in terms of runtime and accuracy?",
              "How sensitive are the theoretical guarantees to the choice of sketching matrix (e.g., Gaussian vs. subsampled Hadamard)?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces SRO, a sketching algorithm for regularized least squares problems with convex or nonconvex regularization, and proposes an iterative variant (Iterative SRO) to achieve geometric error reduction. It claims to establish minimax optimal rates for sparse signal estimation via sketching, providing a unified theoretical framework for both convex and nonconvex cases."
          },
          "strengths": {
            "value": "Originality is evident in the unified theoretical framework for convex and nonconvex sparse learning with sketching, which addresses a gap in existing literature. The paper's theoretical contributions, such as minimax optimal rates and the iterative algorithm's geometric error decay, are significant. Clarity is strong in the introduction and problem formulation, though the truncated content limits full assessment. The significance lies in advancing large-scale optimization methods for high-dimensional data."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with critical sections (e.g., experiments, detailed proofs, and comparisons) missing. The claim of 'first unified theoretical result' lacks sufficient context or references to prior work. The analysis of nonconvex cases and the handling of non-low-rank data matrices are not fully elaborated. The experimental validation, though mentioned, is not provided, making it hard to assess practical efficacy."
          },
          "questions": {
            "value": "1. What specific assumptions are made in the minimax rate analysis for nonconvex regularizers? 2. How does Iterative SRO compare in computational efficiency to existing methods like IHS? 3. Are the minimax rates tight, and how do they compare to baseline methods without sketching? 4. What are the exact conditions under which the non-low-rank analysis holds? 5. How are the theoretical results validated empirically, and what datasets/methods were used?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "7mlvOHL6qJ": {
    "paper_id": "7mlvOHL6qJ",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces LASeR, a framework that integrates large language models (LLMs) with evolutionary search for robot design automation. The key innovation is the DiRect mechanism, which enables LLMs to reflect on past search trajectories to enhance solution diversity while maintaining functional integrity. The approach also leverages task-specific background information to improve cross-task generalization, demonstrated on voxel-based soft robot design."
          },
          "strengths": {
            "value": "The paper addresses critical limitations in LLM-aided evolutionary optimization by proposing a novel reflection mechanism (DiRect) that explicitly balances exploration and exploitation. The method's emphasis on task-specific context for cross-task generalization is significant. Experimental results on soft robot design show clear advantages over baselines. The work bridges LLMs' reasoning capabilities with robotic design automation, a promising direction for future research."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the DiRect mechanism's components. Experimental comparisons are limited to a single robot design domain (voxel-based soft robots) without exploring other robotic systems. The theoretical justification for how DiRect improves diversity and generalization remains underdeveloped. Additionally, the paper does not address potential scalability issues for more complex robotic tasks."
          },
          "questions": {
            "value": "1. How is the DiRect mechanism implemented technically? What specific inputs does it require from past search trajectories?\n2. Are the experiments repeated across multiple robot design tasks or only the voxel-based soft robot case?\n3. How does LASeR handle the trade-off between computational efficiency and the complexity of incorporating task-specific background information?\n4. What metrics were used to quantitatively evaluate solution diversity and generalization capability?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces LASeR, a method that leverages Large Language Models (LLMs) for robot design automation by integrating a novel 'Diversity Reflection Mechanism' (Di-Rect) to enhance exploration-exploitation trade-offs and task generalizability. The approach is evaluated on voxel-based soft robot design, claiming improvements in solution diversity and zero-shot transferability."
          },
          "strengths": {
            "value": "The paper addresses a significant gap in robot design automation by combining LLMs with evolutionary search, which is a novel application domain. The proposed Di-Rect mechanism shows promise in addressing the exploration-exploitation trade-off, a critical challenge in evolutionary optimization. The focus on soft robots, which have complex design spaces, is timely and relevant. The paper also highlights the potential for inter-task reasoning in LLMs, which could inspire future work on generalizable evolutionary frameworks."
          },
          "weaknesses": {
            "value": "The paper lacks detailed descriptions of the Di-Rect mechanism, making it difficult to assess its novelty or technical soundness. The experimental validation is limited to simulated soft robots, with no comparisons to strong baselines or ablation studies to isolate the contributions of Di-Rect. The claims about 'zero-shot robot proposals' and generalizability are not sufficiently supported by empirical evidence. Additionally, the paper does not discuss potential limitations of LLMs in this context, such as bias or computational costs."
          },
          "questions": {
            "value": "1. What is the exact implementation of Di-Rect, and how does it differ from existing exploration strategies in evolutionary algorithms? 2. Which baselines were used for comparison, and what metrics were employed to evaluate diversity and generalizability? 3. How was the 'inter-task reasoning' capability of LLMs quantitatively validated? 4. Are there ablation studies demonstrating the effectiveness of Di-Rect in isolation? 5. What specific challenges arise when applying LLMs to physical robot design, and how does LASeR address them?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 2
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces LASeR, a framework that leverages Large Language Models (LLMs) for robot design automation by addressing the exploration-exploitation tradeoff and enhancing task generalizability. The key contribution is the DiRect mechanism, which guides LLMs to reflect on past designs to improve solution diversity while preserving functional structures. The method also integrates task-specific background information to enable inter-task reasoning and zero-shot robot proposals."
          },
          "strengths": {
            "value": "The paper presents a novel approach to robot design automation by integrating LLMs with evolutionary search, addressing critical limitations in exploration-exploitation balance and generalizability. The DiRect mechanism is creatively designed to harness LLMs' reasoning capabilities for diverse and functional solutions. The experimental setup on voxel-based soft robots demonstrates practical relevance, and the paper highlights the potential for zero-shot transfer across tasks, which is significant for real-world applications. The work also provides a clear motivation for bridging LLMs and evolutionary computation."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient experimental details and comparisons. The simulated experiments on voxel-based soft robots are not thoroughly described, with no clear baselines for comparison (e.g., no direct comparison to existing LLM-aided methods like those in Zhang [2024] or Qiu et al. [2024]). The DiRect mechanism's implementation is not clearly explained, making it difficult to assess its novelty or effectiveness. Additionally, the paper does not address potential limitations of LLMs in robotic design, such as bias in training data or scalability issues. The claims about inter-task reasoning and zero-shot capabilities are not empirically validated with concrete metrics."
          },
          "questions": {
            "value": "1. How exactly does the DiRect mechanism ensure functional substructure preservation while generating diverse designs? What specific constraints or rules are applied?\n2. What are the exact baselines used for comparison in the experiments? How does LASeR outperform existing LLM-aided methods in terms of diversity and efficiency?\n3. Are there quantitative results demonstrating the zero-shot transfer capabilities of LASeR? How is 'zero-shot' defined in this context?\n4. How does the integration of task-specific background information affect the computational complexity of the framework?\n5. What are the limitations of the current approach when applied to more complex or real-world robotic systems beyond voxel-based soft robots?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "7ohlQUbTpp": {
    "paper_id": "7ohlQUbTpp",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes Collab, a controlled decoding method that uses a mixture of pre-trained, task-specific LLM agents to align outputs with target tasks during inference. It dynamically selects the optimal agent at each token using an implicit Q-function as a long-term utility metric, avoiding retraining. The approach is theoretically analyzed and empirically evaluated on diverse tasks, claiming improvements over single-agent baselines."
          },
          "strengths": {
            "value": "Originality: Introduces a novel mixture-of-agents framework for inference-time alignment, leveraging pre-trained models without retraining. The dynamic token-level selection via an implicit Q-function addresses limitations of fixed mixing strategies. Quality: Theoretical analysis of optimal policy selection and empirical validation on diverse tasks suggest rigorous methodology. Clarity: The problem statement, motivation, and high-level approach are well-articulated. Significance: Addressing the critical challenge of LLM alignment without retraining has practical implications for deployment and adaptability."
          },
          "weaknesses": {
            "value": "Theoretical foundation lacks depth: The implicit Q-function's derivation and its connection to the target reward are not clearly explained. The paper assumes the Q-function is given but does not detail how it is computed or optimized during decoding. Empirical evaluation is incomplete: The results section is cut off, leaving critical details (e.g., specific tasks, baselines, metrics) unverified. The paper does not address scalability with more agents or computational overhead of token-level selection. The role of pre-trained agents' alignment with reward functions is not thoroughly discussed, raising questions about robustness to misaligned agents."
          },
          "questions": {
            "value": [
              "How is the implicit Q-function computed during decoding? Is it derived from the agents' outputs, and if so, what specific features or metrics are used?",
              "What are the exact tasks and datasets used in the empirical evaluation? Are the results reproducible with publicly available benchmarks?",
              "How does the method handle cases where pre-trained agents are not aligned with the target reward function? Are there ablation studies to validate the Q-function's effectiveness?",
              "What is the computational cost of the token-level agent selection compared to single-agent decoding? How does this scale with the number of agents?",
              "How are the pre-trained agents selected and pre-trained? Are they from existing works, and if so, how do their reward functions align with the target tasks?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes Collab, a controlled decoding method that uses a mixture of pre-trained LLM agents to align outputs with target tasks at inference time. Each agent is specialized for different tasks, and the method dynamically selects the optimal agent per token using a long-term utility metric (Q-function) to improve alignment without retraining."
          },
          "strengths": {
            "value": "Originality: The mixture-of-agents approach for inference-time alignment is novel, addressing limitations of single-agent decoding. Theoretical analysis of the Q-function as an alignment metric adds rigor. Clarity: The abstract and introduction clearly outline the problem and contributions. Significance: Alignment is critical for practical LLM deployment, and the method's training-free nature could have broad applicability. Quality: The paper mentions empirical evaluations on diverse tasks, suggesting practical relevance."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with key sections (e.g., methodology details, experiments) truncated, making it impossible to assess technical depth or validation. Theoretical claims lack sufficient detail (e.g., how the Q-function is defined or computed). Empirical results are not fully described, so it's unclear if the improvements over baselines are statistically significant or generalizable. The paper does not address potential limitations, such as computational overhead or scalability of the mixture-of-agents approach."
          },
          "questions": {
            "value": "How is the implicit Q-function computed during decoding? What specific pre-trained models are used as agents, and how are they specialized? Are there ablation studies to isolate the impact of the mixture-of-agents strategy? How does the method handle conflicting task requirements (e.g., factual accuracy vs. creativity)? What is the computational cost compared to single-agent decoding?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 2
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes Collab, a controlled decoding method that uses a mixture of pre-trained language models (LLMs) to achieve task-specific alignment during inference without retraining. It dynamically selects the most suitable LLM at each token level based on a long-term utility metric, aiming to improve performance across diverse tasks compared to single-agent decoding approaches."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel mixture-of-agents decoding strategy for inference-time alignment, which is less explored in prior work. Quality: The method is grounded in a theoretical framework using a Q-function for optimal policy selection, and preliminary experiments show significant improvements over baselines. Clarity: The problem statement and motivation are well-articulated, with a clear explanation of the limitations of single-agent approaches. Significance: Addressing alignment without retraining is critical for practical LLM deployment, and the results suggest a promising direction for efficient inference-time adaptation."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-sentence, leaving key details about the Q-function formulation, the selection metric, and experimental setup incomplete. The theoretical analysis appears brief and lacks depth, such as convergence guarantees or bounds. The experiments mention GPT-4 win-tie rates but do not describe how the metric is computed or compared to other methods. The scalability of the approach with more agents or complex tasks is not discussed, and ablation studies are missing."
          },
          "questions": {
            "value": "1. How is the long-term utility metric (Q-function) defined and computed during decoding? Is it task-specific or generalizable? 2. What are the computational costs of the token-level agent selection compared to single-agent decoding? 3. How does the method handle tasks requiring conflicting capabilities (e.g., factual accuracy vs. creativity)? 4. Are there ablation studies showing the contribution of individual agents or the selection mechanism? 5. How is the GPT-4 win-tie rate measured, and what baselines are used for comparison?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "7orD38wzdi": {
    "paper_id": "7orD38wzdi",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes CALL, a Communicative World Model for ego-centric multi-agent reinforcement learning (MARL) in high-dimensional environments like autonomous driving. The approach uses latent representations of states and intentions to enable lightweight, prediction-driven information sharing between agents, addressing challenges of partial observability and non-stationarity. The method is evaluated on trajectory planning tasks in the CARLA platform."
          },
          "strengths": {
            "value": "The paper introduces a novel framework (CALL) that combines world models with ego-centric learning, addressing key MARL challenges. The use of latent representations for communication is theoretically grounded and demonstrates practical benefits in reducing bandwidth. The three key ideas (latent state/intention encoding, prediction-accuracy-driven sharing, and synergization of WM generalization) are clearly structured. The table comparing bandwidth and prediction accuracy provides concrete evidence of the approach's efficiency."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results and comparisons to state-of-the-art methods. The claims about performance gains are based on a single table without sufficient context or statistical significance. The theoretical analysis is superficial, and the paper does not thoroughly address how CALL differs from prior work (e.g., attention mechanisms or neighbor-based sharing). Scalability and robustness in dynamic environments are not discussed. The communication strategy's dependency on prediction accuracy is not rigorously defined or validated."
          },
          "questions": {
            "value": "1. What specific baselines were used for comparison in the experiments? 2. How is 'prediction accuracy' quantified, and what metrics were used to evaluate the impact of information sharing? 3. Are there ablation studies demonstrating the necessity of each component (latent encoding, prediction-driven sharing, etc.)? 4. How does CALL handle scenarios with varying numbers of agents or non-stationary dynamics beyond the tested cases? 5. What are the limitations of the latent representation approach in complex, real-world environments?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces CALL, a Communicative World Model for ego-centric multi-agent reinforcement learning (MARL) in autonomous driving. The approach uses low-dimensional latent representations of states and intentions to enable efficient, prediction-accuracy-driven communication between agents, addressing partial observability and non-stationarity challenges. Experiments on CARLA demonstrate improved prediction accuracy and trajectory planning performance with reduced communication overhead."
          },
          "strengths": {
            "value": "Originality: CALL's integration of world models with adaptive, prediction-driven communication represents a novel approach to MARL, particularly in high-dimensional environments. The focus on lightweight, context-aware information sharing addresses scalability limitations in prior work. Quality: The methodology is well-structured, with clear ablation studies and bandwidth/prediction accuracy comparisons. Clarity: The paper is logically organized, with thorough explanations of key components like latent representation and adaptive sharing. Significance: The work advances practical MARL for real-world applications like autonomous driving, where communication efficiency and adaptability are critical."
          },
          "weaknesses": {
            "value": "Theoretical analysis is limited: While the paper discusses generalization error and epistemic uncertainty, it lacks rigorous mathematical justification for the proposed prediction-accuracy-driven sharing strategy. Experimental scope is narrow: The experiments focus solely on trajectory planning in CARLA, without testing on diverse MARL benchmarks or comparing with state-of-the-art methods like COMA or QMix. Missing ablation studies: The impact of individual components (e.g., latent intention encoding vs. state sharing) is not thoroughly analyzed. Implementation details are sparse: Key hyperparameters, training procedures, and baseline comparisons are under-described."
          },
          "questions": {
            "value": "1. How does CALL handle non-stationarity beyond prediction accuracy? Are there theoretical guarantees for stability in dynamic environments? 2. What specific baselines were used for comparison (e.g., centralized vs. decentralized MARL methods)? 3. How sensitive is the performance to the choice of latent space dimensionality or prediction horizon? 4. Can the prediction-accuracy-driven sharing mechanism be generalized to other domains beyond autonomous driving?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes CALL, a Communicative World Model approach for ego-centric multi-agent reinforcement learning (MARL) in high-dimensional environments like autonomous driving. CALL addresses partial observability and non-stationarity by encoding high-dimensional sensory inputs into low-dimensional latent states and intentions, enabling efficient, prediction-accuracy-driven information sharing among agents."
          },
          "strengths": {
            "value": "The paper presents a novel integration of world models with adaptive communication strategies, addressing scalability and non-stationarity in MARL. The use of latent representations for lightweight communication is practical and supported by bandwidth/accuracy trade-off experiments. The structured methodology with three key ideas (latent encoding, prediction-driven sharing, and synergization) is well-articulated. The focus on ego-centric learning aligns with real-world applications, and the experiments on CARLA demonstrate relevance to autonomous driving."
          },
          "weaknesses": {
            "value": "The paper is truncated, omitting critical experimental results and analysis. The theoretical foundation for prediction-accuracy-driven sharing is underdeveloped, and the paper lacks ablation studies to validate component contributions. Comparisons with baseline methods are limited to a single table without detailed performance metrics. The scalability of CALL in dynamic agent numbers and robustness to world model limitations (e.g., generalization gaps) are not addressed. The mechanism for determining 'prediction accuracy' in sharing decisions remains unclear."
          },
          "questions": {
            "value": "1. What are the full experimental results (e.g., reward metrics, success rates) comparing CALL to baselines? 2. How does CALL handle dynamic environments with varying numbers of agents? 3. What is the computational overhead of latent encoding/decoding and prediction-accuracy calculations? 4. Are there ablation studies showing the impact of each of the three key ideas? 5. How does the prediction-accuracy metric account for epistemic uncertainty in non-stationary settings?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "7tOc6h8bea": {
    "paper_id": "7tOc6h8bea",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces a self-evaluation framework for large language models (LLMs) that enables adaptive inference-time compute by predicting whether restarting generation would yield better results. The method uses a generative reward model trained on real user prompts to decide when to resample, prune, or select responses, achieving significant performance gains with fewer samples compared to traditional Best-of-N sampling."
          },
          "strengths": {
            "value": "The paper presents a novel approach to adaptive inference-time computation without relying on external reward models, which is a significant contribution to efficiency in LLM deployment. The experiments demonstrate clear improvements in task performance (e.g., 13% increase in AlpacaEval win rate and 7% improvement on GSM8K) with substantial reductions in sample count. The method's simplicity (requiring only a single token prediction) and practicality for real-world applications are strong points. The paper is well-structured and provides concrete examples of self-evaluation in action."
          },
          "weaknesses": {
            "value": "The paper lacks critical comparisons to existing adaptive sampling or self-evaluation methods, making it difficult to assess the novelty and superiority of the proposed approach. The dataset construction is under-specified: it is unclear how the 30,000 preferences were collected, how the ArmoRM reward model was used, or whether the training process introduced biases. The self-evaluation prompt design is not justified, and ablation studies on its effectiveness are missing. Additionally, the computational cost of the self-evaluation step itself is not quantified, and the paper does not address potential failure cases (e.g., when the model incorrectly predicts it cannot improve)."
          },
          "questions": {
            "value": [
              "How was the dataset of 30,000 preferences constructed? Specifically, what role did the ArmoRM reward model play in generating these preferences, and how were ties handled?",
              "What is the exact design of the self-evaluation prompt? Are there ablation studies showing how different prompt formulations affect performance?",
              "What is the computational overhead of the self-evaluation step? How does it compare to the cost of generating an additional sample?",
              "How does this method compare to existing adaptive sampling techniques (e.g., dynamic temperature adjustment or early stopping based on confidence scores)?",
              "Are there specific scenarios where the self-evaluation model fails to predict accurately, and how are these handled in practice?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a self-evaluation framework for large language models (LLMs) that enables adaptive inference-time compute by allowing models to predict whether restarting generation would yield better results. The approach uses a generative reward model formulation, avoiding external reward models and requiring only a single token prediction to decide sample pruning, resampling, or selection. Experiments show improved performance on AlpacaEval and GSM8K with significantly fewer samples."
          },
          "strengths": {
            "value": "The paper presents a novel self-evaluation paradigm that shifts from external reward models to internal predictions, offering a cost-effective alternative to Best-of-N sampling. The method's adaptability to task complexity and model capability addresses a critical scalability challenge in LLM inference. Experimental results demonstrate measurable improvements in downstream tasks with minimal computational overhead, and the simplicity of the single-token prediction mechanism is a key strength. The paper also highlights practical benefits like early pruning and temperature adaptation, which could have broad real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks comparative analysis with existing adaptive sampling techniques (e.g., dynamic temperature scaling or Bayesian methods), making it difficult to assess relative effectiveness. The self-evaluation prompt design is not thoroughly justified, and the reliance on an external reward model (ArmoRM) during training undermines the claim of complete independence from external systems. The experiments focus on Llama 3.1 8B, limiting generalizability, and the paper does not address potential failure cases or edge scenarios where the self-evaluation might misfire."
          },
          "questions": {
            "value": "1. How does this method compare to existing adaptive sampling approaches in terms of efficiency and performance? 2. What is the exact mechanism behind the self-evaluation prompt design, and how was it optimized? 3. Are there specific task types or query patterns where this approach underperforms? 4. How sensitive is the method to the choice of temperature annealing schedule? 5. Could the reliance on an external reward model during training introduce biases that affect real-world deployment?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a self-evaluation framework for large language models (LLMs) that enables adaptive inference-time compute by predicting whether restarting generation would yield better responses. The method uses a generative reward model formulation, avoiding external models and requiring only a single token generation for evaluation. Experiments show improved performance on AlpacaEval and GSM8K with significant reductions in sample count and token generation."
          },
          "strengths": {
            "value": "Originality: The paper presents a novel self-evaluation mechanism that eliminates reliance on external reward models, addressing a key limitation of prior work. Quality: The approach is theoretically grounded and supported by empirical results on real-world datasets. Clarity: The methodology is well-explained with concrete examples and figures. Significance: The focus on efficient inference-time compute aligns with practical challenges in deploying LLMs at scale."
          },
          "weaknesses": {
            "value": "The paper lacks comparison with existing adaptive sampling methods (e.g., dynamic stopping criteria). The self-evaluation mechanism's reliability is not thoroughly validated (e.g., accuracy of 'Yes/No' predictions). Dataset construction details are sparse (e.g., how 30k preferences were collected, how LMSYS prompts were filtered). The role of temperature annealing is not explicitly explained. Results on GSM8K and AlpacaEval lack baseline comparisons against standard Best-of-N sampling."
          },
          "questions": {
            "value": [
              "How does the self-evaluation mechanism compare to existing methods like dynamic stopping or adaptive temperature scheduling?",
              "What is the exact process for generating the self-evaluation token (e.g., specific prompts, training procedure)?",
              "How were hyperparameters (e.g., temperature, pruning thresholds) determined, and how sensitive are results to these choices?",
              "Are there cases where the model's self-evaluation fails, and how are these handled?",
              "What is the computational overhead of the self-evaluation step compared to standard sampling?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "7xf50qWFGP": {
    "paper_id": "7xf50qWFGP",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces an online Laplacian-based representation learning framework for reinforcement learning (RL), proposing the Asymmetric Graph Drawing Objective (AGDO) to enable simultaneous policy and representation updates. The authors provide theoretical guarantees for convergence under bounded policy drift and validate their approach empirically."
          },
          "strengths": {
            "value": "Originality: The paper introduces AGDO as a simplified alternative to prior methods like ALLO, offering a novel formulation for online Laplacian-based representation learning. Quality: The theoretical analysis of convergence under mild assumptions is rigorous, and the paper connects to established work on graph-based representations. Clarity: The problem statement and motivation are well-articulated, with clear links to prior literature. Significance: Addressing online representation learning in RL has broad implications for sample efficiency and policy adaptation, making this a relevant contribution to the field."
          },
          "weaknesses": {
            "value": "The paper is truncated, leaving critical details about experiments, implementation, and comparisons with baselines incomplete. The theoretical analysis assumes bounded policy drift but does not explicitly address how this assumption holds in practice. The empirical validation lacks specifics about benchmarks, hyperparameters, or quantitative comparisons to existing methods. The distinction between AGDO and ALLO is not fully justified, and the practical benefits of the proposed approach remain unclear."
          },
          "questions": {
            "value": "1. How does AGDO differ from ALLO in terms of convergence guarantees and computational efficiency? 2. What specific RL environments and baselines were used in the experiments, and how do the results compare to state-of-the-art methods? 3. How is the 'bounded drift' assumption validated in practice, and what are the implications for real-world RL settings? 4. Are there theoretical or empirical limitations to AGDO's applicability in high-dimensional or continuous state spaces?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces an online Laplacian-based representation learning framework for reinforcement learning, where the graph-based representation is updated concurrently with policy learning. The authors propose the Asymmetric Graph Drawing Objective (AGDO) and provide theoretical guarantees for its convergence under bounded policy drift, validated through simulations. The work addresses the challenge of adapting representations dynamically during policy updates, which is critical for complex environments."
          },
          "strengths": {
            "value": "The paper's originality lies in combining online Laplacian representation learning with policy updates, addressing a key gap in the literature. The theoretical analysis of AGDO's convergence under mild assumptions is a strong point, offering novel insights into the interplay between representation and policy learning. The clarity of the problem formulation and connection to prior work (e.g., ALLO, proto-value functions) is commendable. The significance is high, as dynamic representations could improve sample efficiency and adaptability in RL systems."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with the literature review and experiments cut off, limiting the ability to assess the thoroughness of the empirical validation. The theoretical analysis assumes bounded drift, but the paper does not discuss how this assumption holds in practice or how AGDO handles non-stationary environments. The simplification of ALLO into AGDO is not fully justified, and the trade-offs between AGDO and existing methods (e.g., ALLO) are unclear. The lack of ablation studies or comparisons with baselines weakens the evaluation."
          },
          "questions": {
            "value": "1. How does AGDO compare to ALLO in terms of convergence speed and stability? 2. What are the specific conditions under which the bounded drift assumption holds in practice? 3. Are there ablation studies demonstrating the necessity of AGDO's components (e.g., asymmetry, projection)? 4. How does the online learning framework handle non-stationary policies or environments with sparse transitions? 5. What are the limitations of the simulated environments used for validation, and how generalize to real-world scenarios?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces an online Laplacian-based representation learning framework for reinforcement learning (RL), proposing the Asymmetric Graph Drawing Objective (AGDO) to simultaneously update graph-based representations and policies. The authors theoretically analyze the convergence of online projected gradient descent on AGDO under bounded policy drift assumptions and provide preliminary empirical validation."
          },
          "strengths": {
            "value": "Originality: The paper addresses a novel integration of Laplacian-based representation learning with online policy updates, which is underexplored in RL. Quality: The theoretical analysis of AGDO's convergence is rigorous, and the paper contextualizes its contributions within existing work on graph-based representations. Clarity: The introduction and problem formulation are well-structured, with clear motivation for the online learning approach. Significance: If successful, this could improve sample efficiency and adaptability in complex RL environments."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with the literature review and experiments cut off, making it impossible to assess the full scope of contributions. The theoretical analysis assumes bounded policy drift but does not rigorously justify this assumption in practical RL settings. The empirical validation is limited to a single figure (Figure 1), with no quantitative results or comparisons to baselines. The relationship between AGDO and prior methods like ALLO is not thoroughly explained, and the paper lacks ablation studies or analysis of hyperparameter sensitivity."
          },
          "questions": {
            "value": "1. How is the bounded drift assumption validated in practice? What guarantees exist for real-world RL policies? 2. What are the specific differences between AGDO and ALLO, and how does AGDO address the limitations of prior methods? 3. Are there quantitative experiments demonstrating convergence to the true Laplacian representation, or only qualitative visualizations? 4. How does the proposed method compare to standard representation learning approaches (e.g., autoencoders, proto-value functions) in terms of sample efficiency and task performance? 5. What are the computational costs of online Laplacian updates, and how do they scale with environment size?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "82p8VHRsaK": {
    "paper_id": "82p8VHRsaK",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a novel adversarial anonymization framework that leverages large language models (LLMs) to improve text anonymization against adversarial LLM inferences. The authors propose a feedback-guided approach where an LLM adversary iteratively infers private attributes, and an anonymizer LLM adapts the text to mitigate these inferences. They evaluate their method on 13 LLMs and multiple baselines, demonstrating superior performance in privacy and utility compared to existing tools, supported by a human study."
          },
          "strengths": {
            "value": "The paper presents a highly original approach by framing anonymization as a dynamic adversarial game between LLMs, which addresses critical limitations of prior metrics. The experimental evaluation is comprehensive, covering real-world and synthetic texts, and includes a human study that strengthens the validity of their findings. The framework's ability to outperform commercial anonymizers, even with smaller LLMs, highlights its practical relevance. The clarity of the problem formulation, methodology, and figures (e.g., Figure 1) is excellent, making the core ideas accessible."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of why their framework outperforms existing methods, such as specific technical advantages of the adversarial feedback loop. The evaluation focuses on a narrow set of attributes (e.g., location) without exploring broader privacy risks. The comparison to baselines is thorough but does not address potential limitations of their own approach, such as computational costs or scalability. Additionally, the truncated related work section makes it difficult to assess the novelty relative to prior research."
          },
          "questions": {
            "value": "1. How does the framework balance privacy and utility in cases where inferences require more nuanced trade-offs? 2. What specific capabilities of LLMs (e.g., contextual understanding, generalization) enable their anonymization to outperform traditional methods? 3. Are there scenarios where the adversarial feedback loop could inadvertently introduce new vulnerabilities? 4. How generalizable are the results to other types of personal data (e.g., age, gender) beyond the examples provided?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a novel adversarial anonymization framework that leverages large language models (LLMs) to iteratively remove sensitive information from text based on inferences made by an adversarial LLM. The approach is evaluated across 13 LLMs and compared against industry-grade anonymizers, with claims of improved privacy and utility."
          },
          "strengths": {
            "value": "The paper presents a creative framework that addresses a critical privacy problem by combining adversarial LLM inferences with iterative anonymization. The experimental evaluation is comprehensive, including real-world and synthetic datasets, and a human study adds empirical validation. The work highlights the insufficiency of current anonymization tools and proposes a novel problem formulation for adversarial settings."
          },
          "weaknesses": {
            "value": "The paper lacks detailed metrics for evaluating utility and privacy, relying instead on vague claims of 'higher utility and privacy.' The human study has a small sample size (n=50) and does not address potential biases or statistical significance. The framework's dependency on access to an adversarial LLM may limit practical applicability. Additionally, the paper does not compare against state-of-the-art anonymization methods beyond the described baselines, and the adversarial setting's robustness to diverse LLMs or attack scenarios is unexplored."
          },
          "questions": {
            "value": [
              "How were the 13 LLMs selected for evaluation, and what criteria define their diversity?",
              "What specific metrics were used to quantify 'utility' and 'privacy' in the experiments?",
              "Can the framework function without access to an adversarial LLM, and how is this addressed in practice?",
              "How does the method handle dynamic or evolving adversarial models over time?",
              "What are the computational costs and scalability of the iterative adversarial feedback loop?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces a novel adversarial anonymization framework that leverages large language models (LLMs) to improve text anonymization against adversarial LLM inferences. The authors propose a feedback-guided iterative process where an LLM adversary identifies private attributes, and an anonymizer LLM modifies the text to obscure these attributes. They evaluate their approach on 13 LLMs and real-world/synthetic texts, claiming superior utility and privacy compared to industry tools, supported by a human study."
          },
          "strengths": {
            "value": "Originality is demonstrated through the adversarial evaluation setting and feedback-guided anonymization framework, which creatively repurposes LLMs' inferential capabilities. The paper's experimental scope is broad, covering 13 LLMs and including a human study. The significance is strong, addressing critical privacy gaps in LLM-era text anonymization. Clarity is generally good, with clear figures and logical flow, though some details are underdeveloped."
          },
          "weaknesses": {
            "value": "The human study lacks methodological details (e.g., participant selection, control groups, statistical significance). The paper does not compare against academic baselines beyond industry tools, limiting the novelty claim. Privacy/utility metrics are not rigorously defined or quantified. The computational cost and scalability of the framework are unaddressed. The adversarial LLM's capabilities are not thoroughly analyzed for limitations, and the framework's robustness to evolving adversaries is unclear."
          },
          "questions": {
            "value": "1. How was the human study designed? Were participants blinded to the anonymization method? 2. What specific metrics were used to quantify utility and privacy? 3. How does the framework handle adversarial LLMs with different architectures or training data? 4. What are the computational requirements for deploying this framework in practice? 5. Are there cases where the anonymization could inadvertently introduce new vulnerabilities?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "84WmbzikPP": {
    "paper_id": "84WmbzikPP",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Stiefel Flow Matching, a generative model for predicting 3D molecular structures from molecular formulas and exact moments of inertia. By embedding the feasible space of structures with fixed moments in the Stiefel manifold, the method enforces exact moment constraints, enabling more accurate and efficient structure elucidation compared to Euclidean diffusion models."
          },
          "strengths": {
            "value": "The paper presents a novel problem formulation for moment-constrained structure elucidation, which is both theoretically grounded and practically relevant to chemistry. The use of the Stiefel manifold to enforce exact moment constraints is a creative approach that addresses limitations in existing generative models. The method's integration of Riemannian flow matching with equivariant optimal transport demonstrates originality in combining geometric deep learning with transport theory. Empirical results on standard datasets suggest practical improvements in sampling efficiency and success rates, though more detailed analysis is needed."
          },
          "weaknesses": {
            "value": "The mathematical justification for embedding the feasible space of moment-constrained structures in the Stiefel manifold St(n,4) is insufficiently explained. The paper lacks rigorous proofs or derivations to support this claim, which is critical for the method's validity. The equivariant optimal transport objective is mentioned but not elaborated, leaving unclear how it simplifies flows or improves performance. Experimental results are brief, with no quantitative comparisons to baselines or ablation studies to isolate the impact of key components. The paper also does not address computational costs or scalability to larger molecules."
          },
          "questions": {
            "value": "1. How is the feasible space of structures with fixed moments mathematically embedded in St(n,4)? What guarantees exist that this embedding preserves the constraints? 2. What specific challenges arise when applying flow matching to the Stiefel manifold, and how does the proposed method address them? 3. How does the equivariant optimal transport objective differ from standard optimal transport, and what theoretical benefits does it provide? 4. What are the exact metrics used to evaluate 'higher success rates' and 'faster sampling,' and how do they compare to Euclidean diffusion baselines?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Stiefel Flow Matching, a generative model for predicting 3D molecular structures from molecular formulas and exact moments of inertia. It leverages the Stiefel manifold's mathematical structure to enforce precise moment constraints, contrasting with prior methods that use soft conditioning. The approach combines Riemannian flow matching with equivariant optimal transport to improve sampling efficiency and accuracy."
          },
          "strengths": {
            "value": "The paper presents a novel problem formulation for moment-constrained structure elucidation, which is both theoretically grounded and chemically relevant. The integration of Stiefel manifolds and flow matching demonstrates creative application of differential geometry to molecular generation. Empirical results on standard datasets (QM9, GEOM) suggest practical improvements over Euclidean diffusion models, though the significance of these gains requires deeper scrutiny. The clarity of mathematical derivations and the inclusion of visual intuition (e.g., Figure 1) enhance readability."
          },
          "weaknesses": {
            "value": "The theoretical foundation for embedding molecular configurations in the Stiefel manifold lacks rigorous proof or detailed justification. The paper assumes this embedding is valid but does not address potential ambiguities or edge cases (e.g., degenerate moment configurations). Experimental validation is limited: success rates and sampling speed improvements are not quantified against strong baselines, and ablation studies on the equivariant transport component are missing. The claim about leveraging 'many digits of precision' from spectroscopy is not empirically supported."
          },
          "questions": {
            "value": "1. How is the Stiefel manifold embedding formally justified? Are there conditions under which this holds? 2. What specific metrics (e.g., RMSD, energy stability) were used to evaluate 'higher success rates'? 3. How does the equivariant transport objective differ from prior work on Riemannian flows? 4. Are the reported improvements robust across diverse molecular sizes and complexities? 5. How are molecular symmetries (e.g., permutation, reflection) handled in the Stiefel framework?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces Stiefel Flow Matching, a generative model that leverages the Stiefel manifold structure to enforce exact moment-of-inertia constraints when predicting molecular 3D structures from molecular formulas. It combines Riemannian flow matching with equivariant optimal transport to improve sampling efficiency and accuracy compared to Euclidean diffusion models."
          },
          "strengths": {
            "value": "The paper presents a novel approach to a challenging problem in molecular structure elucidation by formalizing the problem on the Stiefel manifold, which offers a mathematically rigorous way to enforce exact physical constraints. The method's theoretical foundation is solid, and the empirical results on benchmark datasets demonstrate practical improvements in sampling efficiency and success rates. The clarity of the problem formulation and the connection to rotational spectroscopy are particularly strong."
          },
          "weaknesses": {
            "value": "The paper lacks critical comparisons with alternative manifold-based generative models or physics-informed constraints. The theoretical analysis of why Stiefel Flow Matching outperforms Euclidean diffusion is insufficient, and the paper does not address how sensitive the method is to hyperparameters or initial conditions. Additionally, the empirical validation is limited to QM9 and GEOM datasets, with no ablation studies on the impact of different moment constraints or molecule sizes."
          },
          "questions": {
            "value": [
              "How was the embedding of molecular structures into the Stiefel manifold validated? Are there theoretical guarantees that all valid structures satisfy the constraints in Eq. (1)?",
              "What specific advantages does equivariant optimal transport provide over standard Riemannian flow matching in this context? Is there quantitative evidence of shorter generation paths?",
              "The paper claims higher success rates but does not define 'success' in terms of structural accuracy metrics (e.g., RMSD, energy minimization). How were generated structures evaluated for chemical stability?",
              "Why is the Stiefel manifold St(n,4) specifically chosen? Could other manifolds (e.g., Grassmann) yield comparable results with less computational cost?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "85Ik12q2hP": {
    "paper_id": "85Ik12q2hP",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper critically evaluates the effectiveness of ReAct-style prompting for enhancing LLM reasoning in sequential decision-making tasks. The authors argue that the perceived performance gains of ReAct are primarily due to high similarity between example tasks and queries, rather than interleaved reasoning traces or plan guidance. They conduct extensive experiments on AlfWorld and WebShop, showing that LLM performance drops significantly when example-query similarity decreases, and that weaker guidance can be as effective as strong reasoning traces."
          },
          "strengths": {
            "value": "The paper addresses a timely and important question about the validity of ReAct-style prompting, which is widely adopted in the field. The experimental design is comprehensive, covering multiple LLMs and domains, and the findings challenge prevailing assumptions about LLM reasoning. The paper also contextualizes its work within recent critiques of LLM reasoning capabilities, strengthening its relevance. The systematic analysis of prompt variations (RQ1-RQ3) provides actionable insights into the limitations of ReAct."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous quantification of 'exemplar-query similarity'—the core claim depends on this metric but does not define or measure it formally. The experiments do not compare ReAct to alternative prompting methods (e.g., Chain-of-Thought) to isolate its unique contributions. The analysis of 'weaker guidance' is vague, with no clear definition of what constitutes 'placebo-guidance.' Additionally, the paper does not address whether the observed brittleness is inherent to ReAct or a result of specific implementation choices."
          },
          "questions": {
            "value": "1. How was exemplar-query similarity quantified? Were metrics like semantic similarity (e.g., BERTScore) or task-specific features used? 2. What were the exact 'weaker guidance' prompts used in the experiments? 3. Were there control experiments where example tasks were deliberately made dissimilar to queries? 4. How do the results hold when using different LLMs (e.g., GPT-4 vs. Llama 3.1)? 5. Could the observed performance drops be due to distributional shifts in the task space rather than similarity alone?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper critically evaluates the effectiveness of ReAct-style prompting for sequential decision-making tasks in large language models (LLMs). Through systematic variations of input prompts, the authors demonstrate that LLM performance is primarily driven by the similarity between example tasks and queries, rather than the interleaving of reasoning traces or the content of these traces. The study challenges the claimed 'emergent reasoning' abilities of LLMs and highlights the limitations of ReAct in handling diverse prompt designs."
          },
          "strengths": {
            "value": "The paper's strengths lie in its systematic experimental design, which rigorously tests the core claims of ReAct through controlled variations of prompt components. The work addresses an important open question about the true sources of performance gains in LLM prompting methods. The empirical analysis is comprehensive, covering multiple LLMs and domains (AlfWorld, WebShop). The paper also contributes to the growing literature questioning the 'reasoning' capabilities of LLMs by aligning with recent studies that highlight their reliance on pattern matching rather than genuine reasoning."
          },
          "weaknesses": {
            "value": "The study's conclusions rely heavily on the assumption that exemplar-query similarity is the sole driver of performance, without ruling out other potential factors like model-specific biases or domain-specific characteristics. The experiments lack comparisons with alternative prompting strategies that might achieve similar performance without requiring instance-specific examples. The paper also doesn't explore whether the observed limitations persist with larger models or more sophisticated prompt engineering techniques. Additionally, the analysis of 'reasoning traces' could be more rigorous, as the paper doesn't quantitatively measure the quality or coherence of these traces."
          },
          "questions": {
            "value": [
              "How did the authors quantify the 'exemplar-query similarity' metric, and what baseline measures were used to determine its significance?",
              "Were there any cases where ReAct-style prompting outperformed baseline methods despite low exemplar-query similarity?",
              "Could the observed brittleness be attributed to the specific LLMs tested (e.g., GPT-3.5 vs. GPT-4) or the experimental setup?",
              "What mechanisms might explain the comparable performance of 'placebo-guidance' vs. strong reasoning traces? Is this a general phenomenon or specific to the tasks studied?",
              "How do the authors reconcile their findings with ReAct's original claims of improved planning abilities in domains like HotPotQA and FEVER?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper critically evaluates the effectiveness of ReAct-style prompting for enhancing LLM reasoning in sequential decision-making tasks. The authors conduct systematic experiments on domains like AlfWorld and WebShop, finding that perceived LLM reasoning abilities stem from high exemplar-query similarity rather than interleaved reasoning traces or guidance quality."
          },
          "strengths": {
            "value": "The paper demonstrates strong methodological rigor by testing multiple LLMs (GPT-3.5, GPT-4, Claude, Llama) across diverse domains. It addresses important open questions about the validity of ReAct's claims, contributing to the growing literature questioning LLM reasoning capabilities. The research questions are well-defined, and the empirical analysis is comprehensive. The work also contextualizes its findings within recent critiques of emergent reasoning in LLMs."
          },
          "weaknesses": {
            "value": "The analysis relies on specific domain implementations (AlfWorld/WebShop) that may not generalize to other planning tasks. The study doesn't fully disentangle the effects of exemplar similarity from other prompt engineering factors. The claim that 'LLMs lack inherent reasoning abilities' is strong and could benefit from additional ablation studies or comparisons with alternative prompting methods. The paper also lacks detailed analysis of how different LLM architectures (e.g., GPT-4 vs. Llama) interact with the prompting strategies."
          },
          "questions": {
            "value": [
              "How was exemplar-query similarity quantitatively measured? Were there control experiments with systematically varied similarity levels?",
              "What specific forms of 'weaker guidance' were tested in RQ2? Could alternative guidance structures yield different results?",
              "How did the authors ensure that differences in LLM performance weren't driven by model-specific capabilities rather than prompting strategies?",
              "Were there experiments with non-domain-specific prompts to test the generality of the findings?",
              "How were 'instance-specific examples' curated? Could alternative curation strategies mitigate the cognitive burden on prompt engineers?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "86HwTRg0qh": {
    "paper_id": "86HwTRg0qh",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes OneFit, a self-supervised garment simulation method that uses function-based representations to overcome the limitations of mesh-specific and garment-specific models. The approach involves converting garment mesh patches into functions via PolyFit and learning deformations through physics-guided, geometry-aware transformations. This enables mesh-agnostic and garment-agnostic generalization across diverse garment types and mesh resolutions."
          },
          "strengths": {
            "value": "Originality: The function-based representation using n-jet functions and the focus on mesh/garment agnosticism are novel contributions. Quality: The methodology addresses a critical limitation in existing garment simulation approaches. Clarity: The paper clearly articulates the problem and presents a structured solution. Significance: The potential impact on virtual try-on systems and computational efficiency makes this work relevant to both industry and academia."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about experiments, baselines, and quantitative results missing. The claims about scalability and computational superiority lack empirical validation. The function-based representation's effectiveness compared to mesh/point-based methods is not thoroughly demonstrated. The paper does not address potential limitations (e.g., handling complex collisions, extreme deformations) or provide ablation studies. The comparison to state-of-the-art methods is vague without specific metrics."
          },
          "questions": {
            "value": "1. What specific metrics were used to evaluate drape quality and computational efficiency? 2. How does OneFit handle garments with drastically different topologies or resolutions compared to training data? 3. What are the limitations of the n-jet function representation in capturing complex garment behaviors? 4. How does the physics-guided deformation compare to traditional physics-based simulations in terms of accuracy? 5. Are there quantitative results showing the 250 fps inference speed and collision artifact removal effectiveness?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces OneFit, a self-supervised neural garment simulation method that uses function-based representations to overcome the limitations of mesh-specific and garment-specific models. The approach involves converting garment meshes into functions via a PolyFit module and learning deformations through physics-guided, geometry-aware transformations of these functions, enabling generalization across diverse garment types and mesh resolutions."
          },
          "strengths": {
            "value": "Originality is evident in the function-based representation of garments, which contrasts with traditional mesh or point-based approaches. The method's potential for computational efficiency and scalability to unseen garments addresses a critical gap in virtual try-on systems. Clarity is maintained through structured exposition of the problem and solution, while the significance lies in its practical impact on reducing online clothing returns and enabling real-time applications. The integration of physics constraints and geometric awareness demonstrates a novel combination of techniques."
          },
          "weaknesses": {
            "value": "The paper lacks detailed quantitative comparisons with state-of-the-art methods, particularly in terms of drape quality metrics (e.g., visual fidelity, collision avoidance). The experimental section is sparse, with no ablation studies to validate the contribution of key components like PolyFit or the function-based representation. The claim of '250 fps' inference speed is not contextualized against existing methods, and the post-processing step for collision removal is not described in sufficient detail to assess its effectiveness."
          },
          "questions": {
            "value": "1. How does PolyFit ensure consistent function representation across varying mesh resolutions and topologies? 2. What specific metrics were used to quantify the 'similar drape quality' compared to baselines? 3. Can the authors provide examples of garment types not seen during training that OneFit successfully generalizes to? 4. How is the physics-guided deformation implemented in practice (e.g., loss functions, constraints)? 5. What is the computational overhead of the post-processing step, and how does it affect real-time performance?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes OneFit, a self-supervised neural garment simulation framework that uses function-based representations to overcome limitations of mesh-specific and garment-specific models. The approach converts garment patches into differentiable n-jet functions (PolyFit) and learns deformations through geometry-aware, physics-guided transformations, enabling mesh-agnostic and garment-agnostic generalization."
          },
          "strengths": {
            "value": "The paper introduces a novel function-based representation for garments, addressing the critical limitation of mesh-specific models in existing methods. The PolyFit module's use of n-jet functions for compact, detail-preserving representations shows promise for computational efficiency. The approach's potential for generalization across garment types and mesh resolutions is a significant contribution. The paper also provides clear motivation for the problem's importance in virtual try-on applications."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive experimental validation to support claims of computational superiority and generalization. Key details about the PolyFit module's implementation (e.g., how n-jet functions handle topology changes) are under-specified. The comparison with existing methods is incomplete, with no quantitative results on drape quality or collision handling. The theoretical justification for the physics-guided deformation mechanism is minimal, and the paper doesn't address potential failure cases (e.g., extreme deformations)."
          },
          "questions": {
            "value": "1. How does PolyFit ensure bijectivity of functions across different garment topologies? 2. What quantitative metrics (e.g., FID, collision rates) demonstrate 250fps performance and drape quality? 3. How does the function-based approach handle garments with complex geometries not shown in the examples? 4. What is the exact training protocol for multi-garment generalization? 5. How does the post-processing step remove collision artifacts without compromising physical realism?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "8DuJ5FK2fa": {
    "paper_id": "8DuJ5FK2fa",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces EVaLS, a method to enhance the robustness of ERM-pretrained models against spurious correlations without requiring group annotations. It leverages loss-based sampling to create a balanced dataset for last-layer retraining and uses environment inference to select models based on worst environment accuracy (WEA), eliminating the need for group labels during training, validation, or model selection."
          },
          "strengths": {
            "value": "Originality: The paper addresses a critical gap by eliminating group annotations entirely, which is a significant departure from prior work relying on such labels. The use of WEA as a surrogate for model selection is novel. Quality: The method is practical, with experiments on standard benchmarks (e.g., CelebA, Waterbirds) showing competitive performance. Clarity: The approach is well-explained, with a clear structure and logical flow. Significance: Reducing reliance on group annotations has broad implications for real-world applications where such labels are unavailable or costly."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art methods that also avoid group annotations (e.g., AFR or DFR variants). The theoretical analysis of why loss-based sampling mitigates group imbalance is superficial, with only brief claims about its effectiveness. The new dataset (Dominoes Colored-MNIST-FashionMNIST) is mentioned but not thoroughly evaluated. The ablation studies on key components (e.g., environment inference, loss sampling) are missing, making it hard to assess their individual contributions."
          },
          "questions": {
            "value": "How does EVaLS handle cases where environment inference fails to capture meaningful group shifts? What is the computational overhead of environment inference compared to existing methods? Can the last-layer retraining approach be generalized to non-image tasks (e.g., NLP)? How does the paper address potential biases in the initial ERM model that might propagate through loss-based sampling?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes EVaLS, a method to enhance model robustness against spurious correlations without requiring group annotations. It leverages losses from an ERM-trained model to create a balanced dataset for last-layer retraining and uses worst environment accuracy (WEA) for model selection. The approach is tested on benchmarks like CelebA and Waterbirds, with claims of achieving near-optimal performance without group annotations."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in fairness and robustness by eliminating reliance on group annotations, which is a significant originality. The method is simple and practical, with empirical validation on multiple benchmarks. Theoretical insights into balanced sampling and environment-based validation are presented. The clarity of the problem statement and the structure of the paper are strong, and the significance of reducing annotation dependencies aligns with real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed descriptions of the environment inference process and how it captures group shifts. The theoretical analysis of balanced sampling is superficial, with no rigorous justification for its effectiveness. Experiments on the new Dominoes Colored-MNIST-FashionMNIST dataset are under-specified, and the paper does not compare against strong baselines. The claim that group annotations are unnecessary for model selection is overstated without ablation studies or analysis of WEA's reliability. The method's dependence on the initial ERM model's quality is unaddressed, and the paper does not evaluate robustness to adversarial spurious correlations."
          },
          "questions": {
            "value": [
              "How exactly are environments inferred, and what guarantees exist that they capture group shifts? What if the initial ERM model is biased?",
              "What is the exact construction of the Dominoes Colored-MNIST-FashionMNIST dataset, and how do its spurious attributes differ from existing benchmarks?",
              "How does EVaLS handle cases where spurious correlations are not captured by environment inference? Are there scenarios where WEA fails as a surrogate?",
              "What ablation studies validate the necessity of balanced sampling versus other sampling strategies?",
              "How does EVaLS compare to methods that use weak supervision or proxy labels for group annotation? Are there cases where it underperforms?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes EVaLS, a post-hoc method to improve robustness against spurious correlations in ERM-pretrained models without requiring group annotations. It leverages loss-based sampling to create a balanced dataset for last-layer retraining and uses environment inference to select models via worst environment accuracy (WEA). The approach is tested on benchmarks like CelebA and Waterbirds, with a new dataset introduced for multi-shortcut scenarios."
          },
          "strengths": {
            "value": "Originality lies in reducing reliance on group annotations for both training and validation, addressing a critical gap in fairness and robustness. The method is simple yet effective, with empirical results showing strong performance on spurious correlation benchmarks. The paper introduces a novel dataset with partial group annotations, demonstrating the method's versatility. Theoretical insights on loss-based sampling and environment inference provide a foundation for the approach. Clarity is strong in the abstract and introduction, though full details are cut off."
          },
          "weaknesses": {
            "value": "The paper is truncated, limiting assessment of experimental details, theoretical analysis, and comparison with prior work. The environment inference method is vaguely described (e.g., 'random linear layer atop a trained model’s feature space'), raising questions about its practical implementation. The claim that group annotations are unnecessary for model selection lacks sufficient empirical validation. Theoretical justification for balancing high- and low-loss samples is minimal. The new dataset's design and validation are not fully explained, reducing confidence in its utility."
          },
          "questions": {
            "value": [
              "How exactly are environments inferred? What specific techniques are used to create diverse environments with correlation shifts?",
              "What is the theoretical basis for using WEA as a surrogate for model selection? Are there cases where WEA might fail?",
              "How does EVaLS handle scenarios where spurious correlations are not captured by environment inference methods?",
              "What are the computational costs of EVaLS compared to existing methods like DFR or AFR?",
              "How does the new Dominoes Colored-MNIST-FashionMNIST dataset differ from existing benchmarks, and what evidence supports its validity for multi-shortcut scenarios?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "8NiTKmEzJV": {
    "paper_id": "8NiTKmEzJV",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces NETS, a non-equilibrium transport sampler that enhances annealed importance sampling (AIS) by incorporating a learned drift term into the stochastic differential equation (SDE). This drift term reduces the variance of importance weights while maintaining unbiasedness. The method leverages Fokker-Planck equations to derive objectives that avoid backpropagation through SDEs, and it allows post-training tuning of diffusion coefficients. Experiments show superior performance on high-dimensional benchmarks and lattice field theory models."
          },
          "strengths": {
            "value": "The paper presents a novel approach to non-equilibrium sampling by integrating learned drifts into AIS, addressing a key limitation of importance weighting variance. The theoretical analysis connects the drift to minimizing objectives that control KL divergence, offering a principled framework. The method's adaptability via post-training tuning of diffusion coefficients is a practical advantage. The work also bridges concepts from physics-informed neural networks (PINNs) and sampling, demonstrating cross-disciplinary innovation. The experimental claims on challenging problems like lattice field theory highlight significant potential impact."
          },
          "weaknesses": {
            "value": "The paper is truncated, leaving critical details about the experimental setup, comparison with baselines, and implementation of the learned drift unexplored. The theoretical proofs of unbiasedness and KL divergence control are not fully elaborated, making it difficult to assess their rigor. The claim that objectives avoid backpropagation lacks concrete explanation, especially how the drift is optimized without gradient-based methods. The comparison to related work (e.g., Vargas et al. 2024) is incomplete, and the role of the PINN loss in practice is not clarified."
          },
          "questions": {
            "value": [
              "How exactly is the learned drift optimized without backpropagating through SDE solutions? What training procedure is used?",
              "What specific objectives are minimized, and how do they relate to the theoretical guarantees (e.g., KL divergence control)?",
              "Can the authors provide details on the lattice field theory experiments, including baseline comparisons and quantitative metrics?",
              "How does the PINN loss differ from prior applications in sampling, and what evidence supports its effectiveness in this context?",
              "What are the limitations of the current theoretical analysis, and how do the authors address potential edge cases (e.g., non-log-concave targets)?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces NETS, a non-equilibrium transport sampler that combines annealed Langevin dynamics with a learnable drift term to improve sampling efficiency. By leveraging Fokker-Planck equations, the method avoids backpropagating through stochastic differential equations (SDEs) and demonstrates unbiased sampling with tunable diffusion coefficients. It claims superior performance on high-dimensional benchmarks and lattice field theory models compared to existing methods."
          },
          "strengths": {
            "value": "Originality is evident in the integration of learned drifts with annealed importance sampling (AIS) and the derivation of novel objective functions from Fokker-Planck equations. The paper's theoretical contributions, such as proving unbiasedness and linking objectives to KL divergence, are significant. The clarity of the methodology and experimental setup is strong, with detailed comparisons to related work. The practical relevance of addressing sampling inefficiencies in non-log-concave distributions is clear, and the potential impact on Bayesian inference and statistical physics applications is notable."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous theoretical justification for why the proposed objectives (e.g., the PINN loss) directly control KL divergence. The experimental validation is superficial, with no comparison to state-of-the-art methods like CMCD or score-based models. The claim about tunable diffusion coefficients is not supported by ablation studies or quantitative analysis. Additionally, the paper does not address computational costs or scalability to extremely high-dimensional data, which limits its practical applicability."
          },
          "questions": {
            "value": "1. How are the derived objectives (e.g., the PINN loss) theoretically guaranteed to minimize KL divergence? 2. What specific baselines were compared against in the experiments, and why are the results considered superior? 3. How is the diffusion coefficient tuned in practice, and what are the empirical trade-offs between integration time-steps and sample quality? 4. Are there theoretical guarantees for the convergence of the learned drift under non-log-concave targets? 5. How does the computational complexity of NETS compare to existing methods like SMC or diffusion models?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces NETS, a non-equilibrium transport sampler that combines annealed Langevin dynamics with a learnable drift term to improve sampling efficiency. The method leverages Fokker-Planck equations to derive objectives for the drift, avoiding backpropagation through SDEs. It claims to reduce variance in importance weights and control KL divergence via a physics-informed neural network (PINN) loss."
          },
          "strengths": {
            "value": "Originality lies in the novel combination of AIS with learned drifts and the derivation of objectives via Fokker-Planck equations, avoiding backpropagation. The method's adaptability post-training (tuning diffusion coefficients) and claims of unbiasedness are significant. The paper contextualizes the work well within related areas like SMC, diffusion models, and optimal control. The PINN loss's connection to KL divergence is a compelling theoretical contribution."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, particularly in comparing NETS to state-of-the-art methods like score-based diffusion models or SMC. The derivation of objectives and optimization process for the drift is not sufficiently explained, leaving questions about practical implementation. The claim that the PINN loss controls KL divergence requires stronger theoretical justification. The truncated content (e.g., missing experiments and proofs) limits the ability to assess rigor and scalability."
          },
          "questions": {
            "value": [
              "How exactly is the drift learned without backpropagating through SDEs? What specific optimization algorithm is used?",
              "What are the concrete objectives minimized by the drift, and how are they validated empirically?",
              "The paper mentions high-dimensional experiments but does not provide results or comparisons to baselines. How does NETS perform on standard benchmarks like MNIST or CIFAR-10?",
              "How does the method handle the trade-off between computational cost and sampling efficiency, especially in high-dimensional spaces?",
              "The claim that the PINN loss controls KL divergence needs a more detailed proof or empirical validation."
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "8OLayNZfvM": {
    "paper_id": "8OLayNZfvM",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces CtrlMol, a novel framework for controllable molecular graph generation using Bayesian Flow Networks (BFN). The approach addresses the challenges of discrete molecular topologies and multi-modality by sampling in a continuous parameter space, incorporating property-guided distributions, and a topological edge sampling strategy. Experimental results show superior performance over existing methods in generating realistic molecules with desired properties."
          },
          "strengths": {
            "value": "Originality is evident in applying Bayesian Flow Networks to molecular graph generation, a novel approach compared to diffusion models. The unified probabilistic modeling of atoms and bonds, along with the topological edge sampling strategy, creatively tackles multi-modality and sparsity. The experimental results demonstrate significant improvements in conditional generation tasks, highlighting the method's practical significance. The paper is well-structured, with clear explanations of the methodology and comparative analysis."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the contribution of individual components like the topological edge sampler and property-guided distribution. The comparison with recent state-of-the-art methods (e.g., GeoBFN) is limited, and the theoretical justification for why BFN outperforms diffusion models in this context is underdeveloped. Additionally, the experimental section is truncated, leaving critical details about evaluation metrics and baselines incomplete."
          },
          "questions": {
            "value": [
              "How does the property-guided output distribution interact with the Bayesian flow framework during training and inference?",
              "What specific challenges in molecular graph generation does the topological complete edge sampling strategy address that existing methods (e.g., GraphDF) fail to resolve?",
              "Are there any cases where CtrlMol's performance degrades compared to baselines, and how does the authors' approach handle such scenarios?",
              "How does the continuous parameter space enable a 100× speedup in conditional generation, and what are the trade-offs in terms of sample quality?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes CtrlMol, a novel framework for controllable molecular graph generation using Bayesian Flow Networks (BFN). The approach addresses challenges in multi-modality and edge sparsity by introducing a property-guided output distribution and a topological complete edge sampling strategy, achieving state-of-the-art results in generating molecules with desired properties."
          },
          "strengths": {
            "value": "The paper presents a creative solution to a significant problem in molecular generation by leveraging Bayesian Flow Networks, which differ from traditional diffusion models. The introduction of property-guided distributions and edge sampling strategies demonstrates originality in addressing multi-modality and sparsity. The experimental claims of superior performance over baselines suggest potential practical impact, and the paper is well-structured with clear contextualization of prior work."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with the methodology and experimental sections cut off, making it impossible to assess technical details, validation, or reproducibility. Key claims (e.g., 26% and 64% improvements in QED/LogP) lack supporting evidence, such as baselines, metrics, or statistical significance. The novelty over existing BFNs like GeoBFN is not clearly explained, and the paper does not address limitations of the proposed approach."
          },
          "questions": {
            "value": "1. How does CtrlMol handle the discrete nature of molecular graphs compared to diffusion models? 2. What specific property-guided distributions are used, and how are they integrated into the Bayesian flow? 3. Are the experiments conducted on standard datasets with proper baselines (e.g., JTVAE, GraphDF)? 4. How is the 'topological complete edge sampling strategy' implemented, and what are its computational trade-offs? 5. What is the exact definition of 'property control' in the experiments?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes CtrlMol, a novel framework for controllable molecular graph generation using Bayesian Flow Networks (BFN). It addresses challenges in generating discrete molecular graphs by leveraging continuous parameter space sampling, incorporating property-guided distributions, and a topological edge sampling strategy. The method claims to outperform existing approaches in generating valid molecules with desired properties."
          },
          "strengths": {
            "value": "Originality: The integration of Bayesian Flow Networks with property-guided generation for molecular graphs represents a novel approach compared to diffusion models. The paper introduces specific components like topological edge sampling to address multi-modality and edge sparsity. Quality: The framework is theoretically grounded in probabilistic modeling, and preliminary experiments show significant improvements in QED and LogP metrics. Clarity: The problem definition and methodology are well-structured, though some sections are cut off. Significance: Molecular generation is critical for drug discovery, and the paper addresses a longstanding challenge in discrete data generation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art methods, such as recent graph generative models or diffusion-based approaches. The experimental validation is incomplete, with no ablation studies or analysis of the edge sampling strategy's effectiveness. The theoretical justification for the Bayesian Flow Network's superiority over diffusion models is underdeveloped. The paper also does not address limitations, such as scalability to larger molecules or handling of specific chemical constraints. The claimed 100× speedup requires more concrete evidence."
          },
          "questions": {
            "value": [
              "Which specific baselines were compared against, and why were other relevant methods (e.g., recent GNN-based approaches) excluded?",
              "How is the property-guided output distribution implemented? What are the exact training objectives and loss functions?",
              "What metrics were used to evaluate unconditional generation, and how do they compare to existing benchmarks?",
              "Can the topological edge sampling strategy handle molecules with varying sizes or complex bonding patterns?",
              "How does CtrlMol ensure chemical validity of generated graphs, and what are the failure cases?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "8QTpYC4smR": {
    "paper_id": "8QTpYC4smR",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper presents a systematic review of Large Language Models (LLMs), covering their types, applications, limitations, and future directions. It provides an overview of key architectures like Transformers, BERT, GPT, and T5, and discusses their use cases and challenges."
          },
          "strengths": {
            "value": "The paper offers a comprehensive and well-structured overview of LLMs, with clear categorization of model types and detailed literature review. It systematically covers foundational concepts (e.g., self-attention, transfer learning) and recent developments. The organization of content into distinct sections (types, literature review, limitations) enhances readability and accessibility for readers seeking a broad understanding of the field."
          },
          "weaknesses": {
            "value": "The paper lacks original research contributions, relying solely on summarizing existing literature without critical analysis or synthesis. It does not address gaps in current research or propose novel frameworks. Experimental validation, baseline comparisons, or in-depth case studies are absent, which limits its value for a machine learning conference like ICLR. Additionally, the discussion of limitations (e.g., bias, computational costs) is superficial, with no actionable solutions or deeper technical insights."
          },
          "questions": {
            "value": [
              "How does the paper differentiate itself from existing surveys on LLMs? Are there specific gaps in the literature that this review addresses?",
              "What criteria were used to select the studies cited in the literature review? Is the coverage of recent advancements (e.g., 2023-2024) sufficiently comprehensive?",
              "The paper mentions 'future directions' but does not propose concrete research agendas. Could the authors elaborate on potential innovations or open problems in LLMs that warrant further investigation?",
              "How does the paper address the ethical concerns of LLMs (e.g., bias, fairness)? Are there specific techniques or frameworks discussed for mitigating these issues?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper presents a systematic review of large language models (LLMs), covering their types, applications, limitations, and future directions. It categorizes LLMs into generative, masked, sequence-to-sequence, and hybrid models, discusses their technical foundations (e.g., Transformer architecture, self-attention), and highlights challenges like computational costs and bias. The review aims to provide a structured overview of LLMs' impact across industries and research opportunities."
          },
          "strengths": {
            "value": "The paper offers a comprehensive and well-structured overview of LLMs, effectively categorizing their types and technical foundations. It addresses critical aspects like computational demands and ethical concerns, which are essential for responsible AI. The literature review covers key developments in deep learning methods and contextual embeddings, providing a solid foundation for understanding LLMs. The clarity of organization and accessibility of content make it a valuable resource for both researchers and practitioners."
          },
          "weaknesses": {
            "value": "As a review paper, it lacks original analysis or novel insights, relying heavily on summarizing prior work without critical evaluation. The absence of experimental validation or case studies limits its practical utility. Some citations are incomplete (e.g., [?]), and the paper does not explicitly address gaps in existing reviews or propose new frameworks for mitigating limitations like bias. The discussion of future directions is general and lacks specific research questions or actionable strategies."
          },
          "questions": {
            "value": "1. What criteria were used to select the literature reviewed, and how were gaps in existing reviews addressed? 2. Are there specific case studies or applications where LLMs demonstrated measurable impact, which could strengthen the paper's practical relevance? 3. How do the authors propose overcoming computational and ethical challenges, and what role might emerging techniques (e.g., model compression) play in this? 4. Could the paper clarify how its contributions differ from existing surveys on LLMs?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper presents a systematic review of Large Language Models (LLMs), covering their types, applications, limitations, and future directions. It provides an overview of LLM architectures, such as generative, masked, sequence-to-sequence, and hybrid models, and discusses their use cases in tasks like translation, summarization, and sentiment analysis. The review also addresses challenges such as computational costs, bias, and ethical concerns, while outlining potential research directions."
          },
          "strengths": {
            "value": "The paper is well-structured and comprehensive, offering a clear categorization of LLM types and their applications. It systematically reviews key developments in LLM research, including the Transformer architecture and techniques like transfer learning. The literature review is thorough, with relevant citations and a logical flow from foundational concepts to modern advancements. The discussion of limitations and future directions is insightful, highlighting critical challenges such as bias and efficiency."
          },
          "weaknesses": {
            "value": "As a review paper, it lacks original contributions and novel insights, relying heavily on existing literature without critical analysis or comparative evaluations of LLMs. The limitations section is somewhat superficial, with limited discussion of specific technical challenges (e.g., energy consumption, model interpretability). The paper does not address recent advancements beyond the cited works (e.g., LLaMA, Mistral) and omits empirical validation or case studies to support its claims."
          },
          "questions": {
            "value": "1. How were the literature sources selected, and what criteria were used to ensure representativeness? 2. Are there specific gaps in current LLM research that the paper identifies, and how do they differ from prior reviews? 3. How does the paper address the rapidly evolving nature of LLMs, given the potential obsolescence of cited works? 4. Could the authors provide more concrete examples of 'future directions' beyond general suggestions like 'multimodal integration'?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "8RCmNLeeXx": {
    "paper_id": "8RCmNLeeXx",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces the 'Forking Tokens Hypothesis,' suggesting that certain intermediate tokens in neural text generation can drastically alter subsequent outputs. The authors propose a method called Forking Paths Analysis to study uncertainty dynamics by re-sampling alternate token paths and analyzing outcome distributions. They demonstrate the approach on GPT-3.5 across multiple tasks, identifying forking tokens including unexpected cases like punctuation."
          },
          "strengths": {
            "value": "The paper presents a novel hypothesis about intermediate uncertainty in LLMs, addressing a gap in prior work focused on final outputs. The methodology is flexible, requiring no model access or fine-tuning, which is a practical advantage. The empirical analysis across diverse tasks shows promise in uncovering dynamic uncertainty patterns. The integration of statistical models (change point detection, survival analysis) for hypothesis testing adds methodological rigor."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of why specific tokens (e.g., punctuation) act as forking points. The experimental validation is incomplete (the paper is cut off), leaving critical questions about scalability, computational cost, and robustness unanswered. The use of a separate LLM for outcome extraction introduces potential biases without justification. The paper also fails to compare against baseline methods or quantify the magnitude of forking effects."
          },
          "questions": {
            "value": "1. How are 'forking tokens' defined quantitatively? Are there specific metrics to distinguish them from non-forking tokens? 2. What is the impact of using a different LLM for outcome extraction on the results? 3. How does the method handle cases where alternate paths lead to non-semantic or invalid outputs? 4. Are the statistical models (change point detection, survival analysis) validated against synthetic data or theoretical guarantees? 5. How are 'unexpected' forking tokens (e.g., punctuation) interpreted in the context of LLM decision-making?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces the concept of 'forking tokens' in neural text generation, hypothesizing that certain tokens, when resampled, lead to significantly different outcomes. The authors propose a method called Forking Paths Analysis to study uncertainty dynamics by re-sampling alternate completions at each token and analyzing divergent paths. They validate their hypothesis on GPT-3.5 across 7 tasks, identifying forking tokens including unexpected ones like punctuation."
          },
          "strengths": {
            "value": "The paper presents a novel hypothesis about forking tokens, which offers a fresh perspective on uncertainty estimation in LLMs. The methodology is creative, combining change point detection and survival analysis to study dynamic uncertainty. The experiments are comprehensive, covering diverse tasks and domains. The clarity of the problem statement, figures, and explanations is strong, and the significance of understanding uncertainty dynamics for safety and reliability is well justified."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with critical sections truncated, limiting the ability to assess full experimental details. The methodology relies on a different LLM for outcome extraction, which may introduce biases or inconsistencies. The paper lacks comparisons to existing uncertainty estimation techniques, making it unclear how their approach advances the state of the art. The analysis of 'unexpected' forking tokens (e.g., punctuation) is superficial, with no rigorous statistical validation. The scalability of the method to larger models or more complex tasks is unaddressed."
          },
          "questions": {
            "value": "1. How were the 7 tasks selected, and what specific metrics were used to evaluate forking tokens? 2. What is the computational cost of re-sampling at each token, and how was this managed? 3. How does the method handle the variability introduced by using a different LLM for outcome extraction? 4. Were statistical controls applied to rule out trivial forking tokens (e.g., rare or out-of-vocabulary words)? 5. How generalizable are the findings to other LLMs beyond GPT-3.5?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces the concept of 'forking tokens' in neural text generation, hypothesizing that certain intermediate tokens can drastically alter the outcome of generated text. The authors propose a method called Forking Paths Analysis to study uncertainty dynamics by re-sampling alternate completions at each token and analyzing their impact on final outcomes. They validate their hypothesis across multiple tasks and domains, finding evidence of forking tokens even in unexpected locations like punctuation marks."
          },
          "strengths": {
            "value": "Originality is strong, as the paper introduces a novel hypothesis about intermediate uncertainty in LLMs, contrasting with prior work focused on final outputs. The methodology is innovative, combining re-sampling, change point detection, and survival analysis to study dynamic uncertainty. The approach is flexible, applicable to any LLM without access to model weights. Empirical results across 7 tasks demonstrate the hypothesis's validity, with clear implications for understanding LLM behavior and safety."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of why specific tokens become forking points (e.g., semantic vs. syntactic factors). The experiments focus on GPT-3.5, and while the method is claimed to be general, the paper does not thoroughly demonstrate applicability to other models. The use of a different LLM for outcome extraction introduces potential inconsistencies. The example of '2021/2024' is compelling, but more concrete cases of unexpected forking tokens (e.g., punctuation) need deeper exploration. The paper is cut off mid-section, leaving critical methodological details (e.g., Eq. 1) incomplete."
          },
          "questions": {
            "value": [
              "How does the method handle varying LLM architectures or training data distributions? Are there cases where the approach fails to detect forking tokens?",
              "What is the exact process for extracting outcome representations (R(·))? How does this affect the reliability of results?",
              "The paper mentions 'unexpected forking tokens' like punctuation, but provides limited examples. Can the authors elaborate on these cases?",
              "The methodology involves re-sampling at each token, which could be computationally expensive. How was this managed at scale?",
              "How does the paper address potential biases introduced by using a different LLM for outcome extraction?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "8TBGdH3t6a": {
    "paper_id": "8TBGdH3t6a",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes H-PAD, a hybrid prototypes learning model for multivariate time series anomaly detection (MTSAD). The method combines patch prototypes (to capture local interval anomalies) and period prototypes (to detect periodic anomalies) to address over-generalization issues in reconstruction-based models. It leverages memory banks to store prototypes at different scales and incorporates periodic patterns for improved anomaly detection."
          },
          "strengths": {
            "value": "The paper introduces a novel framework that integrates local (patch) and global (period) prototypes, addressing limitations of prior methods that focus on point anomalies. The approach is theoretically grounded in mitigating over-generalization by leveraging multi-scale patch learning and periodicity awareness. The experimental claims on five benchmark datasets suggest practical relevance, and the architecture diagram provides a clear overview of the model's components. The paper also contextualizes its contribution within existing MTSAD literature, highlighting gaps in detecting interval and period anomalies."
          },
          "weaknesses": {
            "value": "The methodology lacks critical implementation details, such as how patch sizes are determined, how prototypes are learned (e.g., clustering, embedding methods), or how the hybrid scoring mechanism is computed. The experiments section is incomplete, making it impossible to assess the validity of the results. The paper does not compare against state-of-the-art methods or provide ablation studies to validate the necessity of hybrid prototypes. Additionally, the period prototype mechanism is vaguely described, with no explanation of how periodicity is modeled or detected."
          },
          "questions": {
            "value": "1. How are patch prototypes learned? Are they derived from clustering, autoencoder embeddings, or another method? 2. What is the exact mechanism for integrating prototypes of different patch sizes during reconstruction? 3. How are period prototypes defined? Are they based on Fourier transforms, sliding windows, or another periodicity detection technique? 4. Are there ablation studies demonstrating the contribution of patch vs. period prototypes? 5. Which specific datasets were used, and how do they compare to standard MTSAD benchmarks?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes H-PAD, a hybrid prototypes learning model for multivariate time series anomaly detection (MTSAD). It combines patch-based prototypes (for local interval anomalies) and period-based prototypes (for global periodic anomalies) to address limitations of existing reconstruction-based methods that struggle with over-generalization and periodic patterns."
          },
          "strengths": {
            "value": "Originality: Introduces a novel framework combining local (patch) and global (period) prototypes, addressing gaps in detecting interval and periodic anomalies. Quality: The problem formulation is well-motivated, with clear identification of over-generalization issues in existing methods. Clarity: The abstract and introduction provide a coherent overview of the approach. Significance: Enhances MTSAD by tackling complex anomaly types (interval/periodic) that are critical in real-world applications like equipment monitoring."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, limiting assessment of methodology details (e.g., how period prototypes are learned, anomaly score calculation). Experimental validation lacks specifics (baselines, metrics, ablation studies). The claim of 'extensive experiments' on five datasets is unverifiable without full results. The integration of patch and period prototypes is described conceptually but lacks technical depth."
          },
          "questions": {
            "value": [
              "What specific baselines were used for comparison, and how does H-PAD outperform them quantitatively?",
              "How are period prototypes defined and learned? Is there a mechanism to handle varying periodicity across variables?",
              "What anomaly score metric is used, and how is it derived from prototype distances?",
              "Are there ablation studies to validate the contribution of patch vs. period prototypes?",
              "How are patch sizes determined, and how does the model handle varying interval anomaly lengths?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes H-PAD, a hybrid prototypes learning model for multivariate time series anomaly detection (MTSAD). It combines patch prototypes (for local interval anomalies) and period prototypes (for global periodic anomalies) to address over-generalization issues in reconstruction-based methods. The approach aims to detect both point/interval anomalies and periodical anomalies through a dual-prototype framework."
          },
          "strengths": {
            "value": "The paper introduces a novel framework that integrates local (patch) and global (period) prototypes, addressing limitations of existing reconstruction-based methods. The motivation is well-justified, and the architecture seems logically designed to tackle over-generalization. The claim of comprehensive consideration of local and global information is promising. The paper also references relevant prior work, though the comparison is incomplete."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation. It mentions 'five benchmark datasets' but does not specify which ones or provide comparison results against state-of-the-art methods. Critical details about the memory bank implementation, prototype integration mechanism, and anomaly scoring function are missing. The novelty over existing methods like MEMTO or D3R is not clearly established. The ablation study on prototype types (patch vs. period) is absent. Theoretical analysis of why hybrid prototypes mitigate over-generalization is lacking."
          },
          "questions": {
            "value": [
              "What specific datasets were used for experiments, and how do they compare to standard MTSAD benchmarks?",
              "How does H-PAD's memory bank differ from MEMTO's approach? What evidence supports its superiority?",
              "Can you provide ablation study results showing the contribution of patch prototypes vs. period prototypes?",
              "How exactly are the 'distance between original features and nearest prototype' scores computed? What normalization/weighting is applied?",
              "What is the exact mechanism for learning period prototypes? How are they aligned with the query series' periodicity?",
              "Why does the hybrid approach prevent over-generalization better than single-prototype methods? Is there theoretical justification?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "8U4NGFE0po": {
    "paper_id": "8U4NGFE0po",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes PLHF, a few-shot prompt optimization framework for large language models (LLMs) that reduces reliance on extensive human feedback by introducing a duo-module architecture. The framework includes a responder task for output generation and an evaluator task for quality scoring, using a lightweight evaluator module to approximate human feedback. Experiments on public and industrial datasets demonstrate its effectiveness compared to existing methods."
          },
          "strengths": {
            "value": "Originality is evident in the duo-module design, which decouples responder and evaluator tasks to minimize human feedback. The paper addresses a practical challenge in real-world applications where explicit metrics are unavailable. The experimental validation on industrial data adds practical relevance. Clarity is maintained through structured problem formulation and clear contributions, while the significance lies in advancing few-shot prompt optimization for generative tasks."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of the evaluator module. The industrial dataset is not described, making it hard to assess generalizability. The comparison with state-of-the-art methods (e.g., DSPy, TextGrad) is superficial, with no quantitative analysis of PLHF's superiority. The evaluator module's training process and how it avoids overfitting to limited feedback are under-explained."
          },
          "questions": {
            "value": "1. How is the evaluator module trained without explicit human labels? Does it use self-supervision or transfer learning from other tasks? 2. What specific metrics are used to validate the evaluator's alignment with human preferences? 3. Can the framework handle tasks with highly subjective quality criteria (e.g., creativity in writing)? 4. How does PLHF scale to longer prompts or more complex tasks? 5. What are the exact improvements over GPT-4-based evaluators in terms of F1 or human agreement scores?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces PLHF, a few-shot prompt optimization framework for large language models (LLMs) that reduces reliance on extensive human feedback. PLHF employs a duo-module architecture consisting of a responder task (generating outputs) and an evaluator task (scoring outputs via a lightweight module). The framework aims to address challenges in tasks where output quality metrics are ambiguous, such as essay writing or dialogue systems, by approximating human feedback through an automated evaluator."
          },
          "strengths": {
            "value": "The paper addresses a relevant real-world problem: optimizing prompts with limited human feedback, which is critical for practical LLM applications. The duo-module design is a novel approach that separates generation and evaluation, potentially improving efficiency. The inclusion of industrial data experiments adds practical validity. The framework's efficiency in requiring only a single round of human feedback is a significant advantage over existing methods. The paper also clearly outlines contributions and contextualizes the work within related literature."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental comparisons with state-of-the-art methods, such as specific metrics (e.g., BLEU, ROUGE) to quantify improvements over DSPy or TextGrad. The evaluator module's design and training process are under-specified, making it unclear how it approximates human feedback. The claim of superiority over GPT-4o as an evaluator is not substantiated with ablation studies or statistical analysis. Additionally, the paper does not discuss limitations, such as scenarios where the evaluator might fail or how it adapts to diverse user preferences."
          },
          "questions": {
            "value": "1. How is the evaluator module trained, and what data is used for its supervision? 2. What specific metrics were used to evaluate output quality in the experiments, and how do they correlate with human judgments? 3. Are there cases where PLHF might underperform compared to methods relying on explicit metrics? 4. How does the framework handle tasks with highly subjective quality criteria (e.g., creative writing)? 5. What is the computational cost of the evaluator module compared to existing methods?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces PLHF, a framework for optimizing large language model (LLM) prompts using limited human feedback. It employs a duo-module architecture consisting of a responder task (generating outputs) and an evaluator task (scoring outputs) to reduce reliance on extensive human annotations. The method is validated on public and industrial datasets, claiming superior performance over existing approaches."
          },
          "strengths": {
            "value": "The paper addresses a practical problem of reducing human feedback in prompt optimization, which is significant for real-world applications. The duo-module design offers a novel approach compared to existing methods relying on reinforcement learning or extensive evaluations. The experimental validation on industrial data adds practical relevance. The work also highlights the importance of context-aware evaluation, which is underexplored in prior literature."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results, such as specific metrics, baselines, or ablation studies. The evaluator module's design and training process are not thoroughly explained, making it difficult to assess its effectiveness. The claims of superiority over state-of-the-art methods are not substantiated with concrete comparisons. The absence of analysis on the robustness of the framework to varying task types or feedback quality is a critical gap."
          },
          "questions": {
            "value": [
              "How is the evaluator module trained? What specific data or methods are used to approximate human scoring?",
              "What are the exact metrics used to evaluate output quality, and how do they compare to existing benchmarks?",
              "How does PLHF handle tasks where human feedback is inherently subjective or ambiguous?",
              "Are there any limitations to the evaluator module's generalizability across different domains or tasks?",
              "What is the exact number of human feedback samples required, and how does this compare to existing methods?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "8UFG9D8xeU": {
    "paper_id": "8UFG9D8xeU",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes DPA-OMF, a method for post-training preference alignment in multi-agent motion generation models by leveraging implicit preferences from pre-training expert demonstrations. It uses occupancy measure matching to create preference rankings among model-generated samples, enabling scalable alignment without human annotations. The approach is tested on large-scale traffic simulations, showing improved realism in a lightweight model compared to state-of-the-art imitation-based models."
          },
          "strengths": {
            "value": "Originality: The method introduces a novel way to exploit implicit preferences from pre-training data, avoiding adversarial assumptions. Quality: The experiments demonstrate practical effectiveness in a complex multi-agent setting. Clarity: The problem statement and high-level approach are well-articulated. Significance: Addressing alignment without human annotations is critical for scalable motion generation systems."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of the occupancy measure matching implementation and how implicit preferences are quantified. Experimental comparisons are limited—no direct baselines against existing alignment methods (e.g., AFD or DPO) are provided. The analysis of scaling laws is mentioned but not elaborated. The truncated nature of the paper prevents full evaluation of methodology and results."
          },
          "questions": {
            "value": "1. How is the implicit preference distance function derived from occupancy measure matching? What features are used for semantic alignment? 2. What are the specific baselines used for comparison (e.g., AFD, DPO, or imitation learning)? 3. How does the method handle varying numbers of agents or scene complexities? 4. Are there ablation studies to validate the contribution of occupancy measure matching versus other components?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes DPA-OMF, a method for post-training preference alignment of multi-agent motion generation models using implicit feedback from pre-training expert demonstrations. The approach leverages occupancy measure matching to construct preference rankings among model-generated samples, avoiding adversarial assumptions and reducing reliance on human annotations. It demonstrates effectiveness in large-scale traffic simulations, achieving results comparable to state-of-the-art models with minimal computational cost."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel framework (DPA-OMF) that uses occupancy measure matching for preference alignment, addressing a critical gap in multi-agent motion generation. Quality: The method is theoretically grounded in inverse reinforcement learning and optimal transport, with clear motivation for avoiding adversarial assumptions. Clarity: The problem statement, approach, and experimental setup are well-explained, with figures illustrating key concepts. Significance: The work addresses a practical challenge in embodied AI (e.g., autonomous driving) and demonstrates scalability to 100+ agents, which is impactful for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of how occupancy measure matching is implemented, particularly the semantic feature space and optimal transport formulation. Experimental validation is limited: comparisons with existing Alignment from Demonstrations (AFD) methods are not provided, and the ablation studies on scaling laws or preference data quality are unclear. The claim that a 1M model matches large imitation-based models requires stronger empirical evidence (e.g., quantitative metrics, baselines). Theoretical justification for the implicit preference distance function is insufficient, and the paper does not address potential failure modes (e.g., noisy demonstrations)."
          },
          "questions": {
            "value": "1. How is the semantic feature space for occupancy measure matching defined, and what ensures its meaningfulness for motion generation tasks? 2. What specific metrics are used to evaluate realism in traffic simulations, and how do they compare to human preferences? 3. Are there ablation studies showing the impact of different components of DPA-OMF (e.g., optimal transport vs. simpler distance metrics)? 4. How does the method handle scene context variations, and what is the computational overhead compared to baseline approaches? 5. What is the exact mechanism for generating preference rankings from occupancy measures, and how is this validated?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of aligning pre-trained motion generation models with human preferences in multi-agent environments. The authors propose DPA-OMF, a method that leverages implicit preferences from pre-training expert demonstrations to construct preference rankings among the model's own generations, avoiding adversarial assumptions. They demonstrate its effectiveness in large-scale traffic simulations, achieving results comparable to state-of-the-art models without requiring human annotations or high computational costs."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to preference alignment by utilizing implicit feedback from pre-training data, which addresses a critical gap in existing adversarial methods. The method is theoretically grounded in optimal transport and occupancy measure matching, offering a principled framework for nuanced preference guidance. The experiments on multi-agent traffic simulations showcase practical significance, and the paper provides valuable insights into preference data scaling laws. The clarity of the problem statement, methodology, and figures (e.g., Figure 1) is strong, with a clear connection to real-world embodied tasks."
          },
          "weaknesses": {
            "value": "The paper lacks a direct comparison with existing Alignment from Demonstrations (AFD) methods, making it difficult to assess the relative effectiveness of DPA-OMF. The theoretical justification for why occupancy measure matching captures human preferences is underdeveloped, and the analysis of how implicit feedback improves alignment remains somewhat abstract. Additionally, the experiments focus narrowly on traffic simulation, with limited exploration of other domains or edge cases. The claim of 'zero human cost' assumes pre-training demonstrations are always available, which may not hold in all scenarios."
          },
          "questions": {
            "value": "1. How does DPA-OMF compare to other AFD methods like those in Li et al. (2024) or Chen et al. (2024b) in terms of alignment quality and scalability? 2. What specific aspects of the occupancy measure matching process ensure it captures meaningful human preferences, and how is this validated? 3. Are there scenarios where the implicit feedback from pre-training demonstrations could be misleading, and how does the method mitigate such risks? 4. How generalizable is the approach to tasks beyond traffic simulation, such as robotic manipulation or humanoid locomotion?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "8WpRt9pjeh": {
    "paper_id": "8WpRt9pjeh",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper explores the use of large language models (LLMs) to generate synthetic data for predicting adult attachment styles. The authors create synthetic agents with varied profiles and childhood memories, simulate Adult Attachment Interviews (AAIs), and train machine learning models on the resulting transcripts. They demonstrate that models trained on synthetic data achieve comparable performance to those trained on real human data, with further improvements through embedding standardization using unlabeled human data."
          },
          "strengths": {
            "value": "The paper presents a novel application of LLMs to generate synthetic data for psychological research, addressing a critical challenge in medical data scarcity. The methodology is well-structured, with clear experimentation and validation. The focus on attachment styles—a key factor in mental health—highlights the significance of the work. The paper effectively balances technical details with broader implications for ethical and scalable AI in healthcare. The contributions to synthetic data generation and alignment techniques are both original and practical."
          },
          "weaknesses": {
            "value": "The real-world dataset used for evaluation is extremely limited (only 9 human transcripts), which undermines the robustness of the results. The qualitative analysis of embedding alignment lacks quantitative rigor, making it difficult to assess the validity of the claims. The paper does not address potential biases in the synthetic data or their impact on model predictions. Additionally, the standardization approach is simplistic, and the paper fails to explore more sophisticated alignment techniques or ablation studies to validate its effectiveness."
          },
          "questions": {
            "value": "1. How were the synthetic agents' profiles and childhood memories generated? Were they based on real-world data or arbitrary assumptions? 2. What specific features of the synthetic transcripts were used for training, and how do they differ from real human responses? 3. How were the three classifiers selected, and what were their individual performance metrics on both synthetic and real data? 4. Are there ethical concerns about using synthetic data for mental health predictions, particularly regarding potential misclassification of attachment styles?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper explores the use of large language models (LLMs) to generate synthetic data for predicting adult attachment styles. The authors create synthetic agents with varied profiles and childhood memories, simulate their participation in Adult Attachment Interviews (AAIs), and train machine learning models on these synthetic transcripts. They demonstrate that models trained on synthetic data achieve comparable performance to those trained on real human data, particularly after standardizing embeddings with unlabeled human data."
          },
          "strengths": {
            "value": "The paper introduces a novel application of LLMs to generate synthetic psychological data, addressing the critical challenge of scarce medical datasets. The methodology is technically sound, with clear experimentation on a specific psychological task. The focus on attachment theory, a foundational concept in mental health, adds significance. The work also highlights a practical approach to aligning synthetic and real data through standardization, which could inspire future research on synthetic data adaptation."
          },
          "weaknesses": {
            "value": "The study relies on an extremely small human dataset (n=9), which severely limits the validity and generalizability of results. The synthetic data generation process lacks detailed justification for how agents' responses align with real-world psychological patterns. The standardization method is under-described, making it difficult to assess its robustness. The paper does not compare against alternative synthetic data generation approaches or evaluate model performance on diverse demographic groups. The three-class attachment categorization differs from the standard four-class framework, potentially introducing bias."
          },
          "questions": {
            "value": "1. How were the synthetic agents' profiles and childhood memories designed to reflect realistic psychological variation? 2. What metrics were used to validate the quality of synthetic responses before training models? 3. Could the small human dataset (n=9) have introduced significant sampling bias? 4. How was the standardization process implemented technically, and what ablation studies were conducted to validate its effectiveness? 5. Why was a non-standard three-class categorization chosen, and how might this affect the interpretability of results?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper explores the use of large language models (LLMs) to generate synthetic data for predicting adult attachment styles, a critical aspect of mental health research. The authors create synthetic agents with varying profiles and childhood memories, simulate Adult Attachment Interviews (AAIs), and train machine learning models on the generated transcripts. They demonstrate that models trained on synthetic data achieve comparable performance to those trained on real human data, with improvements possible through standardization of embeddings using unlabeled human data."
          },
          "strengths": {
            "value": "The paper addresses a pressing challenge in mental health research—data scarcity—by proposing a novel use of LLMs for synthetic data generation. The work is timely and relevant, with clear practical implications for privacy-preserving AI development. The experimental setup is structured, and the authors provide qualitative insights into embedding alignment. The focus on attachment theory, a foundational concept in psychology, adds significance to the contribution. The paper also highlights the potential of synthetic data to extend beyond attachment theory to other behavioral prediction tasks."
          },
          "weaknesses": {
            "value": "The methodology lacks critical details, such as how synthetic agents are constructed, how attachment styles are encoded in prompts, and the exact mechanisms for embedding standardization. The evaluation is based on a very small human dataset (n=9), raising concerns about statistical validity. The comparison between synthetic and real data is not rigorously quantified, and the paper omits key ablation studies (e.g., baseline performance of LLMs without standardization). Additionally, the paper fails to address potential biases in synthetic data generation or the generalizability of findings to diverse populations."
          },
          "questions": {
            "value": [
              "How are the synthetic agents' profiles and childhood memories generated? Are they based on real-world data or entirely synthetic? What specific prompts or instructions are used to simulate different attachment styles?",
              "The evaluation uses only 9 human transcripts. How were these selected, and what is their demographic and clinical diversity? How does this small sample size affect the reliability of the results?",
              "What is the exact process for standardizing embeddings using unlabeled human data? How is this alignment quantitatively measured, and what metrics are used to validate its effectiveness?",
              "Are there any known limitations or biases in the LLM-generated data that could affect the model's performance? How were these mitigated?",
              "How do the authors ensure that the synthetic agents' responses are representative of real-world attachment styles? Are there any validation steps to confirm this?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "8XQ1hLbwmU": {
    "paper_id": "8XQ1hLbwmU",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper explores the use of analogical prompting to enhance large language models' (LLMs) ability to perform inductive linguistic reasoning on low-resource languages. The approach involves generating diverse analogical exemplars from seed examples and using them for in-context learning, demonstrating performance improvements over chain-of-thought methods on the modelLing dataset."
          },
          "strengths": {
            "value": "The paper presents a novel application of analogical prompting for linguistic reasoning, addressing a critical gap in low-resource language tasks. The methodology is well-structured, combining in-context learning with self-generated exemplars to leverage cross-lingual knowledge. The experimental results show significant performance gains, particularly for strong models like GPT-4o and Llama-3.1-405B, highlighting the potential of this approach. The discussion on the interplay between model strength and exemplar quality provides valuable insights into LLM reasoning mechanisms."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with alternative methods beyond chain-of-thought prompting, making it difficult to assess the relative novelty of the approach. The description of how language families and similar languages are selected is vague, and the criteria for exemplar correctness are not rigorously defined. Additionally, the paper does not address potential biases in the modelLing dataset or the generalizability of results to other low-resource languages. The absence of ablation studies on key components (e.g., number of generated exemplars) limits the depth of analysis."
          },
          "questions": {
            "value": [
              "How are language families and similar languages selected during the exemplar generation phase? Are there specific criteria or heuristics used?",
              "What metrics are used to evaluate the quality of generated exemplars, given the lack of a validation mechanism?",
              "How do the results generalize to languages outside the Slavic family, and what factors might influence this generalizability?",
              "Are there any statistical significance tests to confirm the performance improvements reported, particularly for smaller models?",
              "How is the 'exemplar correctness' assumption validated, given that the paper relies on exact matches to annotated responses?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates the use of analogical prompting to enhance large language models' (LLMs) ability to perform inductive linguistic reasoning on low-resource languages. The approach involves generating diverse cross-lingual exemplars through a two-stage process, where a strong language model identifies language families and generates related examples, which are then used for in-context learning. The method achieves significant performance improvements over chain-of-thought approaches on the modelLing dataset."
          },
          "strengths": {
            "value": "The paper presents a novel approach to leveraging analogical prompting for linguistic reasoning tasks, particularly in low-resource settings. The method addresses a critical gap in adapting LLMs to underrepresented languages through cross-lingual knowledge transfer. The experimental results demonstrate measurable performance gains (up to 8.1% for GPT-4o), showing the effectiveness of the proposed technique. The paper also provides insightful analysis of factors influencing linguistic reasoning performance, such as model size and the quality of generated exemplars. The clear motivation and practical implications for multilingual NLP make this work significant."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of the limitations of the proposed method, such as its performance on non-Slavic language families or its sensitivity to the quality of generated exemplars. The experimental validation is limited to a single dataset (modelLing) without comparison to alternative approaches like multilingual fine-tuning. The paper does not address potential biases in the generated exemplars or how they might affect downstream tasks. Additionally, the claim about 'self-generated analogical demonstrations' is not thoroughly validated with ablation studies or statistical significance tests."
          },
          "questions": {
            "value": [
              "How were the language families and related languages selected for generating exemplars? Were there any criteria to ensure their linguistic similarity to the target language?",
              "What specific metrics were used to evaluate the quality of the generated exemplars, given the lack of ground truth annotations for low-resource languages?",
              "Could the performance gains be attributed to the increased number of exemplars rather than the cross-lingual nature of the generated examples?",
              "How does the method handle cases where the model's pre-training data lacks sufficient coverage of the target language family?",
              "What ablation studies were conducted to isolate the impact of different components (e.g., language family identification vs. exemplar generation)?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a two-stage analogical prompting approach to enhance large language models' (LLMs) performance on low-resource linguistic reasoning tasks. By generating diverse analogical exemplars from related languages, the method improves in-context learning for translation puzzles, achieving significant performance gains over chain-of-thought approaches."
          },
          "strengths": {
            "value": "Originality is strong, as the paper proposes a novel application of analogical prompting for low-resource language reasoning, addressing a critical gap in NLP. The methodology is well-structured, combining generation of cross-lingual exemplars with in-context learning. Experiments on the modelLing dataset demonstrate measurable improvements, with clear empirical validation. The paper's focus on linguistic families and grammar similarities provides a fresh perspective on reasoning tasks. Clarity is maintained through the figure and detailed problem setup."
          },
          "weaknesses": {
            "value": "The lack of validation for generated exemplar correctness is a critical flaw—assuming all generated examples are correct risks propagating errors. The paper's truncation limits understanding of full experimental scope, such as baseline comparisons or ablation studies. The analysis of why certain models benefit more (e.g., strong models using weaker multilingual models) is superficial. Additionally, the paper does not address potential biases in language family selection or the impact of exemplar diversity on performance."
          },
          "questions": {
            "value": "1. How was exemplar correctness validated given the absence of human annotations? Were there cases where generated examples degraded performance? 2. What criteria were used to select 'similar' languages for exemplar generation? 3. How was diversity among generated exemplars ensured, and what metrics were used? 4. Were there comparisons with alternative in-context learning methods beyond chain-of-thought? 5. How were zero-shot baselines evaluated given the potential for test set leakage?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "8ZA7lrzw7O": {
    "paper_id": "8ZA7lrzw7O",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces a stochastic formulation of data echoing, providing a sharper analysis of its convergence properties and proposing a communication-efficient algorithm for data parallelism. The authors claim linear speedup with reuse steps and demonstrate a method to reduce communication overhead in distributed training."
          },
          "strengths": {
            "value": "Originality: The paper presents a novel analysis of data echoing using a shifted state approach, differing from prior work. Quality: The theoretical framework is well-structured, with clear mathematical formulations. Clarity: The paper is organized logically, with precise notation and definitions. Significance: Addressing data loading bottlenecks and communication efficiency in distributed training is highly relevant for large-scale machine learning."
          },
          "weaknesses": {
            "value": "The analysis lacks concrete bounds or explicit comparisons to prior work (e.g., Agarwal et al. 2020). The communication-efficient algorithm's theoretical justification is underdeveloped, with unclear guarantees on convergence. The experiments section is incomplete, making it impossible to validate the proposed methods. The 'cosine diminishing schedule' is mentioned but not thoroughly explained or evaluated."
          },
          "questions": {
            "value": "1. How does the shifted state analysis explicitly improve upon Agarwal et al. (2020)? Provide concrete mathematical comparisons. 2. What are the theoretical guarantees for the communication-efficient algorithm? How does it ensure convergence under reduced gradient averaging? 3. The experiments are cut off—please provide full results validating the proposed algorithm and schedule. 4. Why is the cosine diminishing schedule effective? What hyperparameters were used in practice?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates data echoing, a technique to mitigate data loading bottlenecks in machine learning training by reusing batches for gradient computation. The authors provide a sharper theoretical analysis showing linear speedup with reuse steps, propose a communication-efficient algorithm for data parallelism, and introduce a practical cosine diminishing schedule for data loading probability."
          },
          "strengths": {
            "value": "The paper addresses a critical practical problem in distributed training (data loading bottlenecks) with clear motivation. The theoretical analysis extends prior work by showing improved convergence guarantees for data echoing, particularly in non-convex settings. The communication-efficient algorithm draws inspiration from FedAvg, offering a novel approach to reduce overhead in data parallelism. The paper is well-structured with precise mathematical formulations and clear contributions."
          },
          "weaknesses": {
            "value": "The theoretical analysis lacks concrete comparisons to prior bounds (e.g., Agarwal et al. 2020) and does not fully clarify how the 'sharper analysis' improves upon existing work. The communication-efficient algorithm's details are vague, and the claim of 'eliminating communication overhead' requires stronger justification. The experimental section is incomplete, making it impossible to validate the proposed methods. The paper also does not address potential limitations of data echoing (e.g., overfitting risks) in depth."
          },
          "questions": {
            "value": [
              "How does the proposed algorithm differ from FedAvg in terms of communication strategy? What specific conditions ensure its effectiveness?",
              "What are the exact assumptions required for the linear speedup claim? Are there scenarios where this does not hold?",
              "How does the cosine diminishing schedule balance exploration and exploitation during training? What hyperparameters were used in experiments?",
              "Can the analysis be extended to non-stochastic settings or other optimization objectives?",
              "What empirical evidence supports the claim that data echoing avoids overfitting despite reusing batches?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper addresses the data loading bottleneck in large-scale machine learning by analyzing data echoing techniques and proposing a communication-efficient algorithm for data parallelism. It provides a sharper convergence analysis of stochastic data echoing, demonstrating linear speedup with reuse steps, and introduces a method to reduce communication overhead in distributed settings."
          },
          "strengths": {
            "value": "The paper tackles a critical practical problem in ML training (data loading bottlenecks) with both theoretical and algorithmic contributions. The analysis builds on prior work while extending to non-convex settings with a less restrictive bias assumption. The communication-efficient algorithm draws meaningful connections to FedAvg, and the paper is well-structured with clear notation and problem formulation. The proposed cosine diminishing schedule for data loading adds practical value."
          },
          "weaknesses": {
            "value": "The experimental validation is underdeveloped in the provided content, with no specific results shown to support the theoretical claims. The analysis relies on assumptions (e.g., smoothness, bounded bias) that require stronger justification. The connection to FedAvg is mentioned but not elaborated, leaving unclear how the proposed algorithm differs from existing methods. The paper lacks ablation studies or comparisons to baseline approaches."
          },
          "questions": {
            "value": "1. What specific empirical results validate the linear speedup claim and the effectiveness of the communication-efficient algorithm? 2. How does the bounded bias assumption compare to the bounded gradient assumption in prior work, and what are its limitations? 3. Can the authors clarify how their algorithm avoids communication overhead while maintaining convergence guarantees in data parallelism? 4. What are the hyperparameters or design choices for the cosine diminishing schedule, and how were they determined?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "8aKygnbEFX": {
    "paper_id": "8aKygnbEFX",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces a hybrid fine-tuning approach for Large Language Models (LLMs) that combines zeroth-order (ZO) optimization for the base model with first-order methods for Parameter-Efficient Fine-Tuning (PEFT) modules. It proposes a theoretical framework called 'hybrid generalized smoothness' to analyze convergence under heterogeneous optimization landscapes, supported by empirical results across multiple tasks and architectures."
          },
          "strengths": {
            "value": "Originality is demonstrated through the novel hybrid approach combining ZO and first-order methods, addressing limitations of full fine-tuning and PEFT. The theoretical framework of 'hybrid generalized smoothness' extends classical optimization theory to account for heterogeneous parameter spaces. The empirical studies show consistent improvements over baselines, and the work addresses a significant practical challenge in LLM adaptation. Clarity is strong in defining the problem and contributions, with clear motivation for the theoretical analysis."
          },
          "weaknesses": {
            "value": "The theoretical analysis lacks depth, particularly in justifying how 'hybrid generalized smoothness' addresses dynamic Lipschitz constants (L) and heterogeneous smoothness. The convergence proof for SGD under this framework is not detailed, and the paper does not rigorously validate assumptions about the relationship between L and learning rates. Empirical results are described but lack specific metrics (e.g., exact accuracy gains, computational cost comparisons) and fail to compare with critical baselines like full fine-tuning or advanced PEFT methods (e.g., LoRA with gradient-based updates). The paper is truncated, leaving key details about the algorithm and experiments incomplete."
          },
          "questions": {
            "value": "1. What is the exact mathematical definition of 'hybrid generalized smoothness' and how does it differ from existing smoothness assumptions? 2. How are dynamic changes in the Lipschitz constant L handled in the convergence analysis? 3. Are there specific scenarios where the hybrid approach fails compared to full fine-tuning or PEFT? 4. What ablation studies confirm the necessity of combining ZO and first-order methods? 5. How is the computational overhead of the hybrid approach quantitatively compared to full fine-tuning and zeroth-order methods?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes a hybrid fine-tuning approach for Large Language Models (LLMs) that combines zeroth-order optimization for the base model with first-order methods for PEFT modules. It introduces a theoretical framework called 'hybrid generalized smoothness' to analyze convergence under heterogeneous optimization landscapes, supported by empirical validation across multiple tasks and architectures."
          },
          "strengths": {
            "value": "The paper's originality lies in addressing the limitations of both full fine-tuning and PEFT through a novel hybrid approach. The theoretical framework extends classical optimization theory to account for dynamic and heterogeneous smoothness in joint LLM-PEFT training, which is a significant contribution. The empirical study demonstrates practical effectiveness, and the discussion of broader applications (e.g., layer-wise fine-tuning) enhances the significance. The clarity of the problem statement and structure is strong, with clear connections to prior work."
          },
          "weaknesses": {
            "value": "The theoretical analysis lacks depth in specific technical details, such as rigorous proofs for the 'hybrid generalized smoothness' definition or explicit comparisons to existing convergence guarantees for similar methods. The empirical results are mentioned but not described in detail (e.g., metrics, baselines, ablation studies). The related work section is incomplete, raising concerns about whether prior literature on zeroth-order optimization or hybrid methods is adequately addressed. The paper also does not clarify how the dynamic Lipschitz constant $L$ is estimated or adapted during training."
          },
          "questions": {
            "value": "1. What specific theorems or lemmas establish the convergence guarantees under the 'hybrid generalized smoothness' assumption? 2. How are the dynamic Lipschitz constants $L$ estimated in practice, and how does this affect the choice of learning rates? 3. Can the authors provide details on the experimental setup (e.g., datasets, model sizes, hyperparameters) and comparisons to state-of-the-art PEFT methods? 4. How does the framework generalize to other hybrid optimization scenarios beyond LLMs, as suggested in the paper? 5. Are there any limitations or failure cases of the hybrid approach that the authors observed during experiments?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes a hybrid fine-tuning approach for Large Language Models (LLMs) that combines zeroth-order optimization for LLMs with first-order methods for PEFT modules. The authors introduce a theoretical framework called 'hybrid generalized smoothness' to analyze convergence under heterogeneous optimization landscapes, supported by empirical validation across multiple tasks and architectures."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in LLM adaptation by proposing a novel hybrid approach that balances computational efficiency and model performance. The theoretical framework of 'hybrid generalized smoothness' is a significant contribution, offering a new perspective on convergence analysis for heterogeneous optimization problems. The work demonstrates practical relevance through empirical studies, and the problem formulation is clear and well-motivated. The paper also highlights the broader applicability of the theoretical framework beyond hybrid fine-tuning."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about the theoretical analysis, experimental setup, and comparisons to baseline methods incomplete. The convergence guarantees for SGD under hybrid generalized smoothness are not fully explained, and the empirical results lack specific metrics or ablation studies. The related work section is truncated, potentially omitting key prior art. Additionally, the paper does not address potential limitations of the hybrid approach, such as scalability or sensitivity to hyperparameters."
          },
          "questions": {
            "value": [
              "What is the precise mathematical definition of 'hybrid generalized smoothness' (Definition 2.1), and how does it differ from classical smoothness assumptions?",
              "How are the dynamic gradient Lipschitz constants $L$ modeled in practice, and what guarantees does the framework provide for SGD convergence under varying $L$?",
              "What specific tasks and model architectures were used in the empirical studies, and how do the results compare quantitatively to state-of-the-art PEFT methods like LoRA or full fine-tuning?",
              "Are there any ablation studies demonstrating the individual contributions of zeroth-order LLM updates versus first-order PEFT updates?",
              "How does the proposed framework handle non-convexity or saddle points in the hybrid optimization landscape?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "8bjspmAMBk": {
    "paper_id": "8bjspmAMBk",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a novel quality metric for evaluating dynamic graph generative models (DGGMs) based on the Johnson-Lindenstrauss (JL) lemma. The proposed method avoids discretizing dynamic graphs into static snapshots, instead using random projections to create a scalar, application-agnostic measure of similarity that captures both topological and feature evolution. The authors also provide an empirical evaluation demonstrating the metric's effectiveness compared to existing approaches."
          },
          "strengths": {
            "value": "Originality: The paper presents a fresh approach by leveraging the JL lemma for dynamic graph evaluation, addressing key limitations of existing methods. Quality: The empirical analysis covers multiple metrics and properties (fidelity, diversity, etc.), showing the proposed method's advantages. Clarity: The paper is well-structured, with clear motivation and technical explanations. Significance: Dynamic graph evaluation remains underdeveloped, and this work addresses a critical gap with practical implications for applications like social networks and biology."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art methods, particularly in terms of computational efficiency and scalability. The theoretical foundation for using JL projections specifically for dynamic graphs is not thoroughly justified. The empirical evaluation appears limited to a single dataset or synthetic examples, with insufficient analysis of edge cases (e.g., graphs with irregular time intervals). The paper also does not discuss how the metric handles graphs with varying sizes or heterogeneous features."
          },
          "questions": {
            "value": "1. How does the proposed metric compare to recent DGGM evaluation methods like Temporal Graph GANs or time-aware graph embeddings? 2. What are the specific datasets used for empirical evaluation, and how do they represent real-world dynamic graph characteristics? 3. Can the authors provide theoretical guarantees for the JL-based metric's ability to preserve temporal dependencies? 4. How does the method scale to large graphs with thousands of nodes/edges, given the runtime claims in the abstract?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a novel quality metric for evaluating dynamic graph generative models (DGGMs) based on the Johnson-Lindenstrauss (JL) lemma. The method addresses limitations of existing approaches by directly analyzing continuous-time dynamic graphs without discretizing them into static snapshots, enabling a unified, scalar measure of similarity that captures both topological and feature evolution. The work also includes an empirical evaluation comparing the proposed metric to existing methods."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by applying the JL lemma to dynamic graphs, a novel approach that directly tackles temporal dependencies and unified measurement. The theoretical foundation is robust, with clear motivation for overcoming limitations of discretization. The empirical evaluation is comprehensive, addressing key properties like fidelity and computational efficiency. The paper also provides a clear connection to prior work in static graph metrics, showing how their approach extends existing ideas."
          },
          "weaknesses": {
            "value": "The empirical evaluation lacks specific details about datasets, baselines, and metrics used for comparison, making it difficult to assess the practical impact. The paper does not address potential edge cases where the JL-based metric might fail (e.g., extreme temporal sparsity or high-dimensional features). The implementation link is provided, but no ablation studies or sensitivity analysis are mentioned to validate the choice of parameters or projection dimensions."
          },
          "questions": {
            "value": "1. How does the JL-based metric handle dynamic graphs with irregular time intervals or sparse events? 2. What specific baselines were used in the empirical evaluation, and how do they compare in terms of computational cost? 3. Are there scenarios where the proposed metric might underestimate or overestimate similarity, and how is this mitigated? 4. Could the authors provide more details about the datasets and experimental setup to enable reproducibility?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a novel quality metric for evaluating dynamic graph generative models (DGGMs) based on the Johnson-Lindenstrauss (JL) lemma. The proposed method uses random projections to directly compare dynamic graphs as continuous-time sequences, addressing limitations of existing approaches that rely on discrete snapshots. The authors also perform a comprehensive empirical evaluation of metrics for continuous-time dynamic graphs, demonstrating the effectiveness of their approach."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by applying the JL lemma to dynamic graph evaluation, a novel combination that addresses key limitations of prior work. The methodology is theoretically grounded and well-structured, with clear motivation for overcoming issues like temporal dependency modeling and feature-topology integration. The empirical evaluation is thorough, covering multiple metrics and properties (fidelity, diversity, etc.). The paper is well-written, with clear problem formulation and contextualization of related work. Its contribution is significant for advancing DGGM evaluation, which is critical for real-world applications."
          },
          "weaknesses": {
            "value": "The empirical evaluation lacks detailed comparisons with specific baselines, such as how the JL-based metric outperforms discrete metrics in particular scenarios. The paper does not fully address the computational complexity of the proposed method or its scalability to large graphs. The theoretical justification for using random projections in this context is somewhat superficial, with limited discussion of how the JL lemma's guarantees translate to dynamic graph similarity. The paper also does not explore the impact of hyperparameters (e.g., projection dimensions) on performance."
          },
          "questions": {
            "value": "1. How does the JL-based metric handle dynamic graphs with irregular time intervals or varying temporal resolutions? 2. What are the computational trade-offs between the proposed method and existing metrics in terms of runtime and memory? 3. Are there specific types of dynamic graph patterns (e.g., bursty interactions) where the JL metric may underperform? 4. How does the metric generalize to graphs with heterogeneous features or complex temporal dependencies?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "8ctju6iFcn": {
    "paper_id": "8ctju6iFcn",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces CT-BaB, a certified training framework for learning Lyapunov-stable neural controllers by integrating branch-and-bound during training. It dynamically maintains subregions of the input space to optimize verified bounds, enabling larger regions of attraction (ROA) and faster verification compared to counterexample-guided methods."
          },
          "strengths": {
            "value": "Originality: CT-BaB is novel in combining certified training with branch-and-bound for Lyapunov stability, addressing limitations of prior counterexample-guided approaches. Quality: Experiments on a 2D quadrotor system demonstrate significant improvements (5x faster verification, 16x larger ROA). Clarity: The paper is well-structured, with clear problem formulation, method description, and empirical validation. Significance: The work advances safety-critical control systems by providing formal guarantees, which is crucial for robotics and autonomous systems."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed theoretical analysis of the branch-and-bound mechanism's convergence or scalability to higher-dimensional systems. While experiments show improvements, comparisons to additional baselines (e.g., LQR-based methods) are limited. The computational overhead of maintaining dynamic subregions is not quantified, and the generalization to other control tasks remains unexplored."
          },
          "questions": {
            "value": [
              "How does CT-BaB scale to higher-dimensional systems beyond the 2D quadrotor? Are there theoretical guarantees on the branch-and-bound process's efficiency?",
              "What is the computational overhead of the dynamic subregion maintenance during training compared to standard methods?",
              "Could the framework be adapted to other stability criteria (e.g., forward invariance) or control tasks beyond Lyapunov stability?",
              "How sensitive is the method to hyperparameters governing subregion splitting and verification bounds?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces CT-BaB, a certified training framework that integrates branch-and-bound during training to produce Lyapunov-stable neural controllers. By optimizing verified bounds on subregions of the input space and dynamically splitting challenging subregions, the method achieves faster verification and larger regions-of-attraction compared to prior counterexample-guided approaches."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel certified training framework that combines branch-and-bound with Lyapunov stability guarantees, addressing a critical gap in verification-friendly neural controller design. Quality: The experiments on a 2D quadrotor system demonstrate significant improvements in verification speed (5X faster) and ROA size (16X larger) over baselines. Clarity: The problem formulation, methodology, and results are clearly presented with logical flow. Significance: The work addresses a key challenge in safety-critical systems, offering practical benefits for verifiable neural control in complex dynamical systems."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of specific components (e.g., branch-and-bound vs. verified bounds). The experiments are limited to a single domain (quadrotor), raising questions about generalizability. The computational overhead of the branch-and-bound procedure during training is not quantified. The theoretical analysis of convergence or stability guarantees for the CT-BaB framework is minimal."
          },
          "questions": {
            "value": [
              "How does the CT-BaB framework handle systems with non-convex dynamics or higher-dimensional state spaces beyond the quadrotor case study?",
              "What is the computational cost (e.g., training time, memory usage) of the branch-and-bound procedure compared to baseline methods?",
              "Are there theoretical guarantees that the dynamic subregion splitting ensures convergence to a globally stable controller?",
              "How sensitive is the method to hyperparameters controlling the branch-and-bound process (e.g., split criteria, subregion granularity)?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces CT-BaB, a certified training framework for neural controllers that ensures Lyapunov asymptotic stability over a large region-of-attraction (ROA). The approach combines branch-and-bound during training to dynamically partition input space into subregions, optimizing verified bounds to improve verification efficiency and ROA size. Experiments show significant improvements in verification speed and ROA magnitude compared to prior methods."
          },
          "strengths": {
            "value": "Originality: The paper presents a novel certified training framework (CT-BaB) that integrates branch-and-bound during training, addressing the lack of verification-friendly model design in prior work. Quality: The methodology is well-structured, with clear ablation studies and comparisons to baselines. Clarity: The problem statement and technical contributions are clearly articulated, though some details are condensed. Significance: The work addresses a critical gap in safety-critical control systems by enabling formally verified neural controllers with practical scalability."
          },
          "weaknesses": {
            "value": "Limited generalizability: The experiments focus on a specific 2D quadrotor system, leaving the framework's applicability to higher-dimensional or diverse dynamical systems untested. Theoretical guarantees: The paper lacks rigorous analysis of the branch-and-bound's convergence or the optimality of subregion splitting criteria. Comparison gaps: Key baseline methods (e.g., LQR, SOS) are not directly compared in terms of ROA size or verification efficiency. Implementation details: The dynamic dataset maintenance and split criteria are described conceptually but lack quantitative justification for their design choices."
          },
          "questions": {
            "value": [
              "How does CT-BaB scale to higher-dimensional systems or more complex dynamical models beyond the 2D quadrotor case study?",
              "What specific criteria determine the input dimension for splitting subregions, and how are these chosen to optimize the training objective?",
              "Can the framework handle non-convex ROAs or systems with discontinuous dynamics, which are common in real-world control scenarios?",
              "How does the computational overhead of the branch-and-bound process affect training time compared to standard counterexample-guided methods?",
              "Are the verified bounds tight enough to ensure practical usability, or do they introduce conservatism that limits the ROA size?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "8eKMxc1SXg": {
    "paper_id": "8eKMxc1SXg",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces a kurtosis concentration (KC) loss to improve image quality in diffusion models by leveraging the natural image statistic that kurtosis values remain nearly constant across band-pass filtered versions (e.g., DWT). It also proposes a condition-agnostic perceptual guidance strategy. The approach is validated across three tasks: few-shot finetuning, unconditional generation, and super-resolution, showing improvements in FID, MUSIQ, and user studies."
          },
          "strengths": {
            "value": "Originality: The application of kurtosis concentration from natural image statistics to diffusion models is novel and creatively bridges signal processing with generative AI. Quality: The methodology is theoretically grounded, with clear definitions of kurtosis and its relationship to image quality. Clarity: The paper is well-structured, with detailed explanations of the KC property and experiments. Significance: Improving image quality is critical for generative models, and the generic, label-free nature of the KC loss makes it broadly applicable."
          },
          "weaknesses": {
            "value": "The paper lacks detailed implementation specifics for the KC loss (e.g., how band-pass filtering is applied, whether DWT/DCT is used, or how kurtosis is computed across scales). The experiments compare against baseline diffusion models but do not include state-of-the-art methods like those using advanced perceptual losses or adversarial training. The theoretical justification for why reducing kurtosis variance improves quality is superficial, and the perceptual guidance strategy is not sufficiently explained. The ablation studies on the KC loss and perceptual guidance components are missing."
          },
          "questions": {
            "value": "1. How exactly is the kurtosis computed across band-pass versions (e.g., DWT levels, windowing, normalization)? 2. What specific band-pass transform (DWT/DCT) is used, and why? 3. Are there ablation studies showing the contribution of the KC loss vs. perceptual guidance? 4. How does the condition-agnostic perceptual guidance work without relying on text/class conditioning? 5. Why is the KC loss effective for tasks like super-resolution, where structural fidelity is critical? 6. How does the KC loss interact with existing diffusion model objectives (e.g., denoising) in practice?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces a kurtosis concentration (KC) loss and a condition-agnostic perceptual guidance strategy to improve image quality in diffusion models. The KC loss leverages the statistical property of natural images having nearly constant kurtosis across band-pass transformations, validated across tasks like text-to-image generation, super-resolution, and unconditional generation."
          },
          "strengths": {
            "value": "Originality: The KC loss is a novel approach grounded in natural image statistics, offering a generic method applicable to diverse diffusion tasks. Quality: Experiments across multiple tasks (few-shot finetuning, super-resolution, unconditional generation) demonstrate consistent improvements in perceptual quality. Clarity: The paper is well-structured, with clear motivation and methodological explanations. Significance: The work addresses a critical gap in generative AI—improving image realism—and provides practical, label-free enhancements."
          },
          "weaknesses": {
            "value": "The paper lacks detailed justification for choosing Discrete Wavelet Transform (DWT) over other band-pass filters like DCT. The computational cost of computing kurtosis across band-pass versions is not analyzed. The user study details (e.g., sample size, metrics) are insufficient. The theoretical analysis of the KC property is minimal, with only an incomplete lemma provided. The paper does not compare against alternative quality-enhancement methods (e.g., adversarial training, perceptual losses)."
          },
          "questions": {
            "value": [
              "Why was DWT specifically chosen over other band-pass transforms (e.g., DCT) for kurtosis computation? What are the trade-offs?",
              "How does the computational overhead of the KC loss affect training/inference speed? Are there optimizations for real-time applications?",
              "What ablation studies were conducted to validate the individual contributions of the KC loss and perceptual guidance?",
              "Could the KC loss be combined with existing techniques (e.g., CLIP-based guidance) for further improvements?",
              "How were the user study participants selected, and what specific metrics (e.g., Likert scale) were used to quantify perceptual quality?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a kurtosis concentration (KC) loss to improve image quality in diffusion models by leveraging the statistical property of natural images having consistent kurtosis across band-pass filters. The method is combined with a condition-agnostic perceptual guidance strategy and validated across tasks like image generation, super-resolution, and few-shot finetuning, showing improvements in FID and perceptual quality."
          },
          "strengths": {
            "value": "The paper presents a novel approach by applying kurtosis statistics to diffusion models, which is both theoretically grounded and broadly applicable. The method's generality across tasks (e.g., super-resolution, text-to-image) and the inclusion of user studies add to its significance. The clarity of the problem statement, figures, and experimental setup is strong, with clear motivation for the KC loss. The work addresses a critical gap in generative AI: improving perceptual image quality beyond realism."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing methods that use statistical priors (e.g., DCT/DWT-based losses) or perceptual guidance strategies. The theoretical justification for why kurtosis relates to perceptual quality is superficial, with limited analysis of how the KC loss reduces artifacts. The condition-agnostic perceptual guidance strategy is mentioned but not elaborated, leaving unclear how it integrates with the diffusion process. Additionally, the experiments do not include ablation studies on the KC loss's components or sensitivity analysis."
          },
          "questions": {
            "value": "How does the KC loss compare to other statistical priors (e.g., DCT-based losses) in terms of effectiveness and computational cost? What is the exact mechanism linking kurtosis variance to perceptual quality, and how does the KC loss address this? Can the condition-agnostic perceptual guidance be formalized or tested with ablation studies? Are there scenarios where the KC loss might degrade performance, and how are these mitigated?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "8ibaVk4mU8": {
    "paper_id": "8ibaVk4mU8",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces COARSE CORRESPONDENCES, a training-free method that enhances 3D and temporal understanding in multimodal language models (MLLMs) by using lightweight tracking models to identify object correspondences across 2D images. This approach improves performance on benchmarks like ScanQA, EgoSchema, and R2R without modifying the model architecture or requiring task-specific fine-tuning."
          },
          "strengths": {
            "value": "The paper's originality lies in its simple yet effective method for improving 3D/4D understanding in MLLMs without architectural changes or fine-tuning. The experiments are comprehensive, demonstrating consistent gains across open-source and closed-source models, as well as generalization to unseen datasets. The efficiency of the approach (e.g., reducing computational costs by using fewer input frames) and its universality across different model types are significant strengths. The paper also addresses a critical bottleneck in MLLMs' visual intelligence through a novel diagnostic benchmark (SOT) to analyze camera motion bias."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed analysis of why COARSE CORRESPONDENCES works, such as how visual prompting interacts with MLLM internals or the limitations of the tracking model. The sparsification step's impact on performance is not thoroughly discussed, and the choice of the tracking model (e.g., Tracking Anything) is not justified. The diagnostic SOT benchmark is under-described, and the paper does not explore failure cases or edge scenarios where the method might falter. Additionally, the method's dependence on high-quality instance segmentation masks could limit its applicability to noisy or complex real-world data."
          },
          "questions": {
            "value": "1. What specific tracking model (e.g., Tracking Anything) was used, and how does its performance affect the method's effectiveness? 2. How is the sparsification step implemented (e.g., frame sampling strategy, criteria for selecting 'prominent' correspondences)? 3. Are there scenarios where COARSE CORRESPONDENCES fails, and how does the method handle occlusions or dynamic scenes? 4. How does the visual prompting mechanism interact with the MLLM's existing vision-language alignment? 5. What ablation studies were conducted to isolate the contributions of tracking, sparsification, and visualization?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces COARSE CORRESPONDENCES, a training-free visual prompting method that enhances 3D and temporal understanding in multimodal language models (MLLMs) by leveraging object-level correspondences between 2D images. The approach uses a lightweight tracking model to identify coarse correspondences, which are then visualized as prompts to improve model performance on tasks requiring spatial and temporal reasoning."
          },
          "strengths": {
            "value": "The paper's main strengths include its simplicity and effectiveness in improving 3D and temporal understanding without architectural changes or fine-tuning. The method demonstrates consistent gains across multiple benchmarks (ScanQA, EgoSchema, R2R, etc.) and generalizes to both closed-source and open-source models. The idea of using visual prompting based on coarse correspondences is novel and addresses a critical gap in MLLMs' spatial-temporal capabilities. The experimental validation is extensive, covering diverse tasks and datasets."
          },
          "weaknesses": {
            "value": "The paper lacks critical details about the tracking model's implementation (e.g., whether it is pre-trained, its computational cost, or how it handles dynamic scenes). The sparsification and correspondence selection steps are not thoroughly explained, making it difficult to assess their robustness. Ablation studies are missing, so the contribution of each component remains unclear. The comparison with other sparse-input methods is limited, and the SOT benchmark's construction and validation are under-specified. The paper also does not address potential limitations in complex or dynamic environments."
          },
          "questions": {
            "value": "1. How is the tracking model (e.g., 'Tracking Anything') integrated into the visual prompting pipeline? Is it pre-trained or fine-tuned? 2. What specific metrics or criteria are used to sparsify frames and select coarse correspondences? 3. Are there ablation studies showing the contribution of each step (tracking, sparsification, visualization)? 4. How does COARSE CORRESPONDENCES compare to other methods that also use sparse input frames (e.g., frame sampling strategies)? 5. What are the computational costs of the tracking model, and how does this affect the overall efficiency of the method? 6. How is the SOT benchmark constructed, and what validation ensures its effectiveness in testing spatial orientation?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces COARSE CORRESPONDENCES, a training-free visual prompting method that enhances 3D and temporal understanding in multimodal language models (MLLMs) by leveraging lightweight tracking models to identify object correspondences across frames or viewpoints. This approach improves performance on benchmarks like ScanQA, EgoSchema, and R2R without modifying model architecture or requiring task-specific fine-tuning."
          },
          "strengths": {
            "value": "Originality: The method creatively applies visual correspondence tracking to MLLMs, offering a novel alternative to specialized architectures or fine-tuning. Quality: Extensive experiments across 6 benchmarks demonstrate consistent gains for both closed-source (e.g., GPT-4-V/O) and open-source models. Clarity: The framework and steps (tracking, sparsification, selection, visualization) are clearly described, with concrete results. Significance: Addresses critical bottlenecks in MLLMs' spatial-temporal understanding, which is vital for real-world applications like navigation and video QA."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical justification for why coarse correspondences improve performance, such as how visual prompting interacts with MLLM internals. The tracking model's role is underexplored, including its robustness to varying camera motions or object occlusions. The SOT benchmark for spatial orientation is briefly mentioned but not thoroughly analyzed. The method's generalization to non-visual modalities or complex scenes remains untested."
          },
          "questions": {
            "value": [
              "How is the visual prompting integrated into MLLMs? Are the coarse correspondences directly embedded as input tokens or used to guide attention mechanisms?",
              "What specific design choices in the SOT benchmark ensure it effectively measures spatial orientation biases? How do the results on SOT correlate with improvements on other tasks?",
              "How does the method handle scenarios with significant camera motion variations beyond the left-right examples mentioned? Are there ablation studies on tracking model parameters?",
              "Could the gains from COARSE CORRESPONDENCES be attributed to reduced input complexity (fewer frames) rather than the correspondence tracking itself?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "8kk9joQCkc": {
    "paper_id": "8kk9joQCkc",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper formalizes the concepts of sufficiency and necessity for feature importance in machine learning models, proposing a unified framework that bridges these notions with existing methods like Shapley values and conditional independence. The authors demonstrate that combining sufficiency and necessity can reveal more comprehensive insights into feature importance compared to traditional post-hoc methods."
          },
          "strengths": {
            "value": "The paper's originality lies in its rigorous mathematical formalization of sufficiency and necessity, which are foundational yet underexplored in XAI. The methodology is sound, with clear theoretical connections to established concepts like Shapley values. The clarity of the presentation is strong, with precise notation and logical structure. The significance is high, as the work addresses critical gaps in explainability for high-stakes ML applications."
          },
          "weaknesses": {
            "value": "The experimental section is underdeveloped in the provided content, lacking specific results, baselines, or comparisons to existing methods. The practical implementation of the unified framework and its computational feasibility are not discussed. Additionally, the paper does not thoroughly address how the framework generalizes across different model architectures or data types."
          },
          "questions": {
            "value": [
              "What specific datasets and baselines were used in the experiments, and how do the results compare to state-of-the-art XAI methods?",
              "How does the unified framework handle varying model types (e.g., CNNs vs. tabular models), and are there any limitations in its applicability?",
              "What is the computational complexity of the proposed approach, and how does it scale to high-dimensional data?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper formalizes the concepts of sufficiency and necessity for feature importance in machine learning models, proposing a unified framework that spans the necessity-sufficiency axis. The authors demonstrate how existing post-hoc explanation methods often focus on sufficiency alone, and show that their unified approach can reveal more comprehensive insights into feature importance through theoretical connections to conditional independence and Shapley values."
          },
          "strengths": {
            "value": "The paper makes a clear theoretical contribution by rigorously defining sufficiency and necessity for model explanations, addressing a critical gap in XAI research. The connection to Shapley values and conditional independence provides valuable theoretical grounding. The motivation for studying the necessity-sufficiency axis is well-justified, and the paper's structure is logically organized. The problem formulation is novel in its explicit focus on the interplay between sufficiency and necessity."
          },
          "weaknesses": {
            "value": "The experimental validation is underdeveloped, with no concrete results or comparisons to existing methods. The paper lacks specific metrics for evaluating the unified framework's effectiveness. The theoretical analysis of how the unified approach improves upon existing methods is superficial. The paper does not address practical implementation challenges or computational feasibility of the proposed framework."
          },
          "questions": {
            "value": "1. How are the metrics for sufficiency and necessity defined in practice (e.g., what ρ function is used)? 2. What specific datasets and baselines were used in the experiments? 3. How does the unified framework handle high-dimensional data or complex models? 4. What quantitative measures were used to demonstrate that existing methods only capture sufficiency? 5. How does the proposed approach differ from existing game-theoretic methods like Shapley values?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper formalizes the concepts of sufficiency and necessity for explaining machine learning model predictions, proposes a unified framework that combines these notions, and demonstrates its connections to existing methods like Shapley values and conditional independence. The authors show that this unified approach can reveal more comprehensive insights into feature importance compared to traditional post-hoc methods."
          },
          "strengths": {
            "value": "The paper introduces a novel mathematical framework that systematically unifies sufficiency and necessity, addressing a critical gap in explainable AI (XAI) research. The formal definitions are rigorous and align with logical principles, while the connections to Shapley values and conditional independence provide theoretical depth. The work is significant for high-stakes applications where reliable explanations are essential, and the clear motivation for the problem underscores its relevance. The experimental claims suggest practical utility, though more details are needed."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results to substantiate its claims. For example, it does not provide specific metrics or comparisons against existing methods to quantify the improvement offered by the unified framework. The computational complexity of the proposed approach is not discussed, nor are the limitations of the framework (e.g., scalability or assumptions about the reference distribution). Additionally, the theoretical analysis of how the unified framework resolves the gaps in sufficiency/necessity definitions is incomplete."
          },
          "questions": {
            "value": [
              "How does the unified framework handle trade-offs between sufficiency and necessity in practice? Are there scenarios where one property dominates the other?",
              "What is the computational cost of implementing the unified approach compared to existing methods like Shapley values or LIME?",
              "The paper mentions experiments of 'increasing complexity' but does not provide specifics. Could the authors share details about the datasets, baselines, and evaluation metrics used?",
              "How sensitive is the framework to the choice of reference distribution $\\mathcal{V}_{S^c}$, and what strategies are recommended for selecting it?",
              "Are there cases where the unified framework might fail to identify important features, and how does it compare to methods like gradient-based explanations or perturbation-based approaches?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "8lwWBSa1pJ": {
    "paper_id": "8lwWBSa1pJ",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a Time-Aware World Model that explicitly incorporates temporal dynamics by conditioning on the time step size Δt. The model uses a 4th-order Runge-Kutta method to enforce dynamical properties and trains with diverse Δt values sampled log-uniformly. It claims improved performance across varying observation rates in control tasks without additional data."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel approach by conditioning dynamic modeling on Δt, addressing a critical gap in existing world models. Quality: The methodology leverages the RK4 integrator and Nyquist-Shannon sampling theory, providing a solid theoretical foundation. Clarity: The paper is well-structured with clear problem formulation and motivation. Significance: The focus on real-world applicability, where observation rates vary, addresses an important practical challenge in model-based RL."
          },
          "weaknesses": {
            "value": "The related work section is incomplete, making it difficult to assess the novelty relative to prior art like MTS3. The experiments are truncated, leaving key details about baseline comparisons and ablation studies missing. The paper lacks analysis of computational costs associated with varying Δt. The theoretical justification for using log-uniform Δt sampling is not thoroughly explained."
          },
          "questions": {
            "value": [
              "How does the proposed method differ from the Multi-Time Scale World Models (MTS3) mentioned in the related work? What are the key technical distinctions?",
              "What is the computational overhead of training with diverse Δt values compared to fixed Δt? Are there efficiency trade-offs?",
              "Are there ablation studies demonstrating the effectiveness of log-uniform Δt sampling versus other distributions?",
              "How does the model handle tasks with highly variable dynamics across different Δt ranges? Are there limitations in this approach?",
              "The paper mentions Meta-World experiments but does not provide quantitative results. How do the empirical results compare to baselines in terms of metrics like reward or planning accuracy?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a time-aware world model that explicitly incorporates temporal dynamics by conditioning on the time step size (Δt). The model is trained across diverse Δt values, enabling it to adapt to varying observation rates in real-world control tasks. The approach leverages the RK4 method to enforce dynamical properties and draws on the Nyquist-Shannon sampling theorem to justify training with mixed time steps."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in model-based RL by highlighting the limitations of fixed Δt in world models. The theoretical foundation using the Nyquist-Shannon theorem is novel and well-reasoned. The proposed method's integration of RK4 for dynamical consistency and the idea of training with mixed Δt values offer original contributions. The empirical focus on real-world applicability and the potential to bridge simulation-real-world gaps are significant strengths."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete due to the paper being cut off, making it impossible to assess the thoroughness of comparisons with baselines. Key details such as specific control tasks, metrics, and ablation studies on the RK4 component are missing. The claim that the model outperforms baselines without additional data lacks concrete evidence. The theoretical justification, while plausible, requires stronger empirical grounding to demonstrate practical benefits."
          },
          "questions": {
            "value": [
              "What specific baselines were used for comparison, and how do they relate to prior work like MTS3 or TD-MPC2?",
              "Which Meta-World tasks were evaluated, and what are the exact Δt values tested (e.g., 0.03 vs. 0.0025)?",
              "How does the model handle extreme cases of low observation rates (e.g., Δt = 0.1)?",
              "Are there ablation studies showing the impact of the RK4 modification versus the baseline TD-MPC2?",
              "What is the computational overhead of training with mixed Δt values, and how does it affect scalability?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces a time-aware world model (M^T) that explicitly incorporates temporal dynamics by conditioning on the time step size Δt. Unlike traditional models that use a fixed Δt, M^T trains on diverse Δt values, enabling it to capture both high- and low-frequency task dynamics. The approach leverages the 4th-order Runge-Kutta method and the Nyquist-Shannon sampling theorem to improve performance across varying observation rates."
          },
          "strengths": {
            "value": "The paper's originality lies in addressing the critical yet overlooked role of Δt in world models, offering a novel framework for adaptive temporal learning. The theoretical motivation from the Nyquist-Shannon theorem provides a principled basis for varying Δt during training. The clarity of the problem statement and the practical relevance to real-world control tasks are strong. The significance is underscored by the potential to bridge simulation-real-world gaps in model-based RL."
          },
          "weaknesses": {
            "value": "The empirical evaluation lacks depth, with limited details on baseline comparisons, specific control tasks, and ablation studies. The integration of the Runge-Kutta method is not thoroughly explained, leaving questions about its implementation. The paper also does not address how M^T handles extreme Δt values or long-term temporal dependencies. Additionally, the theoretical justification for the Nyquist-Shannon connection is somewhat superficial."
          },
          "questions": {
            "value": [
              "How does the model handle scenarios with Δt values outside the training range? Are there any limitations in extrapolation?",
              "What specific modifications were made to the TD-MPC2 framework to incorporate the Runge-Kutta method? Could this introduce additional computational overhead?",
              "Were there ablation studies to validate the effectiveness of training on diverse Δt values versus fixed Δt? How does the log-uniform sampling distribution impact performance?",
              "How does the proposed method compare to prior work like MTS3, which also considers temporal gaps? What are the key differences in their approaches?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "8sSqNntaMr": {
    "paper_id": "8sSqNntaMr",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a framework for training efficient LLM routers that dynamically select between strong and weak models during inference. The approach leverages human preference data and data augmentation to achieve cost savings of over 2x without sacrificing response quality, with demonstrated generalization across unseen model pairs."
          },
          "strengths": {
            "value": "The paper presents a novel framework for LLM routing that addresses a critical practical challenge in deploying large language models. The use of human preference data and data augmentation techniques shows clear methodological innovation. The experimental validation on multiple benchmarks demonstrates strong empirical performance. The paper's contribution to open-source tools for router training and evaluation adds practical value. The clear problem formulation and comparison with related work highlight the significance of the research."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how the router's decision boundaries are learned from preference data, particularly how the win prediction model differentiates between query complexities. The generalization claims are supported primarily by qualitative results rather than quantitative ablation studies on out-of-distribution model pairs. The data augmentation strategies are not thoroughly described, making it difficult to assess their specific contributions. The comparison with existing methods like Hybrid-LLM and Zooter is somewhat superficial, with limited quantitative analysis of performance differences."
          },
          "questions": {
            "value": [
              "How exactly is the win prediction model trained? What features does it use to differentiate between queries that should be routed to strong vs. weak models?",
              "The paper claims strong generalization across unseen model pairs - what specific metrics or experiments demonstrate this? Are there quantitative results showing performance on model pairs not used during training?",
              "The data augmentation techniques are mentioned but not detailed. Could the authors provide more information about the specific augmentation strategies used and their impact on performance?",
              "How does the proposed framework compare to existing methods in terms of latency? While the paper mentions single-LLM routing, the actual inference speed improvements are not quantified."
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a framework for training efficient LLM routers that dynamically select between strong and weak models during inference, leveraging human preference data and data augmentation to achieve cost savings without significant quality loss. The approach is evaluated on public benchmarks, showing over 2x cost reduction and generalization across model pairs."
          },
          "strengths": {
            "value": "The paper addresses a practical problem of balancing LLM performance and cost, which is highly relevant for real-world deployment. The use of human preference data and data augmentation is novel compared to prior work relying on synthetic labels or fixed model ensembles. The framework's generalization to unseen LLMs and domains is a significant contribution. The paper is well-structured with clear problem formulation and experimental setup."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies on data augmentation techniques, making it unclear how much they contribute to performance gains. The generalization claims are not thoroughly validated beyond the mentioned benchmarks, and the evaluation of out-of-domain scenarios is limited. The framework's scalability to more than two models is not discussed. The comparison with existing methods like Hybrid-LLM and Zooter is superficial, and the paper does not address potential limitations in human preference data quality or bias."
          },
          "questions": {
            "value": "1. What specific data augmentation techniques are used, and how do they compare to baselines? 2. How is the generalization to unseen LLMs quantitatively validated beyond the described benchmarks? 3. Are there any limitations in the human preference data sources (e.g., domain coverage, bias)? 4. How does the framework handle queries that require multiple model interactions, which the paper does not address? 5. What is the computational overhead of the router compared to direct model deployment?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes a framework for training efficient LLM routers that dynamically select between strong and weak models during inference, leveraging human preference data and data augmentation. The approach aims to reduce costs by over 2x without sacrificing response quality, with demonstrated generalization across different LLM pairs."
          },
          "strengths": {
            "value": "The paper addresses a practical problem in LLM deployment with clear motivation. The framework's use of human preference data and data augmentation is novel compared to prior work relying on synthetic labels. The experimental results show significant cost savings and generalization capabilities, and the open-source contribution adds practical value. The problem formulation is well-structured, and the paper highlights key challenges in LLM routing."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing routing methods (e.g., LLM-Blender, Zooter) to quantify improvements. The data augmentation techniques are not thoroughly described, making it hard to assess their contribution. The generalization claims are supported by limited evidence, as the paper does not explicitly test routers on LLM pairs not seen during training. The experimental setup lacks ablation studies on key components like the win prediction model. The cost metrics are not fully contextualized (e.g., latency trade-offs, inference time)."
          },
          "questions": {
            "value": [
              "How does the proposed framework compare to existing routing methods like LLM-Blender or Zooter in terms of cost-performance trade-offs?",
              "What specific data augmentation strategies were used, and how were they validated through ablation studies?",
              "How was the generalization capability tested on LLM pairs not included in training? Were there any failure cases?",
              "How are the cost savings calculated? Are they based on inference time, computational resources, or monetary costs?",
              "What is the exact definition of 'response quality' in the experiments, and how is it measured beyond benchmark scores?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "8sglLco8Ti": {
    "paper_id": "8sglLco8Ti",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces ChunkKV, a novel key-value (KV) cache compression method for long-context large language models (LLMs) that preserves semantic chunks instead of discrete tokens. By grouping related tokens and leveraging layer-wise index reuse, ChunkKV aims to reduce GPU memory usage while maintaining semantic information. The method is evaluated on benchmarks like LongBench and GSM8K, claiming state-of-the-art performance with 10% compression."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in LLM inference efficiency by proposing a semantic-preserving compression approach, which is a significant departure from prior discrete token-based methods. The layer-wise index reuse technique is a novel idea that could improve computational efficiency. The experiments on relevant benchmarks and the claim of superior performance over existing methods highlight the practical relevance. The paper is well-structured and clearly articulates the motivation and contributions."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of how semantic chunks are identified and grouped, which is critical for reproducibility. The experimental results are not thoroughly analyzed—e.g., it is unclear how ChunkKV's performance compares to non-compression baselines or other state-of-the-art methods beyond the claimed 'state-of-the-art' assertion. The theoretical justification for why chunking preserves semantics is missing. Additionally, the evaluation metrics and ablation studies are not sufficiently detailed."
          },
          "questions": {
            "value": "1. How are semantic chunks defined and grouped? What criteria or models are used to determine chunk boundaries? 2. What is the exact computational overhead of the chunking process, and how does it compare to existing methods? 3. Are the experiments conducted with the same hyperparameters as baseline methods for a fair comparison? 4. How does the layer-wise index reuse technique specifically reduce computational overhead? 5. What is the trade-off between compression ratio and performance on different benchmarks?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces ChunkKV, a novel key-value (KV) cache compression method for long-context language models (LLMs) that preserves semantic information by grouping related tokens into chunks rather than pruning discrete tokens. The approach includes a layer-wise index reuse technique to reduce computational overhead, demonstrating state-of-the-art performance on benchmarks like LongBench and GSM8K with a 10% compression ratio."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in LLM inference—semantic information loss in KV cache compression—by proposing a novel chunk-based approach. The method is simple yet effective, with clear contributions to both compression efficiency and semantic preservation. The experimental evaluation on relevant benchmarks and the introduction of layer-wise index reuse show practical relevance. The paper is well-structured, with a clear problem statement, motivation, and comparison to prior work."
          },
          "weaknesses": {
            "value": "The paper lacks detailed quantitative results to substantiate claims of 'state-of-the-art performance' and 'superior semantic preservation.' The comparison to prior methods in Table 1 is incomplete, with no metrics provided. The mechanism for determining semantic chunks is not explained, and the theoretical justification for why chunking improves performance is missing. Additionally, the experiments appear to be limited to a small set of models and tasks, raising questions about generalizability."
          },
          "questions": {
            "value": "1. What specific metrics (e.g., accuracy, latency, memory savings) demonstrate ChunkKV's superiority over existing methods? 2. How are semantic chunks defined and extracted—what criteria or models are used for grouping tokens? 3. Are there ablation studies showing the contribution of each component (e.g., chunking vs. index reuse)? 4. How does ChunkKV perform under varying compression ratios or with different model architectures? 5. What is the exact computational overhead reduction from layer-wise index reuse, and how does it scale with model depth?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces ChunkKV, a KV cache compression method for long-context LLM inference that preserves semantic chunks instead of discrete tokens. It proposes a layer-wise index reuse technique to reduce computational overhead and claims superior performance over existing methods, including outperforming full KV cache with 10% compression."
          },
          "strengths": {
            "value": "The paper identifies a critical limitation in existing KV cache compression methods (semantic information loss) and proposes a novel chunk-based approach. The layer-wise index reuse technique addresses computational efficiency, showing potential for practical deployment. The experimental setup on standard benchmarks (LongBench, NIAH, GSM8K) and multiple models suggests relevance to real-world applications. The clear problem statement and motivation are strengths."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology for semantic chunking (e.g., how chunks are defined, trained, or validated). Experimental results are incomplete (the text is cut off), and there is no comparison against all baseline methods. The claim of surpassing full KV cache lacks context (e.g., latency vs. accuracy trade-offs). The paper does not discuss limitations, edge cases, or ablation studies. The theoretical justification for semantic preservation is superficial."
          },
          "questions": {
            "value": "1. How are semantic chunks defined and validated? What metrics or mechanisms ensure semantic coherence? 2. What is the exact implementation of layer-wise index reuse? How is similarity between layers measured? 3. Are there ablation studies showing the contribution of each component (chunking vs. index reuse)? 4. How does ChunkKV handle varying context lengths or heterogeneous content? 5. What is the computational overhead of the index reuse technique compared to baseline methods? 6. Why does ChunkKV outperform the full KV cache? Is this consistent across all benchmarks?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "960Ny6IjEr": {
    "paper_id": "960Ny6IjEr",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes LLRC, a gradient-based method for low-rank compression of language models by learning singular value selection masks through a calibration dataset. It claims to outperform existing fine-tuning-free methods like STRS and ARS, achieving better performance on downstream tasks without post-compression fine-tuning."
          },
          "strengths": {
            "value": "The paper introduces a novel approach (LLRC) for layer-wise rank selection in low-rank matrix decomposition, addressing a critical challenge in model compression. The method's gradient-based optimization of singular value masks is theoretically sound and potentially impactful. The experiments demonstrate competitive results on multiple large language models and tasks, suggesting practical relevance. The paper also provides a clear motivation for the problem and contextualizes its contributions within existing compression techniques."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient experimental details to validate its claims. Key questions remain: (1) How does LLRC compare to gradient-based methods like ARS in terms of performance without fine-tuning? (2) Are the reported improvements statistically significant? (3) What is the exact implementation of the 'any-k' singular value selection? (4) The paper does not analyze the computational efficiency of LLRC or its scalability to larger models. (5) The calibration dataset size (3k documents) is not justified, and its impact on generalization is unclear."
          },
          "questions": {
            "value": [
              "What is the exact formulation of the multi-objective loss function used in LLRC? How is the trade-off between compression and performance balanced?",
              "Why does LLRC outperform ARS without requiring post-compression fine-tuning? Is this due to the proposed mask architecture or other factors?",
              "Are the results consistent across different types of layers (e.g., attention vs. feed-forward) in the models?",
              "How does the method handle varying compression rates? Is there a theoretical analysis of the rank selection process?",
              "What ablation studies were conducted to validate the components of LLRC (e.g., mask learning vs. calibration dataset size)?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces LLRC, a gradient-based method for low-rank compression of language models that learns optimal singular value selection masks without requiring fine-tuning. The approach uses a small calibration dataset to train masks that balance compression rate and downstream task performance, outperforming existing heuristic and gradient-based methods on multiple benchmarks."
          },
          "strengths": {
            "value": "Originality: LLRC presents a novel gradient-based framework for rank selection in low-rank decompositions, addressing a key challenge in model compression. Quality: The experiments are comprehensive, comparing against multiple state-of-the-art methods across diverse models and tasks. Clarity: The paper is well-structured with clear explanations of the methodology and results. Significance: The problem of efficient model compression is critical for deploying large language models, and LLRC offers a practical solution with strong empirical performance."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed theoretical analysis of why the proposed loss function effectively balances compression and performance. The experiments focus on specific tasks (e.g., MMLU, NQ-Open) and models (e.g., Llama-2, Llama-3), limiting generalizability. The calibration dataset size (3k documents) is not thoroughly justified, and its impact on performance remains unclear. Additionally, the paper does not address potential limitations of the mask-based approach, such as sensitivity to calibration data distribution."
          },
          "questions": {
            "value": [
              "What is the theoretical basis for the multi-objective loss function used in LLRC? How does it ensure optimal trade-offs between compression and performance?",
              "How sensitive is LLRC to the size and quality of the calibration dataset? Could the results be affected by distributional shifts in real-world deployment scenarios?",
              "What is the computational overhead of training the singular value masks compared to existing methods? Does LLRC scale to larger models beyond the tested configurations?",
              "Are there ablation studies demonstrating the contribution of individual components (e.g., the mask architecture, loss terms) to the overall performance?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes LLRC, a gradient-based method for low-rank compression of language models that learns optimal per-layer singular value selection masks without requiring fine-tuning. The approach uses a multi-objective loss function to balance compression and task performance, validated on multiple large language models across common-sense reasoning and open-domain QA tasks."
          },
          "strengths": {
            "value": "Originality is strong through the combination of gradient-based rank learning and fine-tuning-free compression. The method addresses a critical challenge in model compression by directly optimizing layer-specific ranks. Experiments demonstrate consistent improvements over existing techniques. Clarity is high with clear problem formulation and structured presentation. Significance lies in enabling efficient deployment of large models without performance trade-offs."
          },
          "weaknesses": {
            "value": "The paper lacks ablation studies on key components like the multi-objective loss function. The calibration dataset size (3k documents) is small but not thoroughly analyzed for sensitivity. Theoretical justification for the mask learning mechanism is limited. Experimental comparisons could include more baselines (e.g., recent quantization methods). The paper is cut off, missing parts of the related work and methodology sections."
          },
          "questions": {
            "value": "1. How sensitive is LLRC to calibration dataset size/quality? 2. What are the computational costs of training the masks compared to baselines? 3. Can the method generalize to non-LLM architectures? 4. How does the multi-objective loss balance compression vs. performance in different scenarios? 5. What's the impact of the 'any-k' singular value selection strategy on different layers?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "98ASXp6oPg": {
    "paper_id": "98ASXp6oPg",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces SEK (Self-Explained Keywords), a technique that enhances code generation by having LLMs extract and explain problem-specific keywords using their own knowledge. The approach involves identifying low-frequency keywords in problem descriptions, re-explaining them with high-frequency terms, and appending these explanations to prompts to improve code generation accuracy."
          },
          "strengths": {
            "value": "Originality: SEK's self-explanation mechanism leverages LLMs' inherent capabilities to address undertrained keywords, offering a novel approach compared to external knowledge integration. Quality: Experiments on three benchmarks (HumanEval, MBPP, APPS) with five LLMs demonstrate consistent performance improvements, such as an 8.8% relative gain on Llama-3.1. Clarity: The methodology is well-structured with visualizations (e.g., Figure 2) and detailed examples. Significance: Code generation is a critical task, and SEK's ability to enhance LLMs' understanding of low-frequency terms addresses a practical limitation in existing models."
          },
          "weaknesses": {
            "value": "The paper lacks technical details on how the frequency-based ranking algorithm is implemented and validated. The ablation studies are superficial, with no explicit analysis of how individual components (e.g., keyword extraction vs. re-ranking) contribute to performance. The comparison to Chain of Thought (CoT) is brief, and the differences in strategies are not thoroughly discussed. Additionally, the paper does not address potential limitations, such as cases where SEK fails or the computational overhead of the process."
          },
          "questions": {
            "value": "1. How is the frequency-based ranking algorithm implemented? What metrics are used to identify undertrained keywords? 2. Are there specific scenarios where SEK's improvements are less pronounced, and why? 3. How does SEK handle ambiguous or context-dependent keywords that require deeper semantic analysis? 4. What is the computational cost of the keyword extraction and explanation process, and how does it scale with larger problem descriptions? 5. How does the paper ensure consistency between the formalized explanations and the test cases provided in the benchmarks?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes SEK (Self-Explained Keywords), a technique where large language models (LLMs) automatically identify and explain problem-specific keywords in code generation tasks. By augmenting prompts with these explanations, SEK aims to improve code generation accuracy, particularly for low-frequency or undertrained terms. The method leverages the LLM's own knowledge to rephrase and prioritize key terms, demonstrated to yield significant performance gains on code generation benchmarks."
          },
          "strengths": {
            "value": "Originality: SEK introduces a novel approach to code generation by leveraging LLMs' self-explanation capabilities, distinct from prior methods relying on external knowledge. Quality: The paper presents experiments across three benchmarks with multiple LLMs, showing consistent improvements (e.g., 8.8% relative gain on Llama-3.1). Clarity: The methodology and motivation are well-articulated, with illustrative examples. Significance: Enhancing code generation accuracy has broad implications for software development, and the results suggest meaningful practical impact."
          },
          "weaknesses": {
            "value": "The paper is truncated, omitting critical details about the frequency-based ranking algorithm, the exact prompt design, and ablation study results. Experimental validation lacks depth, such as statistical significance tests or analysis of failure cases. The claim that SEK shifts attention to high-frequency terms is not substantiated with concrete evidence (e.g., attention visualization). The paper does not address potential limitations, such as the method's effectiveness on non-English code or complex programming paradigms."
          },
          "questions": {
            "value": "1. How is the frequency-based ranking algorithm implemented? What criteria determine which keywords are prioritized? 2. Are the explanations generated by the LLM aligned with standard programming definitions, or do they risk introducing errors? 3. What is the computational overhead of the SEK process, and how does it scale to larger codebases? 4. How does SEK compare to existing methods like Chain of Thought (CoT) in terms of implementation complexity and effectiveness? 5. Are there specific types of keywords (e.g., domain-specific terms) where SEK performs poorly?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes SEK (Self-Explained Keywords), a technique that enhances code generation by having LLMs automatically extract and explain problem-specific keywords using their own knowledge. The method addresses the issue of LLMs overlooking undertrained keywords by augmenting prompts with these explanations, leading to improved code generation performance across multiple benchmarks."
          },
          "strengths": {
            "value": "Originality is evident in leveraging LLMs' internal knowledge for keyword explanation rather than external data. The methodology is well-structured, with comprehensive experiments across three benchmarks and five models. The paper clearly connects the problem of long-tail distributions in training data to the proposed solution. Clarity is maintained through detailed examples and a logical flow. The significance lies in addressing a practical limitation of LLMs in code generation, with measurable improvements in pass rates."
          },
          "weaknesses": {
            "value": "The paper lacks detailed implementation specifics of the frequency-based ranking algorithm and how LLMs identify 'key terms.' The mechanism for prioritizing low-frequency keywords is under-described, making it hard to replicate. The comparison with Chain of Thought (CoT) is superficial; the paper does not sufficiently clarify how SEK's strategy differs in practice. Additionally, the analysis of attention shifts is limited to qualitative claims without quantitative validation."
          },
          "questions": {
            "value": "1. How does the frequency-based ranking algorithm determine which keywords to prioritize? 2. What criteria does the LLM use to identify 'key terms' in the problem description? 3. Are there cases where SEK might degrade performance, and how are these handled? 4. How does the method scale to non-English or less common programming languages? 5. Can the authors provide ablation studies on the impact of different explanation lengths or formats?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "98d7DLMGdt": {
    "paper_id": "98d7DLMGdt",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the inefficiency of visual autoregressive (AR) models by introducing LANTERN, a method that relaxes the acceptance condition in speculative decoding. The key challenge identified is 'token selection ambiguity,' where visual AR models assign uniformly low probabilities to tokens, hindering speculative decoding. LANTERN leverages latent space token interchangeability and incorporates a total variation distance bound to maintain image quality while accelerating generation."
          },
          "strengths": {
            "value": "Originality: The paper pioneers the application of speculative decoding to visual AR models, identifying a novel problem (token selection ambiguity) and proposing a tailored solution. Quality: The methodology is rigorous, combining theoretical analysis (total variation distance bound) with empirical validation. Clarity: The problem statement, approach, and experimental setup are well-articulated. Significance: The work addresses a critical efficiency bottleneck in AR models, with practical benefits for real-world applications."
          },
          "weaknesses": {
            "value": "The experiments are limited to LlamaGen, raising questions about generalizability. The paper lacks detailed comparisons with alternative acceleration techniques (e.g., parallel decoding or hybrid models). The 'token selection ambiguity' phenomenon is not thoroughly analyzed across diverse models or datasets. The total variation distance bound's implementation and empirical validation require more depth. The trade-off between speed and image quality is not quantified beyond qualitative examples."
          },
          "questions": {
            "value": [
              "What are the specific baseline methods used for comparison (e.g., other speculative decoding variants or non-speculative approaches)? How do they differ from the 'naive application' of EAGLE-2?",
              "How is the total variation distance computed in practice? Is it derived from the target model's distribution, and how is it enforced during decoding?",
              "Are there scenarios where LANTERN's relaxed acceptance condition could lead to semantic coherence issues or artifacts? How does the method handle such cases?",
              "How does LANTERN perform on tasks beyond image generation (e.g., video generation or multi-modal tasks)?",
              "What is the computational overhead of the total variation distance bound? Does it offset the speed gains in practice?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of accelerating visual autoregressive (AR) models using speculative decoding, which is effective in language models but underexplored in vision. The authors identify a problem called 'token selection ambiguity' where visual AR models assign uniformly low probabilities to tokens, hindering speculative decoding. They propose LANTERN, a relaxed acceptance condition leveraging latent space token interchangeability, combined with a total variation distance bound to maintain image quality. Experiments on LlamaGen show significant speedups compared to naive speculative decoding."
          },
          "strengths": {
            "value": "The paper introduces a novel problem (token selection ambiguity) specific to visual AR models, which is well-motivated and timely. The proposed solution, LANTERN, is original and addresses a critical bottleneck in applying speculative decoding to vision. The experiments demonstrate practical speed improvements without significant quality loss, and the code is publicly available. The paper also provides clear technical details on the method's design and theoretical guarantees."
          },
          "weaknesses": {
            "value": "The experiments are limited to a single model (LlamaGen) and may not generalize to other visual AR architectures. The analysis of token selection ambiguity lacks depth, such as ablation studies on why visual models exhibit uniform probabilities. The total variation distance bound's practical impact is not thoroughly validated. The paper also does not compare against alternative acceleration methods (e.g., parallel decoding or hybrid approaches), which limits the scope of the contribution."
          },
          "questions": {
            "value": [
              "How does LANTERN perform on other visual AR models beyond LlamaGen? Are the results consistent across different architectures or datasets?",
              "What is the exact mechanism behind the 'token selection ambiguity'? Are there quantitative analyses (e.g., entropy metrics) to support this claim?",
              "How does the total variation distance bound interact with different image quality metrics (e.g., FID, CLIP score)? Are there cases where the bound is insufficient?",
              "What is the trade-off between speed and diversity in generated images under LANTERN? Does the relaxed acceptance condition reduce diversity compared to strict decoding?",
              "Could the proposed method be adapted to non-AR generation paradigms (e.g., diffusion models) or other modalities?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of accelerating visual autoregressive (AR) models using speculative decoding, which is effective in LLMs but underexplored in vision. The authors identify 'token selection ambiguity' as a key problem, where visual AR models assign uniformly low probabilities to tokens, hindering speculative decoding. They propose LANTERN, a relaxed acceptance condition leveraging latent space token interchangeability, combined with a total variation distance bound to maintain quality. Experiments show speedups of 1.75×–1.82× over baseline speculative decoding on LlamaGen."
          },
          "strengths": {
            "value": "Originality is evident in applying speculative decoding to visual AR models, a novel domain for this technique. The method introduces a novel relaxation mechanism (LANTERN) to address token selection ambiguity, which is well-motivated. The experiments demonstrate significant speed improvements, and the paper is technically rigorous in its formulation. The clarity of the problem statement and proposed solution is strong, though some technical details are under-specified. The significance lies in bridging the efficiency gap between AR models and diffusion/GANs."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed analysis of why token selection ambiguity arises in visual AR models—e.g., whether it stems from the model's architecture, training objective, or codebook structure. The experiments are limited to LlamaGen, with no comparisons to other visual AR models or alternative acceleration methods. The total variation distance bound is mentioned but not thoroughly explained or validated. The paper also does not discuss the computational cost of the relaxation mechanism or its scalability to larger models. Additionally, the ablation studies on hyperparameters (e.g., relaxation threshold) are missing."
          },
          "questions": {
            "value": "1. What specific characteristics of visual AR models (e.g., codebook size, token distribution patterns) contribute to token selection ambiguity? 2. How does LANTERN perform on diverse image modalities (e.g., different resolutions, domains)? 3. What is the exact formulation of the total variation distance bound, and how is it computed in practice? 4. Are there scenarios where LANTERN degrades image quality or semantic coherence despite the bound? 5. How does the drafter model's architecture/quality impact LANTERN's effectiveness?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "9D9VoONnn6": {
    "paper_id": "9D9VoONnn6",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates the theoretical complexity of hyperparameter tuning in deep learning through a data-driven framework. It introduces a novel analysis of the dual utility function's piecewise polynomial structure, leveraging tools from differential/algebraic geometry to bound sample complexity for tasks like activation function interpolation and graph kernel tuning in GNNs."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by formalizing hyperparameter tuning as a learning problem with piecewise polynomial dual functions, a novel perspective in deep learning theory. The technical depth is impressive, with rigorous mathematical analysis combining geometry and optimization. The work addresses important practical problems (e.g., multi-task learning) and provides concrete sample complexity bounds. The theoretical framework has potential broader applicability beyond neural networks."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation to support its theoretical claims, which is critical for ICLR's focus on machine learning. The connection between abstract mathematical analysis and practical hyperparameter tuning remains under-specified. The proposed tools for analyzing discontinuities/local maxima are not illustrated with concrete examples. The comparison to existing hyperparameter tuning methods (e.g., Bayesian optimization) is absent, weakening the significance of the contribution."
          },
          "questions": {
            "value": "1. How do the theoretical sample complexity bounds translate to practical hyperparameter tuning performance? 2. What are the limitations of the piecewise polynomial assumptions in real-world neural networks? 3. How does the proposed framework compare to existing methods in terms of computational efficiency and scalability? 4. Are the mathematical tools (e.g., algebraic geometry) accessible to the broader ML community, or do they require specialized expertise?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the theoretical analysis of hyperparameter tuning in deep neural networks through a data-driven framework. The authors introduce a novel approach leveraging differential/algebraic geometry to characterize the structure of dual utility functions, deriving sample complexity bounds for specific applications like activation function interpolation and graph kernel tuning."
          },
          "strengths": {
            "value": "Originality is strong, as the paper bridges hyperparameter tuning with geometric analysis of utility functions, a novel perspective. Methodology is rigorous, combining tools from algebraic geometry and constrained optimization. Clarity is maintained through structured theorems and lemmas. Significance is high, as hyperparameter tuning remains a critical challenge in deep learning with limited theoretical understanding."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation, relying solely on theoretical analysis. The practical applicability of the sample complexity bounds is unclear, as no experiments are presented. The assumptions in the theorems (e.g., piecewise polynomial structures) may be overly restrictive for real-world neural networks. The connection between the abstract framework and concrete applications (e.g., activation interpolation) is underdeveloped."
          },
          "questions": {
            "value": [
              "How do the derived sample complexity bounds translate to practical hyperparameter tuning scenarios? Are the assumptions about piecewise polynomial structures realistic for modern deep networks?",
              "What are the computational costs of implementing the geometric analysis techniques proposed in the paper?",
              "The paper mentions applications to activation functions and graph kernels but provides no details. Can the authors elaborate on how their framework applies to these specific cases?",
              "Are the theoretical guarantees robust to noise or approximations in real-world training processes?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a theoretical framework for analyzing the sample complexity of hyperparameter tuning in deep neural networks, focusing on scenarios where the utility function exhibits piecewise polynomial structures. The authors leverage tools from differential/algebraic geometry to characterize discontinuities and oscillations in the utility function, providing bounds on its learning-theoretic complexity. They apply this analysis to two concrete problems: interpolating neural activation functions and tuning graph kernel parameters in GNNs."
          },
          "strengths": {
            "value": "The paper's originality lies in its novel application of geometric and optimization tools to hyperparameter tuning, a domain traditionally dominated by empirical methods. The theoretical analysis is rigorous, with clear connections between piecewise polynomial structures and learnability. The significance is high, as hyperparameter tuning remains a critical challenge in deep learning. While the paper is dense, the clarity of its mathematical formulations and the structured presentation of lemmas/theorems are commendable."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation, which is a major gap given its focus on sample complexity. The theoretical results depend on strong assumptions (e.g., piecewise polynomial structures) that may not hold in practical settings. The connection between the abstract geometric analysis and concrete applications (e.g., activation function interpolation) is not sufficiently explained. Additionally, the paper does not compare its bounds to existing methods or discuss practical implications."
          },
          "questions": {
            "value": "1. How do the theoretical bounds on sample complexity translate to real-world hyperparameter tuning scenarios? 2. Are the assumptions about piecewise polynomial structures of the utility function realistic for typical deep learning tasks? 3. Can the proposed framework be extended to discrete hyperparameters or non-polynomial architectures? 4. What are the computational costs of applying the geometric tools (e.g., algebraic geometry) in practice?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "9DrPvYCETp": {
    "paper_id": "9DrPvYCETp",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes Shared Recurrent Memory Transformer (SRMT), a multi-agent reinforcement learning framework that introduces a shared memory mechanism to enable implicit information exchange among agents. SRMT extends memory transformers by pooling and globally broadcasting individual working memories, aiming to improve coordination in decentralized multi-agent systems. The approach is evaluated on the Bottleneck navigation task and POGEMA benchmarks, showing competitive performance against existing methods."
          },
          "strengths": {
            "value": "The paper presents a novel approach to multi-agent coordination by leveraging shared recurrent memory, which is theoretically grounded in global workspace theory. The methodology is well-structured, with clear connections to prior work on memory transformers and multi-agent pathfinding. Experiments on controlled tasks (Bottleneck) and established benchmarks (POGEMA) demonstrate consistent performance improvements, particularly in sparse reward scenarios and generalization to longer corridors. The paper also highlights the importance of decentralization, a critical aspect for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of shared memory versus other components. The comparison with state-of-the-art methods like SCRIMP or MAMBA is limited, and it is unclear how SRMT outperforms these approaches in specific scenarios. The generalization claims (e.g., longer corridors) are not rigorously quantified. Additionally, the theoretical analysis of how shared memory reduces deadlocks or improves coordination is underdeveloped, leaving the mechanisms behind the empirical success somewhat opaque."
          },
          "questions": {
            "value": [
              "How does the shared memory mechanism specifically address deadlocks in multi-agent pathfinding compared to existing communication-based methods?",
              "What metrics were used to quantify generalization to longer corridors in the Bottleneck task, and how do they compare to baseline methods?",
              "Are there any limitations to the shared memory approach in dynamic or non-stationary environments, and how does the paper address them?",
              "Why was the global workspace theory chosen as the inspiration, and how does it directly inform the design of SRMT's memory architecture?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes the Shared Recurrent Memory Transformer (SRMT), a method for multi-agent pathfinding that uses a shared memory mechanism to enable implicit information exchange among agents. The approach is evaluated on a novel Bottleneck navigation task and the POGEMA benchmark, showing competitive performance against existing MARL and planning-based methods."
          },
          "strengths": {
            "value": "The paper presents a novel architecture (SRMT) that extends memory transformers to multi-agent settings, addressing coordination challenges in decentralized systems. The experimental validation on both synthetic and benchmark tasks demonstrates practical effectiveness, particularly in sparse reward scenarios. The connection to global workspace theory provides a theoretically grounded motivation. The paper also contextualizes its contributions within existing MARL literature, showing awareness of relevant prior work."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of the shared memory mechanism. The comparison with state-of-the-art methods like SCRIMP (which also uses transformers for communication) is incomplete. The Bottleneck task's design is not sufficiently described, making it hard to assess its relevance to real-world scenarios. The generalization claims are not rigorously validated (e.g., no statistical significance tests for performance gains). The theoretical analysis of how shared memory improves coordination is superficial."
          },
          "questions": {
            "value": [
              "How does SRMT's shared memory mechanism specifically address deadlocks compared to existing methods like MAMBA or SCRIMP?",
              "What are the exact hyperparameters and training details for the Bottleneck task? Are the results robust to parameter variations?",
              "Why is the shared memory not prone to the same scalability issues as centralized planners like RHCR?",
              "Are there any limitations to the memory size or the number of agents that SRMT can handle effectively?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces the Shared Recurrent Memory Transformer (SRMT), a novel architecture that extends memory transformers to multi-agent settings by enabling agents to share and update a global working memory. This approach aims to improve coordination in decentralized multi-agent pathfinding tasks without explicit communication protocols. The method is evaluated on a custom Bottleneck navigation task and the POGEMA benchmark, showing competitive performance against existing reinforcement learning and planning-based approaches."
          },
          "strengths": {
            "value": "The paper demonstrates originality by applying memory transformer architectures to multi-agent pathfinding, offering a novel mechanism for implicit information exchange through shared memory. The work addresses a critical challenge in MARL—coordination without explicit communication—which is both technically innovative and practically relevant. The experiments are well-structured, with clear comparisons to multiple baselines on diverse tasks. The paper also provides a thorough review of related work, contextualizing SRMT within existing MARL and memory transformer research. The potential impact lies in enabling more scalable and generalizable multi-agent systems for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contribution of the shared memory mechanism. While SRMT outperforms baselines, the analysis of why it achieves better performance (e.g., how the shared memory specifically improves coordination) is limited. The Bottleneck task is a simplified environment, and the generalization claims to longer corridors could benefit from more rigorous quantitative validation. Additionally, the comparison to planning-based methods on POGEMA is somewhat superficial, with no discussion of computational efficiency or scalability. The truncated text on page 3 suggests potential gaps in the technical description of SRMT's architecture."
          },
          "questions": {
            "value": [
              "How does SRMT handle varying numbers of agents or dynamic changes in the environment? Are there scalability limitations?",
              "What is the computational overhead of maintaining and updating the shared memory compared to other methods?",
              "Can the shared memory mechanism be adapted to tasks with partial observability beyond the Bottleneck setup?",
              "How does SRMT's performance compare to MAMBA or SCRIMP in terms of communication efficiency and collision avoidance?",
              "Are there specific failure cases where SRMT underperforms, and what are the root causes?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "9EqQC2ct4H": {
    "paper_id": "9EqQC2ct4H",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the problem of attributing global properties of diffusion models to data contributors using Shapley value estimation. The authors propose an efficient framework that leverages model pruning and fine-tuning to reduce the computational cost of retraining and inference, enabling scalable Shapley value calculation. They validate their method across three use cases involving image quality, demographic diversity, and aesthetic quality."
          },
          "strengths": {
            "value": "The paper makes a significant contribution by addressing a critical gap in data attribution for diffusion models, particularly for global properties rather than local image-level attributes. The proposed method of using model pruning and fine-tuning to approximate Shapley values is novel and practical, given the computational challenges of traditional approaches. The experimental validation on diverse datasets and models demonstrates the framework's effectiveness. The paper is well-structured, with clear motivation, methodological rigor, and thorough comparisons to existing methods."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of pruning and fine-tuning on Shapley value estimation accuracy. The theoretical analysis of how pruning preserves model properties relevant to global metrics is insufficient. Additionally, the discussion of how to handle data contributors with multiple samples (e.g., aggregation strategies) is brief and could be expanded. The computational efficiency gains are not quantified in terms of specific time/memory savings compared to baseline methods."
          },
          "questions": {
            "value": [
              "How does the pruning strategy ensure that global model properties (e.g., aesthetic quality) are preserved in the pruned model? Are there experiments validating this assumption?",
              "What specific metrics or criteria were used to determine the pruning rate and fine-tuning schedule? Could these choices introduce bias in Shapley value estimation?",
              "The paper mentions aggregating contributions from multiple data points per contributor but does not elaborate on how this is implemented. Could the authors provide more details on their aggregation strategy and its theoretical justification?",
              "How does the framework handle data contributors who contribute overlapping or highly correlated samples? Is there a risk of over/underestimating contributions in such cases?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces an efficient framework for attributing global properties of diffusion models (e.g., image quality, demographic diversity, aesthetic quality) to individual data contributors using Shapley values. The authors propose a method combining model pruning and fine-tuning to reduce computational costs of retraining and inference, enabling scalable Shapley value estimation. They validate the approach on three distinct use cases across different models and datasets."
          },
          "strengths": {
            "value": "The paper addresses a novel and important problem of fair credit attribution for diffusion model data contributors, which has significant implications for data compensation and incentive design. The Shapley value framework is theoretically sound and well-justified. The method's efficiency through pruning/fine-tuning is a practical innovation, addressing a critical computational barrier. The empirical evaluation covers diverse models (DDPM, LDM, LoRA) and global properties, demonstrating broad applicability. The work fills a key gap by focusing on global properties rather than local image-level attributions."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how pruning affects the accuracy of Shapley value estimation, raising concerns about the validity of the approximations. The experimental comparisons with baselines are limited—specifically, it's unclear how much better the proposed method is compared to existing attribution techniques. The aggregation strategy for multiple data points per contributor is not thoroughly explained, which could introduce biases. The paper also does not discuss potential limitations of Shapley values in this context (e.g., computational complexity for large datasets)."
          },
          "questions": {
            "value": "1. How was the pruning strategy validated to ensure it preserves the model's ability to capture global properties? 2. What specific metrics were used to evaluate image quality, demographic diversity, and aesthetic quality in the experiments? 3. How does the method handle contributors who provided multiple data points, and what evidence is provided that this aggregation is accurate? 4. Are there ablation studies showing the trade-off between computational efficiency and Shapley value accuracy when using pruning vs. full retraining?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes an efficient framework for attributing global properties of diffusion models to data contributors using Shapley values. The method leverages model pruning and fine-tuning to approximate retraining and inference, addressing the computational infeasibility of traditional Shapley value estimation for diffusion models. The approach is evaluated on three use cases involving image quality, demographic diversity, and aesthetic quality across different datasets and models."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem of fair credit attribution in diffusion models, which is essential for data compensation and incentive mechanisms. The use of Shapley values provides a theoretically sound foundation, and the proposed method of pruning/fine-tuning is innovative for reducing computational costs. The experiments span multiple datasets and global properties, demonstrating practical relevance. The paper also clarifies a gap in prior work by focusing on data contributors rather than individual data points."
          },
          "weaknesses": {
            "value": "The computational efficiency claims lack concrete metrics (e.g., GPU hours saved vs. baseline). The approximation error from pruning and fine-tuning is not quantified, raising concerns about the accuracy of Shapley value estimation. The paper does not address how to handle multiple data points from the same contributor, which was highlighted as a gap in prior work. The experimental validation is limited: no ablation studies on pruning/fine-tuning, no comparison against non-Shapley baselines, and no analysis of how global property metrics correlate with contributor importance."
          },
          "questions": {
            "value": "1. How does the pruning/fine-tuning process affect the fidelity of the retrained models used for Shapley value estimation? 2. What ablation studies demonstrate the necessity of pruning vs. fine-tuning? 3. How are multiple data points from the same contributor aggregated, and what is the impact on attribution accuracy? 4. What are the computational savings compared to full retraining (e.g., GPU days saved)? 5. Are the global property metrics (e.g., image quality, diversity) robust to the approximations in the framework?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "9GJ6JKoCVp": {
    "paper_id": "9GJ6JKoCVp",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces NaN Pooling and NaN Convolution, novel techniques to improve the efficiency of U-Nets by skipping computations on numerically unstable or irrelevant voxels. These methods replace unstable values with NaNs, reducing the number of operations in CNNs without sacrificing accuracy, as demonstrated on FastSurfer and MNIST."
          },
          "strengths": {
            "value": "The paper addresses a novel and practical problem in CNN efficiency, leveraging IEEE NaNs to bypass unnecessary computations. The approach is conceptually simple and aligns with the growing need for efficient deep learning models. The experiments on real-world neuroimaging and benchmark datasets (FastSurfer and MNIST) demonstrate tangible efficiency gains. The clarity of the problem statement and the proposed solution is strong, though some technical details are underdeveloped."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous analysis of why numerical instability occurs in max pooling and how NaN Pooling directly addresses it. The thresholds (t1, t2) are not justified or tested across different scenarios. The experiments are limited to two models, with no ablation studies or comparisons to existing optimization techniques. The impact of NaNs on gradient computation during training is unexplored, and the paper focuses only on inference efficiency. Additionally, the method's generalizability to other architectures or tasks is unclear."
          },
          "questions": {
            "value": "How are the thresholds t1 and t2 determined? Are they dataset-specific or model-specific? What is the impact of NaNs on gradient flow during training, and how does the method handle this? Are there cases where skipping convolutions could degrade model performance, and how are such cases mitigated? How does the overhead of NaN detection and handling compare to the computational savings? What are the limitations of applying this method to non-neuroimaging tasks?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces NaN Pooling and Convolution, novel techniques to accelerate U-Nets by skipping operations on numerically unstable voxels. The authors claim these methods reduce computational overhead by up to 69.3% in FastSurfer and 28.38% in MNIST while maintaining accuracy. The approach leverages IEEE NaN values to identify and bypass irrelevant computations."
          },
          "strengths": {
            "value": "The paper presents a creative solution to computational inefficiency in CNNs, leveraging NaNs to skip unnecessary operations. The problem formulation addresses a practical challenge in neuroimaging, and the method shows promising empirical results. The technical description of NaN Pooling and Convolution is detailed, and the experiments demonstrate significant speedups on specific tasks. The work could inspire further research on numerical stability in deep learning."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous validation of the claimed efficiency gains. It does not compare against established optimization techniques (e.g., pruning, quantization) or baseline methods. The numerical instability analysis is superficial, with no formal proof of why NaNs effectively skip irrelevant computations. The MNIST experiments are minimal, and the parameter choices (e.g., thresholds t1, t2) are not justified. The impact of NaNs on downstream layers and overall model robustness is unaddressed. Additionally, the paper does not discuss hardware-specific performance implications or generalizability to other architectures."
          },
          "questions": {
            "value": "1. How were the thresholds t1 and t2 determined? Were they validated across different datasets/models? 2. What is the theoretical basis for assuming NaNs represent 'numerically irrelevant' data? 3. How does the method handle NaNs during training, given the paper focuses on inference? 4. Are the speedup metrics normalized against computational resources (e.g., GPU vs. CPU)? 5. How does this approach compare to existing methods for CNN acceleration in terms of accuracy-efficiency trade-offs?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces NaN Pooling and Convolution, novel techniques to accelerate U-Nets by skipping unnecessary computations on numerically unstable voxels. The authors claim these methods can reduce floating-point operations by up to 69.30% in certain layers while maintaining model accuracy, as demonstrated on FastSurfer and MNIST."
          },
          "strengths": {
            "value": "The paper presents an original approach to computational efficiency in CNNs by leveraging numerical instability. The method's integration with PyTorch and use of IEEE NaNs are technically creative. The experimental results on neuroimaging and standard datasets suggest practical relevance. The problem formulation addresses a real-world challenge in model efficiency."
          },
          "weaknesses": {
            "value": "The paper lacks critical details about the implementation of NaN Convolution, such as how NaNs are handled during gradient computation or backward passes. The experiments are incomplete (e.g., truncated equations, missing baseline comparisons) and do not address whether the approach is applicable to other architectures beyond U-Nets. The theoretical justification for why skipping NaNs does not affect accuracy is weak. The numerical instability analysis in Appendix A.5 is not fully explained."
          },
          "questions": {
            "value": [
              "How are NaNs handled during gradient computation in the backward pass? Does this affect training stability?",
              "What is the exact mechanism for determining when to replace values with NaNs in the convolution operation? How is the threshold t2 chosen?",
              "Are the accuracy results statistically significant? What is the standard deviation across experiments?",
              "Why is this approach not applicable to non-U-Net architectures? Are the numerical instabilities specific to U-Nets?",
              "How does the method perform on larger, more complex models beyond FastSurfer and MNIST?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "9GKMCecZ7c": {
    "paper_id": "9GKMCecZ7c",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates the use of vision pre-trained models (PTMs) for developing generalist robot manipulation policies. It highlights that local features from PTMs outperform global features for multi-task policies, identifies DINOv2 as superior to specialized robot learning models, and proposes a self-distillation approach for few-shot adaptation to bridge domain gaps."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in robotics: building generalist policies that generalize across tasks and domains. Its originality lies in emphasizing local features over global features for multi-task learning, which is a novel perspective. The experiments are comprehensive, comparing multiple PTMs, evaluating augmentation methods, and proposing a practical self-distillation objective. The work's significance is underscored by its potential to improve real-world robot deployment in diverse environments."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed analysis of why local features are more effective than global features, which limits the theoretical understanding. The few-shot adaptation approach, while promising, is not thoroughly compared to existing methods like meta-learning or contrastive learning. Additionally, the experiments focus on Metaworld benchmarks, which may not fully capture real-world domain shifts. The paper also does not discuss computational efficiency or scalability of the proposed methods."
          },
          "questions": {
            "value": "1. What specific characteristics of local features (e.g., spatial resolution, semantic richness) contribute to better performance? 2. How does the self-distillation objective compare to other few-shot adaptation techniques in terms of sample efficiency? 3. Are the findings generalizable to other robot platforms or sensor modalities beyond vision? 4. Could the observed superiority of DINOv2 be attributed to its training data diversity rather than its architecture?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper explores the use of pre-trained vision models (PTMs) for building generalist robot manipulation policies. The authors demonstrate that local features from PTMs outperform global features for multi-task performance and domain generalization, identify DINOv2 as a strong baseline for robot learning, and propose a self-distillation approach for few-shot adaptation to bridge domain gaps."
          },
          "strengths": {
            "value": "Originality: The paper introduces novel insights about the importance of local features vs global features in PTMs for robotics, and proposes a self-distillation objective for few-shot adaptation. Quality: The experimental analysis covers multiple PTMs, domain gaps, and augmentation strategies with quantitative comparisons. Clarity: The problem formulation and technical contributions are well-structured, with clear ablation studies. Significance: The findings provide practical guidance for leveraging vision PTMs in embodied agents, addressing critical challenges in domain generalization and multi-task learning."
          },
          "weaknesses": {
            "value": "The paper lacks thorough analysis of why DINOv2 outperforms robot-specific PTMs like VC-1 - this could be due to dataset diversity vs task-specific optimization. The few-shot adaptation experiments focus on a single domain transfer scenario without exploring more complex cross-domain settings. The self-distillation approach is described but not deeply analyzed - it's unclear how it interacts with different PTM architectures or what specific aspects of the policy it improves."
          },
          "questions": {
            "value": [
              "What specific characteristics of DINOv2's training data (e.g., diversity, scale) contribute to its superior performance compared to VC-1?",
              "How do the authors quantify the 'domain gap' in their experiments, and what metrics are used to evaluate generalization beyond task success rates?",
              "Can the self-distillation approach be generalized to other PTM architectures, or is it specifically tailored to the ones tested?",
              "What are the computational costs of using full local features vs global features, and how does this impact real-time robotic control?",
              "Are there any negative transfer effects when using PTM features in domains with radically different visual characteristics?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper explores the use of pre-trained vision models (PTMs) for building generalist robot manipulation policies. It finds that local features from PTMs outperform global features for multi-task performance and domain generalization, with DINOv2 showing superior results. The work also introduces self-distillation for few-shot adaptation and evaluates augmentation methods to bridge domain gaps."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in robot learning: leveraging pre-trained vision models for multi-task generalization. It provides clear empirical insights into the limitations of global features versus local features, which is a novel perspective. The comparison of PTMs (e.g., DINOv2 vs. VC-1) and the proposed self-distillation objective for few-shot adaptation demonstrate practical value. The work also systematically evaluates augmentation compatibility across PTMs, offering actionable guidance for future research."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with state-of-the-art robot learning methods that combine vision and language models. The evaluation on Metaworld may not fully capture real-world domain gaps. The ablation studies on augmentation methods are superficial, and the paper does not discuss computational trade-offs of using local features. The few-shot adaptation results are promising but require more quantitative analysis (e.g., error bars, statistical significance). The theoretical justification for why DINOv2 outperforms VC-1 is underdeveloped."
          },
          "questions": {
            "value": "1. Why does DINOv2 outperform VC-1 despite being trained on generic datasets? Could there be specific architectural or training differences? 2. How is the self-distillation objective implemented in practice? What hyperparameters were used, and how sensitive is the method to them? 3. Are the domain gaps evaluated on physically distinct environments (e.g., different lighting, objects) or just task variations? 4. How do the results scale to more complex or higher-dimensional tasks beyond Metaworld?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "9HK2rHNAhd": {
    "paper_id": "9HK2rHNAhd",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes SQUEEZEATTENTION, a method for optimizing Key-Value (KV) cache management in Large Language Models (LLMs) by considering both sequence and layer-wise dimensions. The approach leverages cosine similarity of hidden representations to determine attention layer importance, dynamically reallocating KV-cache budgets across layers while integrating with existing sequence-wise compression algorithms."
          },
          "strengths": {
            "value": "The paper introduces a novel 2D KV-cache compression framework that addresses a critical gap in prior work by incorporating layer-wise importance. The method is theoretically grounded in analyzing hidden representation similarity, offering a systematic way to optimize cache allocation. The experiments on multiple LLMs (7B to 70B) demonstrate significant memory reduction (30-70%) and throughput improvements (2.2×), highlighting its practical relevance. The clarity of the problem statement and the motivation for layer-wise optimization are strong, and the paper effectively contextualizes its contributions relative to existing sequence-wise methods."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-experiment, leaving critical details about the evaluation (e.g., specific metrics, baseline comparisons, and ablation studies) incomplete. The layer importance metric (cosine similarity) lacks thorough justification—its robustness across diverse models/tasks is unverified. The method's dependency on prompt prefilling for clustering layers may introduce latency, which is not discussed. Additionally, the claim of orthogonality to sequence-wise algorithms is not rigorously validated, as the combined results are not quantitatively analyzed."
          },
          "questions": {
            "value": "1. How is the cosine similarity metric validated as a reliable proxy for layer importance across different models and tasks? 2. What are the computational costs of the layer importance calculation during inference, and how does this affect end-to-end latency? 3. Are there scenarios where dynamic layer budgeting could harm model accuracy, and how is this mitigated? 4. How does SQUEEZEATTENTION handle heterogeneous batch sizes or variable sequence lengths during cache reallocation?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes SQUEEZEATTENTION, a 2D KV-cache compression method for LLM inference that optimizes cache budgets across both sequence and attention layers. It uses cosine similarity of hidden representations to assess layer importance and dynamically reallocates budgets, combining with existing sequence-wise compression algorithms to achieve memory and throughput improvements."
          },
          "strengths": {
            "value": "Originality: Introduces layer-wise KV-cache budgeting, a novel dimension not explored by prior work. Quality: Demonstrates experiments across multiple LLMs and benchmarks. Clarity: Abstract and figures effectively explain the 2D compression concept. Significance: Addresses critical inference cost challenges in LLMs, with potential for practical impact."
          },
          "weaknesses": {
            "value": "Experiments lack depth: No ablation studies on layer importance metrics or robustness analysis. The cosine similarity metric's validity as a proxy for layer importance is unverified. Comparisons with baselines are superficial—no analysis of trade-offs between accuracy and efficiency. Implementation details (e.g., clustering methods, budget reallocation logic) are under-specified. The paper does not address computational overhead of layer importance calculation."
          },
          "questions": {
            "value": "1. How is the optimal number of clusters for layer grouping determined? 2. What is the computational cost of measuring cosine similarity across layers? 3. How does SQUEEZEATTENTION handle dynamic input lengths or heterogeneous batch sizes? 4. Are there cases where layer-wise budgeting could degrade model accuracy? 5. How is the orthogonality to sequence-wise methods implemented in practice?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces SQUEEZEATTENTION, a 2D KV-cache compression method for LLM inference that optimizes both sequence-wise and layer-wise dimensions. The approach uses cosine similarity of hidden representations to measure layer importance, dynamically reallocating KV-cache budgets across layers. It combines with existing sequence-wise compression algorithms to achieve memory reduction and throughput improvements."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel 2D KV-cache management framework, addressing layer-wise importance which prior work ignores. Quality: The method is theoretically grounded in analyzing hidden representation similarity, and experiments cover diverse LLMs. Clarity: The problem statement and high-level approach are well-explained, with illustrative figures. Significance: Reducing inference costs is critical for LLM deployment, and the orthogonal integration with sequence-wise methods broadens its applicability."
          },
          "weaknesses": {
            "value": "The paper is truncated, omitting full experimental results and detailed analysis. The layer importance metric (cosine similarity) lacks justification for why it captures critical information. The clustering method for layer groups is not described, and ablation studies on the impact of layer budgeting are missing. The claimed 30-70% memory reduction and 2.2x throughput gains require more rigorous validation with baseline comparisons."
          },
          "questions": {
            "value": [
              "How is the cosine similarity computed between hidden representations before and after self-attention layers? Are gradients or specific layers used?",
              "What clustering algorithm is employed to group layers based on cosine similarity? How is the number of clusters determined?",
              "Are there ablation studies demonstrating the effectiveness of layer-wise budgeting compared to uniform allocation?",
              "How does SQUEEZEATTENTION handle varying model architectures (e.g., different numbers of attention layers) or tasks?",
              "What is the exact implementation of the layer-wise budget reallocation mechanism? How is accuracy trade-off managed?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "9Ieq8jQNAl": {
    "paper_id": "9Ieq8jQNAl",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper explores the use of multiple types of human feedback for reinforcement learning from human feedback (RLHF). The authors define six synthetic feedback types (ratings, comparisons, demonstrations, corrections, language instructions, and narrative feedback), implement reward models for each, and evaluate their effectiveness across ten RL environments. They demonstrate that combining diverse feedback types can improve reward modeling performance compared to preference-based baselines."
          },
          "strengths": {
            "value": "The paper makes a strong contribution by systematically defining and simulating six distinct feedback types, which expands the scope of RLHF research beyond traditional preference-based methods. The experimental design is comprehensive, covering multiple environments and comparing diverse feedback combinations. The work addresses an important gap in understanding how different feedback types interact and complement each other. The joint-reward modeling approach using an ensemble of feedback types shows promising empirical results."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how specific feedback types contribute to performance improvements, making it unclear which types are most effective. The synthetic feedback generation method is not thoroughly justified, and its alignment with real-world human behavior is not validated. The experiments focus on simulated environments, and the paper does not address how these findings generalize to real-world human feedback. Additionally, the paper does not discuss potential conflicts between different feedback types or methods for resolving such conflicts during training."
          },
          "questions": {
            "value": [
              "How were the six feedback types selected? Are there specific criteria or real-world applications that motivated this particular set?",
              "What is the rationale behind the synthetic feedback generation methodology? How does it account for human variability and error in real-world scenarios?",
              "The paper mentions 'joint-reward modeling' but does not elaborate on how different feedback types are integrated. Could the authors provide more details on the ensemble approach and its training procedure?",
              "Are there cases where certain feedback types might conflict with others? How does the framework handle such conflicts during reward learning?",
              "How do the results generalize to non-synthetic, human-provided feedback? What steps were taken to ensure the simulated feedback captures meaningful human preferences?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper explores the use of multi-type human feedback for reinforcement learning from human feedback (RLHF), proposing a framework to simulate six distinct feedback types (e.g., ratings, comparisons, demonstrations) and evaluate their effectiveness in reward modeling and downstream RL tasks. The authors demonstrate that combining diverse feedback types can improve reward learning performance compared to single-type baselines."
          },
          "strengths": {
            "value": "The paper addresses a novel and important problem in RLHF by systematically investigating the potential of multi-type feedback, which has been underexplored in prior work. The methodological contributions include a comprehensive framework for simulating diverse feedback types and a joint-reward modeling approach. The empirical analysis across ten environments provides valuable insights into the complementarity of feedback types. The paper also clearly articulates its research questions and connects its work to relevant prior studies."
          },
          "weaknesses": {
            "value": "The reliance on simulated feedback limits the practical relevance of the findings, as the paper does not validate the approach on real human data. The paper lacks detailed analysis of how different feedback types interact or conflict during joint training, and the ensemble-based reward modeling approach is not thoroughly justified. Additionally, the comparison with baselines is limited to preference-based methods, without exploring other RLHF variants. The truncated content also raises concerns about the completeness of the methodology and experimental results."
          },
          "questions": {
            "value": "1. How was the simulated feedback validated against real human feedback, and what are the potential discrepancies? 2. What criteria were used to select the six feedback types, and are there other important feedback types missing? 3. How are conflicts between different feedback types resolved during joint training, and what ablation studies were conducted to assess their individual contributions? 4. Are there specific environments or tasks where multi-type feedback shows the most significant improvement, and why?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper explores the use of multi-type human feedback for reinforcement learning from human feedback (RLHF), introducing a framework to simulate six distinct feedback types (ratings, comparisons, demonstrations, corrections, language instructions, and descriptive feedback). The authors evaluate their effectiveness across ten environments and demonstrate that combining diverse feedback types can improve reward modeling performance compared to single-type baselines."
          },
          "strengths": {
            "value": "The paper addresses a timely and important problem in RLHF by systematically exploring multi-type feedback, which has been under-investigated. The work is novel in its scale (six feedback types) and its focus on simulating diverse feedback for controlled experimentation. The clear problem formulation and structured approach to defining feedback types showcase strong theoretical foundations. The empirical investigation across multiple environments highlights practical relevance, and the open-source release of the simulation framework represents significant value for the community."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete due to the paper's truncation, making it difficult to assess the full scope of results. Key details about the implementation of the joint-reward modeling approach (e.g., how feedback types are combined, handling of conflicting signals) are missing. The paper lacks analysis of computational costs and scalability of the multi-type framework. The comparison to baselines appears limited, with no ablation studies on individual feedback types' contributions."
          },
          "questions": {
            "value": [
              "Please provide details about the ten RL environments used for evaluation and their characteristics.",
              "How is the joint-reward modeling approach implemented? What is the architecture of the ensemble model?",
              "What metrics were used to quantify the complementarity of different feedback types?",
              "Are there any failure cases where multi-type feedback performed worse than single-type baselines?",
              "How does the simulation framework handle edge cases (e.g., conflicting feedback signals)?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "9KxnxWOBA5": {
    "paper_id": "9KxnxWOBA5",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper addresses two open questions in Multi-Draft Speculative Decoding (MDSD) by analyzing the optimal acceptance rate through optimal transport theory. The authors propose a dual formulation to compute the theoretical upper bound of MDSD efficiency, compare draft sampling methods (with/without replacement), and introduce a novel sampling algorithm that approaches the theoretical limit. They also quantify the gap between existing verification algorithms and the upper bound for real text data."
          },
          "strengths": {
            "value": "Originality is strong through the dual optimal transport formulation and novel subset selection approach. Theoretical analysis is rigorous, with practical methods for specific cases. The paper provides the first empirical measurement of MDSD's theoretical upper bound on real text. Clarity is good, with structured sections and clear motivation. Significance lies in improving LLM inference efficiency, a critical challenge in NLP."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental comparisons with state-of-the-art MDSD methods, making it hard to assess practical impact. The proposed 'greedy high-probability' sampling method is not thoroughly validated with ablation studies or real-world benchmarks. The dual approach's scalability to large vocabularies (e.g., 100k+ tokens) is unaddressed. Theoretical claims about total unimodularity and convexity-like structures require more rigorous justification."
          },
          "questions": {
            "value": "1. How does the dual formulation handle vocabularies with sizes beyond the thousands (e.g., 100k tokens)? 2. What are the computational costs of solving the subset selection problem for large n (number of drafts)? 3. Are there any cases where sampling with replacement outperforms the proposed greedy method? 4. How sensitive are the results to the choice of target model distribution (e.g., different language families)? 5. Can the theoretical upper bound be computed for non-English text or specialized domains?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper addresses the efficiency bottleneck in autoregressive decoding for large language models (LLMs) by analyzing the optimal acceptance rate in Multi-Draft Speculative Decoding (MDSD). The authors reformulate the problem of computing the optimal acceptance rate as a subset selection problem using duality theory and total unimodularity. They propose methods to compute theoretical upper bounds for MDSD efficiency, compare draft sampling strategies (with and without replacement), and introduce a novel greedy sampling method and verification algorithm that approach the theoretical limits."
          },
          "strengths": {
            "value": "The paper introduces a novel theoretical framework by transforming the optimal transport problem into a subset selection problem, offering fresh insights into MDSD efficiency. The work provides the first practical method to compute theoretical upper bounds for MDSD efficiency on real text data, which is a significant contribution. The comparison of sampling methods (e.g., without replacement outperforming with replacement) and the proposed greedy sampling strategy demonstrate clear innovation. The paper is well-structured, with clear problem formulation and rigorous theoretical analysis."
          },
          "weaknesses": {
            "value": "The experimental validation is limited to synthetic or small-scale settings, with no results on large LLMs or real-world applications. The paper lacks detailed comparisons of computational complexity between the proposed methods and existing algorithms. The theoretical analysis assumes idealized conditions (e.g., perfect draft distributions), which may not hold in practice. The practical implications of the proposed verification algorithm (e.g., latency, scalability) are not thoroughly discussed."
          },
          "questions": {
            "value": [
              "How do the proposed methods scale to large vocabularies (e.g., 10k+ tokens) common in LLMs?",
              "What are the computational costs of the subset selection algorithms compared to existing verification methods like K-SEQ or RRS?",
              "Are the theoretical upper bounds tight in practice, or do real-world distributions introduce significant gaps?",
              "How does the greedy sampling method perform under imperfect draft model distributions?",
              "What are the specific limitations of the current verification algorithm in achieving the theoretical upper bound?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses two critical open problems in Multi-Draft Speculative Decoding (MDSD): computing the optimal acceptance rate via optimal transport and quantifying the gap between existing verification algorithms and theoretical bounds. The authors propose a novel dual formulation of the transport problem, enabling efficient computation of upper bounds for MDSD efficiency. They also introduce a new draft sampling method that outperforms traditional approaches and demonstrate that current verification algorithms fall short of theoretical limits."
          },
          "strengths": {
            "value": "The paper makes significant theoretical contributions by reframing the optimal acceptance rate problem as a subset selection task, leveraging total unimodularity and convexity-like structures. It provides the first practical method to compute theoretical upper bounds for real text distributions, addressing a key gap in MDSD research. The experimental analysis comparing sampling methods (with/without replacement) and verification algorithms (K-SEQ, RRS) is rigorous, and the proposed greedy sampling method shows promising results. The work clearly connects theoretical insights to practical implications for MDSD efficiency."
          },
          "weaknesses": {
            "value": "The paper focuses on single-step MDSD and does not address multi-step scenarios, which may limit its applicability. The theoretical analysis assumes independence between draft tokens, which may not hold in practice. While the subset selection approach is novel, the paper lacks ablation studies on hyperparameters (e.g., number of drafts, vocabulary size) that could affect results. Additionally, the practical implementation of the proposed verification algorithm that reaches the theoretical upper bound is not detailed, making it difficult to assess feasibility."
          },
          "questions": {
            "value": "1. How does the dual formulation handle dependencies between draft tokens in real-world scenarios? 2. What are the computational costs of the proposed subset selection methods for large vocabulary sizes (e.g., 10k+ tokens)? 3. Can the greedy sampling method be generalized to other types of language models beyond the ones tested? 4. How sensitive are the results to the choice of draft model architecture or training data? 5. Are there practical constraints (e.g., memory, latency) that limit the deployment of the proposed verification algorithm?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "9PYCz4cDuZ": {
    "paper_id": "9PYCz4cDuZ",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper presents a theoretical analysis of Minimum Bayes Risk (MBR) decoding in text generation through bias-diversity decomposition. The authors decompose MBR decoding errors into bias (alignment with human evaluations) and diversity (variation in utility function estimates), propose a pseudo-bias metric, and introduce MAMBR to enhance diversity. Experiments across multiple NLP tasks demonstrate correlations between decomposed terms and performance, with MAMBR showing improvements."
          },
          "strengths": {
            "value": "The paper offers a novel theoretical framework for understanding MBR decoding, which fills a critical gap in the literature. The decomposition into bias and diversity provides interpretable insights into MBR's behavior. The proposed pseudo-bias metric and MAMBR approach are practical innovations. The experiments are well-designed, covering multiple tasks and sampling methods, and the writing is clear and structured."
          },
          "weaknesses": {
            "value": "The theoretical analysis is incomplete due to the paper's truncation, particularly the proof of Theorem 1, which limits the evaluation of its rigor. The experiments lack comparisons with state-of-the-art decoding methods beyond traditional MBR. The practical utility of pseudo-bias as a metric requires further validation. The paper also does not address potential limitations of the bias-diversity decomposition approach, such as sensitivity to specific utility functions or task characteristics."
          },
          "questions": {
            "value": [
              "Please provide the full proof of Theorem 1 to assess the theoretical rigor of the bias-diversity decomposition.",
              "How is the pseudo-bias metric calculated in practice, and what ablation studies support its effectiveness compared to existing metrics?",
              "How does MAMBR compare to other diversity-enhancing methods (e.g., top-p sampling, temperature scaling) in terms of performance and computational efficiency?",
              "Are there scenarios where improving diversity might negatively impact bias, and how does the paper address this trade-off?",
              "What are the limitations of the bias-diversity decomposition framework, and how might it generalize across different NLP tasks or model architectures?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper presents a theoretical analysis of Minimum Bayes Risk (MBR) decoding by decomposing its error into bias and diversity components. The authors introduce pseudo-bias as a metric to approximate bias using gold references and propose Metric-augmented MBR (MAMBR) to enhance diversity. Experimental results across NLP tasks validate the correlation between bias-diversity terms and performance, demonstrating MAMBR's effectiveness."
          },
          "strengths": {
            "value": "The paper's originality lies in its novel theoretical framework for analyzing MBR decoding through bias-diversity decomposition, which offers fresh insights into the trade-offs between bias and diversity. The theoretical analysis is rigorous, with clear mathematical formulations. The paper's clarity is strong, with well-structured explanations of concepts. The significance is high, as MBR decoding is widely used in NLP, and the findings provide actionable guidance for improving text generation systems."
          },
          "weaknesses": {
            "value": "The theoretical decomposition lacks detailed empirical validation of the bias-diversity trade-off in specific scenarios. The paper does not thoroughly compare MAMBR with existing diversity-enhancing methods (e.g., top-k sampling, nucleus sampling) to establish its uniqueness. The pseudo-bias metric's practical utility is not fully justified, and the experiments could include ablation studies to isolate the impact of diversity adjustments. Additionally, the paper does not address potential limitations of using gold references for pseudo-bias in real-world settings."
          },
          "questions": {
            "value": [
              "How does MAMBR differ from existing diversity-enhancing techniques, and what specific advantages does it offer?",
              "Can the theoretical bias-diversity decomposition be validated with synthetic data where ground-truth bias/diversity values are known?",
              "What are the computational costs of MAMBR compared to standard MBR, and how does this affect scalability?",
              "How sensitive is the pseudo-bias metric to the quality of gold references, and what happens when gold references are imperfect?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper presents a theoretical analysis of Minimum Bayes Risk (MBR) decoding by decomposing its error into bias and diversity components. The authors propose pseudo-bias as a metric to approximate bias using gold references and introduce Metric-augmented MBR (MAMBR) to enhance diversity in utility functions. The work bridges theoretical insights with empirical validation across multiple NLP tasks."
          },
          "strengths": {
            "value": "The paper offers original theoretical insights by applying bias-diversity decomposition to MBR decoding, a novel perspective in this domain. The analysis is mathematically rigorous, with clear definitions of bias (alignment with human evaluations) and diversity (variation in utility functions). The empirical validation across tasks like machine translation and summarization demonstrates practical relevance. The proposed MAMBR method shows promise by addressing diversity without altering pseudo-references, and the paper is well-structured with formal equations and logical flow."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-proof, leaving critical details of the bias-diversity decomposition and experimental results incomplete. The theoretical analysis lacks depth in explaining why improving diversity is prioritized over bias. The pseudo-bias metric's implementation and validation are not fully elaborated. Additionally, the paper does not address potential limitations of MAMBR, such as computational overhead or sensitivity to utility function choices. The empirical results section is insufficiently detailed to assess the magnitude of performance gains."
          },
          "questions": {
            "value": "1. How is the bias-diversity decomposition mathematically derived, and what assumptions underpin Theorem 1? 2. What specific adjustments does MAMBR make to utility functions to increase diversity, and how are these validated? 3. How is pseudo-bias calculated using gold references, and what baseline metrics are used for comparison? 4. Are the experimental results reproducible, and what hyperparameters were used for MAMBR? 5. How does the paper address the intractability of Omega in Equation 2, and what sampling strategies were employed?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "9Qptgv0Eyw": {
    "paper_id": "9Qptgv0Eyw",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces PtychoFormer, a transformer-based model for ptychographic phase retrieval, addressing limitations of traditional iterative methods and CNN-based deep learning approaches. It proposes a hierarchical architecture that processes diffraction patterns while preserving spatial relationships, and a hybrid approach (ePF) combining PtychoFormer with ePIE to improve reconstruction quality and reduce global phase shifts."
          },
          "strengths": {
            "value": "The paper's originality lies in applying transformers to ptychography, a novel domain for such models. The hierarchical design and spatial awareness mechanism address a key limitation of prior CNN-based methods. The experimental results demonstrate significant speed improvements (3600x faster than ePIE) and enhanced phase reconstruction. The hybrid ePF approach effectively mitigates global phase shifts, a critical challenge in ptychography. The paper provides a clear problem formulation and contextualizes its contributions within the broader field of phase retrieval."
          },
          "weaknesses": {
            "value": "The related work section is incomplete, making it difficult to assess the novelty relative to existing deep learning methods. The experiments rely heavily on simulations, with no evaluation on real-world data. The paper lacks ablation studies to validate the importance of specific components (e.g., transformer architecture vs. spatial encoding). The mechanism by which PtychoFormer captures spatial dependencies is not thoroughly explained, and the claims about sparse data tolerance require more quantitative justification."
          },
          "questions": {
            "value": [
              "How does PtychoFormer's performance compare to other deep learning methods (e.g., CNNs) on standard ptychography benchmarks?",
              "Are the experiments validated on real experimental data, or solely synthetic datasets?",
              "What specific design choices in PtychoFormer enable it to handle sparse diffraction patterns, and how is this quantified?",
              "How does the hybrid ePF approach address global phase shifts compared to ePIE alone, and what are the computational trade-offs?",
              "Are there any limitations to the spatial awareness mechanism, such as scalability to larger datasets or higher-resolution images?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces PythoFormer, a transformer-based model for ptychographic phase retrieval, which processes diffraction patterns hierarchically to enable faster reconstruction and address global phase shift issues. It also proposes a hybrid approach (ePF) combining PythoFormer with ePIE to improve accuracy."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in ptychography with a novel application of transformers, which is promising given their success in other domains. The proposed method demonstrates significant speed improvements (3600x faster than ePIE) and tackles the global phase shift ambiguity, a known limitation. The hybrid ePF approach is innovative, and the paper provides a clear problem statement and structure. The theoretical foundation of phase retrieval and ptychography is well-explained."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive experimental validation. The speed claim (3600x faster) is not contextualized with baseline comparisons or computational details (e.g., hardware, batch sizes). The hybrid ePF's improvements (73.59% NRMSE reduction) are not corroborated with full results or ablation studies. The related work section is incomplete, making it hard to assess novelty. The model's architecture and training procedure are not sufficiently detailed for reproducibility."
          },
          "questions": {
            "value": "1. How was the 3600x speed improvement calculated? What hardware/software setup was used? 2. What are the exact architectural details of PythoFormer and ePF? 3. Are there comparisons with other DL methods beyond ePIE? 4. How does the model handle noise or varying scan densities? 5. What is the computational cost of the hybrid ePF approach? 6. Are the phase reconstruction results statistically significant compared to baselines?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces PythoFormer, a transformer-based model for ptychographic phase retrieval, aiming to address limitations of traditional iterative methods like ePIE. It proposes a hierarchical architecture that processes diffraction patterns in parallel, achieves faster reconstruction speeds, and introduces a hybrid ePF approach to mitigate global phase shifts. The work claims state-of-the-art performance in phase recovery tasks."
          },
          "strengths": {
            "value": "Originality: The application of transformers to ptychography is novel, addressing spatial relationship limitations in CNNs. Quality: The paper presents clear claims of significant speed improvements (3600x faster than ePIE) and hybrid method benefits. Clarity: The structure is logical, with sections on background, methodology, and results. Significance: Ptychography is critical in imaging, and the proposed methods could impact biological and materials sciences."
          },
          "weaknesses": {
            "value": "The paper is truncated, missing experimental details, datasets, and validation on real-world data. The 3600x speedup claim lacks contextualization (e.g., hardware, implementation specifics). The hybrid ePF approach's integration with ePIE is not elaborated. The spatial awareness mechanism in PythoFormer is not thoroughly explained. No ablation studies or comparison with state-of-the-art DL methods are provided."
          },
          "questions": {
            "value": "1. What datasets/simulations were used for validation? How do they reflect real-world ptychography challenges? 2. How is the 3600x speedup calculated (e.g., algorithmic complexity vs. implementation efficiency)? 3. What is the exact architecture of PythoFormer, and how does it encode spatial relationships? 4. How does ePF integrate with ePIE (e.g., initialization, iterative refinement steps)? 5. Are there comparisons with recent DL methods beyond CNNs (e.g., graph neural networks)?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "9TpgFnRJ1y": {
    "paper_id": "9TpgFnRJ1y",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a generative framework for real-time counterfactual generation in interactive classification tasks, leveraging a label-disentangled regularized autoencoder (RAE) to address efficiency, interpretability, and validity of counterfactual explanations. The method claims to generate interpretable counterfactual images in real-time, validated through a user study demonstrating improved human performance."
          },
          "strengths": {
            "value": "The paper introduces a novel approach combining label disentanglement with RAEs, addressing key limitations of existing counterfactual methods. The real-time efficiency claim is compelling, as it avoids gradient-based optimization. The user study provides empirical evidence of the framework's effectiveness, and the related work section is comprehensive. The focus on interpretability through latent concept extraction is a strong point."
          },
          "weaknesses": {
            "value": "The paper is truncated, leaving critical methodological details (e.g., label-relevant/irrelevant latent space design, training procedure) incomplete. The user study lacks quantitative metrics and methodological details (e.g., task setup, baseline comparisons). The claim of being 'the first' real-time framework is unverified without comparison to existing methods. The evaluation is limited to a single task, raising questions about generalizability."
          },
          "questions": {
            "value": [
              "What specific metrics were used to quantify the user study's results, and how do they compare to baselines?",
              "How is the 'real-time' capability measured (e.g., inference speed, latency thresholds)?",
              "What ablation studies were conducted to validate the contribution of label disentanglement vs. other components?",
              "How is the adversarial distribution defined, and what guarantees exist for counterfactual validity?",
              "What datasets were used for evaluation, and how do they reflect real-world scenarios?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes a generative framework for real-time counterfactual generation using a label-disentangled regularized autoencoder (RAE). The method aims to address challenges in efficiency, interpretability, and validity of counterfactual explanations by leveraging disentangled latent representations and avoiding gradient-based optimization. A user study is presented to demonstrate its effectiveness in enhancing human performance."
          },
          "strengths": {
            "value": "The paper introduces a novel approach combining label disentanglement with RAEs, which could address key limitations in existing counterfactual methods. The focus on real-time generation is timely and practical. The user study provides empirical validation of the framework's impact on human performance. The method's efficiency claim (avoiding gradient-based optimization) is promising for interactive applications."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with critical methodological details missing (e.g., architecture specifics, training process). The user study lacks quantitative results and methodological details (e.g., participant selection, metrics). Comparisons to baselines are not described, making it hard to assess novelty. The claim of being 'the first' real-time framework is unsubstantiated without thorough prior work analysis. Technical claims about label disentanglement and adversarial distribution generation are not rigorously justified."
          },
          "questions": {
            "value": "1. How is the label disentanglement achieved in the RAE? What specific architectural modifications enable this? 2. What baselines were compared against in the user study? 3. How is 'real-time' performance quantified (e.g., latency metrics)? 4. What evidence supports the claim that the generated counterfactuals are valid and interpretable? 5. How are adversarial distributions leveraged to bypass gradient-based optimization?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a generative framework for real-time counterfactual generation using a label-disentangled regularized autoencoder (RAE). The method aims to address challenges in counterfactual explanations by enabling efficient, interpretable, and valid counterfactual image generation through adversarial distribution sampling, with a user study demonstrating improved human performance in classification tasks."
          },
          "strengths": {
            "value": "Originality: The paper presents a novel framework combining label disentanglement with RAEs for real-time counterfactual generation, which the authors claim is the first of its kind. Quality: The approach leverages adversarial distributions to avoid gradient-based optimization, offering a computationally efficient solution. Clarity: The structure is logically organized, with clear problem statements and methodological overview. Significance: Real-time counterfactuals have potential applications in human-in-the-loop systems, and the user study highlights practical impact."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about the methodology, experiments, and quantitative results (e.g., Appendix C.1) unreviewable. The user study lacks methodological specifics (e.g., metrics, participant details, baselines). The claim of 'first' real-time framework requires stronger justification against prior work (e.g., DDPM-based methods). The role of label disentanglement in improving interpretability is not sufficiently explained."
          },
          "questions": {
            "value": "What specific metrics were used to evaluate the user study, and how do they correlate with human performance improvements? How does the framework handle domain shifts (e.g., non-image data)? Are there ablation studies demonstrating the necessity of label disentanglement? What is the computational complexity comparison with gradient-based methods? How is the adversarial distribution defined and sampled?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "9VMW4iXfKt": {
    "paper_id": "9VMW4iXfKt",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces R-Sparse, a training-free activation sparsity method for efficient large language model (LLM) inference. The approach leverages input activation sparsity and low-rank approximations to achieve high sparsity levels without requiring active channel prediction or extensive retraining. Key contributions include a rank-aware decomposition of linear layers and an evolutionary search for optimal sparse-low-rank ratios."
          },
          "strengths": {
            "value": "Originality: The paper addresses a critical gap in activation sparsity for non-ReLU LLMs, proposing a training-free method that avoids active channel prediction. Quality: The methodology is theoretically grounded, with experiments on multiple LLMs and tasks demonstrating significant efficiency gains. Clarity: The problem statement and high-level approach are well-articulated, though some technical details are vague. Significance: The potential for hardware-friendly inference with 50% sparsity and 43% speed improvements addresses a key challenge in deploying LLMs on edge devices."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art sparsity methods, making it hard to assess relative effectiveness. The multi-phase ReLU and low-rank approximation mechanisms are not sufficiently explained, raising questions about their theoretical justification. Experimental results do not include ablation studies or analysis of sparsity vs. accuracy trade-offs. The evolutionary search for optimal ratios is mentioned but its computational overhead and practical feasibility are unclear. The paper is cut off, leaving critical details about the methodology and results unaddressed."
          },
          "questions": {
            "value": "1. How does the multi-phase ReLU function specifically enable high sparsity without sacrificing performance? 2. What is the exact mechanism for decomposing linear layers into sparse and low-rank components, and how is the rank determined? 3. How does R-Sparse compare to existing training-free sparsity methods (e.g., unstructured pruning) in terms of sparsity-accuracy trade-offs? 4. What is the computational cost of the evolutionary search algorithm, and how does it scale to larger models? 5. Are there any limitations on the types of LLM architectures or tasks where R-Sparse is effective?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces R-Sparse, a training-free activation sparsity method for efficient Large Language Model (LLM) inference. The approach leverages input activation sparsity and low-rank approximations of weight matrices to achieve high sparsity without requiring active channel prediction. Key insights include treating non-sparse input components as bias terms and approximating full computations via combinations of input channels and singular value components. Experiments on Llama-2/3 and Mistral models demonstrate 50% model-level sparsity with 43% end-to-end speed improvements."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in LLM inference efficiency by proposing a novel, training-free method that avoids the need for active channel prediction or extensive retraining. The theoretical insights into low-rank structures and input channel importance are original and well-motivated. The experimental validation spans multiple LLM families and tasks, and the potential for integration with quantization adds practical value. The method's compatibility with non-ReLU activations is a significant contribution."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art sparsity methods, making it difficult to assess the novelty and effectiveness of R-Sparse. The claims about achieving 50% sparsity and 43% speed improvements are not contextualized against existing techniques. The evolutionary search for layer-specific sparsity ratios is not thoroughly explained, and the role of the 'customized kernel' in achieving performance gains remains unclear. The theoretical justification for the low-rank approximation and the handling of non-ReLU activations (e.g., SiLU/GELU) require deeper analysis."
          },
          "questions": {
            "value": "1. How does R-Sparse specifically handle non-ReLU activation functions like SiLU/GELU, given the paper's emphasis on this capability? 2. What are the exact implementation details of the 'customized kernel' and how does it contribute to the 43% speed improvement? 3. How does the evolutionary search algorithm determine optimal sparsity ratios, and what metrics are used to evaluate layer-specific trade-offs? 4. Are there ablation studies demonstrating the individual contributions of input sparsity and low-rank approximations? 5. How does R-Sparse compare to structured pruning or quantization methods in terms of inference speed and memory usage?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces R-Sparse, a training-free activation sparsity method for efficient Large Language Model (LLM) inference. The approach leverages input channel sparsity and low-rank approximations via singular value decomposition (SVD) to achieve high sparsity without active channel prediction. Key observations include treating non-sparse input components as bias terms and approximating full computation through input channel and singular value combinations. Experiments on Llama-2/3 and Mistral models show 50% model-level sparsity with 43% end-to-end efficiency gains."
          },
          "strengths": {
            "value": "Originality lies in the training-free design and focus on input activation sparsity, avoiding the need for active channel prediction. The method's compatibility with weight quantization and empirical results across multiple LLM families and tasks demonstrate practical significance. The paper provides clear motivation for addressing limitations of existing activation sparsity techniques, particularly for non-ReLU-based models. Experimental validation is thorough, with detailed ablation studies and comparisons to full models."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons to state-of-the-art sparsity methods (e.g., output sparsity, structured pruning) to contextualize the 50% sparsity claim. The theoretical justification for the two key observations (bias-term approximation and low-rank structure) is underdeveloped, with minimal analysis of why input sparsity outperforms output sparsity. The evolutionary search for layer-wise sparsity ratios is not thoroughly explained, and its computational overhead is unaddressed. The claim of 'high sparsity' (50% model-level) is not validated against alternative approaches, making it difficult to assess relative effectiveness. The paper also omits details about hardware-specific optimizations and their impact on latency."
          },
          "questions": {
            "value": "1. How does R-Sparse handle non-ReLU activations like SiLU/GELU compared to ReLU-based methods? 2. What is the exact mechanism for approximating non-sparse components as bias terms? 3. How does the evolutionary search algorithm scale to larger models or different hardware configurations? 4. Are the 43% efficiency gains consistent across all ten tasks, or are they task-specific? 5. What is the trade-off between sparsity and accuracy for different sparsity levels (e.g., 70% vs. 50%)? 6. How does the low-rank decomposition of weights affect inference latency beyond the end-to-end speed metric?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "9W6Z9IeLzc": {
    "paper_id": "9W6Z9IeLzc",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces CoPS, a novel algorithm for cross-task experience sharing in large language model (LLM) agents. CoPS enhances sequential reasoning by selecting distribution-matched experiences from previous tasks using a pessimism-based strategy, aiming to balance utility and risk from distribution shifts. The method is evaluated on benchmarks like Alfworld, Webshop, and HotPotQA, showing superior performance and sample efficiency compared to existing approaches."
          },
          "strengths": {
            "value": "The paper presents a theoretically grounded approach to experience sharing, addressing limitations in existing reflection-driven and experience-assisted reasoning paradigms. The pessimism-based selection strategy is novel and well-motivated, with clear empirical validation on multiple benchmarks. The work highlights the importance of task-dependent distribution matching, offering insights into improving agent generalization. The method's flexibility (agnostic to base models, task types, and frameworks) enhances its practical appeal."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contribution of the pessimism principle versus other components. The theoretical analysis is limited, with insufficient discussion of how the claimed dependence on LLM quality and distribution matching translates to practical performance. The experimental section is truncated, making it difficult to assess the full scope of results. The distance metric and implementation details of the experience selection strategy are not sufficiently explained."
          },
          "questions": {
            "value": "1. How is the pessimism principle mathematically formalized in the experience selection objective (Equation 2.1)? 2. What specific distance metric $d$ is used in practice, and how does it capture distribution mismatches? 3. Are there ablation studies demonstrating the necessity of distribution-matching versus random experience selection? 4. How does CoPS handle tasks with drastically different state-action spaces from previous experiences? 5. What are the exact metrics used to quantify sample efficiency, and how do they compare to baselines like LATS?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes CoPS, a method for cross-task experience sharing in large language model (LLM) agents. CoPS addresses limitations in existing sequential reasoning paradigms by selecting distribution-matched experiences via a pessimism-based strategy, enabling agents to improve performance on new tasks while mitigating distribution shift risks. The approach is validated on benchmarks like Alfworld, Webshop, and HotPotQA, demonstrating superior sample efficiency and performance compared to state-of-the-art methods."
          },
          "strengths": {
            "value": "The paper presents a theoretically grounded approach with clear methodological innovation in experience selection. The experimental validation on multiple benchmarks demonstrates practical effectiveness, and the generalizability across offline/online settings is a significant strength. The work addresses a critical gap in sequential reasoning by combining external experiences with principled selection mechanisms. The clarity of the problem formulation and the structured presentation of contributions enhance readability."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of the pessimism-based selection mechanism's theoretical guarantees and practical implementation. The algorithm description (Algorithm 1) is overly brief, with insufficient explanation of key components like the optimization objective in Equation (2.1). Experimental comparisons are limited to a few baselines, and ablation studies on hyperparameters or the impact of different distance metrics are missing. The theoretical analysis of performance dependencies on LLM quality and distribution matching is mentioned but not elaborated."
          },
          "questions": {
            "value": "1. How is the distance metric $d$ in Algorithm 1 selected, and what are its properties? 2. What specific decoder architecture and training methodology are used for generating $\\tau^{s_1}$? 3. Are there any limitations or failure cases of the pessimism-based strategy that the authors observed? 4. How does CoPS handle tasks with drastically different distributions from prior experiences? 5. Can the authors provide additional ablation studies on the impact of memory size $k$ and sequence length $H$?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces CoPS, a novel algorithm for cross-task experience sharing in large language model (LLM) agents. CoPS addresses limitations in existing sequential reasoning paradigms by selecting distribution-matched experiences from previous tasks using a pessimism-based strategy, enhancing performance and sample efficiency. The method is evaluated on benchmarks like Alfworld, Webshop, and HotPotQA, demonstrating superior results compared to state-of-the-art approaches."
          },
          "strengths": {
            "value": "The paper presents a creative solution to a significant problem in sequential reasoning for LLM agents, combining theoretical grounding with practical experimentation. The proposed pessimism-based experience selection strategy is novel and addresses key limitations of prior methods. The experimental validation on diverse benchmarks showcases strong performance improvements and sample efficiency. The work's generalizability across task types and frameworks is a notable strength, and the theoretical analysis provides valuable insights into the algorithm's behavior."
          },
          "weaknesses": {
            "value": "The theoretical analysis lacks depth in connecting the pessimism principle to empirical performance, with minimal discussion of how distribution matching directly impacts results. The paper does not thoroughly compare against a broad range of baselines (e.g., other experience-assisted methods beyond RAP, Reflexion, and LATS). The choice of distance metric and its hyperparameters are not justified, and ablation studies on key components (e.g., distribution matching vs. random selection) are missing. The practical implementation details (e.g., memory bank management, scaling to large datasets) are under-specified."
          },
          "questions": {
            "value": "1. How is the pessimism principle formally integrated into the experience selection objective (Equation 2.1)? What guarantees does it provide against distribution shift? 2. Why was the Hellinger distance chosen over other metrics (e.g., KL divergence, cosine similarity) for measuring distribution mismatch? 3. Are there cases where distribution-matched experiences could harm performance, and how does CoPS mitigate this? 4. How does the method scale to tasks with very long sequences or high-dimensional state spaces? 5. What ablation results are available for the impact of memory size (k) and sequence length (H) on performance?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "9Y6QWwQhF3": {
    "paper_id": "9Y6QWwQhF3",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces FoREST, a new benchmark to evaluate large language models' (LLMs) understanding of frame of reference (FoR) in spatial reasoning tasks. It addresses the lack of dedicated benchmarks for FoR by creating a dataset with ambiguous (A-split) and clear (C-split) spatial expressions. The authors propose Spatial-Guided (SG) prompting to improve FoR identification and demonstrate its benefits for text-to-image generation."
          },
          "strengths": {
            "value": "The paper makes a significant contribution by addressing a critical gap in spatial reasoning research through the FoREST benchmark, which systematically evaluates FoR understanding. The methodology is well-structured, with clear distinctions between ambiguous and clear spatial expressions. The proposed SG prompting approach is novel and effectively leverages spatial relations to mitigate model biases. The work connects cognitive linguistics with practical AI applications, such as text-to-image generation, and provides a comprehensive analysis of LLMs' FoR biases."
          },
          "weaknesses": {
            "value": "The paper lacks detailed baseline comparisons for the SG prompting method, making it difficult to assess its relative effectiveness. The dataset construction process is described but lacks specific details on how ambiguous and clear splits were generated. The experiments on text-to-image generation are mentioned but not thoroughly evaluated with quantitative metrics or qualitative examples. Additionally, the paper does not discuss potential limitations of the FoREST benchmark, such as cultural or linguistic biases in the dataset."
          },
          "questions": {
            "value": [
              "How was the FoREST dataset curated? What criteria were used to determine ambiguous vs. clear spatial expressions?",
              "What are the exact metrics used to evaluate FoR identification, and how do they align with the theoretical framework of FoR?",
              "How does the SG prompting method compare to other prompting techniques or existing approaches for spatial reasoning?",
              "Are there specific cases where SG prompting fails, and what are the underlying reasons?",
              "How generalizable are the results to other languages or cultural contexts, given the focus on English spatial expressions?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces FoREST, a new benchmark to evaluate large language models' (LLMs) understanding of frame of reference (FoR) in spatial reasoning tasks. It proposes Spatial-Guided (SG) prompting to improve FoR identification and demonstrates its impact on text-to-image generation. The work highlights biases in LLMs' FoR interpretation and addresses gaps in existing spatial reasoning benchmarks."
          },
          "strengths": {
            "value": "Originality is strong through the creation of a novel benchmark (FoREST) targeting FoR, a concept underexplored in AI. The quality of experiments is solid, with analysis of LLM biases and integration of FoR into text-to-image generation. Clarity is good, with structured sections on spatial roles, relations, and FoR types. Significance lies in addressing a critical gap in spatial reasoning evaluation and proposing a practical prompting method for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of SG prompting. The FoREST dataset construction methodology is incomplete (content truncated), making it hard to assess its robustness. The analysis of LLM biases is superficial, with limited comparison to baselines. The text-to-image experiments lack quantitative metrics (e.g., FID, accuracy) to validate improvements. The connection between FoR identification and image generation is not thoroughly justified."
          },
          "questions": {
            "value": "1. How was the FoREST dataset validated for coverage of all four FoR types? 2. What specific metrics were used to quantify LLM bias in FoR identification? 3. How does SG prompting compare to existing prompting strategies (e.g., chain-of-thought)? 4. Are there qualitative examples showing improved image generation with SG prompting? 5. How generalizable is the SG prompting approach to other spatial reasoning tasks?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces FoREST, a new benchmark to evaluate large language models' (LLMs) understanding of frame of reference (FoR) in spatial reasoning tasks. It proposes Spatial-Guided (SG) prompting to mitigate biases in FoR identification and demonstrates improvements in text-to-image generation by incorporating FoR-aware reasoning."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in AI research by focusing on FoR, a fundamental aspect of spatial cognition. The FoREST benchmark is well-designed with ambiguous (A-split) and clear (C-split) splits to test models under varying conditions. The SG prompting method is innovative, leveraging spatial relations to improve FoR identification. The work bridges theoretical spatial reasoning with practical applications like text-to-image generation. The paper is clearly structured, with detailed explanations of spatial roles, relations, and FoR types."
          },
          "weaknesses": {
            "value": "The paper lacks depth in comparing FoREST with existing benchmarks, such as how it differs from textual-only or text-to-image benchmarks. The SG prompting experiments are limited—only a few models or datasets are evaluated, and the methodology for incorporating spatial relations into prompts is under-described. The dataset construction process (e.g., how A-split/C-split examples are generated) is not sufficiently detailed. The impact of FoR biases on text-to-image generation is demonstrated, but specific metrics (e.g., quantitative improvements in layout accuracy) are missing."
          },
          "questions": {
            "value": "1. How were the A-split and C-split examples generated? Are they balanced in terms of FoR distribution? 2. What spatial relations (directional, topological, distance) are prioritized in SG prompting, and how are they encoded? 3. How does SG prompting compare to other prompting strategies (e.g., instruction tuning or contrastive learning)? 4. Are there cases where SG prompting fails, and what are the limitations of the current FoR classification framework?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "9YhocG0o2l": {
    "paper_id": "9YhocG0o2l",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces ToMValley, a novel benchmark for evaluating large language models' (LLMs) Theory of Mind (ToM) reasoning in realistic social contexts. The benchmark addresses three limitations of existing ToM evaluations: static mental states, independent mental states, and absence of social contexts. It includes 1100 social contexts, 78,100 questions, and manual verification. Experiments show LLMs underperform humans by 11%, particularly in dynamic and compositional reasoning tasks."
          },
          "strengths": {
            "value": "The paper's strengths include a comprehensive framework for constructing ToMValley, which systematically addresses real-world ToM challenges through dynamic mental states, interdependent relationships, and social contexts. The manual quality verification and human baseline add credibility. The evaluation of ten LLMs and detailed analysis of their limitations (e.g., multi-hop reasoning failures) demonstrate practical significance. The paper's structure and clarity are strong, with clear alignment between contributions and methodology."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing benchmarks like SocialIQA or TOMBENCH, making it hard to assess incremental value. Experimental results rely on a single human baseline without statistical significance analysis. The framework's reproducibility is unclear due to the missing figure and truncated README.md link. The 11% performance gap lacks granular breakdowns (e.g., task-specific failures). The absence of ablation studies or analysis of how social contexts impact model performance limits insights."
          },
          "questions": {
            "value": "1. How does ToMValley differ quantitatively from existing benchmarks in terms of task complexity and coverage? 2. What specific compositional reasoning tasks are LLMs failing, and why? 3. How were social contexts and character profiles validated for realism? 4. Can the authors provide more details about the human annotation process and inter-annotator agreement? 5. Are there plans to open-source ToMValley and its framework for reproducibility?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces ToMValley, a new benchmark for evaluating the Theory of Mind (ToM) reasoning of large language models (LLMs) in realistic social contexts. The authors address limitations of existing benchmarks by incorporating dynamic, interdependent mental states, social locations, and character profiles. They evaluate ten LLMs, finding significant performance gaps compared to humans, particularly in multi-hop reasoning tasks."
          },
          "strengths": {
            "value": "The paper's originality lies in its focus on realistic social contexts, addressing gaps in prior benchmarks. The ToMValley framework is methodologically rigorous, with manual quality verification and detailed construction steps. The experimental scope is broad, testing multiple LLMs and prompting strategies. The findings on LLMs' limitations in dynamic and compositional reasoning contribute valuable insights to the field."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with existing benchmarks like SocialIQA or TOMBENCH, making it unclear how ToMValley improves upon them. The 11% human performance advantage is significant but not contextualized with baseline metrics from prior work. The analysis of LLM failures is superficial, with no ablation studies or exploration of whether the benchmark's complexity or model limitations cause the gaps. The paper also omits discussion of practical implications or mitigation strategies for the identified shortcomings."
          },
          "questions": {
            "value": [
              "How does ToMValley specifically address the limitations of prior benchmarks in terms of design and evaluation criteria?",
              "What are the exact metrics used to quantify the 11% performance gap, and how do they compare to baseline results from existing benchmarks?",
              "Are there ablation studies demonstrating the impact of individual components (e.g., social contexts vs. mental state interdependencies) on LLM performance?",
              "How were human annotations conducted, and what was the inter-annotator agreement to ensure reliability of the benchmark?",
              "What specific aspects of multi-hop reasoning in social contexts are most challenging for LLMs, and how could the benchmark be adapted to test targeted improvements?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces ToMValley, a new benchmark for evaluating large language models (LLMs) on their Theory of Mind (ToM) capabilities in realistic social contexts. The authors address limitations of existing benchmarks by incorporating dynamic mental states, interdependent mental states (beliefs, intentions, emotions), and detailed social contexts. They evaluate ten LLMs, finding significant performance gaps compared to human baselines (11% lower) and identify limitations in handling multi-hop reasoning and dynamic state changes."
          },
          "strengths": {
            "value": "The paper's originality lies in addressing three critical gaps in existing ToM benchmarks: static mental states, isolated mental state types, and lack of social context. The ToMValley framework is systematically designed with four steps (social background, mental state sketch, scenario design, question generation) and includes 1100 social contexts with 78,100 questions. The manual verification by human annotators adds credibility. The significance is high, as ToM is crucial for human-like LLM interactions, and the benchmark's scale and realism represent a meaningful advancement."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing benchmarks like SocialIQA and TOMBENCH, making it unclear how ToMValley improves upon them. The human baseline is based on annotator judgments without statistical validation (e.g., inter-annotator agreement). The evaluation of LLMs is limited to ten models, with no analysis of model-specific failures. The paper also does not address how ToMValley's design could generalize to non-English contexts or cultural variations. The claim that LLMs fail multi-hop reasoning is supported by anecdotal evidence rather than systematic ablation studies."
          },
          "questions": {
            "value": "1. How does ToMValley specifically improve upon existing benchmarks like SocialIQA or TOMBENCH in terms of task design and evaluation metrics? 2. What criteria were used to select the 10 LLMs evaluated, and why are other state-of-the-art models (e.g., Llama, Falcon) excluded? 3. How was the human baseline validated statistically (e.g., inter-annotator agreement, error analysis)? 4. Can the framework for generating dynamic mental state questions be adapted to other domains beyond social reasoning? 5. What ablation studies were conducted to isolate the impact of dynamic mental states vs. social context on LLM performance?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "9cQB1Hwrtw": {
    "paper_id": "9cQB1Hwrtw",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates whether transformers can learn to perform graph search tasks, using a controlled graph connectivity problem as a testbed. The authors demonstrate that small transformers can learn to search when trained on a carefully constructed distribution, but struggle with larger graphs. They introduce a novel mechanistic interpretability technique to analyze the learned algorithm, revealing that transformers perform parallel search across all vertices."
          },
          "strengths": {
            "value": "The paper's originality lies in its controlled experimental setup for testing search capabilities, the development of a novel mechanistic interpretability method, and the rigorous analysis of transformer limitations. The experiments are well-designed, with clear ablation studies on training distributions. The significance is high, as it addresses a fundamental challenge in transformer-based reasoning. The paper also provides open-source code, enhancing reproducibility."
          },
          "weaknesses": {
            "value": "The paper lacks detailed justification for how the training distribution was constructed to eliminate heuristics, which is critical to their claims. The mechanistic interpretability analysis is described but not thoroughly validated (e.g., no quantitative measures of the extracted algorithm's accuracy). The experiments on larger graphs are limited in scale, and the comparison to baselines (e.g., rule-based methods) is underdeveloped. The theoretical analysis of why transformers struggle with larger graphs is superficial."
          },
          "questions": {
            "value": "1. How exactly was the training distribution designed to prevent heuristic shortcuts? Were ablation studies conducted to verify this? 2. What metrics were used to validate the correctness of the extracted computation graph from the model? 3. How do the authors ensure that the observed parallel search behavior is not an artifact of the training data? 4. What are the specific limitations of chain-of-thought prompting in this context, and how do they differ from the proposed approach? 5. Are there any theoretical guarantees or hypotheses about the relationship between model depth and search capacity in this setup?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates the ability of transformers to learn search tasks using a graph connectivity problem as a testbed. The authors demonstrate that small transformers can learn to perform search with the right training distribution, but struggle with larger graphs. They introduce a mechanistic interpretability technique to analyze the model's internal algorithm, revealing parallel processing across layers. However, increasing model size or using chain-of-thought prompting does not resolve the limitations."
          },
          "strengths": {
            "value": "The paper's originality lies in its controlled experimental setup using graph connectivity to isolate search capabilities from heuristics. The methodology is rigorous, with careful data generation and mechanistic analysis. The contribution of the novel interpretability technique is significant, offering insights into how transformers process search tasks. The experiments are thorough, addressing scalability, model size, and prompting strategies. The work's significance is high, as it identifies a critical limitation in transformers for search and planning tasks."
          },
          "weaknesses": {
            "value": "The experiments focus on a specific graph connectivity task, which may not generalize to real-world search problems. The mechanistic analysis, while innovative, is limited to the observed computation graph and does not fully explain the model's internal logic. The conclusion that scaling fails to resolve the issue is based on the authors' setup but may not apply to other architectures or training methods. The paper also lacks discussion on potential alternative training approaches to overcome these limitations."
          },
          "questions": {
            "value": "How do the findings translate to more complex or real-world search tasks beyond graph connectivity? What are the limitations of the mechanistic interpretability technique in capturing the full model behavior? Could the training distribution be adapted to other tasks, and how would that affect performance? How do the results compare to non-transformer architectures or alternative training paradigms?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates the ability of transformers to learn search tasks using a graph connectivity problem as a testbed. The authors demonstrate that small transformers can learn to perform search when trained on carefully constructed data but struggle with larger graphs. They introduce a novel mechanistic interpretability technique to analyze the learned algorithm, revealing that transformers perform parallel search across all vertices."
          },
          "strengths": {
            "value": "The paper's originality lies in its focused exploration of transformers' search limitations through a foundational graph problem, combined with a novel mechanistic interpretability approach. The experimental design is rigorous, with synthetic data generation and controlled training distributions. The clarity of the problem statement and methodology is strong, and the significance of addressing search limitations in transformers is high for both theoretical and practical AI research."
          },
          "weaknesses": {
            "value": "The synthetic graph tasks may not generalize to real-world search scenarios, and the paper lacks comparisons with alternative architectures or methods. The interpretability technique is promising but requires more validation through ablation studies or benchmarking against existing approaches. The analysis of scaling laws is limited, and the paper does not fully address how these findings apply to other search-intensive tasks like planning or reasoning."
          },
          "questions": {
            "value": "1. How does the training distribution's design specifically prevent heuristic shortcuts, and what evidence supports its effectiveness? 2. Can the parallel search mechanism be formalized as a specific algorithm (e.g., BFS, DFS) or is it a novel approach? 3. What are the limitations of the interpretability technique in capturing model behavior on more complex or real-world graphs? 4. How do the results compare to other architectures (e.g., graph neural networks) on similar tasks?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "9dBBq2ehY5": {
    "paper_id": "9dBBq2ehY5",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates the relationship between phase transitions in adversarial training (AT) and catastrophic overfitting (CO). The authors analyze the implicit bias of PGD AT, identifying a critical threshold ε_c where the loss landscape transitions, leading to CO. They propose a simplified one-dimensional model to empirically and analytically validate their claims, demonstrating that high curvature solutions emerge for small ε and that CO can occur even with minimal adversarial budgets."
          },
          "strengths": {
            "value": "The paper offers a novel theoretical framework connecting phase transitions in AT to CO, which is a significant and underexplored aspect of adversarial robustness. The use of a simple, analytically tractable model to isolate and study CO is a strong methodological contribution. The work aligns with existing empirical observations and provides a quantitative lens to understand CO, which could guide future regularization strategies. The clarity of the problem statement and the logical flow of the analysis are commendable."
          },
          "weaknesses": {
            "value": "The paper's experimental validation is limited to a highly simplified model, raising questions about the generalizability of the findings to real-world, high-dimensional settings. The theoretical analysis of the phase transition and its link to CO lacks depth, particularly in explaining how the critical threshold ε_c is derived or how it scales with model complexity. The absence of experiments on standard datasets or more complex architectures weakens the practical relevance of the claims. Additionally, the paper does not address how its findings interact with existing regularization techniques for CO."
          },
          "questions": {
            "value": "How do the authors reconcile their findings in the one-dimensional model with the behavior of CO in high-dimensional settings? What are the limitations of their analytical derivation of ε_c, and how might it be extended to more complex models? Are there empirical results demonstrating that their proposed phase transition framework can predict or mitigate CO in practical AT scenarios? How do the high-curvature solutions identified in the paper relate to other known phenomena in AT, such as sharp minima or robustness-accuracy trade-offs?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper investigates the relationship between phase transitions in the loss landscape of adversarial training (AT) and the phenomenon of catastrophic overfitting (CO). It introduces a simplified model with one-dimensional inputs and a single parameter to analytically and empirically demonstrate how a critical threshold ε_c separates regimes where single-step AT is effective from those requiring multi-step PGD or regularization. The work links high curvature solutions in the loss landscape to CO and provides a framework for understanding its onset."
          },
          "strengths": {
            "value": "The paper offers a novel theoretical connection between phase transitions and CO, which is a significant gap in adversarial robustness research. The simplified model enables rigorous analytical treatment, and the controlled experiments provide clear evidence for the proposed mechanism. The clarity of the problem formulation and the focus on implicit bias in PGD AT are strengths. The work also aligns with existing empirical observations, enhancing its relevance."
          },
          "weaknesses": {
            "value": "The analysis is limited to a highly simplified model (1D inputs, single parameter), which may not generalize to real-world settings. The empirical validation lacks diversity (e.g., no experiments on standard datasets like CIFAR-10). The paper does not address how to mitigate CO in practice beyond suggesting regularization, which is already well-known. The theoretical derivation of ε_c assumes bounded parameter norms, which may not hold in complex models. The connection between phase transitions and CO remains abstract without broader implications for AT methods."
          },
          "questions": {
            "value": "How do the findings from the 1D model translate to higher-dimensional settings or real-world models? What are the limitations of the bounded parameter norm assumption in the theoretical analysis? Are there experiments on standard datasets to validate the phase transition hypothesis? How can the insights be leveraged to design more robust AT methods? Does the critical ε_c depend on specific model architectures or training setups?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates the relationship between phase transitions in the loss landscape of adversarial training (AT) and the phenomenon of catastrophic overfitting (CO). The authors propose a theoretical framework linking the critical adversarial budget threshold ε_c to CO, supported by analysis and experiments on a simplified model with one-dimensional inputs and a single parameter. They demonstrate that high curvature solutions emerge in PGD AT, leading to CO when ε exceeds ε_c, and provide an analytical method to compute ε_c under bounded parameter norms."
          },
          "strengths": {
            "value": "The paper offers a novel theoretical perspective by connecting phase transitions in loss landscapes to CO, which is a critical issue in robust ML. The use of a simplified model enables analytical tractability and clear insights into CO's mechanisms. The work aligns with community intuition about AT and provides a structured approach to understanding implicit biases in PGD. The clarity of the problem formulation and the focus on foundational principles are commendable."
          },
          "weaknesses": {
            "value": "The simplicity of the model (1D inputs, single parameter) limits the generalizability of findings to real-world scenarios. The paper lacks empirical validation on more complex architectures or datasets, which weakens the practical relevance. The analysis of high curvature solutions and their connection to CO relies heavily on theoretical assumptions without sufficient experimental corroboration. Additionally, the critical threshold ε_c is derived under specific constraints (e.g., bounded parameter norms), leaving open questions about its applicability to unconstrained settings."
          },
          "questions": {
            "value": "How do the theoretical findings in the 1D model translate to higher-dimensional data and complex neural networks? What are the practical implications of the derived ε_c for real-world adversarial training? Are there empirical comparisons demonstrating the effectiveness of the proposed framework against existing regularization methods? How does the curvature of the loss landscape relate to other factors (e.g., model architecture, data distribution) in inducing CO?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "9dFCm4uZo8": {
    "paper_id": "9dFCm4uZo8",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a framework to analyze compositionality in Vision Transformer (ViT) encoders using Discrete Wavelet Transform (DWT) to define input-specific primitives. The approach draws parallels to compositionality studies in NLP, leveraging DWT's ability to decompose images into meaningful, input-dependent components. Empirical results suggest that ViT representations respect the compositional structure induced by DWT, particularly at the final encoder layer."
          },
          "strengths": {
            "value": "The paper presents a novel application of DWT for analyzing compositionality in ViTs, addressing a gap in vision representation studies. The framework is theoretically grounded in homomorphism principles from \\cite{andreas2019}, offering a structured approach to evaluate compositional properties. The use of DWT as a tool for generating interpretable primitives is creative and well-motivated. The work contributes to the growing body of research on ViT interpretability and provides a foundation for future studies on compositional reasoning in vision models."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, as the content is truncated, making it difficult to assess the thoroughness of the tests or the robustness of the results. The choice of one-level DWT decomposition is not justified, and the impact of wavelet type or multi-level decomposition on compositionality is unclear. The paper does not compare its approach to alternative methods for defining primitives (e.g., CNN features or learned basis sets). Additionally, the empirical analysis does not address whether the observed compositionality is specific to ViTs or generalizable to other architectures."
          },
          "questions": {
            "value": "1. What specific tests were conducted to quantify compositionality, and how do they align with the homomorphism framework? 2. How were DWT primitives selected, and what criteria ensure their 'visually meaningful' nature? 3. Are the results consistent across different ViT architectures (e.g., base vs. large) or datasets? 4. How does the one-level DWT decomposition compare to multi-level approaches in capturing compositional structure? 5. What baseline methods were used to validate the effectiveness of DWT-based primitives?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a framework to analyze compositionality in Vision Transformers (ViTs) using Discrete Wavelet Transforms (DWT). The authors propose leveraging DWT to generate input-specific primitives and evaluate whether ViT encoder representations preserve compositional structure. They empirically show that ViT patch representations at the final encoder layer exhibit compositionality with respect to DWT-based primitives."
          },
          "strengths": {
            "value": "Originality: The paper presents a novel approach to analyzing compositionality in ViTs by using DWT, which is not previously applied to this task. Quality: The framework is theoretically grounded in group theory and compositionality principles. Clarity: The paper is well-structured with clear motivation and technical explanations. Significance: Understanding compositionality in ViTs could improve model interpretability and shed light on their representational learning mechanisms."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation. It does not provide quantitative metrics to measure compositionality (e.g., similarity scores, statistical tests). The comparison to baseline methods or alternative decomposition techniques (e.g., Fourier transforms) is missing. The claim about 'compositional structure' is not rigorously defined or validated. The empirical analysis appears limited to a single ViT variant and dataset, reducing generalizability."
          },
          "questions": {
            "value": [
              "How is compositionality quantified? What specific metrics or statistical tests were used to validate the claim that ViT representations respect DWT-based compositional structure?",
              "Are there ablation studies to evaluate the impact of DWT parameters (e.g., wavelet type, decomposition level) on the results?",
              "How do the results compare to alternative methods for analyzing compositionality in vision models (e.g., CNNs, other decomposition techniques)?",
              "Why is the analysis focused on the last encoder layer? What about intermediate layers?",
              "What is the role of the CLS token in the compositionality analysis? Is it excluded or included in the experiments?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a framework to analyze compositionality in Vision Transformers (ViTs) using Discrete Wavelet Transforms (DWT). The approach leverages DWT to generate input-specific primitives for images, enabling the study of whether ViT representations preserve compositional structure. Empirical results suggest that ViT patch representations at the final encoder layer exhibit compositionality with respect to DWT primitives."
          },
          "strengths": {
            "value": "The paper presents a novel framework for analyzing compositionality in ViTs, which is significant given the lack of prior work on this specific application of DWT. The use of DWT as a tool for defining input-dependent primitives is creative and addresses the challenge of constructing interpretable image dictionaries. The clarity of the theoretical exposition, particularly the connection between homomorphisms and compositionality, is strong. The empirical results, though brief, offer intriguing insights into ViT representations, highlighting the potential for improving model interpretability."
          },
          "weaknesses": {
            "value": "The paper lacks depth in justifying why DWT is the optimal choice for generating primitives compared to alternative methods (e.g., CNN-based feature extractors or other signal decomposition techniques). The empirical analysis is limited in scope, with no discussion of ablation studies, hyperparameter sensitivity, or comparisons to baseline methods. The claim about 'compositionality' is not rigorously quantified, and the paper does not address how DWT-based compositionality relates to downstream task performance. Additionally, the analysis is restricted to a single-level DWT, without exploring multi-level decompositions or their implications."
          },
          "questions": {
            "value": "1. How does the DWT-based framework differ from existing compositionality analysis methods in vision models? 2. What are the limitations of using a single-level DWT, and how might multi-level decompositions affect the results? 3. Are the empirical findings robust across different ViT architectures (e.g., small vs. large models) and datasets? 4. How does the proposed method compare to alternative approaches for analyzing compositional structure in ViTs? 5. Can the observed compositionality be linked to specific architectural components of ViTs (e.g., attention mechanisms or patch embeddings)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "9e5syenoVE": {
    "paper_id": "9e5syenoVE",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces MSB-PRS, a novel variant of multiple-play stochastic bandits that incorporates prioritized resource sharing. The model addresses scenarios where multiple plays compete for arm capacities based on priority weights, with theoretical guarantees on regret bounds and an efficient algorithm for optimal play allocation. The work extends traditional MP-MAB frameworks to capture complex resource allocation dynamics in applications like LLMs and edge computing."
          },
          "strengths": {
            "value": "The paper presents a novel problem formulation (MSB-PRS) that addresses prioritized resource sharing, a critical gap in existing MP-MAB literature. The theoretical contributions are robust, including instance-independent and instance-dependent regret lower bounds, as well as an efficient algorithm (MSB-PRS-OffOpt) with a well-analyzed computational complexity. The work also highlights non-trivial technical challenges arising from nonlinear combinatorial utility functions, demonstrating creativity in addressing these issues. The relevance to real-world applications (e.g., edge computing, LLMs) enhances its significance."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation to support the proposed algorithms, which is a critical omission for a machine learning conference. While the theoretical analysis is strong, the practical feasibility of the O(M³K³) computational complexity for large-scale problems is unclear. Additionally, the connection to prior work on MP-MAB variants could be more explicitly discussed, particularly in terms of how MSB-PRS differs from existing models. The truncated content also prevents a full assessment of the related work and experimental sections."
          },
          "questions": {
            "value": [
              "Are there experimental results demonstrating the effectiveness of the MSB-PRS-OffOpt algorithm and the approximate UCB-based approach in practical scenarios? If not, how do the authors justify the practical relevance of their theoretical contributions?",
              "How does the computational complexity of O(M³K³) scale with large M and K? Are there potential optimizations or approximations to make the algorithm viable for real-world applications?",
              "The paper mentions 'bandit feedback on the capacity' but does not elaborate on how this feedback is modeled or how it interacts with the prioritized sharing mechanism. Can the authors clarify this aspect?",
              "How do the authors compare their regret bounds to existing MP-MAB variants? Are there specific cases where MSB-PRS outperforms prior methods in terms of regret guarantees?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a novel variant of multiple-play stochastic bandits (MSB-PRS) that incorporates prioritized resource sharing, where arms have stochastic capacities and plays compete for them based on priority weights. The authors derive instance-independent and dependent regret lower bounds and propose an algorithm (MSB-PRS-OffOpt) with high computational complexity, along with an approximate UCB-based algorithm. The work addresses challenges from nonlinear combinatorial utility functions induced by prioritized sharing."
          },
          "strengths": {
            "value": "The paper makes significant theoretical contributions by formalizing a new bandit model with prioritized resource sharing, which is relevant to real-world applications like LLMs and edge computing. The derivation of regret lower bounds and the design of algorithms with computational/sample efficiency trade-offs are technically rigorous. The problem formulation extends existing MP-MAB frameworks by introducing nonlinear combinatorial structures, which is a novel direction. The connection between utility maximization and bipartite graph matchings is a creative methodological contribution."
          },
          "weaknesses": {
            "value": "The computational complexity of MSB-PRS-OffOpt (O(M³K³)) is prohibitively high for large-scale applications, and the paper lacks empirical validation to support the theoretical claims. The regret upper bounds for the UCB-based algorithm include factors (e.g., α₁K²) that may not be tight, and the analysis assumes specific structures (e.g., independent groups of classical MABs) that may limit generalizability. The related work section is incomplete, and the paper does not sufficiently contrast its model with existing MP-MAB variants. The practical implications of the prioritized sharing mechanism are not thoroughly discussed."
          },
          "questions": {
            "value": "1. How does the computational complexity of MSB-PRS-OffOpt compare to existing MP-MAB algorithms, and what are the practical limitations of this approach? 2. Are there empirical results demonstrating the effectiveness of the proposed algorithms on real-world or synthetic datasets? 3. How do the derived regret lower bounds compare to existing bounds in similar settings, and what assumptions underlie their tightness? 4. What specific challenges arise from the nonlinear combinatorial utility function, and how does the proposed algorithm address them beyond standard UCB techniques? 5. How does the prioritized resource sharing mechanism differ from existing models (e.g., budget-constrained or switching-cost MP-MAB) in terms of problem formulation and technical challenges?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces MSB-PRS, a variant of multiple-play stochastic bandits that incorporates prioritized resource sharing for resource allocation problems. The work establishes instance-independent and instance-dependent regret lower bounds, proposes an exact optimization algorithm (MSB-PRS-OffOpt) with polynomial complexity, and designs a UCB-based algorithm with matching regret upper bounds. The model addresses nonlinear combinatorial utility functions arising from prioritized capacity allocation."
          },
          "strengths": {
            "value": "Originality: The paper formulates a novel bandit model with prioritized resource sharing, addressing a gap in existing MP-MAB frameworks. Quality: Theoretical analysis is rigorous, with well-constructed lower bounds and algorithmic complexity proofs. Clarity: The problem setup, contributions, and technical details are clearly presented. Significance: The model has direct applications in edge computing and LLM resource allocation, with implications for combinatorial optimization under uncertainty."
          },
          "weaknesses": {
            "value": "The paper lacks empirical validation, which is critical for assessing the practical relevance of the proposed algorithms. The computational complexity of O(M³K³) may be prohibitive for large-scale applications. The connection between the bipartite graph representation and the optimal allocation policy requires more intuitive explanation. The regret upper bounds include factors (e.g., α₁K²) that may not be tight, and the assumptions about reward structures (e.g., σ characterizing reward tails) are not thoroughly discussed."
          },
          "questions": {
            "value": "1. How do the authors validate the practical effectiveness of MSB-PRS-OffOpt and the UCB-based algorithm? 2. What are the limitations of the assumed reward distribution (e.g., sub-Gaussianity)? 3. How does the algorithm scale to large M and K, given the O(M³K³) complexity? 4. Can the prioritized resource sharing mechanism be generalized to non-stationary environments? 5. How do the derived regret bounds compare to existing MP-MAB results in similar settings?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "9juyeCqL0u": {
    "paper_id": "9juyeCqL0u",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes using causal order instead of causal graphs as a more stable output interface for leveraging imperfect experts, such as LLMs or human annotators, in causal inference. The authors introduce a triplet method that queries causal relationships between variable pairs and an auxiliary variable to avoid cycles, improving accuracy and robustness compared to pairwise prompts. They validate this approach on real-world datasets and demonstrate its utility in downstream tasks like effect inference."
          },
          "strengths": {
            "value": "The paper presents a novel perspective by shifting from causal graphs to causal order, addressing the challenge of distinguishing direct vs. indirect effects in pairwise prompts. The triplet method is theoretically justified and empirically shown to reduce cycles and improve accuracy. The work highlights practical benefits of causal order for downstream tasks, such as backdoor adjustment, and demonstrates that smaller models can outperform GPT-4 using their approach. The problem formulation is relevant, and the experiments on real-world graphs provide initial validation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing causal discovery methods that incorporate expert knowledge. The experiments are not sufficiently described (e.g., specific datasets, evaluation metrics, baselines). The theoretical analysis of error reduction with the triplet method is mentioned but not elaborated. The computational efficiency and scalability of the triplet method are not discussed. Additionally, the claim that causal order is sufficient for backdoor adjustment requires further proof or examples."
          },
          "questions": {
            "value": "1. What specific real-world datasets were used for experiments, and how were they selected? 2. How were human annotators and LLMs evaluated for their 'imperfection' in the experiments? 3. What is the exact implementation of the triplet method, and how does the voting-based ensemble work? 4. Are there any limitations to the causal order's applicability in complex graphs? 5. How does the triplet method handle cases where the auxiliary variable introduces noise or bias?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes using causal order instead of causal graphs as a more stable interface for eliciting domain knowledge from imperfect experts, such as LLMs or human annotators. The authors introduce a triplet-based querying strategy to reduce cycles and improve accuracy in causal order estimation, demonstrating its effectiveness in downstream tasks like effect inference and graph discovery."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel perspective by shifting from causal graphs to causal order, addressing the limitations of pairwise prompts in distinguishing direct/indirect effects. The triplet method offers a creative solution to mitigate cycles in imperfect expert outputs. Quality: Theoretical claims about the superiority of causal order over graphs are supported by illustrative examples. Clarity: The abstract and introduction are well-structured, and figures (e.g., Figure 1) effectively highlight key concepts. Significance: The work addresses a critical gap in leveraging imperfect experts for causal inference, with potential applications in real-world scenarios where perfect knowledge is unavailable."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive experimental validation due to truncated content, making it difficult to assess the triplet method's empirical performance. The theoretical analysis of the triplet method's error reduction is not fully developed, and the paper does not discuss potential limitations of causal order (e.g., cases where it is insufficient for downstream tasks). Additionally, the comparison to baselines (e.g., pairwise prompts) is not rigorously evaluated, and the claim that small models outperform GPT-4 lacks detailed evidence. The paper also does not address how causal order integrates with existing causal discovery algorithms beyond theoretical assertions."
          },
          "questions": {
            "value": [
              "What specific real-world graphs were used in the experiments, and how were they validated? How do the results generalize across different domains?",
              "How was the 'voting-based ensemble method' implemented? What criteria were used to resolve conflicts in expert predictions?",
              "What baselines were compared against the triplet method? How does it perform against state-of-the-art causal discovery algorithms?",
              "How does the paper address the scenario where causal order is ambiguous or incomplete? Are there guarantees for downstream task performance?",
              "What metrics were used to quantify the improvement in effect inference and graph discovery tasks? How do these metrics correlate with the proposed $D_{top}$ metric?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 2
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes using causal order as a more reliable output interface for leveraging imperfect experts (e.g., LLMs or human annotators) in causal inference. The authors introduce a triplet prompting strategy that reduces cycles and improves accuracy compared to traditional pairwise prompts, demonstrating its effectiveness across real-world datasets and showing that smaller models can outperform GPT-4 in certain scenarios."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to causal inference by shifting from causal graphs to causal order, addressing the limitations of pairwise prompts in distinguishing direct/indirect effects. The triplet method shows theoretical and empirical advantages in reducing errors and cycles, with experiments validating its effectiveness. The work highlights the practical utility of causal order for downstream tasks like effect inference and graph discovery, and the clear motivation for why causal order is more stable than graphs. The paper also provides concrete algorithms for integrating causal order into existing methods."
          },
          "weaknesses": {
            "value": "The experimental evaluation lacks depth in several areas: (1) The paper does not thoroughly compare the triplet method against state-of-the-art causal discovery algorithms that incorporate expert knowledge. (2) The theoretical analysis of error reduction in the triplet method is limited to a simplified model and lacks rigorous proof. (3) The claims about smaller models (e.g., Phi-3, Llama-3 8B) outperforming GPT-4 are not substantiated with detailed ablation studies or statistical significance tests. (4) The paper does not address how the triplet method scales to very large graphs or handles noisy expert inputs beyond the tested scenarios."
          },
          "questions": {
            "value": "1. How are auxiliary variables selected in the triplet method, and what criteria ensure their effectiveness in breaking cycles? 2. Are the experiments on real-world datasets sufficiently diverse to generalize the findings across domains? 3. What specific downstream tasks (e.g., backdoor adjustment, effect estimation) demonstrate the practical benefits of causal order, and how are these metrics quantitatively evaluated? 4. How does the voting-based ensemble method handle conflicting expert responses within triplets, and what is its computational efficiency? 5. Are there cases where the triplet method fails to produce a valid causal order, and how are such edge cases addressed?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "9ljHiYuRHl": {
    "paper_id": "9ljHiYuRHl",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates the limitations of large language models (LLMs) in causal reasoning from narratives, identifying three key failure modes: reliance on narrative order, over-reliance on parametric knowledge, and struggles with long narratives. The authors validate these through synthetic and real-world experiments, demonstrating that causal graph extraction improves performance compared to chain-of-thought methods."
          },
          "strengths": {
            "value": "The paper's strengths lie in its rigorous experimental design, including controlled synthetic experiments and real-world evaluations on CauseNet. It clearly articulates novel failure modes, such as the impact of narrative order and parametric knowledge conflicts, which are critical for advancing causal reasoning in LLMs. The focus on practical prompting strategies (e.g., causal graph extraction) offers actionable insights. The problem formulation is well-motivated, and the writing is structured to highlight key contributions."
          },
          "weaknesses": {
            "value": "The experiments focus primarily on simple chain graphs, which may not capture the complexity of real-world causal relationships. The synthetic data generation process is not fully detailed, raising questions about its realism. The analysis of parametric knowledge conflicts is superficial, with limited discussion of how these conflicts arise or could be mitigated. Additionally, the real-world evaluations on CauseNet are not sufficiently described, leaving gaps in understanding their scope and limitations."
          },
          "questions": {
            "value": [
              "How do the observed failure modes generalize to more complex causal structures (e.g., Bayesian networks) beyond simple chains?",
              "What specific examples of parametric knowledge conflicts were encountered, and how were they resolved in the experiments?",
              "Could the causal graph extraction method be automated or adapted to other tasks, and what are its computational requirements?",
              "How were the synthetic narratives validated for fidelity to the underlying causal graphs, and what metrics were used?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates the limitations of large language models (LLMs) in causal reasoning tasks involving narratives. The authors identify three key failure modes: over-reliance on event order, parametric knowledge conflicts, and reduced performance with longer narratives. They validate these through synthetic and real-world experiments, demonstrating that explicit causal graph extraction improves performance compared to chain-of-thought methods."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in understanding LLMs' causal reasoning capabilities, which is essential for AI systems requiring robust decision-making. The experimental design is rigorous, with controlled synthetic datasets and real-world validation using CauseNet. The focus on isolating failure modes (e.g., event order, parametric knowledge) provides actionable insights. The paper also highlights the potential of causal graph extraction as a mitigation strategy, which is a novel practical contribution."
          },
          "weaknesses": {
            "value": "The real-world experiments on CauseNet are under-described, with unclear details on how narratives were generated or validated. The comparison to baselines (e.g., chain-of-thought) lacks depth, and the paper does not thoroughly analyze why causal graph extraction outperforms other methods. The related works section is incomplete, potentially obscuring prior art. Additionally, the paper does not address how these failure modes generalize to non-chain graph structures or more complex causal relationships."
          },
          "questions": {
            "value": "1. How were the real-world narratives generated from CauseNet? Were they manually curated or LLM-generated, and how was coherence ensured? 2. What specific metrics were used to evaluate the consistency between LLM outputs and extracted causal graphs? 3. How do the authors account for potential biases in LLM-generated narratives, which might skew results? 4. Are the failure modes observed in simple chain graphs applicable to more complex causal structures? 5. What ablation studies were conducted to isolate the impact of each failure mode?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates failure modes in large language models (LLMs) for causal reasoning on narratives. The authors identify three key limitations: reliance on narrative order, over-reliance on parametric knowledge, and difficulty with long narratives. They validate these through synthetic and real-world experiments, showing that explicit causal graph generation improves performance compared to chain-of-thought methods."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by focusing on narrative-based causal reasoning, a less-explored application of LLMs. The experimental design is rigorous, combining synthetic control experiments with real-world data from CauseNet. The findings on failure modes (e.g., positional shortcuts, parametric knowledge conflicts) are significant and provide actionable insights. The clarity of the problem formulation and structure is excellent, and the work addresses an important gap in understanding LLM reasoning capabilities."
          },
          "weaknesses": {
            "value": "The synthetic experiments lack detailed ablation studies to isolate the impact of specific variables (e.g., narrative length vs. order). The real-world experiments on CauseNet may not fully capture the complexity of natural narratives. The causal graph extraction method is not thoroughly explained, leaving unclear how it outperforms chain-of-thought. Additionally, the paper does not propose solutions to mitigate the identified failure modes beyond suggesting graph generation."
          },
          "questions": {
            "value": "1. How is the causal graph extraction method implemented? Is it based on explicit prompting, post-hoc analysis, or a specialized module? 2. Could the parametric knowledge conflicts be mitigated by fine-tuning LLMs on domain-specific causal data? 3. Are the synthetic narratives sufficiently diverse to generalize across different causal structures? 4. How do the results compare to alternative methods like causal inference models or symbolic reasoning systems?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "9mBodivRIo": {
    "paper_id": "9mBodivRIo",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces LocoVR, a large-scale dataset of 7000+ two-person trajectories in indoor VR environments, capturing geometric and social navigation dynamics. The dataset includes precise spatial data, head orientations, and socially motivated behaviors like proxemics. The authors evaluate its utility on three tasks: global path prediction, trajectory prediction, and goal area prediction, claiming improved model performance compared to existing datasets."
          },
          "strengths": {
            "value": "The paper's primary strength lies in the creation of a novel, large-scale dataset with rich annotations of social navigation behaviors, addressing a gap in existing indoor trajectory datasets. The VR-based data collection method is innovative for capturing complex indoor interactions. The practical evaluation on three tasks demonstrates the dataset's utility. The paper also contextualizes its contribution within relevant literature, highlighting the importance of social proxemics in indoor navigation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental comparisons with state-of-the-art baselines, making it difficult to assess the magnitude of performance gains. The VR system's technical specifics (e.g., tracking accuracy, calibration methods) are under-described, raising questions about data quality. The social navigation analysis is superficial, with no quantitative measures of proxemics adherence. The paper also fails to address limitations of VR-based data, such as potential discrepancies from real-world behavior or biases introduced by simulated environments."
          },
          "questions": {
            "value": [
              "How does the VR-based data collection system ensure fidelity to real-world human motion and social behavior? What metrics were used to validate this?",
              "The paper mentions 'socially-aware navigation patterns' but provides no quantitative analysis of how well models trained on LocoVR capture proxemics. How were social behaviors encoded or evaluated?",
              "What specific challenges arise from using VR for indoor locomotion data, and how were they mitigated? Are there known biases or artifacts in the dataset?",
              "How does LocoVR compare to existing datasets like SIT, JRDB, or SACSON in terms of scene diversity, trajectory density, and social interaction complexity?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces LocoVR, a large-scale indoor trajectory dataset capturing two-person social navigation behaviors in virtual reality. The dataset includes 7000+ trajectories across 130 home environments, focusing on geometric and social proxemics. The authors demonstrate its utility through three tasks: global path prediction, trajectory prediction, and goal area prediction, showing improvements in model performance for socially-aware navigation."
          },
          "strengths": {
            "value": "Originality: LocoVR addresses a critical gap in existing datasets by focusing on multi-person social navigation dynamics in indoor environments, particularly proxemics-based behaviors. Quality: The dataset's scale (7000+ trajectories) and inclusion of precise spatial data, head orientations, and social motion behaviors are promising. Clarity: The paper's structure is logical, with clear motivation, contributions, and task definitions. Significance: The dataset has direct implications for improving AI systems like home robots, which require understanding both geometric and social navigation constraints."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results comparing LocoVR to existing datasets, making it difficult to assess the magnitude of performance improvements. The related work section is incomplete, missing key references that could contextualize LocoVR's novelty. The VR data collection methodology is not thoroughly explained, raising questions about reproducibility and data validity. Additionally, the paper does not address potential biases in the dataset (e.g., cultural, demographic, or scenario diversity)."
          },
          "questions": {
            "value": "1. How were the VR environments designed to ensure realistic social navigation behaviors? Were participants instructed to follow specific social norms? 2. What metrics were used to evaluate the three tasks, and how do they quantify 'socially-aware' navigation? 3. How does LocoVR compare to existing datasets like SIT or JRDB in terms of scene diversity and social interaction complexity? 4. Are there limitations in the dataset's representation of cultural or demographic factors that could affect generalizability?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces LocoVR, a large-scale dataset of two-person indoor locomotion trajectories captured in virtual reality (VR). It addresses the lack of socially-aware indoor trajectory data by leveraging VR to collect detailed spatial and social interaction data across 130+ home environments. The dataset is evaluated on three tasks (global path prediction, trajectory prediction, and goal area prediction) to demonstrate its utility for socially-aware navigation models."
          },
          "strengths": {
            "value": "Originality is evident in the VR-based data collection approach, which enables efficient capture of complex indoor social dynamics. The dataset's scale (7000+ trajectories) and focus on proxemics (e.g., personal space, collision avoidance) address critical gaps in existing datasets. Clarity is strong, with a well-structured presentation of the problem, methodology, and applications. The significance is high, as the dataset fills a key need for training AI systems to navigate human-centric indoor spaces."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing datasets (e.g., SIT, JRDB) to quantify LocoVR's improvements in social dynamics. The evaluation of social navigation performance is limited to three tasks without ablation studies on specific social factors (e.g., cultural differences, relationship types). The VR data collection method's accuracy in capturing real-world physics and social norms is not thoroughly validated. Additionally, the paper does not discuss potential biases in the dataset (e.g., demographics of participants or scene diversity)."
          },
          "questions": {
            "value": "How does LocoVR's social dynamics annotation compare to ground-truth human annotations? What specific challenges were encountered in translating VR locomotion to real-world indoor navigation? Are there plans to expand the dataset to include multi-person interactions beyond two individuals? How was the dataset validated for generalizability across different cultural or demographic contexts?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "9mO9CNgNrh": {
    "paper_id": "9mO9CNgNrh",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces TableTextGrad, a framework for table understanding that leverages textual gradients to optimize prompts for large language models (LLMs). It combines chain-of-thought reasoning with non-destructive function calls (e.g., italicizing relevant cells) to iteratively refine prompts and improve table reasoning accuracy, achieving state-of-the-art results on benchmarks like WikiTableQA."
          },
          "strengths": {
            "value": "Originality is demonstrated by extending TextGrad to table reasoning, integrating textual gradients for prompt optimization, and introducing non-destructive functions. The quality of experiments is strong, with competitive results on established benchmarks. Clarity is maintained through structured sections and a figure illustrating the framework. Significance is high due to the practical importance of table understanding and the potential of prompt optimization for LLMs without fine-tuning."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology on how textual gradients are calculated or implemented, particularly for non-destructive functions like italicization. The comparison with existing prompt optimization methods (e.g., TextGrad, CoT) is insufficient. The computational cost and scalability of the approach are not discussed. Additionally, the paper does not address potential limitations, such as dependency on LLM feedback quality or performance on tables with extreme structural complexity."
          },
          "questions": {
            "value": "1. How are non-destructive functions (e.g., italicizing cells) implemented in practice without altering the table's underlying data? 2. What specific mechanisms are used to compute textual gradients for prompt refinement? 3. Are ablation studies provided to validate the contribution of key components (e.g., chain-of-thought steps vs. non-destructive functions)? 4. How does TableTextGrad compare to program-aided methods (e.g., Text-to-SQL) in terms of robustness to complex tables?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces TableTextGrad, a framework for table understanding that leverages textual gradients to automatically optimize prompts for large language models (LLMs). It combines multi-step chain-of-thought reasoning with non-destructive functions (e.g., soft selection of table cells) to improve table reasoning tasks without fine-tuning. The approach achieves state-of-the-art results on benchmarks like WikiTableQA and TabFact."
          },
          "strengths": {
            "value": "The paper presents a novel framework that addresses a critical challenge in table understanding: prompt optimization without fine-tuning. The integration of multi-step reasoning and non-destructive functions (e.g., italicizing relevant cells) is a significant contribution, as it preserves contextual information while improving accuracy. The experimental results on key benchmarks demonstrate strong performance, and the paper effectively contextualizes its work within existing literature on LLM prompting and table reasoning. The clarity of the methodology and the visual representation of the framework are also notable strengths."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of specific components (e.g., textual gradients, non-destructive functions). The mechanism for calculating textual gradients is not thoroughly explained, leaving ambiguity about how feedback from LLMs is translated into prompt refinements. Additionally, the comparison with existing methods like TextGrad is superficial, and the paper does not fully address potential limitations (e.g., scalability to larger tables or multilingual settings). The experimental section also omits baseline comparisons with other inference-only methods beyond the ones mentioned."
          },
          "questions": {
            "value": [
              "How are textual gradients computed in practice? What specific feedback from LLMs is used to guide the gradient updates?",
              "What are the limitations of the non-destructive functions (e.g., italicizing cells)? How do they handle cases where hard selection might be necessary?",
              "How does TableTextGrad compare to program-aided methods (e.g., Text-to-SQL) in terms of performance and flexibility?",
              "Are there any quantitative analyses of the computational cost or inference time of TableTextGrad compared to existing approaches?",
              "How generalizable is the framework to other table-based tasks beyond question-answering (e.g., data summarization or schema matching)?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces TableTextGrad, a framework that leverages textual gradients to automatically optimize prompts for table understanding tasks. By iteratively refining Chain-of-Thought steps and function calls, the method improves table reasoning without requiring model fine-tuning. It also introduces non-destructive functions for soft selection of table elements, achieving state-of-the-art results on benchmarks like WikiTableQA and TabFact."
          },
          "strengths": {
            "value": "The paper presents a novel approach by extending TextGrad to table reasoning, combining Chain-of-Thought with textual gradient optimization. The non-destructive function design (e.g., italicizing cells) addresses a key limitation of hard selection methods. The experiments demonstrate significant improvements on standard benchmarks, and the framework's simplicity and effectiveness are highlighted as valuable contributions. The work bridges the gap between inference-only techniques and data-driven learning, offering a promising direction for prompt engineering."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how sensitive TableTextGrad is to the initial prompt design, which is critical for its success. The experiments focus on specific datasets (WikiTableQA, TabFact) but do not evaluate generalization to other table types or domains. The description of the framework's architecture and textual gradient computation is vague, making it difficult to assess the technical novelty. Additionally, the paper does not compare against baseline methods that use similar prompting strategies, weakening the claims of superiority."
          },
          "questions": {
            "value": [
              "How does TableTextGrad handle tables with highly ambiguous or non-standard structures? Are there specific cases where the framework fails?",
              "What ablation studies were conducted to validate the contribution of non-destructive functions versus traditional hard selection methods?",
              "How is the 'textual gradient' computed? Are there quantitative metrics to measure the effectiveness of prompt refinement during training?",
              "The paper claims state-of-the-art results but does not provide a detailed comparison with existing prompting methods (e.g., CoT, LEVER, or program-aided approaches). How does TableTextGrad outperform these methods?",
              "What are the computational costs of the framework compared to fine-tuning or other prompting strategies?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "9mOs2Bxd3Q": {
    "paper_id": "9mOs2Bxd3Q",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper extends linear stability analysis to adaptive optimization algorithms like Adam and RMSProp by introducing a generalized coherence measure that quantifies the interaction between adaptive preconditioners and the Hessian of the loss function. The authors derive theoretical conditions for linear stability near stationary points and validate their findings through experiments on benchmark architectures like ResNet and Vision Transformers."
          },
          "strengths": {
            "value": "The paper demonstrates originality by applying stability analysis to adaptive optimizers, a less-explored area. The theoretical framework is novel, offering insights into how adaptive methods interact with loss surface geometry. The experiments on diverse architectures (ResNet, Vision Transformers) and the practical hyperparameter tuning guidelines highlight the significance of the work. The clarity of the problem statement and notation is commendable, though some sections could be more detailed."
          },
          "weaknesses": {
            "value": "The paper lacks depth in explaining how the generalized coherence measure is computed or implemented in practice. The experimental validation is limited to a single dataset (CIFAR-10) and a brief comparison, which may not fully demonstrate the generality of the findings. The theoretical analysis assumes idealized conditions (e.g., stationary points), which may not hold in real-world training scenarios. Additionally, the paper does not address how the coherence measure could be optimized during training."
          },
          "questions": {
            "value": "1. How is the generalized coherence measure computed in practice, and what are its computational costs? 2. Are the theoretical conditions for stability applicable to non-stationary points or dynamic loss landscapes? 3. How do the proposed hyperparameter tuning guidelines compare to existing methods like learning rate schedules or adaptive adjustments? 4. Could the coherence measure be extended to other adaptive methods (e.g., AdaGrad) or non-convex optimization settings?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper extends linear stability analysis to adaptive optimization algorithms like Adam and RMSProp, introducing a generalized coherence measure to quantify the interaction between adaptive preconditioners and the Hessian of the loss function. The authors derive stability conditions and provide hyperparameter tuning guidelines to improve generalization, validated on benchmark datasets and architectures."
          },
          "strengths": {
            "value": "The paper addresses a significant gap in understanding the stability and generalization of adaptive optimizers, which is critical for deep learning. The theoretical framework extends prior work on SGD stability to adaptive methods, and the introduction of a coherence measure offers new insights. The experiments on ResNet and Vision Transformers demonstrate practical relevance, and the paper clearly connects theoretical findings to empirical results."
          },
          "weaknesses": {
            "value": "The theoretical analysis lacks depth in addressing the dynamic nature of adaptive preconditioners, such as Adam's momentum and bias correction. The experiments are limited to a single dataset (CIFAR-10) and architecture (ResNet-50), with no comparison to other adaptive methods like RMSProp or AdaGrad. The hyperparameter tuning guidelines are not empirically validated across diverse settings, and the paper does not discuss how the coherence measure is computed in practice."
          },
          "questions": {
            "value": "1. How does the proposed coherence measure account for the non-stationary nature of adaptive preconditioners (e.g., Adam's moving averages)? 2. Are the theoretical stability conditions validated on additional datasets/architectures beyond CIFAR-10/ResNet-50? 3. How sensitive are the results to the choice of hyperparameters, and what specific guidelines are recommended for tuning? 4. Why was Vision Transformers not used in the experiments, given their mention in the abstract?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper extends linear stability analysis to adaptive optimization algorithms like Adam and RMSProp, introducing a generalized coherence measure to quantify their interaction with loss surface geometry. The work provides theoretical conditions for stability and practical hyperparameter tuning guidelines, validated through experiments on benchmark datasets and architectures."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in understanding the generalization behavior of adaptive optimizers, which is a significant problem in deep learning. The theoretical framework is novel, with the generalized coherence measure offering a fresh perspective on optimizer dynamics. The experiments on standard architectures (e.g., ResNet, Vision Transformers) and datasets (e.g., CIFAR-10) demonstrate practical relevance. The paper is well-structured, with clear definitions and logical flow."
          },
          "weaknesses": {
            "value": "The theoretical analysis is incomplete due to the paper being cut off, making it difficult to assess the rigor of the derivations. The motivating example (ResNet-50 on CIFAR-10) is limited in scope and does not generalize to other architectures or datasets. The coherence measure's practical implementation and computational complexity are not discussed. The paper lacks ablation studies or comparisons with alternative approaches to validate the necessity of the proposed measure."
          },
          "questions": {
            "value": [
              "How is the generalized coherence measure computed in practice, and what are its computational costs?",
              "What specific hyperparameters are tuned based on the theoretical conditions, and how do they compare to existing heuristics?",
              "Are the experimental results reproducible across different datasets and architectures beyond CIFAR-10 and ResNet?",
              "How does the coherence measure relate to existing sharpness metrics, and does it capture unique aspects of adaptive optimizer behavior?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "9p2YMVs1Tl": {
    "paper_id": "9p2YMVs1Tl",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes SHARP, a novel framework for solving Mixed-Integer Linear Programming (MILP) problems by integrating a Sinkhorn-normalized Edge Attention Network (SKEGAT) with an adaptive regret-greedy search algorithm. The approach emphasizes edge features in bipartite graph representations of MILP problems, addressing limitations in existing ML-based methods that focus primarily on node features. Experiments on combinatorial optimization tasks demonstrate significant improvements in solution accuracy and computational efficiency compared to state-of-the-art methods."
          },
          "strengths": {
            "value": "The paper introduces a novel integration of edge features into MILP solving, which addresses a critical gap in existing work. The use of Sinkhorn normalization for feature stabilization and the adaptive regret-greedy search strategy show creative problem-solving. The experimental results on benchmark problems (e.g., Combinatorial Auction and Item Placement) suggest practical relevance. The framework's scalability and reduced reliance on hand-crafted tuning are promising contributions to the field."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with critical sections (e.g., full experimental results, detailed ablation studies) missing, making it difficult to assess the validity of claims. The novelty of combining EGAT with Sinkhorn normalization is not sufficiently contextualized against prior work (e.g., how does this differ from existing graph neural network approaches?). The adaptive regret-greedy algorithm's mechanism is under-described, and its theoretical justification is unclear. The comparison to SOTA methods lacks statistical significance analysis and detailed runtime benchmarks."
          },
          "questions": {
            "value": [
              "Please provide the full experimental results, including comparisons with additional baselines (e.g., Neural Diving, SelectiveNet) and statistical significance tests.",
              "Clarify the novelty of the Sinkhorn-normalized EGAT architecture. How does it differ from existing graph attention mechanisms? Are there theoretical guarantees for the normalization process?",
              "Explain the adaptive regret-greedy search algorithm's design. How does it dynamically adjust to problem structures, and what is its computational complexity?",
              "What specific limitations of prior work (e.g., Neural Diving) does SHARP overcome? Provide quantitative evidence of improvements in constraint satisfaction rates.",
              "Include detailed runtime metrics to demonstrate computational efficiency claims, especially for large-scale problems."
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes SHARP, a novel framework for solving Mixed-Integer Linear Programming (MILP) problems by integrating a Sinkhorn-normalized Edge Attention Network (EGAT) with an adaptive regret-greedy search algorithm. The approach emphasizes edge feature representation in bipartite graph formulations of MILP, aiming to improve solution accuracy and computational efficiency compared to existing methods."
          },
          "strengths": {
            "value": "The paper introduces a novel combination of edge-aware graph neural networks (EGAT) with Sinkhorn normalization, addressing a gap in prior work that focused primarily on node features. The adaptive regret-greedy search method shows promise for handling scalability and reducing manual tuning. The experimental results on combinatorial problems (Combinatorial Auction and Item Placement) demonstrate significant improvements over state-of-the-art solvers like Gurobi and SCIP, suggesting practical relevance. The paper also provides a clear problem formulation and contextualizes its contributions within existing ML-based MILP approaches."
          },
          "weaknesses": {
            "value": "The experimental validation is limited to two specific problem types (Combinatorial Auction and Item Placement), without testing on standard MILP benchmarks like MIPLIB. The comparison to SOTA methods lacks detailed statistical analysis (e.g., p-values, confidence intervals) and fails to address how SHARP performs on larger-scale or more complex instances. The adaptive regret-greedy algorithm is described at a high level, with minimal technical details on its implementation or theoretical guarantees. The paper also does not thoroughly discuss how the Sinkhorn normalization interacts with the EGAT architecture or justify its superiority over alternative normalization techniques."
          },
          "questions": {
            "value": "1. Were the experiments conducted on standard MILP benchmarks (e.g., MIPLIB) or only on custom combinatorial problems? 2. How does the adaptive regret-greedy algorithm differ from existing post-search heuristics, and what specific theoretical guarantees does it provide? 3. What ablation studies were performed to isolate the contribution of Sinkhorn normalization versus EGAT? 4. How does SHARP scale to problems with significantly larger constraint matrices or higher-dimensional variable spaces? 5. Could the reported improvements over Gurobi/SCIP be attributed to the specific problem instances used, and how generalizable are these results?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper proposes SHARP, a novel framework for solving Mixed-Integer Linear Programming (MILP) problems by integrating edge-aware graph neural networks (SKEGAT) with an adaptive regret-greedy search algorithm. It addresses the underutilization of edge features in existing ML approaches for MILP and claims significant improvements in solution accuracy and computational efficiency over state-of-the-art methods."
          },
          "strengths": {
            "value": "The paper introduces a novel focus on edge features in MILP representation, which is a critical gap in existing literature. The integration of Sinkhorn normalization for feature stabilization and the adaptive regret-greedy search algorithm show potential for scalability and generalization. The experimental evaluation on combinatorial problems with multiple metrics (Primal Gap, Survival Rate, Primal Integral) demonstrates practical improvements over traditional solvers and ML-based methods. The framework's ability to capture subtle problem structure differences is a notable strength."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the contribution of edge features versus node features. The adaptive regret-greedy algorithm is described briefly without sufficient technical depth or justification for its design. Experimental results are limited to two problem types (Combinatorial Auction and Item Placement), and the paper does not discuss scalability to larger or more complex MILP instances. The claims of 'notable enhancements' lack statistical significance testing or comparisons with recent ML-based methods beyond Gurobi and SCIP."
          },
          "questions": {
            "value": [
              "How does the Sinkhorn normalization specifically improve the training stability and performance of SKEGAT compared to other normalization techniques?",
              "What ablation studies were conducted to quantify the contribution of edge features versus node features in the model's performance?",
              "Can the adaptive regret-greedy algorithm be generalized to other problem domains beyond the tested combinatorial optimization tasks?",
              "How does SHARP handle the computational overhead of Sinkhorn normalization on large-scale MILP instances?",
              "What are the specific limitations of the framework in terms of problem size or constraint complexity that were not addressed in the experiments?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "9soA8GWQ9g": {
    "paper_id": "9soA8GWQ9g",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces outer-PPO, a framework that decouples the inner-loop update estimation from the outer-loop application in Proximal Policy Optimization (PPO). By treating the outer loop as a gradient-based optimization problem with arbitrary learning rates and momentum, the authors investigate whether non-unity learning rates, momentum, and biased initialization improve performance compared to standard PPO. They evaluate these modifications on Brax, Jumanji, and MinAtar environments, reporting statistically significant improvements on Brax and Jumanji."
          },
          "strengths": {
            "value": "The paper provides a novel decomposition of PPO into inner and outer loops, offering fresh insights into its design. The empirical investigation is rigorous, with extensive hyperparameter sweeps (38,400 agent trains) and open-sourced results for reproducibility. The questions addressed (e.g., optimality of unity learning rates) are relevant to PPO's practical deployment. The work also highlights practical implications for practitioners by suggesting non-unity learning rates as a simple improvement."
          },
          "weaknesses": {
            "value": "The theoretical justification for outer-PPO's improvements is limited, with no analysis of why non-unity learning rates or momentum yield better results. The experiments focus on a narrow set of environments (Brax, Jumanji, MinAtar), and the lack of improvement on MinAtar raises questions about the generality of findings. The comparison to standard PPO is constrained to a fixed hyperparameter tuning budget, leaving open whether more extensive tuning could close the gap. The paper also lacks ablation studies on the impact of individual components (e.g., momentum vs. learning rate)."
          },
          "questions": {
            "value": [
              "How do the improvements on Brax and Jumanji compare to other state-of-the-art RL algorithms beyond PPO? Are the gains specific to PPO's architecture?",
              "What is the computational cost of outer-PPO compared to standard PPO? Does the use of arbitrary optimizers introduce significant overhead?",
              "Why did none of the proposed methods improve performance on MinAtar? Is this due to the environment's specific challenges or limitations in the outer-PPO framework?",
              "Were the hyperparameters for outer-PPO (e.g., learning rates, momentum values) tuned as thoroughly as the baseline PPO? Could the results be sensitive to this tuning?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes Outer-PPO, a framework that decouples the inner-loop update estimation and outer-loop parameter application in Proximal Policy Optimization (PPO). By treating the outer loop as a gradient-based optimization step with arbitrary learning rates and momentum, the authors empirically investigate whether PPO's conventional design choices (unity learning rate, no momentum, and non-biased initialization) are optimal. They demonstrate improvements on Brax and Jumanji environments with non-unity learning rates and momentum, while MinAtar shows no gains."
          },
          "strengths": {
            "value": "The paper presents a clear and structured decomposition of PPO into inner/outer loops, enabling novel empirical investigations. The methodological approach is original in challenging conventional PPO design choices. The experiments are extensive, with 38,400 agent training runs and open-sourced hyperparameter sweeps, enhancing reproducibility. The questions raised about PPO's implicit assumptions are theoretically relevant. The writing is precise, with illustrative diagrams and a logical flow from problem formulation to empirical validation."
          },
          "weaknesses": {
            "value": "The theoretical analysis is limited, with no formal justification for why non-unity learning rates or momentum improve performance. The comparison to the baseline PPO is constrained by fixed hyperparameter tuning budgets, which may not reflect optimal configurations. The lack of ablation studies on the impact of different outer-loop optimizers (e.g., Adam vs. SGD) limits insight into the generalizability of findings. The failure to improve MinAtar results is unexplained, raising questions about the approach's scalability. The paper also lacks discussion of computational overhead from the decoupled framework."
          },
          "questions": {
            "value": "1. How do the improvements on Brax/Jumanji scale to larger or more complex environments? 2. What is the mechanism behind the effectiveness of non-unity learning rates—does it relate to gradient noise, curvature, or exploration? 3. Why does the biased initialization only improve Jumanji and not Brax? 4. Could the MinAtar results be due to suboptimal hyperparameter choices rather than inherent limitations of Outer-PPO? 5. How does the decoupling affect sample efficiency or stability in long-term training?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces outer-PPO, a framework that decouples the inner-loop estimation of policy updates from the outer-loop application of these updates using arbitrary gradient-based optimizers. The work empirically investigates whether PPO's design choices—such as unity learning rates, lack of momentum, and initialization at behavior parameters—are optimal, demonstrating improvements with non-unity learning rates and momentum on certain environments."
          },
          "strengths": {
            "value": "The paper offers a novel decomposition of PPO into inner and outer loops, enabling new experimental insights. The empirical investigation is thorough, with extensive hyperparameter sweeps (38,400 agent training runs) and open-sourced results for reproducibility. The framework is well-motivated, and the questions addressed (e.g., optimal learning rates, momentum, and initialization) are relevant to PPO's design. The results on Brax and Jumanji show statistically significant improvements, highlighting practical value."
          },
          "weaknesses": {
            "value": "The theoretical analysis is limited, with no justification for why non-unity learning rates or momentum improve performance. The experiments focus on specific environments (Brax, Jumanji, MinAtar), and the lack of improvement on MinAtar raises questions about generalizability. The paper does not explore alternative optimizers beyond learning rates and momentum, and the connection between the outer-PPO framework and existing literature (e.g., trust-region methods) is underdeveloped."
          },
          "questions": {
            "value": "1. How sensitive are the results to the choice of inner-loop optimization method (e.g., ADAM vs. SGD)? 2. What is the impact of the biased initialization on environments beyond Jumanji? 3. Why did MinAtar not benefit from outer-PPO, and could this be due to specific task characteristics? 4. Are the observed improvements due to the decoupling of loops or other factors (e.g., hyperparameter tuning differences)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "9spNhEw6qf": {
    "paper_id": "9spNhEw6qf",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates grokking in data regimes below the critical threshold by leveraging knowledge distillation (KD) from a grokked model to induce generalization on different distributions. The authors demonstrate that KD can accelerate grokking, reduce the critical data size, and enable generalization in low-data scenarios. They also challenge the necessity of weight decay for grokking and explore applications in larger models and continual pretraining settings."
          },
          "strengths": {
            "value": "Originality: The paper introduces novel applications of KD to induce grokking in low-data regimes and challenges prior assumptions about weight decay's role. Quality: Experiments are well-designed, with controlled tasks (e.g., addition/subtraction) and clear comparisons. Clarity: The problem formulation, methodology, and figures (e.g., Figure 1) are clearly presented. Significance: The findings have practical implications for continual learning, domain adaptation, and resource-constrained scenarios, addressing gaps in prior work focused on single-distribution grokking."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with alternative regularization techniques (e.g., dropout, data augmentation) to validate KD's superiority. The experiments are limited to algorithmic tasks (e.g., arithmetic), which may not generalize to real-world data. The theoretical analysis of why KD enables grokking below critical data is superficial, with no rigorous justification for the observed phenomena. The paper also does not address potential limitations of KD, such as overfitting to the teacher model's biases or scalability to more complex architectures."
          },
          "questions": {
            "value": "1. How do the results generalize to real-world datasets beyond algorithmic tasks? 2. What is the exact mechanism by which KD enables grokking below the critical data threshold? 3. How sensitive are the results to the quality of the teacher model (e.g., if the teacher fails to grok)? 4. Are there scenarios where KD fails to induce grokking, and how do these compare to prior methods? 5. How is the 'critical data size' defined across different tasks, and does this vary significantly?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates the grokking phenomenon in data regimes below the critical threshold, focusing on how knowledge distillation (KD) from a pre-grokked model can induce grokking on different distributions, reduce critical data requirements, and challenge the necessity of weight decay/weight norm reduction. It also explores distilling knowledge into larger models and continual pretraining scenarios."
          },
          "strengths": {
            "value": "Originality is strong in applying KD to transfer grokking across distributions and questioning weight decay as a sole driver. The experiments on algorithmic tasks (addition/subtraction) provide controlled validation. Clarity is good, with figures illustrating key findings. Significance lies in addressing low-data generalization, a critical challenge in ML. The paper also introduces practical applications for continual learning and domain adaptation."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with prior work on grokking (e.g., Power et al. 2022, Varma et al. 2023) to contextualize its contributions. Experimental details are sparse (e.g., hyperparameters, baseline comparisons, critical data size determination). The claim that weight norms are not necessary is under-supported, with no visualizations or statistical analysis of weight dynamics. The continual pretraining setup is vaguely described, and the role of KD in mitigating catastrophic forgetting is not rigorously analyzed."
          },
          "questions": {
            "value": "1. How were the critical data thresholds determined in the experiments, and how do they align with prior definitions (e.g., Liu et al. 2022b)? 2. Were the experiments conducted on the same algorithmic tasks as prior studies (e.g., Power et al. 2022) for direct comparison? 3. What specific metrics or analyses were used to conclude that weight norms are not necessary for grokking? 4. How was the transition between distributions in continual pretraining managed, and what strategies were used to prevent catastrophic forgetting beyond KD?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates grokking in low-data regimes, demonstrating that knowledge distillation (KD) from a grokked model can induce generalization even when training data is below the critical threshold. The authors explore scenarios involving KD transfer, larger models, and continual pretraining, showing that weight decay is not a necessary condition for grokking and that KD accelerates generalization under data scarcity."
          },
          "strengths": {
            "value": "Originality is strong, as the paper introduces novel applications of KD to induce grokking in low-data settings, expanding beyond prior work focused on single distributions. The quality of experiments is solid, with controlled tasks (e.g., arithmetic) and clear empirical validation of claims. Clarity is good, with figures effectively illustrating key findings. Significance is high, addressing practical challenges in data-scarce scenarios and offering insights into knowledge transfer mechanisms."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons with alternative methods (e.g., other regularization techniques) for inducing grokking. The experiments focus on narrow tasks (e.g., addition/subtraction), limiting generalizability. The claim that weight decay is not a driver is supported but not deeply analyzed, leaving open questions about other potential factors. The critical data size definition is referenced but not critically evaluated in the context of the proposed methods."
          },
          "questions": {
            "value": "How do the results generalize to more complex tasks or architectures beyond arithmetic? What is the mechanism by which KD accelerates grokking—does it encode distributional knowledge or stabilize training? Are there cases where KD fails to induce grokking despite low data? How does the continual pretraining setup handle distributions beyond p1/p2? Could the observed effects be attributed to specific KD hyperparameters rather than the method itself?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "9uswuRBLm0": {
    "paper_id": "9uswuRBLm0",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces Cyclic Neural Networks (Cyclic NN), a novel architecture that deviates from the traditional layer-by-layer Directed Acyclic Graph (DAG) structure of artificial neural networks (ANNs). Inspired by biological neural systems, which form complex graph-like structures with cycles, the authors propose a model allowing flexible, non-DAG neuron connections. They claim experimental superiority over existing methods, particularly with their Forward-Forward (FF) training algorithm, and introduce a 'Graph Over Multi-layer Perceptron' model as a concrete example."
          },
          "strengths": {
            "value": "The paper's originality lies in challenging the long-standing DAG constraint in ANN design, drawing inspiration from biological neural systems. The conceptual framework is novel, and the methodological proposal of a cyclic architecture with localized training aligns with biological plausibility. The significance is high if the claims about flexibility and performance gains hold, and the paper's structure is logically organized, though lacking in technical depth."
          },
          "weaknesses": {
            "value": "The paper lacks concrete experimental validation. It mentions 'widely tested datasets' and 'superiority over current DAG networks' but provides no specific results, metrics, or comparisons. The Forward-Forward training algorithm's claimed advantage over backpropagation is not substantiated. The computational neuron design is under-specified, with no details on parameterization or how it differs from traditional neurons. The paper also ignores critical challenges in cyclic networks, such as training stability, gradient flow, and potential issues like vanishing gradients. The biological analogies are vague and not rigorously connected to the proposed architecture."
          },
          "questions": {
            "value": [
              "Which specific datasets were used to validate the Cyclic NN, and what were the quantitative results compared to baseline DAG models?",
              "How exactly does the Forward-Forward algorithm outperform backpropagation? What metrics (e.g., accuracy, loss, training speed) were used, and how were they measured?",
              "What is the exact parameterization of the 'computational neuron'? How does it differ from traditional neurons, and what evidence supports its increased computational capacity?",
              "How are cycles in the network handled during training? Are there mechanisms to prevent instability or divergence?",
              "What ablation studies were conducted to isolate the impact of cyclic connections versus other components of the proposed framework?",
              "How is the graph structure of the Cyclic NN determined? Is it learned or manually designed, and what criteria guide its construction?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Cyclic Neural Networks (Cyclic NN), a novel architecture that challenges the traditional layer-by-layer Directed Acyclic Graph (DAG) structure of artificial neural networks (ANNs). Inspired by biological neural systems, Cyclic NN allows for flexible, graph-structured connections, including cycles, and proposes the Graph Over Multi-layer Perceptron (GOMLP) as a concrete model. The authors claim superior performance over DAG-based ANNs, particularly when trained with the Forward-Forward (FF) algorithm, and argue for a paradigm shift in ANN design."
          },
          "strengths": {
            "value": "The paper's primary strength lies in its conceptual innovation, challenging the long-standing DAG assumption in ANNs and drawing inspiration from biological neural systems. The idea of localized training and computational neurons with enhanced capacity is promising, offering a fresh perspective on neural network design. The paper also highlights the potential of the FF algorithm to outperform backpropagation, which could spark further research. However, the lack of detailed technical descriptions and experimental validation limits the impact of these contributions."
          },
          "weaknesses": {
            "value": "The paper lacks concrete experimental results to substantiate its claims. While it mentions validation on 'widely tested datasets,' no specific datasets, metrics, or baselines are provided. The GOMLP model is not clearly defined, and the technical details of how Cyclic NN avoids the limitations of DAG structures (e.g., gradient propagation) are insufficient. The claim that FF outperforms backpropagation is unverified without proper comparisons. Additionally, the paper does not address critical challenges in cyclic networks, such as stability, convergence, or handling feedback loops, which are essential for practical deployment."
          },
          "questions": {
            "value": "1. How is the Cyclic NN trained without backpropagation, and what mechanisms ensure stability in cyclic architectures? 2. What specific datasets and metrics were used to validate the superiority of Cyclic NN and FF training? 3. How does GOMLP differ from existing cyclic models (e.g., RNNs, GRUs, or graph neural networks)? 4. Are there theoretical guarantees for the convergence of localized training in Cyclic NN? 5. How does the computational neuron's increased capacity (e.g., more parameters) affect training efficiency and generalization?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Cyclic Neural Networks (Cyclic NN), a novel architecture that deviates from traditional layer-by-layer Directed Acyclic Graph (DAG) structures by allowing cyclic, graph-like connections inspired by biological neural systems. The authors propose a 'Graph Over Multi-layer Perceptron' model and claim superiority over conventional ANNs through localized training and Forward-Forward (FF) algorithms, emphasizing flexibility and biological plausibility."
          },
          "strengths": {
            "value": "The paper presents a compelling conceptual shift by challenging the DAG constraint in ANNs, drawing meaningful parallels to biological systems. The methodological innovation of localized training and computational neurons offers a fresh perspective on network design. The experimental claims of FF outperforming backpropagation are significant, and the paper's emphasis on biological inspiration aligns with growing interest in biologically plausible AI. The clarity of the problem statement and structure is strong, with a clear narrative connecting theory to applications."
          },
          "weaknesses": {
            "value": "The paper lacks concrete technical details on how cyclic structures are trained without backpropagation, leaving critical gaps in understanding. The experimental validation is superficial, with no ablation studies or comparisons to non-DAG baselines. The claim that FF outperforms backpropagation is overstated without rigorous benchmarks. The biological analogies are metaphorical rather than technically grounded, and the computational neuron design is under-explained. The paper also fails to address potential issues like training stability in cyclic networks."
          },
          "questions": {
            "value": "1. How is gradient-free training implemented in cyclic networks? What mechanisms prevent divergence in cyclic structures? 2. What specific architectural details define the 'Graph Over Multi-layer Perceptron' model? 3. Are there ablation studies showing the contribution of cyclic connections vs. localized training? 4. How does the computational neuron's increased capacity relate to its parameterization (e.g., linear layers)? 5. What datasets and baselines were used for FF vs. backpropagation comparisons?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "9ut3QBscB0": {
    "paper_id": "9ut3QBscB0",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces 'normality normalization,' a novel normalization layer for deep neural networks that encourages normality in feature representations using the power transform and additive Gaussian noise. The method is motivated by the information-theoretic properties of the normal distribution, such as maximal robustness to noise and maximal entropy under moment constraints. The authors claim improvements in generalization, robustness, and adaptability across model configurations."
          },
          "strengths": {
            "value": "Originality is strong, as the paper creatively combines the power transform with normalization layers, addressing a gap in prior work on distributional prescriptions for activations. The theoretical foundation in information theory (mutual information games, entropy maximization) is rigorous and well-articulated. Clarity is high, with structured sections and clear explanations of motivations. Significance is notable, as normalization is a core component of deep learning, and the method's broad applicability across model architectures and tasks is emphasized."
          },
          "weaknesses": {
            "value": "The experimental validation is insufficiently detailed. While the paper claims 'comprehensive' results, it lacks direct comparisons with established normalization methods (e.g., batch normalization) on standard benchmarks. The ablation studies are minimal, leaving unclear the necessity of the power transform versus simpler alternatives. The theoretical analysis of how normality normalization interacts with training dynamics (e.g., gradient propagation) is underdeveloped. Additionally, the paper does not address potential limitations, such as computational overhead or sensitivity to hyperparameters."
          },
          "questions": {
            "value": "How does normality normalization compare to batch normalization in terms of training speed, convergence, and final performance on standard datasets? What ablation studies confirm the importance of the power transform versus other normalization strategies? How was the hyperparameter for the power transform selected, and is it sensitive to dataset or model architecture? Are there scenarios where normality normalization fails or degrades performance compared to existing methods?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Normality Normalization, a novel normalization layer that encourages normality in neural network activations using the power transform and additive Gaussian noise. It leverages information-theoretic properties of the normal distribution to improve generalization, robustness, and adaptability across model configurations."
          },
          "strengths": {
            "value": "The paper presents a strong theoretical foundation rooted in information theory, including the mutual information game and entropy maximization principles. The motivation for normality in representations is well-articulated, and the connection to existing normalization techniques (e.g., batch, layer normalization) is clear. The experiments are structured to evaluate generalization, robustness, and scalability across model factors, demonstrating the method's versatility."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with state-of-the-art normalization methods (e.g., group normalization, weight normalization) to establish competitive performance. The implementation details of the power transform are ambiguous, and the paper does not address potential limitations (e.g., computational overhead, sensitivity to hyperparameters). Additionally, the theoretical analysis does not rigorously justify why normality improves robustness or generalization in practice."
          },
          "questions": {
            "value": "1. How does Normality Normalization compare to existing normalization layers in terms of performance on standard benchmarks? 2. What are the specific hyperparameters (e.g., power transform exponent, noise magnitude) used in the experiments, and how were they tuned? 3. Are there scenarios where enforcing normality could degrade performance (e.g., non-Gaussian data distributions)? 4. How does the additive noise interact with other regularization techniques (e.g., dropout, weight decay)?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces 'normality normalization,' a novel normalization layer for neural networks that encourages normality in feature representations using a power transform and additive Gaussian noise. The authors argue that normality improves generalization, robustness, and representational efficiency, supported by experiments across various model and dataset configurations."
          },
          "strengths": {
            "value": "The paper provides a strong theoretical foundation by linking normality to information-theoretic principles (e.g., mutual information games, maximum entropy). The motivation for normality in neural networks is well-articulated, and the paper's structure is clear. The experimental section claims comprehensive validation, including ablation studies on model width, depth, and batch size, which suggests practical relevance."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with established normalization methods (e.g., batch normalization, layer normalization), making it difficult to assess the relative effectiveness of normality normalization. The power transform's implementation details (e.g., parameter selection, handling of non-positive data) are under-specified. The experiments do not rigorously test robustness to adversarial perturbations or provide ablation studies isolating the impact of the power transform versus noise. Theoretical claims about 'maximal representational capacity' lack empirical validation."
          },
          "questions": {
            "value": "1. How does the power transform handle non-positive activations, and what hyperparameters were used? 2. What ablation studies were conducted to disentangle the effects of the power transform and additive noise? 3. How does normality normalization compare to existing normalization techniques in terms of training stability and convergence? 4. Were adversarial examples or targeted perturbations used to evaluate robustness? 5. How is the 'normality' of activations quantitatively measured during training?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "9z9PvXPisj": {
    "paper_id": "9z9PvXPisj",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces ROSE, a reward-oriented data selection framework for task-specific instruction tuning of large language models (LLMs). ROSE shifts the data selection objective from minimizing cross-entropy loss to maximizing a reward signal derived from pairwise preference loss, aiming to better align with real-world task performance. The method uses gradient-based influence estimation to select high-quality training data, demonstrating competitive results with 5% of the training data compared to full fine-tuning and state-of-the-art methods."
          },
          "strengths": {
            "value": "The paper presents a novel approach by redefining data selection from loss minimization to reward maximization, addressing a critical limitation of existing similarity-based methods. The theoretical analysis highlights the misalignment between traditional loss metrics and practical task performance, which is a significant insight. The experimental validation covers multiple datasets and model architectures, showing consistent improvements. The clarity of the problem statement and methodology is strong, and the paper emphasizes practical relevance for domain-specific instruction tuning."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contribution of key components (e.g., the reward formulation vs. influence estimation). The experimental results are not fully described (the text is truncated), making it difficult to assess the robustness of claims. The comparison with baselines is limited, and the paper does not address potential limitations of the reward signal (e.g., sensitivity to preference set quality). Additionally, the theoretical analysis of why reward maximization outperforms ERM is superficial."
          },
          "questions": {
            "value": "1. How is the pairwise preference validation set constructed, and what guarantees exist that it reflects true task performance? 2. What ablation studies were conducted to validate the necessity of the reward maximization objective versus traditional loss minimization? 3. Are there specific tasks or domains where ROSE underperforms, and why? 4. How sensitive is the method to hyperparameters (e.g., the size of the preference set or influence estimation parameters)? 5. What are the computational costs of ROSE compared to existing methods, and how does this impact scalability?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces ROSE, a reward-oriented data selection framework for task-specific instruction tuning of large language models (LLMs). The method addresses the limitation of existing similarity-based approaches, which rely on minimizing cross-entropy loss (a proxy for task performance) and often fail to align with actual task performance. ROSE instead uses pairwise preference loss (inspired by Direct Preference Optimization) to select training data that maximizes task-specific rewards, demonstrated to achieve competitive results with only 5% of the training data compared to full fine-tuning."
          },
          "strengths": {
            "value": "Originality: ROSE introduces a novel shift from loss minimization to reward maximization for data selection, addressing a critical gap in existing methods. Quality: The approach leverages established techniques (DPO, influence estimation) in a creative way, with clear theoretical motivation. Clarity: The paper presents a structured problem definition and analysis of existing methods. Significance: Efficient data selection for instruction tuning has broad implications for reducing training costs and improving task-specific performance in LLMs."
          },
          "weaknesses": {
            "value": "The experimental validation appears limited: the paper lacks ablation studies to isolate the contribution of the reward-based selection vs. other components. The comparison to baselines is not fully detailed (e.g., exact metrics, statistical significance). The theoretical analysis of why reward maximization outperforms ERM is superficial. The paper does not address how ROSE scales to very large datasets or different model architectures beyond what's tested."
          },
          "questions": {
            "value": [
              "How does ROSE perform on tasks with varying levels of complexity or different modalities (e.g., code vs. text)?",
              "What is the computational overhead of the influence estimation step compared to existing methods?",
              "How sensitive is ROSE to the size and quality of the few-shot preference validation set?",
              "Are there cases where ROSE might fail, and how does it compare to baselines in those scenarios?",
              "What ablation studies demonstrate the necessity of the reward maximization objective versus other components of the framework?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes ROSE, a reward-oriented data selection framework for task-specific instruction tuning of large language models (LLMs). It addresses the limitation of existing similarity-based methods, which rely on minimizing next-token prediction loss that doesn't align with real-world task performance. ROSE leverages pairwise preference loss (inspired by DPO) and gradient-based influence estimation to select high-quality training data, achieving competitive results with only 5% of the full dataset."
          },
          "strengths": {
            "value": "The paper identifies a critical limitation of existing data selection methods for instruction tuning, which is a relevant and timely problem. The proposed approach introduces a novel reward maximization objective instead of traditional loss minimization, aligning with recent trends in RLHF. The experimental validation across multiple datasets and models demonstrates consistent improvements over existing methods. The theoretical analysis of similarity-based methods provides a solid foundation for the proposed approach."
          },
          "weaknesses": {
            "value": "The paper lacks detailed descriptions of how the reward function is defined or how the few-shot preference validation set is constructed. The implementation of the DPO loss for data selection is not thoroughly explained, and there are no ablation studies to isolate the contributions of different components. The computational efficiency of ROSE compared to baseline methods is not discussed, which is crucial for practical deployment. The paper also does not address potential issues with influence estimation techniques, such as scalability or hyperparameter sensitivity."
          },
          "questions": {
            "value": "1. How is the reward function defined in ROSE, and how are the positive/negative samples in the preference validation set obtained? 2. What specific datasets and model architectures were used in the experiments, and how were hyperparameters tuned? 3. Are there any ablation studies to demonstrate the effectiveness of the DPO loss and influence estimation components? 4. How does ROSE's computational cost compare to existing similarity-based methods? 5. What are the limitations of the influence estimation approach in this context?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "9zKm3TytBG": {
    "paper_id": "9zKm3TytBG",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes a machine learning approach called Contextualized Similarity Detection (CSD) to quantify copyright infringement in AI-generated art by comparing 'plaintiff' (copyrighted) and 'defendant' (potentially infringing) artwork sets. The method involves fine-tuning a pre-trained classifier to measure similarity via averaged validation logits, with an emphasis on accessibility for non-experts. The authors demonstrate the approach on curated datasets involving Mickey Mouse and Maria Prymachenko's art."
          },
          "strengths": {
            "value": "The paper addresses a timely and important problem at the intersection of AI and law, with clear practical motivations. The proposed method is simple, low-resource, and designed for non-expert use, which is a significant advantage. The focus on contextual similarity rather than generic similarity detection aligns with legal realities. The work also engages with legal frameworks and highlights potential applications beyond copyright, such as artist compensation."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous experimental validation. Key details about dataset curation (e.g., selection criteria for 'defendant' artworks) and baseline comparisons are missing. The method's generalizability is untested beyond two specific datasets. The claim that the approach provides 'quantitative support' for legal arguments is not substantiated with empirical evidence. Additionally, the paper does not address limitations, such as how the method handles style variations or adversarial cases."
          },
          "questions": {
            "value": "1. How were the defendant artworks selected for the Mickey Mouse and Maria Prymachenko datasets? What criteria ensured their relevance to real legal cases? 2. Were there baseline methods (e.g., existing copyright detection systems) used for comparison? 3. How does the method handle cases where AI-generated art diverges significantly from the plaintiff's style? 4. What steps were taken to validate the interpretability of the model's 'differentiation-relevant features'?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper proposes a novel approach called Contextualized Similarity Detection (CSD) to quantify copyright infringement in AI-generated artwork by comparing stylistic similarity between plaintiff (copyrighted) and defendant (potentially infringing) sets. The method involves fine-tuning a pre-trained classifier on curated plaintiff-defendant pairs and using averaged validation logits as a similarity score, with demonstrations on datasets featuring Mickey Mouse and Maria Prymachenko's art."
          },
          "strengths": {
            "value": "The paper introduces a fresh perspective by redefining copyright similarity detection as a contextual problem (CSD) rather than a general similarity detection (GSD) task, aligning with legal practices. The method's simplicity and low-resource accessibility make it practical for non-experts, addressing a critical gap in the field. The use of real-world legal cases and curated datasets demonstrates relevance, while the focus on expert-validated differentiation highlights its alignment with judicial reasoning. The work's potential to empower artists and legal practitioners with quantifiable tools is significant."
          },
          "weaknesses": {
            "value": "The paper lacks depth in addressing how the method scales to diverse art styles beyond the specific examples (Mickey Mouse, Prymachenko). The evaluation metrics are limited to accuracy and logit averages, without thorough analysis of false positives/negatives or robustness to varying artistic styles. The reliance on pre-trained models may not capture nuanced legal distinctions, and the threshold for 'similarity' is not rigorously justified. Additionally, the paper does not discuss how to handle defendant sets with multiple classes or ambiguous differentiation criteria."
          },
          "questions": {
            "value": "1. How generalizable is the CSD approach to artworks outside the curated datasets (e.g., abstract art, non-figurative styles)? 2. What criteria were used to select defendant sets, and how are they validated for relevance to the plaintiff? 3. How does the method handle cases where defendant artwork contains both similar and dissimilar elements to the plaintiff? 4. What is the process for determining the threshold α for similarity, and how is it validated against legal standards? 5. Are there plans to incorporate human expert feedback loops to refine the model's outputs?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes a machine learning approach to quantify copyright infringement in AI-generated artwork by measuring stylistic similarity between plaintiff (copyrighted) and defendant (potentially infringing) art sets. The method involves fine-tuning a pre-trained classifier to distinguish between these sets and using averaged validation logits as a similarity score, with case studies on Mickey Mouse and Maria Prymachenko's art."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel framework for contextualized similarity detection (CSD) that aligns with legal practices, contrasting with existing general similarity detection (GSD) approaches. The method is practical, using low-resource models accessible to non-experts, which addresses a critical gap in legal tech tools. Significance is high due to the growing importance of AI-generated art copyright issues. Clarity is maintained through detailed methodological steps and formal definitions, though some sections could be more concise."
          },
          "weaknesses": {
            "value": "The experimental validation is limited, with only two datasets (Mickey Mouse and Maria Prymachenko) and no comparison to existing methods for similarity detection. The paper lacks details on how defendant artworks were selected from 'real legal cases' or how the threshold α was determined. The similarity score's legal validity is not rigorously evaluated, and the method's generalizability beyond the tested cases remains unproven. The paper also does not address potential biases in the curated datasets or the interpretability of the model's decisions."
          },
          "questions": {
            "value": [
              "How were the defendant artworks selected from real legal cases? What criteria were used to ensure their relevance?",
              "What baselines or existing methods were compared against in the experiments? How does the proposed CSD-basic method differ from prior work?",
              "How was the threshold α determined for similarity scoring, and was it validated across multiple datasets?",
              "What metrics besides accuracy were used to evaluate model performance, and how do they correlate with legal notions of 'substantial similarity'?",
              "How does the method handle variations in art styles or non-visual features (e.g., thematic elements) that might be relevant in legal contexts?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "A0W7VCSQev": {
    "paper_id": "A0W7VCSQev",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces a method to analyze attention heads in large language models (LLMs) for multiple-choice question answering (MCQA) tasks. The authors identify 'select-and-copy' heads responsible for choosing correct answers and propose two novel scoring mechanisms—Query-Key (QK) score and Attention score—to extract knowledge from these heads. They demonstrate improvements in accuracy across multiple datasets and model sizes, particularly in zero-shot scenarios and synthetic data."
          },
          "strengths": {
            "value": "The paper presents a novel approach to understanding LLM decision-making in MCQA tasks, with clear methodological innovation in identifying select-and-copy heads. The experimental scope is broad, covering models from 1.5B to 70B parameters and multiple datasets. The synthetic dataset results (60% accuracy gain) are compelling, and the consistency of select-and-copy heads across datasets is a significant finding. The work also advances interpretability by linking attention patterns to semantic representation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with existing methods for MCQA evaluation, such as instruction tuning or in-context learning. The synthetic dataset's construction is not adequately explained, raising concerns about its generalizability. The QK-score and Attention score mechanisms are described briefly, with no ablation studies or analysis of their robustness to hyperparameter choices. The claim that select-and-copy heads outperform final model outputs is not thoroughly validated, and the paper does not address potential biases in head selection or model-specific limitations."
          },
          "questions": {
            "value": [
              "How were the select-and-copy heads identified? Were they manually curated or automatically detected via a specific criterion?",
              "What is the exact computation for the QK-score and Attention score? Are these scores derived from specific layers or attention patterns?",
              "How does the method handle answer options with varying lengths or complex structures (e.g., nested reasoning)?",
              "What is the computational overhead of the proposed scoring mechanisms compared to standard MCQA evaluation?",
              "Are the synthetic dataset results reproducible, and how were the 'correct' answers explicitly defined?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a novel method for improving multiple-choice question answering (MCQA) by analyzing specific attention heads in large language models (LLMs). The authors identify 'select-and-copy' heads responsible for choosing correct answers and propose two new scoring mechanisms—Query-Key (QK) scores and Attention Scores—to extract knowledge from these heads. Their approach achieves significant accuracy gains across multiple datasets and model sizes, demonstrating robustness to option permutations and supplementary options."
          },
          "strengths": {
            "value": "The paper's originality lies in its focus on intrinsic attention head analysis for MCQA, a novel angle compared to existing methods like instruction tuning. The quality of experiments is strong, with systematic testing across 1.5B–70B parameter models, zero-shot, and few-shot settings. Clarity is maintained through precise definitions of QK-scores and Attention Scores, while the significance is evident in the 16% accuracy improvement for LLaMA2-7B and 60% gain on a synthetic dataset, addressing a critical limitation in MCQA evaluation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contribution of select-and-copy heads versus other mechanisms. While the synthetic dataset results are promising, the construction methodology is under-specified, raising questions about generalizability. The claim about middle-layer dominance of select-and-copy heads could benefit from deeper analysis of why later layers degrade performance. Additionally, the paper does not compare directly with mechanistic interpretability approaches that also study attention heads (e.g., induction heads)."
          },
          "questions": {
            "value": "1. How are select-and-copy heads identified—through empirical analysis or explicit training objectives? 2. What is the exact construction process of the synthetic dataset, and how does it differ from real-world MCQA tasks? 3. Are the QK-scores and Attention Scores interpretable, or are they purely heuristic metrics? 4. How does the method handle MCQA tasks with non-sequential answer options (e.g., randomized labels)? 5. Could the observed middle-layer performance be influenced by the specific architecture of the models tested?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper addresses limitations in multiple-choice question answering (MCQA) evaluations for large language models (LLMs) by analyzing specific attention heads responsible for selecting and copying correct answers. The authors introduce Query-Key (QK) scores and Attention Scores based on these heads, demonstrating improved accuracy across models and datasets."
          },
          "strengths": {
            "value": "The paper's originality lies in focusing on intrinsic attention head analysis for MCQA, a novel angle compared to prior work on instruction tuning or in-context learning. The experiments are comprehensive, covering diverse model sizes (1.5B–70B parameters) and settings (zero-shot/few-shot). Clarity is strong, with clear explanations of the QK-score and Attention Score. The significance is high, as MCQA is a critical evaluation benchmark, and the method addresses a practical limitation in model evaluation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology for identifying 'select-and-copy' heads, making it hard to replicate the approach. The comparison with baseline methods is superficial, with no ablation studies or analysis of how QK-scores outperform existing techniques. The synthetic dataset's construction is under-described, raising questions about its validity. Additionally, the theoretical justification for why these heads are effective remains vague, relying on empirical results alone."
          },
          "questions": {
            "value": "1. How exactly are 'select-and-copy' heads identified? Are they hand-picked or automatically detected? 2. What is the exact mechanism behind the QK-score and Attention Score? How do they differ from existing attention-based metrics? 3. How is the synthetic dataset generated, and how does it differ from real-world MCQA tasks? 4. Are there ablation studies showing the contribution of each component (QK-score vs. Attention Score)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "A0mk2Wi68Y": {
    "paper_id": "A0mk2Wi68Y",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces a framework to enforce interpretability in time series Transformers by aligning their representations with predefined interpretable concepts (e.g., time features and an autoregressive surrogate model) using Centered Kernel Alignment (CKA). The approach modifies the training objective of the Autoformer model, demonstrating that interpretability can be improved without significant performance degradation."
          },
          "strengths": {
            "value": "Originality: The paper adapts Concept Bottleneck Models (CBMs) to time series Transformers, a novel application area. The use of a linear surrogate model and timestamp features as interpretable concepts addresses domain-specific challenges. Quality: The framework is evaluated on six benchmark datasets, with detailed CKA analysis and visualization of concept contributions. Clarity: The methodology and experiments are well-explained, with a clear structure. Significance: Enhancing interpretability in high-stakes domains like finance and energy is critical, and the framework enables local interventions without retraining."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with existing interpretability methods for Transformers, making it hard to assess the novelty of the CKA-based approach. The surrogate model's linear nature may limit its ability to capture complex temporal patterns, yet the paper does not discuss this trade-off. The intervention example (time shift) is brief and lacks quantitative evaluation of its effectiveness. The framework's generalizability to other Transformer architectures (beyond Autoformer) is not tested. Additionally, the paper does not address potential issues like overfitting to predefined concepts or the scalability of CKA in large models."
          },
          "questions": {
            "value": "1. How were the predefined concepts (e.g., AR surrogate model) selected, and what criteria ensure their relevance to the downstream task? 2. Why is CKA preferred over other similarity metrics (e.g., cosine similarity, mutual information)? 3. What ablation studies were conducted to validate the necessity of each concept (timestamp vs. surrogate model)? 4. How does the framework handle cases where the predefined concepts are incomplete or noisy? 5. Are there quantitative measures of interpretability (e.g., human evaluation) beyond CKA scores?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a concept bottleneck framework to enhance the interpretability of time series Transformers without compromising performance. By aligning model representations with predefined concepts (e.g., time features and an autoregressive surrogate model) using Centered Kernel Alignment (CKA), the approach enables local interventions while maintaining forecasting accuracy. The method is evaluated on the Autoformer model across multiple benchmarks."
          },
          "strengths": {
            "value": "The paper's originality lies in adapting Concept Bottleneck Models (CBMs) to time series forecasting, a domain where such methods are underexplored. The use of CKA to measure alignment with interpretable concepts is a novel technical contribution. The framework's practical value is demonstrated through a successful intervention example (time shift mitigation) without retraining. The clarity of the problem statement, methodology, and experiments is strong, with visualizations aiding understanding. The significance is high, as it addresses a critical gap in making Transformers interpretable for high-stakes applications."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons with alternative interpretability methods (e.g., attention visualization or feature attribution) that could contextualize the proposed framework's effectiveness. The choice of concepts (e.g., AR surrogate model) is not thoroughly justified, and the paper does not explore whether other concept sets might yield better results. The experiments focus on Autoformer, leaving open questions about the framework's generalizability to other Transformer architectures. Additionally, the theoretical analysis of how CKA alignment impacts interpretability and intervenability is limited."
          },
          "questions": {
            "value": "1. How does the framework compare to existing interpretability techniques (e.g., SHAP, LIME) in terms of practical utility? 2. Why was the autoregressive surrogate model chosen as a concept, and could alternative concepts (e.g., domain-specific features) yield better results? 3. Are the interventions demonstrated (e.g., time shift correction) applicable to real-world scenarios beyond synthetic experiments? 4. How sensitive is the framework to hyperparameters in the CKA objective? 5. Could the approach be extended to non-Transformer time series models?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a framework to enforce interpretability in time series Transformers by aligning their representations with predefined interpretable concepts (e.g., time features and an autoregressive surrogate model) using Centered Kernel Alignment (CKA). The approach modifies the training objective of the Autoformer model to maintain performance while improving interpretability and intervenability, demonstrated through experiments on benchmark datasets and a time-shift intervention case study."
          },
          "strengths": {
            "value": "The paper presents a novel application of Concept Bottleneck Models (CBMs) to time series Transformers, addressing a critical gap in interpretability for this domain. The methodology is well-justified, with CKA used effectively to measure alignment between model representations and concepts. Experiments are comprehensive, covering multiple benchmarks and providing detailed interpretability analysis. The framework's ability to enable local interventions (e.g., time-shift corrections) without retraining is a significant practical contribution. The writing is clear, and the figures (e.g., Figure 1) effectively illustrate the approach."
          },
          "weaknesses": {
            "value": "The paper lacks comparison with alternative interpretability methods (e.g., attention visualization or feature attribution) to validate the effectiveness of the CKA-based framework. The predefined concepts (time features and AR surrogate) are limited in scope and may not generalize to all time series tasks. The intervention example focuses on a specific scenario (time shift), leaving the framework's applicability to other types of interventions (e.g., anomaly detection) unclear. Additionally, the theoretical analysis of how CKA enforces interpretability is superficial, with limited discussion on potential limitations of the approach."
          },
          "questions": {
            "value": "1. How do the predefined concepts (e.g., time features) interact with domain-specific characteristics of different time series datasets? 2. What ablation studies have been conducted to assess the contribution of CKA to the framework's performance and interpretability? 3. Are there cases where enforcing alignment with the surrogate model might degrade performance or introduce bias? 4. How does the framework handle complex, non-linear concepts that cannot be captured by simple linear surrogates or timestamp features?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "A1JdcLawSu": {
    "paper_id": "A1JdcLawSu",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates the 'stability gap' in continual learning, where performance on prior tasks drops sharply during the initial stages of new task training. The authors identify abrupt knowledge loss in deeper network layers and propose Adaptive Angular Replay (AAR), a method leveraging hyper-spherical feature spaces with normalization and adaptive scaling to mitigate stability gaps and preserve class structure."
          },
          "strengths": {
            "value": "The paper introduces novel insights into the dynamics of knowledge retention during continual learning, particularly highlighting the stability gap's impact beyond the final fully connected layer. The analysis using Centered Kernel Alignment (CKA) and the observation of abrupt class structure disruption after a single gradient step demonstrate originality. The proposed AAR method is theoretically grounded in addressing degenerate solutions in softmax cross-entropy loss, with clear motivation for its design. The paper's structure and clarity are strong, with well-defined contributions and a focused problem statement."
          },
          "weaknesses": {
            "value": "The experimental validation is limited, with no comparisons to state-of-the-art continual learning methods or ablation studies on AAR's components. The paper lacks analysis of how the adaptive scaling strategy interacts with different network architectures or datasets. The claim about 'complete loss of class structure' in feature space requires stronger empirical support, as the current evidence is based on qualitative observations. The connection between hyper-spherical normalization and stability gap mitigation is not thoroughly explained."
          },
          "questions": {
            "value": [
              "How does AAR compare to established methods like Elastic Weight Consolidation or memory replay on standard continual learning benchmarks?",
              "What ablation studies have been conducted to isolate the contributions of feature normalization, angular replay, and adaptive scaling in AAR?",
              "Can the adaptive scaling strategy be generalized to other loss functions or architectures beyond the experiments presented?",
              "How sensitive is the observed 'stability gap' to hyperparameter choices in the experimental setup?",
              "Are the findings about abrupt knowledge loss in deeper layers reproducible across different network depths or task configurations?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates the 'stability gap' in continual learning, where models experience sharp performance drops on prior tasks during new task training. The authors analyze how knowledge is lost abruptly in deeper network layers and propose Adaptive Angular Replay (AAR), a method leveraging hyperspherical feature space with normalization to preserve class structure. They also introduce an adaptive scaling strategy to mitigate the stability gap."
          },
          "strengths": {
            "value": "The paper addresses a critical and underexplored issue in continual learning—the stability gap—by providing empirical insights into the dynamics of knowledge retention and loss. The proposed AAR method is conceptually novel, leveraging hyperspherical geometry to stabilize learning. The work contributes to understanding how neural networks degrade during task transitions, which is significant for safety-critical applications. The clarity of the problem formulation and the connection to prior work on catastrophic forgetting are strong."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive experimental validation. Key claims, such as the effectiveness of AAR and the adaptive scaling strategy, are not supported by sufficient comparisons to state-of-the-art methods or ablation studies. The analysis of the stability gap is based on limited experiments (e.g., CKA analysis and single-gradient-step experiments), which may not generalize. The truncation of the paper prevents evaluation of full experiments, baselines, and implementation details. The identification of 'degenerate solutions' in cross-entropy loss is not rigorously justified or quantified."
          },
          "questions": {
            "value": [
              "How does AAR compare to established replay-based methods (e.g., ER, MIR) on standard continual learning benchmarks? What are the specific baselines used for comparison?",
              "The paper claims that memory samples have higher feature norms than new samples. What metrics or experiments support this claim? Are these norms computed on the full network or specific layers?",
              "The adaptive scaling strategy is mentioned but not elaborated. How is the scaling factor determined? Is it task-dependent or learned during training?",
              "The analysis of class structure disruption after a single gradient step is intriguing. What are the exact experimental settings (e.g., dataset, model architecture) for this result?",
              "How does AAR handle non-stationary data distributions beyond the scope of the experiments presented? Are there theoretical guarantees for its stability?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper investigates the 'stability gap' in continual learning, where performance on prior tasks drops sharply during the initial stages of new task training. The authors analyze how knowledge is lost abruptly in deeper network layers and propose Adaptive Angular Replay (AAR), a method that learns features in hyperspherical space with normalization to preserve class structure. They also introduce an adaptive scaling strategy to reduce the stability gap."
          },
          "strengths": {
            "value": "The paper addresses a critical and underexplored aspect of continual learning—the stability gap—and provides empirical insights into how knowledge is lost across network layers. The proposed AAR method is theoretically grounded in hyperspherical geometry, offering a novel approach to mitigate catastrophic forgetting. The analysis using Centered Kernel Alignment (CKA) and observations about single-gradient-step disruptions demonstrate rigorous experimentation. The work also bridges theoretical understanding with practical solutions, such as adaptive scaling."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art continual learning methods, making it difficult to assess AAR's relative effectiveness. The adaptive scaling strategy is mentioned but not thoroughly explained or evaluated. The experimental section appears incomplete, with the paper cut off mid-sentence, leaving key details about datasets, baselines, and ablation studies missing. The theoretical justification for why hyperspherical normalization addresses the stability gap is not fully developed."
          },
          "questions": {
            "value": [
              "How does AAR compare to established methods like Elastic Weight Consolidation or memory replay in terms of performance and stability gap mitigation?",
              "What specific datasets and benchmarks were used to evaluate AAR, and how do they align with standard continual learning benchmarks?",
              "Can the authors provide more details on the adaptive scaling strategy and its integration with Angular ER?",
              "Are there ablation studies demonstrating the individual contributions of hyperspherical normalization and adaptive scaling to the overall performance?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "A6QotWIQim": {
    "paper_id": "A6QotWIQim",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper investigates energy efficiency in on-device streaming speech recognition models, focusing on the Neural Transducer architecture. It identifies that power consumption is influenced by memory traffic, invocation frequency, and component placement, proposing a compression strategy that reduces energy use by 47% while maintaining accuracy."
          },
          "strengths": {
            "value": "The paper's originality lies in its detailed analysis of component-specific energy consumption dynamics in Neural Transducer models, which is underexplored in prior work. The large-scale experimentation (180 models) demonstrates thoroughness, and the proposed design principles address a critical practical challenge. The clarity of the problem statement, methodology, and results is strong, with clear figures and definitions. The significance is high, as energy efficiency is a key barrier for on-device AI."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons against state-of-the-art compression methods, making it hard to validate the 47% energy reduction claim. The exponential accuracy-encoder size relationship is not rigorously validated with statistical analysis or ablation studies. The compression strategy's implementation details (e.g., quantization, pruning) are vague, hindering reproducibility. The analysis of memory hierarchy's impact on power consumption is superficial, with no experiments on memory placement optimization. The baseline models used for comparison are not explicitly described."
          },
          "questions": {
            "value": "1. How exactly was the 47% energy reduction achieved? What specific compression techniques were applied to the Joiner, Predictor, and Encoder? 2. Which state-of-the-art methods were used as baselines, and how do they differ from the proposed approach? 3. How was the exponential accuracy-encoder size relationship validated? Were there statistical tests or visualizations? 4. What are the limitations of the proposed method in terms of model size, latency, or accuracy trade-offs? 5. How does the memory placement strategy (e.g., local vs. global memory) affect power consumption in practice?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper investigates how weight parameter configurations in Neural Transducer models affect energy efficiency for on-device streaming speech recognition. The authors identify that component-specific factors like invocation frequency and memory placement significantly influence power consumption, and propose a compression strategy prioritizing components with high power sensitivity and low accuracy sensitivity. Their approach achieves up to 47% energy reduction while maintaining accuracy."
          },
          "strengths": {
            "value": "The paper introduces a novel perspective on energy efficiency by analyzing component-specific operational dynamics (invocation frequency, memory placement) rather than just model size. The large-scale evaluation of 180 models across diverse architectures demonstrates rigorous experimentation. Key findings, such as the disproportionate power consumption of the Joiner component and the exponential accuracy-encoder size relationship, challenge conventional assumptions and open new research directions. The practical design guidelines for energy-efficient model compression are well-motivated and impactful for on-device AI applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed implementation specifics for the compression strategy, particularly how 'energy-efficient local memory' is utilized for the Joiner. The exponential accuracy-encoder size relationship requires deeper analysis (e.g., statistical significance, dataset dependencies). While experiments on LibriSpeech and Public Video datasets show promise, broader validation across device architectures and use cases is needed. The comparison to state-of-the-art methods could be strengthened with more quantitative benchmarks and ablation studies."
          },
          "questions": {
            "value": [
              "How was the exponential relationship between encoder size and accuracy quantified? What statistical evidence supports this claim?",
              "What specific compression techniques were applied to the Joiner component, and how does storing its weights in 'energy-efficient local memory' work technically?",
              "Are there scenarios where the proposed strategy might not generalize, such as non-Transformer-based architectures or different hardware platforms?",
              "How does the method handle trade-offs between energy savings and latency improvements in real-world deployment?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates how weight parameter configurations in Neural Transducer models affect energy efficiency for on-device streaming speech recognition. The authors identify that component-specific factors like invocation frequency and memory placement significantly influence power consumption, and propose a compression strategy prioritizing high-power-sensitivity, low-accuracy-sensitivity components (e.g., the Joiner). Their approach achieves up to 47% energy reduction while maintaining accuracy."
          },
          "strengths": {
            "value": "Originality: The paper presents novel insights into component-specific energy dynamics in Neural Transducer models, particularly the disproportionate power consumption of the frequently invoked Joiner. Quality: The large-scale evaluation of 180+ models with diverse architectures provides robust empirical evidence. Clarity: The methodology and findings are well-explained with illustrative figures. Significance: The work addresses a critical challenge in on-device AI, offering practical design guidelines for energy-efficient ASR systems."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with existing energy-optimization techniques for ASR, making it difficult to assess the novelty of their compression strategy. The exponential accuracy-encoder size relationship is not thoroughly validated with statistical analysis. The power consumption measurements are not detailed (e.g., hardware platform, measurement methodology). The impact of compression on other metrics like model size or inference latency beyond RTF is underexplored. The paper does not address how their approach generalizes across different device architectures or workloads."
          },
          "questions": {
            "value": [
              "How was power consumption measured? Was it on actual hardware or simulated? What specific metrics (e.g., CPU/GPU utilization, battery drain) were used?",
              "What is the exact methodology for determining component power/accuracy sensitivity? Are these metrics task-specific or generalizable?",
              "How does the proposed compression strategy compare to established techniques like quantization or pruning in terms of implementation complexity and trade-offs?",
              "What are the limitations of the exponential accuracy-encoder size relationship? Are there edge cases where larger encoders still provide significant gains?",
              "How does the Joiner's memory placement in 'energy-efficient local memory' affect other system constraints (e.g., memory bandwidth, thermal management)?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "A6Y7AqlzLW": {
    "paper_id": "A6Y7AqlzLW",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces Process Advantage Verifiers (PAVs), a novel approach to training process reward models (PRMs) for improving large language model (LLM) reasoning. PAVs measure 'progress' as the change in the likelihood of producing a correct response after each step, using a distinct 'prover' policy. The method demonstrates significant improvements in accuracy and computational efficiency over outcome reward models (ORMs) in both test-time search and online reinforcement learning (RL)."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel definition of process rewards based on 'progress' rather than direct correctness, addressing limitations of prior PRMs. Theoretical analysis of prover-policy complementarity provides a principled framework. Quality: Experiments on Gemma2 models show consistent gains (8%+ accuracy, 6× sample efficiency). The work bridges theoretical insights with practical workflows for PAV training. Clarity: The problem formulation and key concepts (e.g., progress, prover policies) are well-explained. Significance: Improving LLM reasoning via efficient PRMs has broad implications for AI alignment and capability scaling."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons to alternative PRM approaches (e.g., value-based or trajectory-level methods) that could contextualize the novelty. The prover policy design is not fully detailed—how is it trained? What are its architectural choices? The theoretical analysis is abstract; concrete examples of 'complementary' provers would strengthen the argument. Empirical validation focuses on accuracy and efficiency but omits analysis of how PAVs affect downstream tasks (e.g., robustness to distribution shifts). The claim of 'one of the first results' requires justification against existing work on automated PRMs."
          },
          "questions": {
            "value": "1. How is the prover policy trained? Is it a pre-trained model, a fine-tuned version of the base policy, or a separate architecture? 2. What metrics are used to quantify 'progress'—specifically, how is the likelihood change calculated? 3. Are there ablation studies on the impact of prover strength (e.g., strong vs. weak provers) on PAV performance? 4. How does the method handle cases where the prover itself is imperfect or misaligned with the base policy? 5. What is the computational overhead of training PAVs compared to ORMs, and how does this scale to larger models?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces Process Advantage Verifiers (PAVs), a novel approach to training process reward models (PRMs) for improving reasoning in large language models. PAVs measure 'progress' at each step of a reasoning trace, defined as the change in the likelihood of a correct final answer before and after the step, using a distinct 'prover' policy. The method demonstrates improved accuracy and efficiency in test-time search and online reinforcement learning (RL) compared to outcome reward models (ORMs)."
          },
          "strengths": {
            "value": "The paper presents a theoretically grounded and empirically validated approach to PRMs, focusing on progress as a more effective metric than traditional correctness-based supervision. The work addresses a critical scalability challenge in PRM training by proposing a complementary prover policy. The experiments show significant improvements in accuracy (8%+), compute efficiency (5x), and sample efficiency (6x) over ORMs, with thorough validation across multiple model sizes. Theoretical analysis of prover-base policy alignment and the role of complementary policies adds depth."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons to other automated PRM methods beyond ORM baselines, making it difficult to assess relative improvements. The prover policy design is not fully detailed, leaving unclear how to select or train such policies in practice. The theoretical analysis is abstract and could benefit from concrete examples or empirical validation. The experiments focus on math reasoning tasks, limiting generalizability, and ablation studies on key components (e.g., prover strength, rollout ratios) are missing."
          },
          "questions": {
            "value": [
              "How is the 'complementary' prover policy defined and trained? What specific metrics or criteria ensure it aligns with the base policy while distinguishing steps?",
              "What are the exact evaluation metrics used for accuracy? How do PAVs compare to other PRM methods like value-based or Q-value-based approaches?",
              "Are the improvements consistent across different types of reasoning tasks, or are they limited to math problems? How does the method handle tasks with sparse feedback?",
              "What ablation studies were conducted to validate the impact of prover strength, seed rollout ratios, and other hyperparameters?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces process advantage verifiers (PAVs) that measure 'progress' in reasoning traces of large language models (LLMs) to improve multi-step reasoning. By defining process rewards as changes in the likelihood of correct outcomes under a 'prover' policy, the authors demonstrate improved efficiency in test-time search and online reinforcement learning (RL) compared to outcome reward models (ORMs)."
          },
          "strengths": {
            "value": "The paper presents a novel approach to process reward design by focusing on 'progress' rather than step-wise correctness, addressing scalability challenges in automated PRM training. Theoretical insights on complementary provers and empirical validation across multiple model sizes (2B, 9B, 27B Gemma2) show significant gains in accuracy (8%+ improvement) and compute efficiency (5x faster search, 6x more sample-efficient RL). The work is well-structured with clear figures and addresses a critical problem in LLM reasoning."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons to alternative PRM methods beyond ORMs, leaving open questions about the uniqueness of their approach. The prover policy's training methodology is under-specified, and the theoretical analysis of 'complementary' provers is brief. Experimental results focus on a single task (math reasoning) and specific model architectures, limiting generalizability. The claims about 8% accuracy improvements require more detailed baseline comparisons."
          },
          "questions": {
            "value": "1. How is the prover policy trained, and what ensures its complementarity to the base policy? 2. Are there ablation studies showing the impact of prover strength on performance? 3. How do PAVs perform on non-math reasoning tasks or diverse problem types? 4. What are the computational costs of training PAVs compared to ORMs? 5. How are 'progress' values calculated in practice, and what safeguards prevent overfitting to the prover's biases?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "A7LTIuhH4k": {
    "paper_id": "A7LTIuhH4k",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces a novel method leveraging the proximal point method (PPM) to approximate multiple Pareto-efficient robust solutions for optimization problems under uncertainty. By generating solutions along the PPM trajectory, the approach reduces computational cost compared to solving multiple robust optimization instances with varying hyperparameters. Theoretical guarantees are provided for robust linear programs, while empirical validation is demonstrated on portfolio optimization and adversarial machine learning tasks."
          },
          "strengths": {
            "value": "The paper presents a theoretically grounded approach with clear computational benefits for generating multiple robust solutions. The connection between PPM and robust optimization is novel, particularly in framing PPM trajectories as Pareto-efficient solutions. Theoretical results for linear robust optimization problems are rigorous, and the practical applications to portfolio optimization and deep learning highlight the method's relevance. The paper also addresses the gap in existing methods by avoiding repeated solves of robust counterparts."
          },
          "weaknesses": {
            "value": "The theoretical analysis is limited to specific classes of problems (e.g., robust linear programs with ellipsoidal uncertainty), leaving the generalizability of the method unclear. The empirical validation is brief and lacks detailed comparison with state-of-the-art methods, such as the computational efficiency gains claimed. The paper does not fully explain how the approximate PPM steps are implemented for nonlinear objectives or how the Pareto efficiency of the generated solutions is rigorously verified. Additionally, the paper assumes the uncertainty set structure (e.g., ellipsoidal) without discussing robustness to other types of uncertainty."
          },
          "questions": {
            "value": [
              "How does the method handle non-ellipsoidal uncertainty sets, and what assumptions are required for the theoretical guarantees to hold?",
              "What are the specific implementation details of the approximate PPM steps for nonlinear objectives, and how are convergence properties ensured?",
              "Are there quantitative comparisons with existing methods (e.g., globalized robust optimization or hyperparameter grid search) in terms of solution quality and computational cost?",
              "How is the Pareto efficiency of the generated solutions formally defined and verified in the experiments?",
              "What are the limitations of the current approach in terms of problem size or dimensionality, and how does the computational cost scale with $ N $?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a novel method leveraging the proximal point method (PPM) to approximate multiple Pareto-efficient robust solutions in robust optimization problems. The approach reduces computational cost compared to solving multiple robust optimization instances separately, with theoretical guarantees for linear programs and empirical validation on portfolio optimization and adversarial machine learning tasks."
          },
          "strengths": {
            "value": "The paper presents a novel application of PPM to generate multiple robust solutions efficiently, addressing a key challenge in robust optimization. The theoretical contributions are strong, particularly for linear programs, with rigorous proofs of Pareto efficiency. The computational efficiency claim (reduced cost from N×T_RC to T_RC + (N−1)×T_PPM) is significant. The paper is well-structured, with clear notation and logical flow. The practical relevance to machine learning and operations research problems adds to its significance."
          },
          "weaknesses": {
            "value": "The theoretical results are limited to linear programs and probabilistic bounds for random polyhedron domains, leaving gaps for nonlinear or non-convex problems. The empirical validation is described but not detailed, making it hard to assess the practical impact. The paper lacks a thorough comparison with existing methods for nonlinear objectives, and the approximation quality of PPM steps is not fully quantified. The assumption of convexity and specific uncertainty set structures may limit applicability."
          },
          "questions": {
            "value": "1. How generalizable are the theoretical results to non-linear or non-convex robust optimization problems? 2. What are the specific implementations of the approximate PPM steps, and how are their approximation errors controlled? 3. Are there limitations to the uncertainty set assumptions (e.g., ellipsoidal vs. arbitrary sets)? 4. How does the method scale to high-dimensional problems common in machine learning? 5. What metrics were used to evaluate the 'comparable performance' of approximate solutions in the experiments?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a novel algorithm based on the proximal point method (PPM) to approximate multiple Pareto-efficient robust solutions for optimization problems. The method reduces computational cost compared to solving multiple robust optimization instances separately, leveraging the PPM trajectory to generate solutions across varying robustness levels. Theoretical guarantees are provided for robust linear programs, and empirical validation is demonstrated on portfolio optimization and adversarial machine learning tasks."
          },
          "strengths": {
            "value": "The paper presents a creative application of PPM to generate multiple robust solutions efficiently, addressing a critical challenge in robust optimization. The theoretical contributions are strong, including exact Pareto efficiency guarantees for specific robust LPs and probabilistic bounds for random polyhedron domains. The practical relevance is highlighted through experiments on real-world problems. The writing is clear, with well-defined notation and structured organization."
          },
          "weaknesses": {
            "value": "The theoretical results are limited to specific problem classes (e.g., linear programs with ellipsoidal uncertainty), leaving the generality of the approach unclear. The empirical validation lacks detailed comparisons with baseline methods and quantitative analysis of solution quality. The paper does not thoroughly discuss how PPM parameters (e.g., step size) affect the trade-off between efficiency and robustness in practice."
          },
          "questions": {
            "value": [
              "How does the PPM-based approach generalize to non-linear or non-convex robust optimization problems beyond the linear cases analyzed in the paper?",
              "What are the practical limitations of the method when applied to high-dimensional or non-differentiable objectives, such as those in complex machine learning models?",
              "Can the paper provide a more detailed analysis of how the choice of PPM parameters (e.g., step size, termination criteria) impacts the quality and computational efficiency of the generated solutions?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "A8Vuf2e8y6": {
    "paper_id": "A8Vuf2e8y6",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes NeoMLP, an architecture that transforms traditional MLPs into a fully connected graph with self-attention mechanisms. By reinterpreting MLPs as graphs and applying message passing with weight-sharing via self-attention, NeoMLP enables conditional neural fields through learnable latent codes in hidden and output nodes. The method demonstrates effectiveness in fitting high-resolution signals and improving downstream tasks compared to existing approaches."
          },
          "strengths": {
            "value": "The paper introduces a novel graph-based perspective on MLPs, integrating self-attention for scalability and conditioning. The approach addresses limitations of existing neural fields by embedding latent codes directly into the architecture, which is theoretically sound. The experimental claims (e.g., outperforming state-of-the-art methods) suggest potential significance, and the connectionist principles align with broader neural network design trends. The paper also proposes new concepts like ν-reps and ν-sets, which could inspire future work."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-explanation, leaving critical details about the architecture (e.g., how self-attention is implemented, how latent codes are optimized) and experimental results incomplete. The claims about scalability and performance improvements lack empirical validation due to missing data. The comparison with existing methods like cross-attention is not thoroughly analyzed, and the paper does not address potential limitations (e.g., computational cost, generalization to other modalities). The theoretical analysis of how NeoMLP overcomes parameter symmetries is also unclear."
          },
          "questions": {
            "value": "1. How exactly does the self-attention mechanism in NeoMLP differ from standard attention in transformers? 2. What specific baselines were used for comparison, and how do they relate to the claimed state-of-the-art results? 3. Are the ν-reps and ν-sets explicitly defined, and how are they trained? 4. How does the paper address the computational complexity of fully connected graphs compared to traditional MLPs? 5. What ablation studies were conducted to validate the contribution of self-attention vs. other components?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 2
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces NeoMLP, an architecture that transforms traditional MLPs into a fully connected graph with self-attention mechanisms to improve scalability and conditioning for neural fields. By treating MLPs as graphs and leveraging self-attention across input, hidden, and output nodes, NeoMLP enables conditional neural fields and demonstrates effectiveness on high-resolution signals and downstream tasks."
          },
          "strengths": {
            "value": "The paper presents a novel architecture (NeoMLP) that creatively combines MLPs with self-attention, addressing key limitations in neural field conditioning. The approach is theoretically grounded in connectionism and offers a unified framework for scalable conditioning. The experiments show promise in fitting high-resolution data and outperforming prior methods, though details are sparse. The paper also introduces new concepts like ν-reps and ν-sets, which could inspire future work."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies and quantitative comparisons to baseline methods, making it hard to assess the magnitude of improvements. The theoretical justification for self-attention over cross-attention is underdeveloped, and the experimental results are not sufficiently detailed (e.g., metrics, datasets). The figure describing the architecture is cut off, hindering understanding. The claim of outperforming state-of-the-art methods is unsubstantiated without explicit results."
          },
          "questions": {
            "value": "1. How does the self-attention mechanism in NeoMLP specifically improve conditioning compared to cross-attention? 2. What are the exact metrics and datasets used to demonstrate superiority over existing methods? 3. Are there any limitations or edge cases where NeoMLP fails? 4. How are the hidden/output node features initialized, and how does this affect training stability?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces NeoMLP, an architecture that transforms traditional MLPs into a fully connected graph with self-attention mechanisms to address limitations in neural fields (NeFs). By reinterpreting MLPs as graphs and leveraging self-attention for message passing, NeoMLP enables conditional neural fields with latent code representations, demonstrating effectiveness on high-resolution signals and downstream tasks."
          },
          "strengths": {
            "value": "Originality is evident in reimagining MLPs as graph structures and integrating self-attention for message passing, which offers a novel perspective on neural field conditioning. The methodology is technically sound, with clear design choices for scalability through high-dimensional features and weight-sharing. The paper's clarity is strong, with logical organization and visual aids (e.g., Figure 1) to explain the architecture. The significance lies in addressing critical limitations of existing NeFs, such as parameter symmetries and limited conditioning mechanisms, with promising experimental results."
          },
          "weaknesses": {
            "value": "The experimental validation lacks depth, particularly in comparing NeoMLP to state-of-the-art set-latent methods like those using cross-attention. Key details about hyperparameter tuning, computational efficiency, and ablation studies on self-attention vs. traditional MLPs are missing. The theoretical analysis of how self-attention mitigates parameter symmetries is underdeveloped, and the paper does not address potential overfitting risks with the proposed latent code approach."
          },
          "questions": {
            "value": "1. How does NeoMLP specifically address parameter symmetries compared to prior work? 2. What ablation studies were conducted to isolate the impact of self-attention versus traditional MLP components? 3. Are the high-dimensional features explicitly designed, or are they a byproduct of the architecture? 4. How does the method scale to extremely large datasets in terms of memory and computation? 5. What metrics were used to evaluate downstream task performance, and how do they compare to baselines?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "A9loYh0RgU": {
    "paper_id": "A9loYh0RgU",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes FORMED, a foundation model for medical time series (MedTS) classification that leverages a pre-trained backbone and domain-specific adaptation to address inter- and intra-dataset heterogeneity. It claims to achieve generalizable adaptation across diverse MedTS datasets with minimal task-specific tuning."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in MedTS classification—generalizability across heterogeneous datasets—which is highly relevant to real-world clinical applications. The idea of repurposing foundation models with domain-specific adaptation is novel and aligns with broader trends in foundation model research. The experimental results suggest promising performance improvements over task-specific models, and the focus on lightweight parameter updates for adaptation is practical for deployment."
          },
          "weaknesses": {
            "value": "The paper lacks critical details about the repurposing cohort (e.g., dataset curation process, clinical relevance of tasks, and statistical significance of results). The claims of superior performance over 11 baselines are not substantiated with concrete metrics or comparisons to state-of-the-art MedTS models. The method for integrating 'medical domain knowledge' into the 'specialized shell' is unclear, and ablation studies or analysis of the model's generalization mechanisms are missing. Additionally, the paper does not address how the pre-trained backbone's generalization capabilities are validated on non-medical tasks."
          },
          "questions": {
            "value": [
              "What specific criteria were used to curate the 5 MedTS datasets in the repurposing cohort? How representative are they of real-world clinical scenarios?",
              "How is the 'specialized shell' trained? What medical knowledge is encoded, and how is it integrated with the pre-trained backbone?",
              "Which 11 baseline models were compared? Are they representative of the current state of the art in MedTS classification?",
              "What ablation studies were conducted to validate the contribution of the backbone, specialized shell, and adaptation mechanism?",
              "How does the paper address the risk of overfitting to the repurposing cohort, especially given the limited dataset size (340K samples)?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces FORMED, a foundation model for medical time series (MedTS) classification that leverages pre-trained backbones and domain-specific adaptation to address inter- and intra-dataset heterogeneity. The approach combines general temporal feature extraction with a medical domain shell trained on a curated MedTS cohort, enabling adaptation to unseen datasets with minimal parameter updates."
          },
          "strengths": {
            "value": "Originality is demonstrated through the novel repurposing of foundation models for MedTS classification, addressing key challenges like heterogeneity and data insufficiency. The methodology integrates generalizable adaptation with domain knowledge, showing strong experimental performance. Clarity is maintained through structured explanations and a figure illustrating adaptation paradigms. The significance lies in advancing clinical deployment of MedTS models, a critical healthcare challenge."
          },
          "weaknesses": {
            "value": "The paper lacks specifics on the pre-trained backbone model (e.g., architecture, pre-training data), which hinders reproducibility. The repurposing cohort (5 datasets with 340K samples) may not be sufficiently diverse or representative of real-world MedTS heterogeneity. Experimental details are sparse, including statistical significance of results, ablation studies, and comparisons with state-of-the-art methods beyond the claimed 11 baselines. The parameter update strategy for adaptation is not clearly explained."
          },
          "questions": {
            "value": [
              "What is the specific foundation model used as the backbone, and what data was it pre-trained on?",
              "How was the domain-specific shell trained? What medical knowledge sources were incorporated?",
              "Are the 5 datasets in the repurposing cohort representative of real-world MedTS variability (e.g., channel counts, sampling rates)?",
              "What is the exact parameter update strategy for adaptation? How does it handle varying channel configurations and time-series lengths?",
              "How are results validated statistically? Are there p-values or confidence intervals for performance metrics?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes FORMED, a foundation model repurposed for medical time series (MedTS) classification. It addresses challenges like inter- and intra-dataset heterogeneity and data insufficiency by combining a pre-trained backbone with domain-specific adaptation. FORMED claims to generalize across datasets with varying channel configurations, time lengths, and tasks without task-specific adaptation, outperforming existing models on 5 MedTS datasets and unseen data."
          },
          "strengths": {
            "value": "Originality lies in repurposing foundation models for MedTS, a novel approach given the focus on forecasting in prior work. The paper addresses critical challenges in medical time series classification with a structured solution. Experimental validation includes comparisons against 11 baselines and tests on unseen datasets, demonstrating practical utility. The problem's significance in healthcare applications is well-motivated, and the methodology is logically structured."
          },
          "weaknesses": {
            "value": "The paper lacks critical details about the pre-trained backbone model (e.g., architecture, training data), making it hard to assess its suitability for MedTS. The repurposing cohort's selection criteria and dataset characteristics are under-specified, raising questions about generalizability. Experimental results lack statistical significance tests and ablation studies to validate key components. The claim of 'lightweight parameter updates' for new datasets is vague without quantifying the parameter count or training procedure. The related work section is incomplete, omitting key MedTS literature."
          },
          "questions": {
            "value": "1. What specific pre-trained foundation model is used as the backbone, and why is it suitable for MedTS? 2. How were the 5 MedTS datasets in the repurposing cohort selected, and what are their exact characteristics (e.g., channel counts, task types)? 3. Are the performance gains statistically significant across all 5 datasets, and how do they compare to state-of-the-art MedTS models? 4. How does FORMED handle variations in sampling rates or channel configurations not seen during repurposing? 5. What is the exact size and structure of the 'specialized shell' for medical domain knowledge?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "AAZ3vwyQ4X": {
    "paper_id": "AAZ3vwyQ4X",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces Multimodal Structure Preservation Learning (MSPL), a framework that leverages the clustering structure of one data modality to enhance the utility of another. The method aims to preserve structural information across modalities, demonstrated on synthetic time series data and real-world epidemiological datasets (e.g., MALDI-TOF and WGS data)."
          },
          "strengths": {
            "value": "The paper addresses a relevant problem in multimodal learning by proposing a novel framework that bridges utility gaps between data modalities. The method's structure-level alignment approach is original and well-motivated, particularly for applications like epidemiology where cost and data availability are critical. The practical use case of enhancing MALDI-TOF data with WGS clustering structure demonstrates real-world relevance. The paper is well-structured with clear problem formulation and theoretical framework."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about the method (e.g., the exact form of $f_{\\mathrm{struct}}$) and experimental results incomplete. The structure preservation loss function lacks specificity, and it is unclear how the external dissimilarity matrix $\bm{d}$ is computed or validated. The pretext task's labels and their relationship to the structure preservation objective are ambiguous. The absence of ablation studies or comparisons to baseline methods weakens the evaluation."
          },
          "questions": {
            "value": "1. How is the external dissimilarity matrix $\bm{d}$ derived from the other modality? Is it based on ground-truth labels or an unsupervised method? 2. What is the role of the pretext task in the overall framework, and how does it interact with the structure preservation objective? 3. Are there ablation studies demonstrating the necessity of each component (reconstruction, pretext, structure loss)? 4. How does MSPL compare to existing multimodal alignment methods in terms of performance and efficiency?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces Multimodal Structure Preservation Learning (MSPL), a novel framework that leverages the clustering structure of one data modality to enhance the utility of another. By aligning dissimilarity-based clustering structures across modalities, MSPL aims to bridge utility gaps in multimodal data, demonstrated on synthetic time series and real-world epidemiological data (WGS, MALDI-TOF, AMR)."
          },
          "strengths": {
            "value": "Originality: MSPL's focus on structure-level alignment instead of traditional feature-level alignment is a fresh perspective. The application to real-world biomedical data (e.g., MALDI-TOF for outbreak detection) highlights its practical significance. Quality: The framework is well-structured with clear objectives (reconstruction, pretext task, structure preservation). Clarity: The problem statement and motivation are clearly articulated. Significance: The potential to reduce data acquisition costs while improving clustering performance addresses a critical challenge in multimodal machine learning."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how the structure preservation loss (Eq. 7) interacts with other objectives. While experiments show improvements, ablation studies on the loss components (e.g., reconstruction vs. structure preservation) are missing. The synthetic data experiments are limited in complexity compared to real-world scenarios. Implementation details (e.g., U-Net hyperparameters, dissimilarity matrix computation for real data) are under-specified, hindering reproducibility. The comparison with existing methods (e.g., feature-aligned models) is superficial."
          },
          "questions": {
            "value": [
              "How does the structure preservation loss (Eq. 7) compare to other structure-aware objectives (e.g., clustering losses in self-supervised learning)?",
              "What specific hyperparameters were used for the U-Net architecture and the structure preservation term?",
              "How was the external dissimilarity matrix (d) computed for real-world data (e.g., WGS/AMR vs. MALDI)?",
              "Are there ablation studies demonstrating the individual contributions of reconstruction, pretext, and structure preservation losses?",
              "What are the limitations of using mean squared error for f_struct? Could alternative metrics (e.g., KL divergence) yield better results?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Multimodal Structure Preservation Learning (MSPL), a framework that aligns data representations across modalities by preserving the clustering structure of one modality to enhance another. It demonstrates applications in epidemiology, using WGS data to improve MALDI-TOF mass spectrometry for outbreak detection."
          },
          "strengths": {
            "value": "The paper presents a novel approach by focusing on structure-level alignment rather than feature-level alignment, addressing a gap in multimodal learning. The motivation is strong, with a clear real-world application in medical diagnostics. The method is well-structured, with a formal mathematical formulation and references to relevant prior work. The potential impact on reducing data acquisition costs in healthcare is significant."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-methods, leaving critical details about the structure preservation loss function (e.g., how $ f_{\\mathrm{struct}} $ is defined) and experimental results incomplete. The lack of ablation studies, comparisons with baselines, and analysis of limitations (e.g., sensitivity to noise, scalability) undermines the validity of claims. The synthetic and real-world experiments described are not fully detailed, making it impossible to assess the robustness of MSPL."
          },
          "questions": {
            "value": [
              "What is the exact definition of $ f_{\\mathrm{struct}} $, and how is it trained? Is it a learned function or a fixed metric?",
              "Are there ablation studies demonstrating the contribution of each loss term ($ \\mathcal{L}_{\\mathrm{recon}}, \\mathcal{L}_{\\mathrm{pretext}}, \\mathcal{L}_{\\mathrm{struct}} $)?",
              "How does MSPL handle cases where the structural information from the source modality is noisy or incomplete?",
              "What baselines were compared against (e.g., standard autoencoders, multimodal alignment methods)?",
              "Are there qualitative results (e.g., t-SNE plots) showing how structure preservation improves clustering?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 2
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "AAjCYWXC5I": {
    "paper_id": "AAjCYWXC5I",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces zero-shot in-context adversarial learning inspired by GANs to enhance research ideation via multi-LLM-agent interactions. It proposes a novel relative quality ranking metric to evaluate open-ended generation, claiming significant improvements in idea novelty (21%) and feasibility (322%) using GPT-4o."
          },
          "strengths": {
            "value": "Originality: The paper is the first to combine adversarial learning with zero-shot in-context learning for research ideation, offering a novel framework. Quality: The method addresses convergence issues in adversarial training by avoiding bi-level optimization. Clarity: The problem statement and motivation are well-articulated. Significance: If validated, the approach could transform scientific ideation by leveraging LLMs without additional training."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental methodology, such as how the relative quality metric (S) was validated or compared to human evaluation. The 322% feasibility improvement is implausibly high without supporting evidence. No ablation studies or comparisons with baseline methods (e.g., standard in-context learning) are provided. The related work section is incomplete, omitting key prior work on multi-LLM-agent systems. The theoretical foundation for the adversarial framework is underdeveloped."
          },
          "questions": {
            "value": "1. How was the relative quality ranking metric (S) validated against human evaluations? 2. What baselines were used to demonstrate the 21% novelty and 322% feasibility improvements? 3. Are the results specific to GPT-4o, or do they generalize to other LLMs? 4. How does the proposed method differ from existing multi-LLM-agent frameworks (e.g., debate or collaboration models)? 5. What is the exact definition of 'feasibility' in the context of research ideas?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a novel framework called zero-shot in-context adversarial learning, inspired by Generative Adversarial Networks (GANs), to enhance research ideation using large language models (LLMs). The approach combines adversarial learning with in-context learning to optimize the utilization of LLMs' parametric knowledge, while avoiding bi-level optimization. The authors also propose a relative quality ranking metric as a proxy for human evaluation of open-ended generation, demonstrating significant improvements in idea novelty and feasibility with GPT-4o."
          },
          "strengths": {
            "value": "Originality: The paper creatively combines GANs with in-context learning, a novel approach for research ideation. The relative quality ranking metric addresses a critical gap in evaluating open-ended generation. Quality: The experiments with GPT-4o show substantial improvements (21% novelty, 322% feasibility). Clarity: The problem statement and contributions are well-articulated, with clear motivation for adversarial learning in this context. Significance: The work has potential to transform scientific discovery by improving LLM-based hypothesis generation."
          },
          "weaknesses": {
            "value": "The paper is truncated, leaving critical details about the multi-LLM-agent framework, the adversarial learning mechanism, and the relative quality metric unexplained. The theoretical foundation for the zero-shot in-context adversarial learning formulation is underdeveloped, with limited connection to GANs. The experimental validation lacks baselines for comparison and detailed analysis of the metric's reliability. The claim about GPT-4o's performance requires verification, as the paper does not provide raw data or statistical significance tests."
          },
          "questions": {
            "value": [
              "How exactly is adversarial learning integrated with in-context learning? What are the specific roles of the LLM agents in the multi-agent framework?",
              "What is the exact implementation of the relative quality ranking metric? How was it validated against human evaluation?",
              "Are there baselines for comparison (e.g., standard in-context learning, non-adversarial methods)? How do the results depend on the choice of LLMs?",
              "What are the limitations of the approach? How does it handle tasks requiring domain-specific knowledge or long-term reasoning?",
              "How was the 322% feasibility improvement measured? What defines 'feasibility' in this context, and how is it quantified?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces zero-shot in-context adversarial learning, inspired by GANs, to enhance research ideation via multi-LLM-agent interactions. It proposes a novel relative quality ranking metric for evaluating open-ended generation and reports significant improvements in idea novelty (21%) and feasibility (322%) using GPT-4o."
          },
          "strengths": {
            "value": "The paper presents a novel framework combining adversarial learning with in-context learning for scientific ideation, addressing a gap in leveraging LLMs' parametric knowledge. The relative quality ranking metric is a practical contribution for proxy human evaluation. The potential impact on accelerating research creativity is promising, and the approach's elimination of bi-level optimization is technically interesting. The abstract's quantitative results suggest strong empirical motivation."
          },
          "weaknesses": {
            "value": "The paper is incomplete, making it impossible to assess the full methodology, experiments, or comparisons with existing work. Key details about the adversarial learning formulation (e.g., how GANs are adapted to zero-shot settings) and the metric's validation are missing. The claims of 322% feasibility improvement require deeper scrutiny, as such large gains may be context-dependent or statistically questionable without proper baselines."
          },
          "questions": {
            "value": "1. How exactly is the adversarial component implemented in the zero-shot in-context setting? 2. What are the baselines used for comparing the 21% and 322% improvements? 3. How was the relative quality ranking metric validated against human evaluations? 4. What specific LLMs were used in the multi-agent system, and how do they interact? 5. Are there limitations or failure cases of the proposed approach that the authors acknowledge?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 2
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "AFMi0kUtDr": {
    "paper_id": "AFMi0kUtDr",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces PruneFuse, a novel data selection method for active learning that leverages structured pruning and network fusion. The approach first prunes a neural network to create a smaller model for efficient data selection, then fuses the pruned network with the original model to enhance training. The method aims to reduce computational costs while maintaining or improving performance."
          },
          "strengths": {
            "value": "Originality: PruneFuse combines model pruning and network fusion for data selection, offering a fresh perspective on reducing computational overhead in active learning. The structured pruning approach for data selection is innovative, as it avoids retraining large models during the selection phase. Quality: The paper presents extensive experiments across multiple datasets, demonstrating superior performance and computational efficiency. Clarity: The methodology is well-explained with a clear two-stage process. Significance: The method addresses a critical challenge in active learning—scalability under resource constraints—making it valuable for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed comparison with state-of-the-art methods like SubSelNet and SVP, particularly in terms of computational efficiency and generalization. The fusion mechanism between the pruned network and the original model is not thoroughly explained, leaving questions about how knowledge is transferred. Additionally, the paper does not analyze the impact of different pruning strategies (e.g., unstructured vs. structured) on data selection performance. The theoretical justification for why pruned networks are effective for data selection is underdeveloped."
          },
          "questions": {
            "value": [
              "How does PruneFuse determine the most informative samples during the selection phase? Are uncertainty estimates or diversity maximization used?",
              "What specific fusion technique is employed (e.g., knowledge distillation, parameter averaging)? How does this affect the final model's convergence and generalization?",
              "Are there ablation studies demonstrating the contribution of each component (pruning, fusion, etc.) to the overall performance?",
              "How does PruneFuse compare to SubSelNet in terms of computational cost and data selection accuracy? What are the trade-offs?",
              "What is the theoretical basis for the effectiveness of pruned networks in data selection? Are there any guarantees about the quality of selected samples?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces PruneFuse, a method for efficient data selection in active learning by leveraging pruned networks. It first prunes a neural network to create a smaller model for selecting informative samples, then fuses the trained pruned network with the original model to improve training efficiency and performance."
          },
          "strengths": {
            "value": "The paper presents a novel approach combining model pruning and network fusion for data selection, addressing the computational inefficiencies of traditional active learning methods. The method's two-stage process is well-structured, and the experiments demonstrate significant computational savings and performance improvements over baselines. The work also highlights the practical benefits of architectural coherence between pruned and original models, which is a thoughtful design choice. The paper's applicability across multiple datasets and architectures adds to its significance."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of pruning vs. fusion. The experimental comparisons are not sufficiently comprehensive (e.g., no direct comparison with SVP or SubSelNet in terms of computational cost). The theoretical justification for why pruned networks are effective for data selection is minimal. The fusion mechanism is described but not thoroughly explained, leaving questions about how knowledge distillation or parameter integration works. Additionally, the paper does not address potential limitations, such as sensitivity to pruning rates or compatibility with non-structured pruning methods."
          },
          "questions": {
            "value": [
              "How were the pruning parameters (e.g., sparsity levels, structured pruning criteria) determined? Were they validated through ablation studies?",
              "What specific fusion strategy is used (e.g., parameter averaging, knowledge distillation, or architectural merging)? How does this affect the final model's performance?",
              "How does PruneFuse compare to SVP in terms of computational efficiency, given that both use small proxy models?",
              "Are there any cases where the pruned network's data selection performance degrades, and how is this mitigated?",
              "What is the exact role of the 'PruneFuse V2' feedback loop mentioned in Figure 1, and how does it differ from the base method?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces PruneFuse, a method for efficient data selection in active learning that leverages pruned neural networks. It operates in two stages: first, a pruned network is trained to select informative samples, and second, this pruned network is fused with the original model to enhance training efficiency and performance. The approach aims to reduce computational costs while maintaining or improving model accuracy."
          },
          "strengths": {
            "value": "Originality: The use of pruned networks as data selectors is a novel approach, addressing the computational inefficiencies of traditional active learning methods. The fusion mechanism with the original model is also a creative contribution. Quality: The paper includes experiments on multiple datasets (CIFAR-10, CIFAR-100, etc.), demonstrating competitive performance. Clarity: The methodology is well-structured, with clear sections on background, related work, and experimental setup. Significance: The problem of efficient data selection is critical for practical deep learning, and the proposed method addresses a key bottleneck in scalability and resource constraints."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art baselines, making it difficult to assess the relative improvement of PruneFuse. The fusion process is not thoroughly explained, and the paper does not provide ablation studies to validate the impact of key components (e.g., structured pruning vs. unstructured pruning). The computational cost reduction claims are not substantiated with quantitative analysis (e.g., FLOPs, training time). Additionally, the theoretical justification for why pruned networks are effective for data selection is minimal, and the paper does not address potential limitations, such as cases where the pruned network's architecture diverges significantly from the target model."
          },
          "questions": {
            "value": "1. How is the fusion process between the pruned network and the original model technically implemented? What specific techniques (e.g., parameter averaging, knowledge distillation) are used? 2. What ablation studies were conducted to evaluate the contribution of structured pruning versus other pruning strategies? 3. How does PruneFuse handle cases where the pruned network's architecture differs from the original model, potentially leading to suboptimal data selection? 4. Can the authors provide quantitative metrics (e.g., FLOPs, training time) to support their claims of computational efficiency? 5. What is the role of knowledge distillation in the final training phase, and how is it implemented in practice?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "AJAStQYZaL": {
    "paper_id": "AJAStQYZaL",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces DIVERSEAGENTENTROPY, a novel method for quantifying uncertainty in black-box LLMs by leveraging multi-agent interactions. The approach evaluates consistency across diverse questions about the same query, using the entropy of agents' final answers as a reliability metric. It also includes an abstention policy to avoid hallucinations, demonstrating improved performance over self-consistency-based methods."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in LLM reliability with a fresh perspective by using multi-agent interactions to assess uncertainty. The method is practical for black-box settings, avoiding reliance on internal model access. The motivation is strong, and the approach aligns with the need for scalable oversight in AI. The paper also highlights the limitations of existing self-consistency methods, providing a clear rationale for the proposed solution."
          },
          "weaknesses": {
            "value": "The paper is truncated, leaving critical details about the agent interaction mechanism, experimental setup, and results incomplete. Key aspects like how agents are initialized, the exact process of their interactions, and the criteria for the abstention policy are unclear. The evaluation metrics (e.g., AUROC scores) and comparisons to baselines lack specificity. The ablation studies and analysis of model retrievability are not detailed, limiting the paper's ability to demonstrate its claims."
          },
          "questions": {
            "value": [
              "How are the agents initialized with diverse background knowledge? What specific variations are introduced in the questions to ensure diverse perspectives?",
              "Can the authors clarify the exact process of multi-agent interactions and how they lead to self-correction? Are there constraints or rules governing these interactions?",
              "What are the specific metrics used to evaluate hallucination detection and model reliability? How do the results compare to state-of-the-art methods in terms of accuracy and AUROC?",
              "How is the abstention policy calibrated? What thresholds determine when the model withholds responses?",
              "What computational costs or trade-offs are associated with the proposed method compared to existing approaches?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces DIVERSEAGENTENTROPY, a method for quantifying uncertainty in black-box LLMs by simulating multi-agent interactions. The core idea is that a model's certainty can be assessed by checking consistency across diverse questions about the same original query, with agents (same model with different contexts) collaboratively refining answers. The approach uses entropy of final answers as an uncertainty metric and includes an abstention policy for high-uncertainty cases."
          },
          "strengths": {
            "value": "Originality: The use of multi-agent interactions with the same model (rather than cross-model agents) to evaluate uncertainty is novel. The method addresses limitations of self-consistency-based approaches by incorporating diverse perspectives. Quality: The paper presents empirical results showing improvements in AUROC and accuracy compared to existing methods. Clarity: The problem statement and motivation are well-articulated, with clear connections to prior work. Significance: The black-box setting and practical applicability of the method are important for real-world LLM deployment."
          },
          "weaknesses": {
            "value": "The method relies on generating diverse questions, but the paper lacks details on how these are created (e.g., manual vs. automated). The experiments are incomplete (method section is cut off), and critical implementation details (e.g., agent interaction protocols, ablation studies) are not described. The paper does not compare with state-of-the-art methods beyond self-consistency baselines, and the theoretical justification for the core assumption (consistency under diverse questions implies certainty) is weak. The abstention policy's effectiveness is not thoroughly validated."
          },
          "questions": {
            "value": [
              "How are the diverse questions generated? Are they manually crafted, or is there an automated method (e.g., paraphrasing, perturbation)?",
              "What specific agent interaction protocols are used (e.g., number of agents, rounds of refinement, collaboration mechanisms)?",
              "How is the weighted entropy calculated, and what is the role of the abstention policy in practice?",
              "Are there cases where the method fails, and how does it compare to non-self-consistency baselines (e.g., calibration methods)?",
              "What is the computational cost of the multi-agent interaction process, and how does it scale to large models?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper proposes DIVERSEAGENTENTROPY, a method to quantify uncertainty in Large Language Models (LLMs) by leveraging multi-agent interactions. The approach assumes that a confident model should provide consistent answers across diverse questions about the same original query. It introduces a weighted entropy metric for uncertainty estimation and an abstention policy to avoid hallucinations, demonstrating improved performance over self-consistency-based methods."
          },
          "strengths": {
            "value": "The paper's originality lies in its novel use of multi-agent interactions to assess consistency across diverse perspectives, addressing limitations of existing self-consistency methods. The significance is high, as uncertainty quantification is critical for reliable AI deployment. The clarity of the problem statement and methodology is strong, with a clear connection to real-world applications. The potential impact is notable, as the method could improve trustworthiness in black-box LLMs."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as specific metrics (e.g., AUROC values) or comparisons to baseline methods. The implementation details of the multi-agent interaction process are vague, including how varied questions are generated and how agents collaborate. The abstention policy's effectiveness is not thoroughly evaluated. Additionally, the paper does not address scalability or computational efficiency of the proposed method."
          },
          "questions": {
            "value": "1. How are the 'varied questions' generated to ensure they capture diverse perspectives while relying on the same underlying information? 2. What specific criteria define the 'controlled one-on-one agent interactions' and how are they structured? 3. How is the weighted entropy calculated, and what factors influence the weighting scheme? 4. What thresholds or criteria determine when the abstention policy is triggered? 5. Are there ablation studies demonstrating the contribution of each component (e.g., agent interaction vs. entropy calculation)?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "AJM52ygi6Y": {
    "paper_id": "AJM52ygi6Y",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper addresses decentralized optimization with coupled affine constraints, proposing a first-order algorithm that achieves optimal convergence rates for such problems. It establishes lower complexity bounds and claims the first linearly convergent method for general affine constraints, with applications in resource allocation, power systems, and vertical federated learning."
          },
          "strengths": {
            "value": "The paper presents a novel theoretical contribution by deriving lower complexity bounds and a linearly convergent algorithm for decentralized optimization with general affine constraints, a gap in prior work. The problem formulation is well-motivated with practical applications, and the structure of the paper is logically organized. The authors acknowledge prior work on consensus and constrained optimization, showing awareness of the literature. The focus on decentralized settings with local data storage aligns with modern distributed machine learning trends."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about the algorithm, proofs, and experimental validation unreviewable. Without access to the full analysis, it is impossible to verify the correctness of the theoretical claims or the practical effectiveness of the proposed method. The novelty claim requires rigorous comparison to existing work, which is not fully addressed in the provided text. Additionally, the absence of empirical results (e.g., convergence plots, comparisons to baselines) limits the paper's impact and validation."
          },
          "questions": {
            "value": [
              "What specific lower complexity bounds are derived, and how do they compare to existing results in decentralized optimization?",
              "Can the authors provide a detailed comparison of their algorithm to prior methods for constrained decentralized optimization, particularly in terms of assumptions and convergence guarantees?",
              "What are the experimental results (e.g., convergence rates, scalability) on benchmark problems, and how do they validate the theoretical claims?",
              "How does the algorithm handle non-uniform communication graphs or heterogeneous node capabilities, which are common in real-world decentralized systems?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper addresses decentralized optimization problems with coupled affine constraints, proposing lower complexity bounds and a first-order algorithm that achieves these bounds. The method is claimed to be the first linearly convergent algorithm for such problems, with applications in resource allocation, systems control, and vertical federated learning."
          },
          "strengths": {
            "value": "The paper presents a well-defined problem with clear applications in distributed systems. The focus on coupled constraints, which generalize consensus optimization, represents a novel direction. The theoretical contribution of lower complexity bounds and a linearly convergent algorithm is significant, especially given the lack of prior work on general affine constraints. The paper also contextualizes its work within existing literature and highlights practical relevance."
          },
          "weaknesses": {
            "value": "The paper is incomplete, with critical sections (e.g., algorithm details, experiments) missing. The claims about lower bounds and linear convergence lack concrete analysis or comparisons to existing methods. The novelty is not sufficiently justified against prior work on consensus optimization and constrained decentralized algorithms. The absence of empirical validation undermines the practical relevance of the theoretical results."
          },
          "questions": {
            "value": [
              "What are the exact lower complexity bounds derived in the paper, and how do they compare to existing bounds for similar problems?",
              "How does the proposed algorithm handle different types of affine constraints, and what assumptions are required for its convergence?",
              "Are there any limitations or special cases where the algorithm fails, and how does the paper address them?",
              "What experimental results are presented to validate the theoretical claims, and how do they compare to existing methods?",
              "How does the paper's approach differ from prior work on consensus optimization and constrained decentralized algorithms in terms of assumptions and complexity?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper addresses decentralized optimization problems with coupled affine constraints, proposing a first-order algorithm that achieves lower complexity bounds and linear convergence. It highlights applications in resource allocation, systems control, and vertical federated learning, positioning itself as the first linearly convergent method for general affine constraints."
          },
          "strengths": {
            "value": "Originality: The paper introduces a novel algorithm for decentralized optimization with general affine coupled constraints, which is claimed to be the first linearly convergent method in this setting. The problem formulation is well-motivated with practical applications in resource allocation and vertical federated learning. Quality: The theoretical analysis includes convergence rates and lower complexity bounds, which are critical for optimization literature. Clarity: The introduction and problem setup are clearly articulated, with detailed examples of application scenarios. Significance: The work addresses a gap in decentralized optimization, particularly for problems where constraints are not communication-friendly, which has implications for distributed machine learning and control systems."
          },
          "weaknesses": {
            "value": "The paper is cut off, so the full experimental validation, comparison with existing methods, and detailed algorithmic implementation are missing. The claim of being 'the first linearly convergent algorithm' requires stronger justification, as similar results may exist in recent literature. The theoretical analysis of lower bounds may lack sufficient depth without the full paper. The practical challenges of handling general affine constraints (e.g., matrix structure, scalability) are not discussed."
          },
          "questions": {
            "value": "1. How do the proposed lower complexity bounds compare to existing results in decentralized optimization? Are they tight? 2. What specific challenges arise from the general affine constraint structure, and how does the algorithm address them? 3. Are there empirical results demonstrating the algorithm's performance on real-world problems like vertical federated learning or power systems? 4. How does the algorithm's communication complexity scale with network size and constraint structure?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "AJpUZd8Clb": {
    "paper_id": "AJpUZd8Clb",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces 'coherent factuality' to ensure language model outputs are both factually correct and logically coherent in reasoning tasks. It proposes a conformal prediction method applied to a deducibility graph, which captures dependencies between claims. The approach is evaluated on math reasoning datasets, demonstrating high factuality while preserving most claims."
          },
          "strengths": {
            "value": "The paper's originality lies in defining coherent factuality, which addresses inter-claim dependencies in reasoning tasks—a critical gap in prior work that assumes independence. The method's use of a deducibility graph for conformal prediction is novel and well-motivated. The experiments are thorough, showing strong performance on MATH and FELM datasets, with clear metrics (e.g., 90% factuality while retaining 80% of claims). The writing is clear, and the paper effectively contextualizes its contributions relative to prior work on conformal prediction and LLM reasoning."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed comparison with graph-based conformal prediction methods in other domains, limiting the understanding of its generalizability. The definition of 'coherent factuality' is not fully elaborated, making it difficult to assess how it differs from existing notions of factuality. The evaluation focuses on math problems, but the approach's applicability to other reasoning tasks (e.g., scientific reasoning or legal arguments) is unclear. Additionally, the paper does not provide error analysis or examples of cases where the method fails, which would strengthen the empirical validation."
          },
          "questions": {
            "value": "1. How is the deducibility graph constructed? Are there specific heuristics or rules used to identify dependencies between claims? 2. What are the limitations of the graph-based approach in handling non-linear or dynamic reasoning paths? 3. How does the method scale to larger or more complex reasoning problems? 4. Can the paper provide examples of 'coherent factuality' violations in baseline methods that the proposed approach resolves?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces the concept of 'coherent factuality' to evaluate the correctness of language model (LM) outputs in reasoning tasks, where claims depend on prior steps. The authors propose a method combining split conformal prediction with a 'deducibility' graph to filter claims while maintaining logical coherence. They validate their approach on mathematical reasoning datasets, demonstrating improved factuality while retaining a high proportion of original claims."
          },
          "strengths": {
            "value": "The paper's originality lies in addressing the limitations of prior work that assumes claim independence in reasoning tasks, offering a novel framework for structured reasoning verification. The method's integration of graph-based dependencies with conformal prediction is creative and well-motivated. The experiments on MATH and FELM datasets provide concrete evidence of the approach's effectiveness, and the emphasis on 'legibility' (third-party verifiability) adds practical value. The paper is clearly structured, with detailed explanations of the algorithm and thorough comparisons to baseline methods."
          },
          "weaknesses": {
            "value": "The definition of 'coherent factuality' remains somewhat abstract, and the paper could clarify how it quantitatively differs from traditional factuality metrics. The deducibility graph construction relies on heuristics without rigorous analysis of its impact on performance. While the experiments show promise, they focus on mathematical reasoning—additional evaluation on diverse reasoning domains (e.g., logical puzzles, scientific reasoning) would strengthen generalizability. The paper also lacks ablation studies to isolate the contribution of the graph-based calibration vs. other components."
          },
          "questions": {
            "value": "1. How is 'coherent factuality' formally defined in terms of claim ordering and dependency? 2. What specific heuristics are used to construct the deducibility graph, and how robust are they to noisy or incomplete LM outputs? 3. Can the method handle non-linear reasoning structures (e.g., branching arguments) that challenge graph-based representations? 4. How does the 'strict definition' of factuality relate to existing metrics, and what trade-offs exist between coverage and factuality? 5. Are there cases where filtering 'true but non-essential' claims (as in Figure 1) could harm downstream tasks relying on complete reasoning chains?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces 'coherent factuality' as a new metric for evaluating the correctness of language model (LM) outputs in reasoning tasks, where claims depend on prior steps. The authors propose a graph-based conformal prediction method that filters claims while preserving logical coherence, validated on math problem datasets."
          },
          "strengths": {
            "value": "Originality: The concept of 'coherent factuality' addresses a critical gap in existing conformal prediction methods by accounting for inter-claim dependencies. Quality: The method leverages graph structures and split conformal prediction with clear theoretical grounding. Clarity: The problem statement and approach are well-structured, with illustrative examples showing the limitations of prior work. Significance: The work advances reliable LM reasoning, crucial for high-stakes applications where logical consistency matters."
          },
          "weaknesses": {
            "value": "The empirical evaluation is limited to math problems from MATH and FELM datasets, with no analysis of generalization to other reasoning domains. The graph construction method lacks detail - it's unclear how deducibility graphs are automatically generated from LM outputs. The claim about '90% factuality' requires clarification: is this measured on filtered vs. original outputs, and how does it compare to baselines? Theoretical guarantees are not fully developed for the graph-based approach."
          },
          "questions": {
            "value": [
              "How are deducibility graphs constructed for arbitrary reasoning tasks? Is this process automated or manual?",
              "What specific metrics were used to quantify 'coherent factuality' compared to traditional factuality measures?",
              "How does the method handle dynamic reasoning structures where claim dependencies change based on input?",
              "Can the approach be adapted to non-mathematical reasoning tasks like legal reasoning or scientific argumentation?",
              "What ablation studies were performed to validate the necessity of the graph structure for conformal guarantees?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "AKsfpHc9sN": {
    "paper_id": "AKsfpHc9sN",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces LoRD, a novel model extraction attack (MEA) method tailored for large language models (LLMs) by aligning with their reinforcement learning with human feedback (RLHF) training process. The approach uses a policy-gradient-style training task to mimic LLM alignment, achieving higher query efficiency and watermark resistance compared to traditional MEAs that rely on supervised learning (e.g., MLE). Theoretical analysis and experiments on domain-specific tasks demonstrate its effectiveness in extracting commercial LLMs with significantly fewer parameters."
          },
          "strengths": {
            "value": "The paper presents a highly original approach by addressing the critical mismatch between traditional MEAs (designed for DNNs) and LLM alignment procedures. The theoretical analysis linking LoRD's convergence to LLM alignment is a strong contribution, offering rigorous justification for the method's design. The experimental validation on diverse NLP tasks and commercial LLMs demonstrates practical relevance, while the focus on query efficiency and watermark resistance highlights important security implications. The paper is well-structured, with clear problem formulations and logical flow."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with state-of-the-art MEAs for LLMs, making it difficult to assess the relative performance gains of LoRD. The theoretical analysis, while promising, is somewhat abstract and could benefit from more concrete examples or empirical validation. The claim about 'reducing query complexity' is not quantified, and the watermark resistance is primarily theoretical without practical demonstrations. Additionally, the paper does not address potential countermeasures against LoRD or ethical considerations of model extraction attacks."
          },
          "questions": {
            "value": [
              "How does LoRD's query efficiency compare quantitatively to existing MEAs (e.g., in terms of number of queries required for comparable performance)?",
              "What specific metrics or benchmarks were used to validate the watermark resistance claims? Are there empirical results showing reduced stealthiness compared to MLE-based methods?",
              "Can the theoretical guarantees about alignment consistency be generalized to other LLM architectures beyond the ones tested?",
              "How does the paper address the ethical implications of developing effective MEAs for commercial LLMs?",
              "Are there any limitations to the domain-specific extraction setup that could affect the broader applicability of LoRD?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces LoRD, a novel model extraction attack (MEA) method tailored for large language models (LLMs) by aligning with their reinforcement learning with human feedback (RLHF) training. The approach uses a policy-gradient-style training task to mimic LLM alignment, theoretically ensuring consistency with LLM training procedures and improving query efficiency and watermark resistance. Experiments on domain-specific NLP tasks demonstrate its effectiveness in extracting commercial LLMs with significantly smaller local models."
          },
          "strengths": {
            "value": "The paper presents a new perspective on MEAs by addressing the misalignment between traditional methods and LLM training, which is a significant originality. The theoretical analysis linking LoRD's convergence to LLM alignment procedures is a strong contribution. The experimental validation on diverse tasks and datasets shows practical relevance, and the method's focus on query efficiency and watermark resistance addresses critical practical challenges in MEAs."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental comparisons with existing MEA methods (e.g., MLE, KD) and does not specify the exact baselines used. The domain-specific extraction setup is underdefined, making it unclear how generalizable the results are. The theoretical guarantees are abstract and require more concrete examples. Additionally, the paper does not discuss practical limitations, such as robustness to adversarial defenses or scalability to larger LLMs."
          },
          "questions": {
            "value": "1. What specific datasets and tasks were used for domain-specific extractions, and how were they defined? 2. How were the baselines (e.g., MLE, KD) selected and implemented for comparison? 3. What is the exact mechanism for watermark resistance, and how was it quantitatively evaluated? 4. How does LoRD perform on LLMs with different architectures or training data? 5. Are there any limitations in the theoretical analysis regarding convergence guarantees?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces Locality Reinforced Distillation (LoRD), a novel model extraction attack (MEA) tailored for large language models (LLMs). LoRD addresses the limitations of existing MEA methods by aligning with the reinforcement learning with human feedback (RLHF) procedure inherent to LLMs. The method uses a policy-gradient-style training task that leverages the 'locality direction' between local and victim model outputs as an implicit reward signal, avoiding reliance on human feedback. Theoretical analysis and experiments demonstrate LoRD's query efficiency, watermark resistance, and effectiveness in extracting commercial LLMs."
          },
          "strengths": {
            "value": "Originality: The paper presents a fresh perspective on MEAs by explicitly addressing the misalignment between traditional MEA strategies (e.g., MLE/KD) and LLM alignment procedures (e.g., RLHF). The policy-gradient-based approach is novel in the context of MEAs. Quality: The theoretical analysis rigorously connects LoRD's convergence to LLM alignment, and the experiments validate its effectiveness across diverse NLP tasks. Clarity: The paper is well-structured, with clear explanations of technical concepts (e.g., policy gradients, RLHF) and a logical flow from problem statement to solution. Significance: Model extraction attacks on LLMs are critical security concerns, and LoRD's practical results (e.g., extracting 175B-parameter models with 8B locals) underscore its real-world relevance."
          },
          "weaknesses": {
            "value": "The paper lacks a comprehensive comparison with existing LLM-specific MEA methods (e.g., Wallace et al. 2020, Li et al. 2023b), making it harder to assess LoRD's relative novelty. The theoretical guarantees are abstract and could benefit from concrete examples or simplifications. While experiments are extensive, they focus on domain-specific tasks, and the paper does not discuss performance on open-ended generation or multi-turn dialog tasks. The watermark mitigation mechanism is vaguely described, with no quantitative analysis of how effectively LoRD removes or bypasses watermarks."
          },
          "questions": {
            "value": "1. How does LoRD specifically handle different watermarking techniques (e.g., token-level vs. structural watermarks)? 2. What are the hyperparameters or design choices that enable LoRD's query efficiency, and how sensitive is it to these settings? 3. The paper mentions 'domain-specific extractions' but does not clarify how LoRD adapts to tasks requiring external knowledge (e.g., math reasoning). 4. Are there any scenarios where LoRD fails, and how does its performance scale with larger victim models or query budgets?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "AMbIvaD4Rr": {
    "paper_id": "AMbIvaD4Rr",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces MTMDVRP, a novel multi-task multi-distribution vehicle routing problem setting that better captures real-world customer distribution complexities. The proposed SHIELD model combines sparsity via a Mixture-of-Depths (MoD) decoder and hierarchical clustering to improve generalization across tasks and distributions, demonstrating state-of-the-art performance on 9 real-world maps with 16 VRP variants."
          },
          "strengths": {
            "value": "The paper makes a significant contribution by proposing MTMDVRP, a more realistic extension of multi-task VRP that addresses critical gaps in existing foundation models. The SHIELD architecture innovatively integrates sparsity (MoD) and hierarchy (clustering) principles, offering a novel approach to balance computational efficiency with generalization. The empirical evaluation is comprehensive, covering diverse VRP variants and real-world scenarios. The work is well-structured with clear motivation, technical details, and strong experimental validation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of MoD and clustering components. The real-world distribution generation methodology is not sufficiently explained, making it difficult to assess generalizability. The theoretical justification for the hierarchical clustering approach is limited, and the paper does not discuss potential limitations in handling extreme distribution shifts or scalability to larger problem sizes."
          },
          "questions": {
            "value": [
              "How were the 9 real-world maps selected, and what specific distribution characteristics do they represent?",
              "What baselines were compared against in the experiments, and how do they relate to the MTMDVRP setting?",
              "Can the authors provide additional analysis on how MoD adapts to different task/distribution contexts during inference?",
              "How does the clustering layer handle dynamic customer distributions that evolve over time?",
              "What is the computational overhead of SHIELD compared to standard multi-task VRP solvers?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces MTMDVRP, an extension of the Multi-Task VRP (MTVRP) setting to incorporate real-world customer distribution diversity. It proposes SHIELD, a neural solver leveraging sparsity via a Mixture-of-Depths (MoD) decoder and hierarchical clustering to improve generalization across tasks and distributions. The approach is evaluated on 9 real-world maps with 16 VRP variants."
          },
          "strengths": {
            "value": "Originality: The MTMDVRP setting addresses a critical gap in real-world distribution diversity, which is a novel extension of MTVRP. The integration of sparsity (MoD) and hierarchy (clustering) principles demonstrates creative problem-solving. Quality: The paper provides a clear technical framework with well-defined components (MoD and clustering). Clarity: The structure is logical, and the motivation for MTMDVRP is well-justified. Significance: Addressing real-world distributional shifts in VRP has practical relevance, and the potential for a unified solver could impact industrial applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with strong baselines, particularly state-of-the-art unified solvers for MTVRP. The MTMDVRP setup is novel but not rigorously benchmarked, making it hard to assess the magnitude of improvements. The MoD technique is referenced but not thoroughly explained in the context of NCO, and its customization is unclear. The clustering layer's design and effectiveness are under-specified, with no ablation studies to validate its contribution. The 'real-world' maps are not described in terms of diversity, scale, or relevance to industrial scenarios. Theoretical justification for sparsity and hierarchy in this context is minimal. Experiments focus on in-distribution performance but do not quantify generalization benefits rigorously."
          },
          "questions": {
            "value": [
              "What specific baselines were used for comparison? How do SHIELD's results compare to the state-of-the-art unified solvers for MTVRP?",
              "How were the 9 real-world maps selected? Are they representative of diverse distributional patterns in industrial settings?",
              "What is the computational overhead of the MoD and clustering components? How does this affect scalability?",
              "Are there ablation studies demonstrating the individual contributions of sparsity and hierarchy to performance?",
              "How are 'unseen' tasks/distributions defined? What metrics (e.g., solution quality, runtime) were used to evaluate generalization?",
              "What is the theoretical basis for combining sparsity and hierarchy in this problem? How do these principles align with the Information Bottleneck perspective?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces the Multi-Task Multi-Distribution VRP (MTMDVRP) setting, extending traditional multi-task VRP (MTVRP) by incorporating realistic customer distributions. The proposed SHIELD model combines sparsity via a Mixture-of-Depths (MoD) decoder and hierarchical clustering to improve generalization across tasks and distributions, demonstrating strong performance on 9 real-world maps with 16 VRP variants."
          },
          "strengths": {
            "value": "The paper's originality lies in formalizing MTMDVRP as a novel, more realistic VRP setting that accounts for distributional diversity. The integration of sparsity (MoD) and hierarchical clustering mechanisms demonstrates creative problem-solving for efficient generalization. The experimental scope is extensive, covering 16 VRP variants across real-world maps, and the paper addresses a critical gap in existing unified VRP solvers. The clarity of the problem formulation and technical contributions is strong, with clear connections to information bottleneck principles."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with specific baselines, particularly in how SHIELD outperforms prior unified solvers like those in Liu et al. (2024) or Zhou et al. (2024). The MoD implementation is referenced but not thoroughly explained in the context of VRP decoding. The clustering layer's design and its impact on performance are not empirically validated through ablation studies. Additionally, the paper does not address how SHIELD handles extreme distributional shifts or the computational trade-offs between sparsity and model complexity."
          },
          "questions": {
            "value": [
              "How does the MoD mechanism specifically adapt to different VRP distributions during decoding? Are there quantitative results showing its efficiency gains?",
              "What metrics were used to evaluate the effectiveness of the hierarchical clustering layer, and how does it compare to alternative clustering approaches?",
              "The paper mentions 'real-world maps' but does not describe their source or validation. How were these datasets curated to reflect realistic customer distributions?",
              "Are there theoretical guarantees or empirical analysis showing that the sparsity-hierarchy trade-off improves generalization compared to non-structured decoders?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "AN3VTbqM1N": {
    "paper_id": "AN3VTbqM1N",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces a new framework called LIIPA, which uses large language models (LLMs) to infer implicit character portrayals in narrative texts. It proposes a synthetic dataset (ImPortPrompts) with enhanced diversity and coverage compared to existing benchmarks and evaluates different LLM prompting strategies (LIIPA-SENTENCE, LIIPA-STORY, LIIPA-DIRECT) for character analysis across dimensions like intellect, appearance, and power. The work highlights a fairness-accuracy trade-off in LLM-based methods and demonstrates superior performance over non-LLM baselines."
          },
          "strengths": {
            "value": "The paper's strengths include the creation of a novel dataset (ImPortPrompts) with improved cross-topic similarity and lexical diversity, which addresses a critical gap in existing benchmarks. The LIIPA framework introduces flexible prompting strategies (e.g., intermediate attribute lists, chain-of-thought) to model implicit portrayal, showcasing adaptability. The exploration of fairness-accuracy trade-offs aligns with algorithmic fairness literature, adding sociotechnical relevance. The work also emphasizes the importance of implicit portrayal in literary analysis, a dimension often overlooked by prior tools."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology for synthetic dataset generation, making it unclear how the dataset's quality and representativeness were validated. Experimental results are insufficiently contextualized: it is unclear how LIIPA compares to state-of-the-art LLM baselines beyond COMET, and the fairness-accuracy trade-off is not rigorously quantified. The paper does not address potential biases in the synthetic data generation process or provide human evaluation to validate the inferred portrayals. Additionally, the claims about robustness to increasing character counts are not substantiated with ablation studies or statistical significance tests."
          },
          "questions": {
            "value": [
              "How was the synthetic dataset (ImPortPrompts) generated? What criteria were used to ensure cross-topic similarity and lexical diversity?",
              "What specific fairness metrics were used to quantify the fairness-accuracy trade-off? How do these metrics align with existing literature?",
              "Why were non-LLM baselines (e.g., COMET) the only comparison points? How do LIIPA variants compare to other LLM-based methods (e.g., fine-tuned models) or advanced prompting techniques?",
              "Are the results statistically significant? Were p-values or confidence intervals reported for the accuracy and fairness metrics?",
              "How were the classification guidelines (e.g., low/neutral/high for intellect, appearance, power) validated? Was there human annotation or inter-annotator agreement analysis?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a novel framework called LIIPA (LLMs for Inferring Implicit Portrayal for Character Analysis) to analyze implicit character portrayal in narratives using large language models (LLMs). The authors generate a synthetic dataset with enhanced lexical diversity and cross-topic similarity, and evaluate LIIPA's performance against existing methods, highlighting a fairness-accuracy tradeoff in LLM-based approaches."
          },
          "strengths": {
            "value": "Originality: The paper addresses a critical gap in existing tools that focus on explicit character indicators, proposing a new framework and dataset for implicit portrayal analysis. Quality: The methodology includes a systematic comparison of LLM prompting strategies and a thorough investigation of fairness implications. Clarity: The paper is well-structured with clear explanations of the task formulation, dataset curation, and experimental setup. Significance: The work has practical implications for literary analysis, bias detection in narratives, and the development of socially responsible AI tools."
          },
          "weaknesses": {
            "value": "The dataset is synthetic, and the paper lacks validation against human-annotated benchmarks, which could limit the reliability of results. The fairness-accuracy tradeoff is discussed but not thoroughly analyzed in terms of measurement metrics or real-world implications. The paper does not address potential biases in the synthetic data generation process or evaluate LIIPA's performance on diverse narrative genres and character types."
          },
          "questions": {
            "value": "1. How were the synthetic labels for the dataset validated against human annotations or existing benchmarks? 2. What specific metrics were used to quantify fairness in the experiments, and how do they relate to established fairness criteria? 3. How does LIIPA's performance vary across different narrative structures (e.g., short stories vs. novels) or character roles (e.g., protagonists vs. secondary characters)? 4. Are there any limitations in the synthetic data generation process that could affect the generalizability of the results?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces LIIPA, a framework leveraging large language models (LLMs) to infer implicit character portrayals in narratives, addressing the gap in existing tools that rely on explicit textual indicators. The authors propose a novel dataset (ImPortPrompts) with enhanced cross-topic similarity and lexical diversity, and evaluate LIIPA's performance across three dimensions (intellect, appearance, power), highlighting a fairness-accuracy tradeoff among its variants."
          },
          "strengths": {
            "value": "The paper addresses a meaningful problem in literary analysis by focusing on implicit character portrayal, which is critical for understanding narrative design. The proposed dataset (ImPortPrompts) offers improvements over existing benchmarks in diversity and coverage. The LIIPA framework introduces novel prompting strategies (e.g., intermediate attribute lists) that demonstrate superior performance compared to prior methods like COMET. The work also contributes to algorithmic fairness analysis in NLP, identifying a tradeoff between fairness and accuracy that aligns with existing literature. The clarity of the problem formulation and experimental design is strong, with clear ablation studies and comparisons."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology on how the synthetic dataset (ImPortPrompts) was generated, which is critical for reproducibility. The fairness-accuracy tradeoff is mentioned but not thoroughly analyzed—e.g., the specific demographics or metrics used to measure fairness are unclear. The experimental evaluation is limited to a single dataset, and comparisons with non-LLM baselines (e.g., rule-based or traditional NLP methods) are not sufficiently detailed. The paper also does not address how LIIPA handles ambiguous or contradictory portrayals in complex narratives, which is a key challenge in implicit analysis."
          },
          "questions": {
            "value": "1. How was the synthetic dataset (ImPortPrompts) generated, and what ensures its diversity and alignment with real-world narratives? 2. What specific fairness metrics were used to quantify the tradeoff between fairness and accuracy, and how do they relate to existing algorithmic fairness frameworks? 3. Are the results generalizable to other narrative genres or languages, or are they limited to the specific dataset described? 4. How does LIIPA handle dynamic character portrayals that evolve over time, and what mechanisms ensure consistency in multi-turn narratives?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "APCjgjFy5M": {
    "paper_id": "APCjgjFy5M",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces Value Explicit Pretraining (VEP), a contrastive learning method that pretrains an encoder using Bellman return estimates from offline play data to learn transferable visual representations. VEP focuses on task progress rather than visual similarity, enabling efficient adaptation to new tasks with improved sample efficiency and reward performance."
          },
          "strengths": {
            "value": "Originality: VEP introduces a novel approach by explicitly incorporating value functions (Bellman returns) into pretraining, distinguishing it from prior methods like image reconstruction or temporal consistency. Quality: The method is theoretically grounded in RL principles, with experiments showing significant gains on benchmark tasks. Clarity: The paper clearly articulates the problem of misaligned pretraining objectives and positions VEP as a solution. Significance: The focus on transferable representations for RL has broad implications for robotics and sequential decision-making."
          },
          "weaknesses": {
            "value": "The experimental evaluation lacks depth, with limited details on baseline comparisons, hyperparameters, and ablation studies. The method's reliance on Bellman return estimates is not fully explained, particularly how these are computed without action labels. The paper's figures and related work section are incomplete, hindering full assessment of technical contributions. The claim of 'up to 3× improvement' lacks quantitative justification."
          },
          "questions": {
            "value": "How are Bellman return estimates computed without action labels in the offline play datasets? What is the exact formulation of the contrastive loss, and how are positive/negative samples defined? Are there ablation studies to validate the components of VEP (e.g., value-based contrastive learning vs. other objectives)? How does VEP handle tasks with drastically different dynamics from the pretraining set? What is the computational cost of VEP compared to existing methods?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces Value Explicit Pretraining (VEP), a method for learning transferable visual representations in reinforcement learning by leveraging Bellman return estimates. VEP uses a contrastive loss to align states with similar value function estimates across tasks, aiming to create representations invariant to environment dynamics and appearance. The approach is evaluated on Atari and navigation benchmarks, showing improvements in reward and sample efficiency compared to existing pretraining methods."
          },
          "strengths": {
            "value": "The paper presents a novel approach by explicitly incorporating value function estimates (Bellman returns) into pretraining, addressing a gap in existing methods that focus on visual or temporal consistency. The motivation is clear, and the method's alignment with downstream control tasks is well-justified. The experimental results on standard benchmarks suggest promising performance gains, and the problem statement is well-articulated. The contrastive learning framework is conceptually sound and offers a fresh perspective on representation learning for RL."
          },
          "weaknesses": {
            "value": "The paper lacks critical details about the implementation of VEP, such as how Bellman returns are computed without action labels, the exact contrastive loss formulation, and the sampling strategy for positives/negatives. The experimental section is underdeveloped, with no ablation studies, statistical significance tests, or analysis of representation properties (e.g., invariance to dynamics). The claimed 2× and 3× improvements are not substantiated with sufficient quantitative evidence or comparison to baselines. The related work section is incomplete, and the paper does not address potential limitations of using value estimates for pretraining."
          },
          "questions": {
            "value": "1. How are Bellman return estimates computed without action labels, and what assumptions are made about the offline play data? 2. What is the exact contrastive loss function, and how are positive/negative samples selected (e.g., based on task similarity, return thresholds, or other criteria)? 3. Are there ablation studies demonstrating the contribution of the value-based contrastive loss versus other components? 4. How is the encoder's invariance to environment dynamics and appearance validated? 5. What specific metrics or analyses confirm that VEP's representations capture task objectives rather than task-specific features?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces Value Explicit Pretraining (VEP), a method for learning transferable representations in visual reinforcement learning by leveraging Bellman return estimates to create task-agnostic encoders. The approach uses a contrastive loss to align representations based on task progress rather than visual similarity, demonstrating improved performance on Atari and navigation benchmarks."
          },
          "strengths": {
            "value": "Originality: VEP's focus on task progress via Bellman returns instead of visual similarity is a novel approach to representation learning. Quality: The experiments on standard benchmarks (Atari, navigation) and comparison to state-of-the-art methods like VIP and SOM show promise. Clarity: The abstract and introduction are well-structured, though some technical details are missing. Significance: Transferable representations are critical for RL, and VEP addresses a key challenge in generalization across tasks."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results (e.g., specific metrics, ablation studies, or analysis of component contributions). The method description is incomplete, with Figure 2 and related work sections cut off. The theoretical justification for using Bellman returns is underdeveloped. The comparison to baselines is superficial, and implementation details (e.g., hyperparameters, network architecture) are missing. The paper also fails to address potential limitations, such as scenarios where tasks have divergent objectives."
          },
          "questions": {
            "value": "1. How are Bellman return estimates computed (e.g., using a pre-trained value function, and if so, how is it trained)? 2. What is the exact formulation of the contrastive loss, and how does it differ from existing methods? 3. How does VEP handle tasks with dissimilar objectives, and what are its failure cases? 4. Are there ablation studies showing the impact of key components (e.g., contrastive loss, Bellman estimates)? 5. What are the computational costs of VEP compared to baselines?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "APWIZgehDT": {
    "paper_id": "APWIZgehDT",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces a framework for studying human perceptual variability through image generation on artificial neural network (ANN) perceptual boundaries. The authors propose a generative model to create images that induce high human perceptual variability, construct the varMNIST dataset, and use subject-specific fine-tuning to align ANN predictions with human behavior. They also demonstrate the ability to manipulate individual decision-making through tailored stimuli."
          },
          "strengths": {
            "value": "Originality is demonstrated through the novel use of ANN perceptual boundaries to generate stimuli that probe human variability, which is underexplored in prior work. The scale of human experiments (246 participants, 116k trials) and the creation of a dedicated dataset (varMNIST) show strong methodological rigor. Clarity is generally good, with clear sectioning and visual summaries. The significance lies in bridging human and machine perception gaps, with potential applications in personalized AI systems."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparison with existing generative models for perceptual boundary sampling, making it hard to assess the novelty of their approach. The subject-specific fine-tuning methodology is not thoroughly explained, including how individual-level alignment is achieved. The validation of 'controversial stimuli' lacks controls (e.g., baseline comparisons with non-tailored stimuli). The paper also omits ablation studies to isolate the impact of key components like the generative model or fine-tuning process."
          },
          "questions": {
            "value": "1. How was the ANN's perceptual boundary defined mathematically, and what metrics were used to ensure sampled images lie on this boundary? 2. What baseline models were used to demonstrate the superiority of the proposed generative approach? 3. How were hyperparameters for subject-specific fine-tuning determined, and what is the generalization ability of the fine-tuned models? 4. Were there control conditions in the manipulation experiments to rule out confounding factors (e.g., stimulus salience)? 5. How does varMNIST differ from existing datasets like MNIST or CIFAR-10 in terms of perceptual variability metrics?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a counterfactual-based approach to study human perceptual variability by generating images along artificial neural network (ANN) perceptual boundaries. The authors create the varMNIST dataset through human experiments, demonstrate subject-specific fine-tuning to align ANN and human perceptual variability, and validate the ability to manipulate individual decision-making behaviors using tailored stimuli."
          },
          "strengths": {
            "value": "The paper presents a novel framework for investigating human perceptual variability by leveraging ANN decision boundaries, which is both innovative and relevant to cognitive science and AI alignment. The creation of the varMNIST dataset addresses a critical gap in capturing high-perceptual variability. The integration of generative models with large-scale human behavioral experiments demonstrates rigorous methodology. The practical implications for personalizing AI systems and understanding human-machine perceptual differences are significant. The work also builds on prior literature while introducing clear, actionable contributions."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving key details about the generative model's efficiency, the exact fine-tuning process, and experimental validation incomplete. The explanation of how ANN perceptual boundaries are defined and sampled lacks technical depth. The claims about 'high variability' in human perception require stronger empirical justification, such as statistical analysis of inter-subject differences. The comparison to existing methods like adversarial examples and model metamers is superficial, and the paper does not fully address potential limitations of the varMNIST dataset (e.g., generalizability beyond MNIST)."
          },
          "questions": {
            "value": [
              "How is the efficiency of the generative model quantified, and what specific techniques are used to ensure generated images appear natural despite sampling from noisy ANN boundaries?",
              "Can the authors provide more details on the subject-specific fine-tuning approach, including hyperparameters, loss functions, and how individual-level alignment is validated?",
              "What metrics are used to quantify 'high perceptual variability' in human participants, and how are these metrics compared to baseline conditions?",
              "How do the authors control for confounding variables (e.g., participant demographics, task instructions) in their behavioral experiments?",
              "What is the theoretical basis for the hypothesis that ANN perceptual boundaries correspond to human classification boundaries, and how is this validated?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces a method to study human perceptual variability by generating images along artificial neural network (ANN) decision boundaries. The authors create the varMNIST dataset through human behavioral experiments, use subject-specific fine-tuning to align ANN and human perceptual variability, and demonstrate the ability to manipulate individual decisions via tailored stimuli. The approach bridges gaps between machine and human perception while addressing variability in cognitive tasks."
          },
          "strengths": {
            "value": "The paper presents a novel framework for investigating human perceptual variability through ANN-generated stimuli, which is both methodologically innovative and theoretically grounded. The construction of varMNIST with 246 participants and 116,715 trials demonstrates significant empirical effort. The alignment of ANN and human perceptual variability via fine-tuning shows practical utility, and the manipulation of individual decisions through 'controversial stimuli' highlights the potential for personalized AI applications. The work addresses a critical gap in understanding human-machine perception differences."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving key details about the generative model's architecture, training process, and evaluation metrics incomplete. The claim that generated images evoke high perceptual variability lacks quantitative validation (e.g., inter-subject variance measures). The alignment of ANN and human variability relies on behavioral data but does not explicitly address how fine-tuning accounts for individual differences. The manipulation experiments lack controls (e.g., baseline comparisons) and fail to quantify the extent of behavioral change. Theoretical justification for the ANN-perceptual boundary hypothesis is underdeveloped."
          },
          "questions": {
            "value": "1. How is the generative model trained to sample along ANN perceptual boundaries? What specific techniques ensure generated images are perceptually meaningful? 2. What metrics are used to quantify perceptual variability in varMNIST, and how do they correlate with human behavioral data? 3. How is subject-specific fine-tuning implemented—via parameter updates, adapter layers, or other methods? 4. What baseline conditions are used to validate the 'controversial stimuli' manipulation experiments? 5. How does the paper address potential confounding factors (e.g., task difficulty, participant demographics) in behavioral experiments?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "AT64R0ivUO": {
    "paper_id": "AT64R0ivUO",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes SteerPrompt, an inference-time method that enhances large language models' (LLMs) reading comprehension by explicitly steering attention scores to highlight key contextual information. Unlike traditional prompting, which implicitly identifies key information, SteerPrompt uses semantic embeddings to map key sentences back to the original context and adjusts attention scores to prioritize them, improving performance on open-book QA tasks."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in LLMs: their struggle to focus on relevant context in long or complex inputs. SteerPrompt's innovation lies in combining iterative prompting with attention steering, offering a novel approach to explicitly guide model attention without modifying parameters. The method is efficient, as demonstrated by a 4.5× reduction in search overhead compared to prior work. Experimental results show significant performance improvements (e.g., 8.99% EM score gain on LLAMA3-70B-Instruct), validating its practical value. The clarity of the problem statement, methodology, and experimental design is strong, with well-structured figures and logical flow."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with alternative attention steering methods, such as gradient-based or rule-based approaches, which limits the understanding of SteerPrompt's relative advantages. The experiments focus on specific models (Vicuna-7B, LLAMA3-8B/70B) and tasks (open-book QA), leaving questions about generalizability to other architectures or domains. The mechanism for attention score manipulation is briefly described, with limited analysis of how specific attention heads contribute to performance. Additionally, the paper does not address potential computational costs or scalability issues when applying SteerPrompt to very large models."
          },
          "questions": {
            "value": "1. How does SteerPrompt handle cases where key sentences are ambiguous or overlapping in the original context? 2. What is the exact implementation of attention score upweighting, and how does it interact with the model's native attention mechanisms? 3. Are there ablation studies demonstrating the individual contributions of key sentence identification, semantic embedding mapping, and attention steering? 4. How does the method perform on non-English datasets or tasks beyond open-book QA, such as summarization or code generation?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces SteerPrompt, an inference-time method for improving large language models' (LLMs) reading comprehension by explicitly steering attention scores to highlight key contextual information. Unlike traditional prompting, SteerPrompt identifies key sentences via free-text generation and manipulates attention scores to prioritize them, achieving significant performance gains on open-book QA tasks without modifying model parameters."
          },
          "strengths": {
            "value": "Originality is demonstrated through the novel use of attention score manipulation for explicit context steering, addressing limitations of implicit textual prompting. The method's inference-only design avoids retraining, making it broadly applicable. Experiments across multiple LLM sizes (Vicuna-7B, LLAMA3-8B, LLAMA3-70B) on standard QA benchmarks (Natural Questions, HotpotQA) show consistent improvements (e.g., 8.99% EM score gain for LLAMA3-70B). The paper also introduces an efficient coarse-to-fine attention head search strategy, reducing computational overhead. The problem formulation and practical implications for reliable QA are well justified."
          },
          "weaknesses": {
            "value": "The paper lacks detailed analysis of how key sentence selection interacts with attention steering mechanisms. The semantic embedding approach for mapping key sentences to contexts is not thoroughly explained, leaving questions about its robustness. Experiments focus on open-book QA but do not evaluate SteerPrompt on other tasks (e.g., summarization, dialogue) to assess generalizability. The comparison with iterative prompting is limited to a single example (Figure 1), and ablation studies on hyperparameters or attention head configurations are missing. The claim of 'outstanding generalization ability' of attention head sets is not empirically validated across diverse tasks."
          },
          "questions": {
            "value": "1. How are key sentences selected during free-text generation—via a separate model component or through the same LLM? 2. What specific semantic embedding technique (e.g., BERT, sentence-T5) is used for mapping key sentences to the original context? 3. How does the coarse-to-fine search strategy for attention heads differ from prior work (Zhang et al., 2024), and what metrics were used to evaluate its efficiency? 4. Are the attention head configurations transferable across different LLM architectures or tasks? 5. What is the computational overhead of SteerPrompt compared to iterative prompting in terms of inference time and memory usage?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces SteerPrompt, an inference-time method that enhances large language models' (LLMs) reading comprehension by automatically identifying key contextual information and explicitly highlighting it through attention score manipulation. Unlike iterative prompting, SteerPrompt avoids appending key sentences to the prompt, instead using semantic embeddings to map identified sentences back to the original context and steering attention via scaled attention scores. Experiments on open-book QA tasks show significant performance improvements over baseline prompting methods."
          },
          "strengths": {
            "value": "The paper presents a novel approach to attention steering without requiring human annotations, addressing a critical limitation of prior work. The coarse-to-fine search for attention heads demonstrates efficiency gains, and the method's inference-only nature avoids parameter modifications. The experiments are comprehensive, covering multiple LLMs and tasks, and the figure provides a clear illustration of the method's mechanics. The focus on reducing error propagation in iterative prompting is a practical contribution."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with alternative attention steering methods (e.g., gradient-based or explicit token masking techniques). The claimed 'excellent generalization ability' of attention heads is not empirically validated across diverse tasks or model sizes. The improvement percentages (e.g., 7.95%) are significant, but the paper does not analyze whether these gains are consistent across different question types or context complexities. Additionally, the method's applicability to non-QA tasks is not discussed, limiting its perceived scope."
          },
          "questions": {
            "value": "1. How does SteerPrompt compare to gradient-based attention visualization methods in terms of effectiveness and efficiency? 2. Are the performance gains consistent across single-hop vs. multi-hop QA tasks, or are they concentrated in specific scenarios? 3. What is the exact mechanism of the coarse-to-fine search, and how does it avoid overfitting to specific attention heads? 4. How does the method handle contexts where key sentences are not explicitly stated but implied?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "AWg2tkbydO": {
    "paper_id": "AWg2tkbydO",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces PEARL, a novel framework for learning efficient positional encodings (PEs) for graphs using message-passing graph neural networks (GNNs). The authors address limitations of existing eigenvector-based PEs by leveraging GNNs to generate PEs with linear or quadratic complexity, while ensuring stability, expressiveness, and scalability. They theoretically analyze PEARL's properties and demonstrate its effectiveness on graph classification and regression tasks."
          },
          "strengths": {
            "value": "The paper presents a novel approach to learn PEs via GNNs, addressing critical gaps in existing methods. The theoretical analysis of PEARL's stability, expressiveness, and scalability is rigorous. The framework's efficiency (linear/quadratic complexity) compared to cubic complexity of full eigenvector methods is a significant contribution. The paper also provides empirical evidence of PEARL's competitiveness with state-of-the-art methods on benchmark datasets."
          },
          "weaknesses": {
            "value": "The experimental evaluation lacks detailed comparisons with key baseline methods (e.g., substructure-based PEs, relative PEs) and specific metrics on standard datasets. The theoretical claims about basis universality and WL test superiority are not empirically validated. The initialization strategy with random samples and pooling functions is not thoroughly explained, and the choice of hyperparameters (e.g., number of samples) remains unclear. The paper does not address how PEARL generalizes to dynamic or heterogeneous graphs."
          },
          "questions": {
            "value": "1. How does PEARL handle graphs with varying sizes or structures? 2. What specific statistical pooling functions are used, and how were they validated? 3. Are there ablation studies demonstrating the contribution of key components (e.g., random initialization vs. basis vectors)? 4. How does PEARL compare to non-eigenvector-based methods like substructure encodings? 5. What is the exact computational complexity of PEARL in practice, and how does it scale with graph size?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces PEARL, a novel framework for learning efficient positional encodings (PEs) for graphs using graph neural networks (GNNs). The method addresses limitations of existing eigenvector-based PEs by leveraging GNNs as nonlinear mappings of eigenvectors, enabling linear complexity while maintaining expressive power, stability, and permutation equivariance through random initialization and statistical pooling."
          },
          "strengths": {
            "value": "The paper makes a clear contribution by identifying four key criteria for effective graph PEs (stability, expressive power, scalability, genericness) and addressing them through a novel GNN-based approach. The theoretical analysis demonstrates PEARL's basis universality, stability, and expressive power beyond the Weisfeiler-Leman test. The experimental results show strong performance compared to both structure-based and eigenvector-based PEs, with significant computational efficiency gains. The writing is structured and well-organized, with clear motivation and technical depth."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to validate the necessity of key components like random initialization and statistical pooling. The claim about sample complexity being independent of graph size is not thoroughly validated with empirical evidence. The comparison to full eigenvector methods focuses on computational complexity but does not analyze trade-offs in downstream task performance. The theoretical guarantees are mentioned but not fully elaborated, leaving some aspects of the analysis opaque."
          },
          "questions": {
            "value": "1. How were the random initializations for node features selected, and what ablation studies were performed to verify their effectiveness? 2. What is the exact mechanism of the statistical pooling functions, and how do they ensure permutation equivariance in practice? 3. Are there specific graph properties (e.g., size, density) where PEARL's performance degrades compared to existing methods? 4. How does the alternative basis vector initialization approach compare to full eigenvector methods in terms of expressive power versus computational cost?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces PEARL, a novel framework for learning efficient positional encodings (PEs) for graphs using graph neural networks (GNNs). PEARL addresses the limitations of existing eigenvector-based PEs by leveraging message-passing GNNs to generate PEs with linear or quadratic complexity, while ensuring stability, expressiveness, and permutation equivariance through random initialization and statistical pooling."
          },
          "strengths": {
            "value": "The paper presents a creative and theoretically grounded approach to positional encoding for graphs, addressing critical challenges in stability, scalability, and expressiveness. The framework's novelty lies in its use of GNNs as nonlinear mappings of eigenvectors, enabling efficient PE generation. Theoretical analysis demonstrates PEARL's basis universality, stability, and ability to generalize substructure counting. Experimental results show strong performance against baselines, particularly in scalability. The paper also provides clear connections between GNN message-passing and spectral graph theory, advancing the understanding of structural representation learning."
          },
          "weaknesses": {
            "value": "The paper lacks comparisons with recent GNN-based PE methods beyond eigenvector and substructure approaches, which could limit the perceived novelty. The experimental evaluation focuses on molecular and social network datasets, leaving the generalizability to other graph types (e.g., heterogeneous or temporal graphs) unexplored. The theoretical analysis assumes specific GNN architectures, and the role of hyperparameters (e.g., number of random samples) is not thoroughly ablated. Additionally, the paper does not address potential limitations in handling very sparse graphs or the sensitivity of statistical pooling to sample variance."
          },
          "questions": {
            "value": [
              "How does PEARL perform on graphs with varying levels of sparsity or irregular structures compared to dense graphs?",
              "What is the impact of the number of random samples (M) on PE quality and computational efficiency, and how is M chosen in practice?",
              "Are there scenarios where the basis vector initialization outperforms random initialization, and how does this depend on graph properties?",
              "How does PEARL compare to other GNN-based PE methods (e.g., those using attention or graph contrastive learning) not explicitly mentioned in the paper?",
              "Can the statistical pooling mechanism be theoretically justified for preserving permutation equivariance under different GNN architectures?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "AZR4R3lw7y": {
    "paper_id": "AZR4R3lw7y",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces BoostCL, a method for continual learning (CL) that leverages random projection (RP) to enhance feature separability and generalization. It proposes a multi-view RP scheme with AdaBoost-inspired diversity and a self-improvement process for prompt selection, demonstrating superior performance over existing baselines."
          },
          "strengths": {
            "value": "The paper presents a theoretically grounded analysis of RP's role in CL, which addresses a gap in prior work. The multi-view approach with AdaBoost principles is novel and promising. The self-improvement process for prompt selection adds practical value. The experiments show consistent improvements across datasets, though details are limited."
          },
          "weaknesses": {
            "value": "The theoretical analysis lacks rigorous proofs, particularly the claim about margin increases by O(√d'). The self-improvement process is under-described, making it hard to assess its effectiveness. Experimental details (e.g., baselines, ablation studies) are insufficient, and the paper is cut off mid-section, limiting evaluation of methodology and results."
          },
          "questions": {
            "value": "1. How was the theoretical margin analysis derived? Are there missing proofs or assumptions? 2. What specific baselines were compared, and how do they relate to prior work? 3. Can the authors clarify the self-improvement process's implementation and validation? 4. Are there ablation studies to isolate the contributions of RP, multi-view, and self-improvement? 5. How does the computational cost scale with K and d'?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes BoostCL, a method for continual learning (CL) that combines multi-view random projection (RP) with an ensemble classifier inspired by AdaBoost. The approach leverages RP to increase feature separability, employs task-specific prompts for representation learning, and introduces a self-improvement process to mitigate feature shifts. The method is evaluated on multiple datasets, showing superior performance over existing baselines."
          },
          "strengths": {
            "value": "The paper introduces a novel multi-view RP framework that addresses high-dimensional challenges in CL, combining theoretical insights with practical ensemble learning. The theoretical analysis of RP's margin increase and its implications for generalization is a significant contribution. The integration of AdaBoost with CL, along with task-specific prompts and a self-improvement process, demonstrates creative problem-solving. The experimental results are promising, showing consistent improvements over state-of-the-art methods."
          },
          "weaknesses": {
            "value": "The theoretical analysis of RP's margin increase lacks detailed mathematical rigor, particularly in proving the $O(\\sqrt{d'})$ rate. The paper is cut off mid-section, leaving critical details about the self-improvement process and multi-view implementation incomplete. The comparison with baselines is limited to specific variants (e.g., BoostCL-$m(d'=10,000;K=15)$), and the ablation studies on hyperparameters (e.g., $K$, $d'$) are not provided. The scalability of the multi-view approach to more tasks or larger datasets is not discussed."
          },
          "questions": {
            "value": "1. Can the authors provide the missing theoretical proofs for the margin increase analysis? 2. How exactly does the self-improvement process select prompts, and what metrics are used to evaluate its effectiveness? 3. Why were specific values of $d'$ and $K$ chosen (e.g., $d'=10,000$, $K=15$), and how do they affect performance? 4. Are there any limitations to the multi-view approach when applied to tasks with drastically different data distributions? 5. How does the method handle computational costs with increasing $K$ or $d'$?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces BoostCL, a method for continual learning (CL) that leverages multiple views of random projections (RP) to enhance model performance. It theoretically analyzes RP's role in increasing feature separability and generalization, proposes a multi-view ensemble classifier using AdaBoost principles, and includes a self-improvement process for prompt selection. Experiments show improvements over state-of-the-art baselines."
          },
          "strengths": {
            "value": "The theoretical analysis of RP's impact on feature margins and separability is novel and well-motivated. The multi-view ensemble approach with AdaBoost principles addresses a gap in applying boosting to CL. The self-improvement process for prompt selection offers a practical solution to mitigate feature shifts. The paper demonstrates consistent empirical gains across datasets, highlighting the practical value of their approach."
          },
          "weaknesses": {
            "value": "The theoretical analysis lacks detailed proofs or derivations, such as the O(√d') margin increase claim. The self-improvement process is described as 'simple yet effective' but lacks implementation specifics. The experiments omit comparisons with key baselines (e.g., other multi-view methods) and ablation studies on hyperparameters (K, d'). The paper also does not address scalability to larger task sequences or datasets."
          },
          "questions": {
            "value": [
              "How is the theoretical analysis of the margin increase (O(√d')) derived? Are there missing proofs or assumptions?",
              "What are the exact mechanics of the self-improvement process for prompt selection? How is it integrated into the training pipeline?",
              "Are there ablation studies on the number of views (K) and projection dimensions (d')? How sensitive is the method to these hyperparameters?",
              "How does BoostCL compare to other multi-view or ensemble methods in CL (e.g., MC-DNN, VIM)?",
              "What are the computational costs of the multi-view approach compared to existing methods?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "AZVvTBxTdZ": {
    "paper_id": "AZVvTBxTdZ",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces NARes, a large-scale neural architecture dataset comprising 15,625 WRN-style architectures adversarially trained and evaluated against multiple attacks. The dataset includes detailed metrics like stable accuracy and empirical Lipschitz constants, aiming to enable rapid exploration of adversarial robustness (AR) and provide insights into architectural design for AR."
          },
          "strengths": {
            "value": "Originality: The dataset addresses a critical gap by focusing on macro search spaces for AR, unlike prior micro-based datasets. Quality: The scale (15,625 architectures) and inclusion of diverse metrics (e.g., AutoAttack, Lipschitz constants) demonstrate rigorous experimental design. Clarity: The paper is well-structured, with clear explanations of the dataset's design and key findings. Significance: The dataset has the potential to catalyze AR research by enabling empirical validation of architectural hypotheses and bridging theoretical and practical studies."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology on how the 15,625 architectures were selected (e.g., uniform sampling vs. heuristic criteria). The analysis of insights (e.g., 'increasing MACs is preferred over parameters') is superficial and requires deeper statistical validation. The comparison with existing datasets (Jung et al. 2023, Wu et al. 2024) is limited, making it hard to assess NARes' relative advantages. The reproducibility claims (e.g., 'open-source code') are not substantiated with concrete details."
          },
          "questions": {
            "value": [
              "How were the 15,625 WRN architectures sampled? Were they uniformly distributed across depth/width parameters, or were specific criteria used to prioritize certain configurations?",
              "What is the exact definition of 'stable accuracy' and how was it computed? Is it a novel metric, and how does it correlate with standard AR benchmarks?",
              "The paper claims prior architectural principles (e.g., Huang et al. 2023) are unreliable. What statistical evidence supports this assertion, and how were these principles tested against NARes data?",
              "How were the 44 GPU years of training resources allocated? Were there systematic ablation studies to ensure the dataset's robustness to hyperparameter choices?",
              "What specific challenges might researchers face when using NARes for NAS? Are there any limitations in the dataset's design (e.g., fixed pre-activation blocks) that could bias downstream experiments?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces NARes, a large-scale neural architecture dataset comprising 15,625 WRN-style architectures adversarially trained and evaluated against multiple attacks, including AutoAttack. It addresses limitations of prior datasets by focusing on macro search spaces, providing detailed metrics, and offering insights into how architectural choices affect adversarial robustness."
          },
          "strengths": {
            "value": "Originality is strong, as NARes is the first comprehensive macro search space dataset for adversarial robustness. The quality of the dataset is high, with extensive evaluations and rich metrics (e.g., stable accuracy, empirical Lipschitz constants). Clarity is maintained through structured explanations and a helpful figure. The significance lies in lowering computational barriers for robust architecture research and enabling empirical validation of theoretical claims."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with prior datasets in experiments, making it hard to quantify improvements. The claimed contradictions with prior studies are not thoroughly validated or contextualized. The computational cost of building NARes (44 GPU years) may limit reproducibility, despite the dataset's intent to lower barriers. The analysis of architectural insights (e.g., depth vs. width trade-offs) is descriptive rather than causal, requiring further investigation."
          },
          "questions": {
            "value": [
              "How do the contradictions with prior studies (e.g., 'reducing last stage capacity improves AR') specifically challenge existing principles? Provide concrete examples from the literature.",
              "What criteria were used to select the 15,625 architectures? Are there biases in the design space that could affect generalizability?",
              "How does the empirical Lipschitz constant correlate with adversarial robustness in NARes? Are there statistical validations of this relationship?",
              "What are the limitations of using AutoAttack as the primary evaluation metric, and how do other attacks (e.g., PGD) compare in capturing robustness?",
              "How do the four checkpoints per architecture enable meaningful fine-tuning or analysis? Are there examples of such use cases?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces NARes, a large-scale neural architecture dataset focused on adversarial robustness, comprising 15,625 WRN-style architectures adversarially trained and evaluated against multiple attacks. The dataset includes detailed metrics like stable accuracy and empirical Lipschitz constants, aiming to enable systematic exploration of architecture-robustness relationships and address limitations in prior datasets."
          },
          "strengths": {
            "value": "Originality: NARes is the first comprehensive dataset for adversarial robustness research in macro search spaces (WRN-style), addressing gaps in existing micro-based datasets. Quality: The scale (15,625 architectures) and inclusion of diverse metrics (e.g., AutoAttack, Lipschitz constants) are substantial. Clarity: The paper clearly outlines the motivation, design, and key findings. Significance: The dataset fills a critical need for reproducible research in adversarial robustness and provides actionable insights for architecture design."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology on how the 15,625 architectures were sampled or optimized. Experimental validation of the claimed insights (e.g., 'increasing MACs budget is preferred for AR') is limited. The comparison with existing datasets (e.g., Jung et al. 2023, Wu et al. 2024) is superficial, and the computational cost (44 GPU years) is not contextualized against prior work. The paper also does not address potential biases in the design space or the generalizability of findings to other architectures."
          },
          "questions": {
            "value": [
              "How were the 15,625 WRN-style architectures selected? Were they uniformly sampled, or based on specific criteria (e.g., depth/width ranges, performance thresholds)?",
              "What is the exact definition of 'stable accuracy' and 'empirical Lipschitz constant' used in NARes? How were these metrics computed during adversarial training?",
              "The paper claims prior robustness principles (e.g., Huang et al. 2023) are unreliable. What specific contradictions were observed, and how were they validated statistically?",
              "How does NARes compare in terms of computational efficiency to existing datasets? Could the 44 GPU years be reduced with alternative training strategies?",
              "Are the findings generalizable beyond WRN-style architectures, or are they specific to the design space explored in NARes?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "AecVG5CXdp": {
    "paper_id": "AecVG5CXdp",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes a novel reinforcement learning (RL) approach for optimizing Elevator Group Control Systems (EGCS). The authors introduce an MDP formulation with innovations such as 'infra-steps' to handle continuous passenger arrivals, a tailored reward signal, and a novel action space encoding. They evaluate their method on a custom simulation of a 6-elevator, 15-floor system at VU Amsterdam, claiming improvements over traditional rule-based systems."
          },
          "strengths": {
            "value": "Originality: The paper introduces novel concepts like 'infra-steps' and a tailored action space encoding for elevator dispatching, addressing combinatorial complexity. Quality: The custom simulation environment is grounded in real-world data from VU Amsterdam, enhancing practical relevance. Clarity: The problem statement and methodology are clearly articulated, with a structured presentation of contributions. Significance: Efficient EGCS optimization has direct real-world impact on energy consumption and passenger experience, making the work relevant to both academia and industry."
          },
          "weaknesses": {
            "value": "The paper is truncated, omitting critical experimental details (e.g., results, baseline comparisons, ablation studies). The claimed improvements over rule-based systems lack quantitative validation. The action space encoding and 'infra-step' formulation are described conceptually but not technically detailed, making it difficult to assess their novelty or effectiveness. The reward signal design is mentioned but not analyzed for its impact on learning efficiency."
          },
          "questions": {
            "value": "1. What specific technical details define the 'infra-step' formulation and how does it integrate with the MDP? 2. How was the action space encoding implemented, and what metrics were used to quantify its reduction of combinatorial complexity? 3. What were the exact performance metrics (e.g., passenger travel time, energy use) and how do they compare to the rule-based system? 4. Were there ablation studies to isolate the impact of individual innovations (e.g., reward signal, discounting strategy)? 5. How was the simulation validated against real-world data beyond the initial passenger arrival reconstruction?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces a novel reinforcement learning (RL) approach for optimizing elevator group control systems (EGCS). The authors propose an end-to-end RL framework with innovations including a custom simulation environment, a novel action space encoding, 'infra-steps' to model continuous passenger arrivals, and a tailored discounting strategy. The method is evaluated on a 6-elevator, 15-floor system at VU Amsterdam, demonstrating improvements over traditional rule-based approaches."
          },
          "strengths": {
            "value": "The paper makes methodological contributions by formalizing elevator routing as an MDP and introducing 'infra-steps' to handle continuous passenger dynamics. The custom simulation environment is well-motivated and grounded in real-world data. The action space encoding addresses combinatorial complexity, and the tailored reward signal improves learning efficiency. The work bridges RL methodology with practical EGCS challenges, showing promise for real-world applications."
          },
          "weaknesses": {
            "value": "The paper is truncated, so key details about experimental results, baseline comparisons, and ablation studies are missing. The claim of outperforming rule-based systems lacks quantitative metrics (e.g., wait times, energy consumption). The action space encoding's effectiveness is not thoroughly analyzed, and the simulation's validation against real-world data is unclear. The relationship to prior RL work (e.g., Wan et al. 2024) is not fully discussed."
          },
          "questions": {
            "value": "1. What specific metrics (e.g., average wait time, energy use) were used to evaluate performance? 2. How does the action space encoding reduce combinatorial complexity compared to prior work? 3. What is the exact implementation of 'infra-steps' and how do they differ from existing time-discretization methods? 4. How was the simulation validated against real-world elevator data beyond the 2023 VU Amsterdam dataset? 5. Are there limitations to the simulation's realism (e.g., passenger behavior assumptions)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper proposes a novel reinforcement learning (RL) approach for optimizing Elevator Group Control Systems (EGCS). The authors introduce innovations such as a custom simulation environment, a new action space encoding, 'infra-steps' to model continuous passenger arrivals, and a tailored reward signal. They demonstrate that their RL-based EGCS outperforms traditional rule-based systems in terms of passenger travel time."
          },
          "strengths": {
            "value": "Originality is evident in the 'infra-steps' formulation and action space encoding, which address combinatorial complexity in elevator dispatching. The practical contribution of a realistic simulation environment based on real-world data from VU Amsterdam is significant. The paper provides a structured MDP formulation and explores tailored discounting strategies. The clarity of problem description and methodological breakdown is strong, with clear connections to prior work. The significance lies in advancing RL applications for real-world control systems with stochastic dynamics."
          },
          "weaknesses": {
            "value": "The experimental validation is incomplete, with limited comparison to state-of-the-art RL methods beyond a rule-based baseline. The simulation environment's generalizability is unclear, as it is tailored to a specific 6-elevator, 15-floor setup. The action space encoding's effectiveness is not thoroughly analyzed, and the paper lacks ablation studies to isolate the impact of key innovations. The 'infra-steps' concept requires deeper theoretical justification. The reward signal design is described but not empirically evaluated for its impact on learning efficiency."
          },
          "questions": {
            "value": [
              "How does the proposed action space encoding specifically reduce combinatorial complexity compared to existing approaches?",
              "What baseline RL methods were excluded from comparison, and why are they irrelevant to this problem domain?",
              "Can the 'infra-step' formulation be generalized to other continuous-time control problems beyond elevator systems?",
              "What metrics were used to quantitatively evaluate the reward signal's impact on learning efficiency?",
              "How was the simulation environment validated against real-world performance metrics beyond passenger travel time?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "AfZH9EEuRR": {
    "paper_id": "AfZH9EEuRR",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces EgoQR, a system for efficient QR code reading in egocentric settings, specifically tailored for wearable devices. The approach combines a lightweight detection model with an enhanced decoding algorithm, addressing challenges like wide field-of-view, motion blur, and resource constraints. The authors report a 34% improvement in decoding success compared to existing methods on a custom dataset."
          },
          "strengths": {
            "value": "Originality is evident in targeting egocentric QR code reading, a niche domain with unique challenges. The methodology combines traditional computer vision techniques with deep learning, showcasing a creative approach to address specific constraints of wearable devices. The paper demonstrates significant performance improvements (34% better than state-of-the-art) on a custom dataset, highlighting its practical relevance. The structure is clear, with a logical flow from problem motivation to technical contributions and evaluation."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical specifics about the detection and decoding components, making it difficult to assess their novelty or efficiency. The dataset description is minimal, leaving questions about its size, diversity, and how it reflects real-world egocentric scenarios. The 34% improvement claim lacks context—e.g., baseline methods, metrics, and statistical significance. The paper also does not address how EgoQR handles extreme lighting conditions or motion blur beyond vague mentions."
          },
          "questions": {
            "value": "1. What specific techniques are used in the detection and decoding components? Are they based on existing architectures (e.g., CNNs, traditional vision) or novel designs? 2. How was the custom dataset collected? What are its key characteristics (e.g., size, lighting conditions, motion blur levels)? 3. How does EgoQR balance computational efficiency with accuracy on resource-constrained devices? 4. Are there ablation studies to validate the contribution of individual components (e.g., detection vs. decoding)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces EgoQR, a system for efficient QR code reading in egocentric (wearable) settings. It addresses challenges like wide field-of-view, motion blur, and resource constraints by combining a lightweight detection model with an enhanced decoding algorithm, claiming a 34% improvement over existing methods on a custom dataset."
          },
          "strengths": {
            "value": "The paper highlights a relevant and timely problem: adapting QR code reading to wearable devices. It identifies specific challenges unique to egocentric imagery and proposes a system tailored for resource-constrained edge devices. The 34% improvement over baselines is a concrete result, and the focus on efficiency aligns with practical deployment needs. The work also acknowledges prior methods and situates itself within the broader context of QR code detection research."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about the dataset, experimental setup, and technical contributions incomplete. The 34% improvement claim lacks supporting evidence (e.g., ablation studies, comparison with multiple baselines, or analysis of failure cases). The method's efficiency (power/latency) is not quantified, and the integration with ZXing is not clearly explained. The evaluation on a custom dataset raises questions about its diversity, size, and whether it addresses real-world egocentric challenges like extreme angles or dynamic motion blur."
          },
          "questions": {
            "value": "1. Is the custom dataset publicly available, and how was it collected (e.g., diversity of lighting, motion, and perspectives)? 2. What specific modifications were made to ZXing for decoding, and how do they address egocentric challenges? 3. How is the system's power consumption and latency measured, and how does it compare to existing solutions? 4. Are there ablation studies showing the contribution of each component (detection vs. decoding)? 5. How does EgoQR handle extreme motion blur or highly distorted QR codes not covered in the dataset?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 2
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces EgoQR, a system for efficient QR code reading in egocentric (wearable device) settings. It addresses challenges like wide field-of-view, motion blur, and resource constraints by proposing a lightweight detection model and enhanced decoding algorithm, claiming a 34% improvement over existing methods on a custom dataset."
          },
          "strengths": {
            "value": "The paper identifies a relevant and practical problem (QR code reading on wearables) with clear motivation. The approach addresses specific challenges of egocentric imagery, such as distortion and motion blur, which are well-articulated. The focus on lightweight deployment aligns with wearable device constraints, showing practical relevance. The paper also references prior work comprehensively, establishing context for the problem."
          },
          "weaknesses": {
            "value": "The paper is truncated, leaving critical details about the methodology, dataset, and experimental setup incomplete. The claimed 34% improvement lacks context (e.g., baseline comparisons, statistical significance, or ablation studies). The use of ZXing as a baseline is not thoroughly justified, and the paper does not address how it handles varying lighting conditions or environmental variability. The resource efficiency claims (e.g., power consumption, latency) are not quantified. The novelty of the approach is unclear, as it appears to combine existing techniques without explicit innovation."
          },
          "questions": {
            "value": "1. What specific techniques are used to handle motion blur and distortion in egocentric images? 2. How was the custom dataset collected, and what are its statistics (e.g., number of samples, diversity of scenarios)? 3. What metrics were used to evaluate decoding success (e.g., precision, recall, F1-score)? 4. How does the lightweight detection model compare to existing lightweight architectures (e.g., MobileNet, YOLOv5) in terms of speed and accuracy? 5. What are the exact resource constraints (e.g., FLOPs, memory usage) of EgoQR, and how does it compare to baselines?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "Ah3n8U3kRT": {
    "paper_id": "Ah3n8U3kRT",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces median clipping techniques for zeroth-order convex optimization and multi-armed bandit (MAB) problems under heavy-tailed symmetric noise. The authors propose novel algorithms (ZO-clipped-med-SSTM, ZO-clipped-med-SMD, Clipped-INF-med-SMD) that achieve improved convergence rates compared to prior work, particularly for noise with κ ≤ 1. They theoretically analyze the methods and validate their performance on synthetic and real-world data."
          },
          "strengths": {
            "value": "Originality: The paper innovates by applying median estimators to zeroth-order optimization and MAB, addressing a gap in handling symmetric heavy-tailed noise. Quality: Theoretical analysis rigorously establishes convergence rates matching optimal bounds for bounded variance cases. Clarity: The paper is well-structured with clear problem formulations, algorithm descriptions, and comparative experiments. Significance: The work advances practical optimization under realistic noise conditions, with applications to MAB and zeroth-order settings where gradient information is unavailable."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with recent SOTA methods beyond the cited works (e.g., [19, 20]). The assumption of symmetric noise may limit applicability to real-world scenarios where asymmetry is common. Theoretical guarantees rely on bounded κ-th moments for κ > 0, but the interplay between symmetry and heavy-tailedness is not thoroughly analyzed. Experimental validation could include ablation studies on hyperparameter sensitivity or scalability to higher dimensions."
          },
          "questions": {
            "value": [
              "How does the median clipping technique handle non-symmetric heavy-tailed noise, which is common in practice?",
              "What is the computational complexity of the median estimator compared to existing clipping methods, and how does it scale with dimensionality?",
              "Are there theoretical guarantees for the case when the noise distribution is not exactly symmetric but only approximately symmetric?",
              "How sensitive are the experimental results to the choice of clipping parameters (e.g., λ, batch size b)?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces median clipping techniques for zeroth-order convex optimization and multi-armed bandit (MAB) problems under symmetric heavy-tailed noise. It proposes novel algorithms (ZO-clipped-med-SSTM, ZO-clipped-med-SMD, Clipped-INF-med-SMD) that achieve improved convergence rates compared to existing methods, particularly for cases with κ ≤ 1. The theoretical analysis claims to handle unbounded moments and symmetric noise, with experiments showing competitive performance on synthetic and real-world data."
          },
          "strengths": {
            "value": "Originality: The paper addresses a critical gap in zeroth-order optimization under heavy-tailed noise by combining median estimation with clipping, which is novel. Theoretical contributions include convergence rates that do not degenerate for κ approaching 1. Quality: The analysis is rigorous, with detailed bounds for convex and strongly convex cases. Clarity: The paper is well-structured, with clear notation and tables comparing results. Significance: The methods improve robustness in practical scenarios with heavy-tailed noise, such as finance or blockchain, and match optimal rates for bounded variance cases."
          },
          "weaknesses": {
            "value": "The paper lacks detailed justification for the median-based gradient estimation under zeroth-order oracles, which is critical for understanding its effectiveness. The experiments on real-world data are insufficiently described, making it hard to assess practical relevance. The theoretical guarantees assume symmetric noise, but the paper does not discuss robustness to mild asymmetry. Additionally, the claims about handling 'any κ > 0' need stronger justification, as the analysis seems to rely on bounded moments for the median estimator. The comparison with SOTA methods is limited, with no discussion of computational overhead or scalability."
          },
          "questions": {
            "value": "1. How is the median gradient estimate computed in the zeroth-order setting? What is the exact procedure for sampling and clipping gradient differences? 2. The paper assumes symmetric noise, but real-world heavy-tailed distributions may be asymmetric. How robust are the methods to such deviations? 3. Are there theoretical guarantees for the median estimator when the noise has unbounded variance, or does the analysis implicitly assume some moment conditions? 4. How does the batch size affect the convergence rates in practice, and what is the optimal choice for different κ values? 5. The experiments mention 'real-world data' but do not specify the datasets or tasks. Can the authors provide more details on these experiments?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces median clipping techniques for zeroth-order convex optimization and multi-armed bandit problems under symmetric heavy-tailed noise. It proposes novel algorithms (ZO-clipped-med-SSTM, ZO-clipped-med-SMD, Clipped-INF-med-SMD) that achieve improved convergence rates compared to existing methods, particularly for cases with unbounded noise moments. The theoretical analysis demonstrates high-probability convergence guarantees and optimal regret bounds for MAB."
          },
          "strengths": {
            "value": "Originality is strong through the application of median estimators to zeroth-order optimization, a novel approach in this domain. Theoretical analysis is rigorous, with clear convergence rate comparisons and explicit handling of symmetric heavy-tailed noise. Experiments on synthetic and real-world data validate practical effectiveness, especially for κ ≤ 1. The paper provides detailed algorithmic frameworks and contextualizes contributions relative to prior work on gradient clipping and robust estimation."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with state-of-the-art zeroth-order methods for heavy-tailed settings, making it hard to assess relative performance. The symmetric noise assumption may limit applicability to real-world scenarios where asymmetry is common. The convergence rates in Table 1 for convex cases appear suboptimal compared to existing results, requiring clarification. The analysis of the median gradient estimator's robustness to non-symmetric noise is underdeveloped. Experimental results do not include ablation studies on hyperparameters or sensitivity analysis."
          },
          "questions": {
            "value": "How does the median clipping technique compare to alternative robust estimation methods (e.g., trimmed mean, Huber loss) in zeroth-order settings? What are the practical limitations of the symmetric noise assumption in real-world applications? Can the theoretical guarantees be extended to non-convex problems or non-symmetric noise distributions? The paper claims 'dramatic improvements' for κ ≤ 1 but does not provide quantitative comparisons with existing methods in this regime. Clarification is needed on how the batch size scaling (b/κ) affects convergence in practice."
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "AjXkRZIvjB": {
    "paper_id": "AjXkRZIvjB",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces GSM-Symbolic, a new benchmark for evaluating mathematical reasoning in large language models (LLMs) using symbolic templates to generate controllable and diverse questions. It demonstrates that LLMs exhibit performance degradation when numerical values or irrelevant clauses are introduced, suggesting reliance on pattern matching rather than formal reasoning."
          },
          "strengths": {
            "value": "The paper's originality lies in creating a controllable benchmark (GSM-Symbolic) to systematically study LLM limitations, addressing gaps in static benchmarks like GSM8K. The methodology is rigorous, with experiments showing consistent performance drops under varying conditions. The clarity of the problem statement, contributions, and figures is strong. The significance is high, as it highlights critical flaws in LLM reasoning capabilities, prompting better evaluation practices."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology on how symbolic templates are generated (manual vs. automated). The GSM-NoOp dataset's construction is under-specified (e.g., types of irrelevant clauses added). Comparisons to GSM8K are superficial, and statistical significance of performance drops (e.g., 65% decline) is not thoroughly justified. The paper also does not address how GSM-Symbolic's diversity compares to existing benchmarks."
          },
          "questions": {
            "value": "1. How were the symbolic templates for GSM-Symbolic created? Were they manually designed or generated algorithmically? 2. What criteria define 'irrelevant clauses' in GSM-NoOp, and how were they incorporated into questions? 3. How does GSM-Symbolic's difficulty or diversity compare to GSM8K? 4. Are the performance drops statistically significant across all 25 models tested? 5. How were the 25 state-of-the-art models selected, and what baseline metrics exist for GSM-Symbolic?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces GSM-Symbolic, a new benchmark for evaluating mathematical reasoning in large language models (LLMs) by generating diverse question variants through symbolic templates. It highlights LLMs' limitations, such as performance drops when numerical values or irrelevant clauses are altered, and questions the reliability of existing benchmarks like GSM8K."
          },
          "strengths": {
            "value": "The paper's originality lies in creating a controllable benchmark (GSM-Symbolic) to study LLM reasoning, addressing gaps in existing evaluations. The methodology is comprehensive, involving 25 state-of-the-art models and analyzing performance variations. The significance is high, as it challenges assumptions about LLMs' logical reasoning capabilities and underscores the need for robust evaluation frameworks. The problem statement is clearly articulated, and the paper's structure is logical."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results to substantiate claims (e.g., 65% performance drop without specific model comparisons or statistical analysis). The comparison between GSM-Symbolic and GSM8K is superficial, and the analysis of why models fail (e.g., pattern-matching vs. formal reasoning) is speculative. The GSM-NoOp dataset is not thoroughly described, and the paper does not address potential confounding factors in its experiments. Theoretical claims are not empirically validated."
          },
          "questions": {
            "value": "1. How were the symbolic templates for GSM-Symbolic generated, and what ensures their diversity? 2. What specific models and baselines were tested, and how do they compare to prior work? 3. How was the 65% performance drop calculated, and what is its statistical significance? 4. How does GSM-Symbolic address data contamination risks in GSM8K? 5. What controls were in place to isolate the effect of added clauses in GSM-NoOp?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "{\n  \"summary\": \"The paper introduces GSM-Symbolic, a new benchmark for evaluating mathematical reasoning in large language models (LLMs) using symbolic templates to generate diverse question variants. It demonstrates that LLMs exhibit significant performance variability when numerical values or clauses in questions change, suggesting reliance on pattern matching rather than formal logical reasoning. The authors also present GSM-NoOp, a dataset with irrelevant information that causes substantial performance drops, highlighting LLMs' inability to discern relevant information.\",\n  \"strengths\": \"The paper's originality lies in creating a controllable benchmark (GSM-Symbolic) to systematically study LLMs' mathematical reasoning, addressing limitations of static datasets like GSM8K. The experimental design is robust, showing clear trends in performance degradation under controlled variables. The hypothesis about pattern-matching vs. formal reasoning is well-motivated and connects to foundational AI debates (e.g., Chinese Room Argument). The work's significance is high, as it challenges the reliability of current evaluation metrics and underscores the need for better benchmarks.\",\n  \"weaknesses\": \"The paper lacks detailed comparisons with existing benchmarks beyond GSM8K, making it unclear how GSM-Symbolic improves upon them. The GSM-NoOp experiments are described but not thoroughly analyzed—e.g., how many irrelevant clauses were added, or how they were selected. The claim that LLMs \"cannot perform formal reasoning\" is strong but underpinned by indirect evidence (e.g., performance drops). The paper also omits ablation studies on the symbolic template design and does not address potential confounding factors in the experiments.\",\n  \"questions\": \"1. How do the symbolic templates in GSM-Symbolic ensure diversity while maintaining mathematical validity? 2. What specific metrics (e.g., accuracy, F1 score) were used to quantify performance drops in GSM-NoOp? 3. Are the results reproducible with open-source code and data? 4. How were the irrelevant clauses in GSM-NoOp selected—randomly or based on domain knowledge? 5. What is the relationship between the number of clauses and performance degradation (e.g., linear vs. exponential)?\",\n  \"rating\": 3,\n  \"confidence\": 3,\n  \"soundness\": 3,\n  \"presentation\": 3,\n  \"contribution\": 3\n}"
          },
          "strengths": {
            "value": "Failed to parse"
          },
          "weaknesses": {
            "value": "Failed to parse"
          },
          "questions": {
            "value": "Failed to parse"
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "Aly68Y5Es0": {
    "paper_id": "Aly68Y5Es0",
    "reviews": [
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces L-RHO, a learning-guided rolling horizon optimization (RHO) framework for long-horizon combinatorial optimization problems (COPs), with a focus on the Flexible Job-Shop Scheduling Problem (FJSP). L-RHO uses a neural network to identify variables that do not require re-optimization across consecutive RHO subproblems, reducing computational overhead and improving solution quality. The approach is validated on FJSP with significant speedups and quality improvements compared to baselines."
          },
          "strengths": {
            "value": "The paper presents a novel framework (L-RHO) that bridges machine learning and temporal decomposition for COPs, addressing a critical gap in accelerating RHO. The application to FJSP is well-motivated, with empirical results showing up to 54% faster solve times and 21% better solution quality. The theoretical analysis provides insights into conditions where learning improves RHO, and the experiments cover diverse FJSP scenarios, including online settings. The work’s emphasis on reducing redundant computations in RHO is both practical and innovative."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of the neural network’s design on performance. The theoretical analysis is high-level and does not rigorously quantify the conditions under which L-RHO outperforms baselines. The comparison with existing learning-based decomposition methods (e.g., those using neural solvers for subproblems) is insufficient. Additionally, the paper does not provide explicit details on the neural network architecture or training process, which limits reproducibility."
          },
          "questions": {
            "value": [
              "How does the neural network architecture and training process specifically enable the identification of non-re-optimizable variables? What features or embeddings are used?",
              "Are there ablation studies demonstrating the contribution of the learning component versus heuristic rules for variable fixation?",
              "How does L-RHO handle dynamic changes in online FJSP scenarios? What is the exact mechanism for adapting to new orders?",
              "What are the limitations of the theoretical analysis? Are the FP/FN rate assumptions valid across different FJSP distributions?",
              "How does L-RHO compare to recent learning-based decomposition methods (e.g., those using reinforcement learning or graph neural networks) on FJSP?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces L-RHO, a learning-guided rolling horizon optimization framework for long-horizon combinatorial optimization problems (COPs), with a focus on the Flexible Job-Shop Scheduling Problem (FJSP). L-RHO uses a neural network to identify variables that do not require re-optimization across consecutive RHO iterations, reducing subproblem size and improving efficiency and solution quality. The method achieves up to 54% faster solve times and 21% better solution quality compared to baselines, with theoretical analysis and extensive experiments across diverse FJSP scenarios."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in temporal decomposition for long-horizon COPs by introducing L-RHO, the first learning-guided RHO framework. It provides comprehensive empirical validation across multiple FJSP settings, including online scenarios, and includes a theoretical analysis of conditions under which learning improves RHO. The work is well-motivated, with clear contributions to both algorithmic design and practical applications in scheduling."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparison with state-of-the-art FJSP solvers, particularly those using neural methods. The neural network architecture and training process are under-described, raising concerns about reproducibility. The generalizability of L-RHO to COPs beyond FJSP is not thoroughly discussed. Additionally, the theoretical analysis assumes idealized conditions that may not align with practical FJSP instances."
          },
          "questions": {
            "value": "1. How does L-RHO compare to recent FJSP solvers like those using graph neural networks or reinforcement learning? 2. What specific design choices (e.g., network architecture, loss function) enable the neural network to accurately predict non-reoptimizable variables? 3. Are the theoretical conditions for L-RHO's superiority empirically verified on real-world FJSP instances? 4. How does the framework handle dynamic changes in online scenarios, and what is its computational overhead in such settings?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper introduces L-RHO, a learning-guided rolling horizon optimization (RHO) framework for long-horizon combinatorial optimization problems (COPs), specifically applied to the Flexible Job-Shop Scheduling Problem (FJSP). L-RHO uses a neural network to identify and fix variables that do not require re-optimization across consecutive RHO subproblems, reducing computational overhead and improving solution quality. The approach demonstrates up to 54% faster solve times and 21% better solution quality compared to baselines, with theoretical analysis and extensive experiments across FJSP variants."
          },
          "strengths": {
            "value": "Originality: L-RHO is the first learning-guided RHO framework for COPs, addressing temporal redundancy in long-horizon problems. Quality: The experiments are comprehensive, covering multiple FJSP settings, distributions, and online scenarios, with strong quantitative results. Clarity: The paper is well-structured, with clear explanations of the methodology and theoretical analysis. Significance: FJSP is a critical industrial problem, and the framework’s adaptability and generalization potential make it impactful for real-world applications."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contribution of the neural network versus other components. The neural network architecture and training methodology are not sufficiently described, making it hard to replicate. The theoretical analysis, while insightful, is abstract and could benefit from concrete examples or empirical validation. Additionally, comparisons with recent learning-based decomposition methods (e.g., those using graph neural networks) are missing, limiting the context of the results."
          },
          "questions": {
            "value": "1. How is the neural network trained? What is the source of the training data (e.g., synthetic FJSP instances, real-world data)? 2. Are there ablation studies showing the impact of the learning component versus heuristic fixes? 3. How does L-RHO handle FJSP instances with highly dynamic or non-stationary constraints? 4. Could the framework be adapted to other COPs beyond FJSP, and what modifications would be required?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "Antib6Uovh": {
    "paper_id": "Antib6Uovh",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper provides a theoretical analysis of self-supervised learning (SSL) for vision transformers (ViTs), comparing masked autoencoders (MAE) and contrastive learning (CL). The authors model visual data with global and local features, analyze gradient descent dynamics for one-layer softmax-based ViTs under MAE and CL objectives, and show that MAE learns both global and local features while CL focuses on global patterns, even under mild imbalance."
          },
          "strengths": {
            "value": "The paper makes a significant theoretical contribution by offering the first end-to-end convergence guarantees for ViTs under self-supervised learning. It introduces a novel framework to analyze attention patterns, linking empirical observations (e.g., diverse attention maps in MAE vs. global focus in CL) to mathematical properties. The work addresses a critical gap in SSL theory for ViTs, which are widely used in practice. The clarity of the problem formulation and mathematical analysis is strong, with well-defined notations and structured arguments."
          },
          "weaknesses": {
            "value": "The theoretical analysis is limited to one-layer softmax-based ViTs, which simplifies the complexity of real-world ViTs. The data distribution assumptions (e.g., clusters with global/local features) may not capture the richness of real-world visual data. The experiments are not described in detail, leaving unclear how the theoretical findings are validated. The paper also lacks comparisons with other SSL methods or ablation studies on hyperparameters like the information gap Δ."
          },
          "questions": {
            "value": "1. How do the theoretical results extend to multi-layer ViTs, which are standard in practice? 2. What are the practical implications of the 'information gap' Δ, and how is it estimated in real-world scenarios? 3. Are the convergence guarantees dependent on specific initialization or optimization settings? 4. How do the attention pattern analyses generalize to other architectures or datasets beyond the simplified setup? 5. Could the observed differences between MAE and CL be influenced by the choice of softmax-based ViTs rather than the SSL objectives themselves?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper provides a theoretical analysis of how Vision Transformers (ViTs) learn distinct attention patterns under masked autoencoder (MAE) and contrastive learning (CL) objectives. The authors model visual data with imbalanced global and local features and analyze gradient descent dynamics for one-layer softmax-based ViTs, showing MAE learns diverse attention patterns while CL focuses on global features."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in theoretical understanding of SSL for ViTs, which is highly relevant given their dominance in vision. The problem formulation is novel, introducing a structured data distribution with global/local feature imbalance. The analysis of training dynamics for MAE and CL provides rigorous insights into their differing behaviors. The work connects empirical observations (e.g., attention map diversity in MAE) to theoretical guarantees, offering a clear contribution to the field."
          },
          "weaknesses": {
            "value": "The analysis is limited to one-layer softmax-based ViTs, which simplifies real-world models with deeper architectures and complex attention mechanisms. The paper lacks empirical validation, making it hard to assess practical relevance. The theoretical framework assumes a specific data distribution that may not generalize to real-world scenarios. The role of the 'information gap' Δ is not thoroughly justified, and the paper does not discuss how to estimate it in practice."
          },
          "questions": {
            "value": "1. How are the global and local features mathematically defined in the data distribution model? 2. Can the theoretical analysis be extended to multi-layer ViTs or more complex attention mechanisms? 3. What are the practical implications of the 'information gap' Δ, and how is it determined in real-world settings? 4. How do the assumptions about the data distribution affect the generality of the results? 5. Are the convergence guarantees dependent on specific initialization or hyperparameters?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper provides a theoretical analysis of how Vision Transformers (ViTs) trained with masked autoencoder (MAE) and contrastive learning (CL) objectives capture different spatial features. The authors model data distributions with global and local features, showing that MAE learns both, while CL focuses on global patterns, even under mild imbalance. They establish convergence guarantees and analyze attention dynamics for one-layer ViTs."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in understanding SSL for ViTs, offering novel theoretical insights into how MAE and CL objectives lead to distinct attention patterns. The problem formulation is rigorous, with a clear focus on spatially structured data and feature imbalance. The analysis of training dynamics and attention correlations is comprehensive, and the paper's structure is logically organized. The theoretical contributions are significant for a field lacking formal guarantees for ViTs under SSL."
          },
          "weaknesses": {
            "value": "The analysis is limited to one-layer ViTs, which is a major simplification of real-world architectures. The data distribution model assumes a binary split between global and local features, which may not reflect real-world complexity. The paper lacks experimental validation of the theoretical claims, making it difficult to assess practical relevance. The comparison to prior work is superficial, and the implications of the findings for multi-layer ViTs or more complex data scenarios are not discussed. The paper also does not address how these results might inform practical SSL design choices."
          },
          "questions": {
            "value": "1. How do the theoretical findings for one-layer ViTs generalize to multi-layer architectures? 2. What are the limitations of the binary global/local feature model in capturing real-world data distributions? 3. Are there experiments or empirical validations to support the convergence guarantees and attention pattern analyses? 4. How sensitive are the results to hyperparameters like the information gap Δ or learning rates? 5. What are the practical implications of the theoretical differences between MAE and CL for downstream tasks?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "AoIKgHu9Si": {
    "paper_id": "AoIKgHu9Si",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes L-WISE, a method that leverages robustified artificial neural networks (ANNs) to enhance human visual category learning by selecting images based on model-estimated difficulty and applying perturbations to improve recognition. It demonstrates significant accuracy gains (33-72%) and reduced training time in tasks ranging from basic animal classification to clinical domains like histology and dermoscopy."
          },
          "strengths": {
            "value": "The work introduces a novel application of ANNs to directly improve human visual learning, which is a significant contribution. The methodology combines model-based image selection and enhancement, showing clear empirical validation across diverse tasks. The experiments are well-structured, with thorough statistical analysis (e.g., AUC=0.72 for difficulty prediction). The paper also addresses clinically relevant domains, highlighting practical relevance. The clarity of the problem statement and figures (e.g., Fig. 1) is strong, though the full paper is cut off."
          },
          "weaknesses": {
            "value": "The paper lacks critical comparisons with existing human learning augmentation techniques, such as traditional curriculum learning or other image enhancement methods. The specific robustified ANN architecture and perturbation generation details are not clearly described, limiting reproducibility. The statistical significance of results (e.g., 33-72% gains) is not contextualized against baseline variability or effect size thresholds. The mechanism by which perturbations improve recognition is not explained, raising questions about whether they exploit task-specific features or general image properties. Additionally, the paper does not address potential ethical concerns, such as over-reliance on model-generated data in educational settings."
          },
          "questions": {
            "value": "1. What specific robustified ANN models were used, and how were they trained? 2. How were the perturbations generated (e.g., gradient-based methods, adversarial attacks)? 3. What baseline methods were compared against (e.g., random image selection, traditional curriculum learning)? 4. How was the 'recognition difficulty' score validated across different domains? 5. Were the results replicated across multiple participant groups, and what was the sample size? 6. How are the perturbations ensured to preserve task-relevant features rather than introducing noise?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes L-WISE, a method that leverages robustified artificial neural networks (ANNs) to enhance human visual category learning by selecting images based on model-estimated difficulty and applying perturbations to improve recognition. The approach demonstrates significant accuracy gains (33-72%) and reduced training time in tasks spanning natural images, histology, and dermoscopy."
          },
          "strengths": {
            "value": "The paper introduces a novel application of ANNs to augment human visual learning, which is significant given the lack of prior work in this area. The methodology is well-structured, combining difficulty-based image selection and perturbation-based enhancement. The experiments are comprehensive, covering diverse domains and including statistical validation (e.g., logistic regression for difficulty prediction). The clarity of the framework (L-WISE) and figures is strong, and the potential impact on education and medical training is highlighted."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental controls, such as comparisons to alternative image augmentation techniques or baseline methods. The generalizability of the model's difficulty scores across domains is not thoroughly validated. The sample size and statistical power of the human experiments are not explicitly reported, raising concerns about reproducibility. Additionally, the paper does not address potential ethical implications of image perturbations or their long-term effects on learning. The claims about accuracy gains rely on unverified assumptions about the test set's representativeness."
          },
          "questions": {
            "value": "1. How was the model's difficulty score validated across different domains (e.g., histology vs. natural images)? 2. Were there any issues with the perturbation methods that required adjustments during the experiments? 3. What was the exact number of participants, and how were they selected to ensure representativeness? 4. How were the model's logit values calculated, and is there a risk of overfitting to specific datasets? 5. The paper mentions 'fine-grained categorization tasks' but does not specify which ones—could this affect the generality of the results?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper proposes L-WISE, a method that leverages robustified artificial neural networks (ANNs) to enhance human visual category learning by selecting images based on model-estimated difficulty and applying perturbations to improve recognition. The approach demonstrates significant accuracy gains (33-72%) and reduced training time in tasks ranging from basic animal classification to clinical domains like histology and dermoscopy."
          },
          "strengths": {
            "value": "The paper introduces a novel application of ANNs to human learning, combining image selection and enhancement strategies. The methodology is creative, addressing both difficulty estimation and feature amplification for novice learners. The experiments are well-designed, with clear empirical results across diverse domains. The significance of bridging AI models with human perceptual learning is high, offering potential applications in education and medical training. The paper also provides a strong theoretical foundation by linking model predictions to human performance metrics."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of the perturbation techniques used for image enhancement, such as the specific mechanisms of 'ground truth logit maximization.' The experimental setup could benefit from more rigorous controls, such as comparisons with alternative learning strategies. The generalizability of the findings to other domains or model architectures is not thoroughly discussed. Additionally, the paper does not address potential limitations of using pre-trained ANNs, such as biases in the training data or domain-specific constraints."
          },
          "questions": {
            "value": "1. How exactly are the image perturbations generated to maximize ground truth logit activation values? Are there specific constraints or optimization techniques employed? 2. What is the exact process for sampling images based on the 'increasing schedule of maximal difficulty'? How is this schedule determined? 3. Are there any potential ethical concerns or biases associated with using ANNs to influence human learning, particularly in clinical domains? 4. How do the results generalize to other ANN architectures or tasks beyond the ones tested? 5. What is the role of the 'logit-weighted' selection in the curriculum, and how does it compare to other difficulty-based sampling methods?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "AqueuvXErD": {
    "paper_id": "AqueuvXErD",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates the relationship between classification robustness and explanation robustness in deep learning models. It challenges the common assumption of a strong correlation between the two by proposing a clustering-based method for evaluating explanation robustness and analyzing input loss landscapes. The authors find that enhancing explanation robustness does not necessarily flatten the input loss landscape, and a new training method that flattens the explanation loss landscape actually reduces explanation robustness without affecting classification robustness."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely question about the interplay between different forms of robustness in deep learning. The proposed clustering-based method for efficient evaluation of explanation robustness is novel and practical. The experimental analysis of input loss landscapes provides visual insights into the relationship between robustness and loss geometry. The paper also introduces a new training method that directly manipulates the loss landscape, offering a concrete contribution to the field."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient experimental validation to support its key claims. The clustering-based evaluation method is not thoroughly described, making it difficult to assess its effectiveness. The experiments on the relationship between loss landscape flattening and robustness are limited, with no ablation studies or comparisons to alternative approaches. The paper's conclusion that explanation and classification robustness are uncorrelated is not convincingly supported by the evidence, as the analysis of TRADES-based models and the new training method appears superficial. The supplementary material is referenced but not accessible for verification."
          },
          "questions": {
            "value": [
              "How exactly is the clustering-based method implemented, and what metrics are used to select representative pairs for evaluation?",
              "What specific metrics are used to quantify explanation robustness, and how do they compare to existing baselines?",
              "Why does flattening the explanation loss landscape decrease explanation robustness? Is this a general phenomenon or dataset/model-specific?",
              "Are there ablation studies to validate the components of the proposed training method (e.g., the new loss function)?",
              "How does the new training method compare to existing approaches for improving explanation robustness in terms of performance and efficiency?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper challenges the assumption that classification robustness and explanation robustness in deep learning models are strongly correlated. The authors introduce a clustering-based method to efficiently evaluate explanation robustness and demonstrate that enhancing explanation robustness does not necessarily flatten the input loss landscape. They also propose a training method to adjust the loss landscape with respect to explanation loss, showing that this does not affect classification robustness, thereby questioning the previously held correlation between the two forms of robustness."
          },
          "strengths": {
            "value": "The paper addresses a critical and underexplored question about the relationship between two key robustness metrics in deep learning. The proposed clustering-based method for evaluating explanation robustness is innovative and efficient. The experimental analysis, including visualizations of loss landscapes, is thorough and provides clear evidence against the assumed correlation. The work is well-structured, with clear motivation, methodology, and results. The significance lies in challenging a widely accepted assumption and opening new research directions for understanding robustness in deep learning."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed analysis of why the correlation between classification and explanation robustness might not hold, which could strengthen the theoretical foundation. The experiments are primarily conducted on CIFAR-10, and the generalizability to other datasets or architectures is not thoroughly discussed. The proposed training method for adjusting the explanation loss landscape is described but not fully explained in terms of its underlying mechanism or limitations. Additionally, the connection to existing work on loss landscape flatness (e.g., the role of sharp vs. flat minima) is not sufficiently contextualized."
          },
          "questions": {
            "value": "1. How does the clustering-based method for selecting representative pairs ensure that it captures the full spectrum of explanation robustness? 2. What specific aspects of the proposed training method cause a decrease in explanation robustness without affecting classification robustness? 3. Are the findings consistent across different model architectures (e.g., ResNet vs. ViT) or datasets beyond CIFAR-10? 4. How does the proposed loss function for flattening the explanation loss landscape differ from existing methods like TRADES or other adversarial training approaches? 5. Could the observed independence between the two robustness metrics be attributed to the specific choice of adversarial attacks or evaluation metrics used?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper challenges the assumption that classification robustness and explanation robustness in deep learning models are strongly correlated. It introduces a clustering-based method for efficiently evaluating explanation robustness and proposes a training approach to modify the input loss landscape with respect to explanation loss. Key findings include that enhancing explanation robustness does not flatten the input loss landscape, and adjusting the landscape for explanation loss does not affect classification robustness, undermining the previously held correlation between the two."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely question about the relationship between two forms of robustness in deep learning, which is highly relevant to model reliability and interpretability. The clustering-based method for evaluating explanation robustness offers a novel and efficient approach. The experimental design is rigorous, with clear visualizations of loss landscapes and controlled experiments using TRADES to manipulate robustness. The paper also provides code, enhancing reproducibility. The findings challenge a widely accepted assumption, contributing to a deeper understanding of robustness in neural networks."
          },
          "weaknesses": {
            "value": "The paper lacks detailed justification for the clustering-based method's effectiveness in capturing explanation robustness, and its validation against existing metrics is insufficient. The experiments focus on CIFAR-10 and a limited set of models, which may limit generalizability. The proposed training method to flatten the explanation loss landscape is described but not thoroughly compared to alternative approaches. Additionally, the paper does not discuss practical implications of the findings for model design or real-world applications. Theoretical analysis of why the correlation between robustness types might break down is also missing."
          },
          "questions": {
            "value": [
              "How was the clustering-based method validated against established metrics for explanation robustness? Were ablation studies conducted to assess its sensitivity to hyperparameters?",
              "What specific modifications were made to TRADES to control explanation robustness, and how were the trade-off parameters tuned?",
              "How does the proposed training method compare in efficiency and effectiveness to existing techniques for improving explanation robustness?",
              "Are the findings specific to CIFAR-10, or were experiments conducted on other datasets to confirm generalizability?",
              "What are the limitations of the input loss landscape analysis in capturing the interplay between classification and explanation robustness?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "ArJikvI6xo": {
    "paper_id": "ArJikvI6xo",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces GFLAgent, a novel federated learning (FL) system leveraging a Large Language Model (LLM)-based agent to address data and device heterogeneity. GFLAgent dynamically monitors clients, allocates them to layers for asynchronous training, and uses a buffer zone to handle outliers. It claims to reduce energy costs and improve efficiency while maintaining accuracy compared to existing methods."
          },
          "strengths": {
            "value": "Originality is evident in combining LLMs with FL for automated decision-making, addressing both statistical and system heterogeneity. The focus on green computing and energy efficiency aligns with critical trends. The paper proposes a buffer zone for outlier clients, a novel mechanism for robustness. The significance is high, as FL faces major challenges in heterogeneity and scalability, and the approach could inspire future work on agent-based FL systems."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, with no specific datasets, metrics, or comparisons provided. The LLM-based agent's training methodology is unclear—how is it trained, and what data does it use? The buffer zone mechanism is described conceptually but not technically. The related work section is incomplete, making it hard to assess novelty. The claims about 'outperforming SOTA methods' are unsubstantiated without concrete results."
          },
          "questions": {
            "value": "1. How is the LLM-based agent trained? What training data or objectives are used? 2. What specific datasets and metrics were used in experiments? 3. How is the buffer zone implemented—what criteria determine client relocation? 4. How does GFLAgent handle scalability with a large number of clients? 5. Are there ablation studies to validate the contribution of the buffer zone and LLM agent?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces GFLAgent, a novel federated learning (FL) system leveraging a large language model (LLM)-based agent to address data and device heterogeneity. The system dynamically selects clients, manages asynchronous training, and incorporates a buffer zone for outlier clients. It claims to improve efficiency, reduce energy costs, and outperform existing methods on standard datasets."
          },
          "strengths": {
            "value": "Originality lies in applying LLMs for client selection and system management in FL, which is a fresh perspective. The system's energy efficiency and asynchronous training framework are promising. The paper addresses practical challenges like stragglers and outlier clients, demonstrating relevance to real-world FL deployment. The clarity of the problem statement and high-level architecture is strong, though implementation details are sparse."
          },
          "weaknesses": {
            "value": "The paper lacks concrete experimental details (e.g., datasets, baseline comparisons, hyperparameters) to validate claims of SOTA performance. The integration of the LLM-based agent is under-specified—how is it trained? What input features does it use? The buffer zone mechanism is described conceptually but not quantified. The theoretical analysis of heterogeneity mitigation is minimal, and the energy efficiency claims are not supported by measurements."
          },
          "questions": {
            "value": "1. How is the LLM-based agent trained? Is it pre-trained on FL-specific data or fine-tuned on task-specific data? 2. What specific metrics (e.g., accuracy, communication cost, energy consumption) were used to demonstrate superiority over SOTA methods? 3. How is the buffer zone's effectiveness quantified—e.g., through ablation studies or statistical analysis? 4. Are there comparisons of energy consumption between GFLAgent and existing methods? 5. How does the agent dynamically adjust its decisions during training?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper introduces GFLAgent, a novel Federated Learning (FL) system that leverages a Large Language Model (LLM)-based agent to address data and device heterogeneity. The system dynamically selects and schedules clients, incorporates a buffer zone for outlier clients, and aims to improve efficiency and energy sustainability in FL. The authors claim significant performance improvements over existing methods."
          },
          "strengths": {
            "value": "Originality is evident in the integration of LLM-based agents for dynamic client scheduling and the proposed buffer zone for handling outliers. The paper addresses critical FL challenges like statistical heterogeneity and system robustness. The methodology appears well-structured, with clear motivation for the Green FL approach. The experimental claims (e.g., superior performance over SOTA) suggest potential significance for the FL community."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation (e.g., specific metrics, baselines, or ablation studies). The integration of the LLM agent is not thoroughly explained, including how it is trained or adapted for FL tasks. The buffer zone mechanism and its impact on system robustness remain under-specified. The paper also does not address potential limitations of LLM-based agents in FL, such as computational overhead or interpretability issues."
          },
          "questions": {
            "value": "1. How is the LLM-based agent trained? Is it pre-trained on FL-specific data or fine-tuned for this task? 2. What metrics (e.g., accuracy, communication cost, energy consumption) were used to evaluate GFLAgent's performance? 3. How does the buffer zone mechanism handle outliers during training? 4. Are there comparisons with other Green FL approaches (e.g., FedProx, SCAFFOLD) in terms of energy efficiency? 5. What is the computational overhead of the LLM agent compared to traditional scheduling methods?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "ArwsbHBoxA": {
    "paper_id": "ArwsbHBoxA",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper addresses the problem of contextual dueling bandits under adversarial feedback, proposing a new algorithm called RCDB. The algorithm uses uncertainty-weighted maximum likelihood estimation to achieve a regret bound of $\\tilde{O}(d\\sqrt{T} + dC)$, where $d$ is the context dimension, $T$ is the number of rounds, and $C$ is the number of adversarial feedback. The authors also establish a lower bound demonstrating the near-optimality of their result."
          },
          "strengths": {
            "value": "The paper introduces a novel algorithm for contextual dueling bandits, a more general setting than prior finite-armed models. The theoretical analysis includes both upper and lower bounds, showing near-optimality. The regret bound improves on existing methods, and the problem is highly relevant to applications like RLHF. The paper also provides a comprehensive comparison table of related works."
          },
          "weaknesses": {
            "value": "The experiments are not detailed in the provided text, making it difficult to assess their thoroughness or validity. The paper lacks a clear discussion of practical limitations, such as assumptions about the context distribution or computational complexity. The novelty compared to prior work (e.g., Agarwal et al. 2021) is not fully justified, and the lower bound analysis is not elaborated. The adversarial feedback model's real-world applicability remains unclear."
          },
          "questions": {
            "value": [
              "How does the adversarial model in this paper differ from Agarwal et al. (2021) in terms of assumptions and practical relevance?",
              "What is the exact mechanism for incorporating uncertainty-dependent weights into the MLE, and how is this theoretically justified?",
              "How does the algorithm handle the estimation of $C$ when it is unknown, and what guarantees exist for the optimistic estimator?",
              "What specific types of adversarial feedback were tested in the experiments, and how do they compare to real-world scenarios?",
              "Are there any assumptions about the context vectors (e.g., boundedness, distribution) that could limit the algorithm's applicability?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper addresses the problem of contextual dueling bandits under adversarial feedback, proposing a novel algorithm called RCDB that integrates uncertainty-weighted maximum likelihood estimation. The algorithm achieves a nearly optimal regret bound of $\\tilde{O}(d\\sqrt{T} + dC)$, where $d$ is the context dimension, $T$ is the number of rounds, and $C$ is the adversarial feedback count. The work also establishes a lower bound to demonstrate the optimality of their result, both with and without adversarial feedback."
          },
          "strengths": {
            "value": "The paper introduces a novel algorithm (RCDB) for contextual dueling bandits under adversarial feedback, addressing a critical gap in existing literature. The theoretical analysis is rigorous, with a regret bound that improves upon prior work, particularly in contextual settings. The paper's contributions include a more realistic adversarial model (adversary observes only selected actions) and a clear comparison to prior methods via a comprehensive table. The significance lies in its application to real-world scenarios like aligning large language models with human feedback, where adversarial manipulation is a concern. The writing is structured and accessible, with well-defined notation and clear problem formulations."
          },
          "weaknesses": {
            "value": "The experimental section appears incomplete in the provided content, making it difficult to assess the empirical validation of RCDB. The paper mentions 'extensive experiments' but lacks specific details on datasets, baselines, or adversarial feedback types tested. Additionally, the analysis assumes a specific adversarial model (e.g., adversary observes only the selected actions), but it is unclear how robust this assumption is to variations in adversarial behavior. The lower bound proof is not detailed here, so its validity and tightness require further scrutiny."
          },
          "questions": {
            "value": [
              "What specific types of adversarial feedback were tested in the experiments, and how do they differ from the assumptions in the theoretical analysis?",
              "How does the uncertainty-weighted MLE in RCDB handle scenarios where adversarial feedback is non-uniformly distributed across actions?",
              "Can the algorithm's performance be guaranteed under weaker assumptions about the adversary's knowledge (e.g., if the adversary does not observe the selected actions)?",
              "Are there any practical limitations to the algorithm's implementation, such as computational complexity or sensitivity to hyperparameters?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the problem of contextual dueling bandits under adversarial feedback, proposing a new algorithm called RCDB. The algorithm uses uncertainty-weighted maximum likelihood estimation to achieve a regret bound of $\\tilde{O}(d\\sqrt{T} + dC)$, where $d$ is the context dimension, $T$ is the number of rounds, and $C$ is the number of adversarial feedbacks. The authors also establish a lower bound showing the near-optimality of their result, making it the first work to achieve nearly minimax optimal regret in this setting."
          },
          "strengths": {
            "value": "The paper demonstrates strong originality by introducing a novel algorithm for contextual dueling bandits with adversarial feedback, addressing a gap in existing literature. The theoretical analysis is rigorous, with both upper and lower bounds provided to establish optimality. The clarity of the problem formulation and the structured presentation of results (e.g., the comparative table) enhance readability. The significance of the work is high, as it tackles a critical challenge in real-world applications like RLHF, where adversarial feedback is a major concern."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation to support the theoretical claims, as the provided content is truncated and does not include results. The adversarial feedback model assumes the adversary observes only the selected actions, but the practical implications of this assumption are not thoroughly discussed. Additionally, the lower bound proof may require more detailed justification to confirm its correctness and tightness. The comparison with prior work in the table is useful, but the paper could better contextualize its contributions relative to existing methods."
          },
          "questions": {
            "value": [
              "What specific adversarial scenarios were tested in the experiments, and how were they implemented?",
              "How does the algorithm handle cases where the adversary's strategy is adaptive or non-stationary?",
              "Are there any limitations to the uncertainty-weighted MLE approach in high-dimensional contexts?",
              "Can the lower bound be generalized to other adversarial feedback models beyond the one considered in the paper?",
              "How sensitive is the algorithm to the choice of hyperparameters in the uncertainty weighting?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "As2ZyaNoHa": {
    "paper_id": "As2ZyaNoHa",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper investigates biases in Large Language Models (LLMs) regarding financial knowledge by analyzing over 190k questions about U.S. publicly traded companies. It identifies temporal and cross-sectional knowledge gaps, showing LLMs struggle with historical data and smaller companies but are more accurate for recent and larger firms. The study also reveals that larger companies are more prone to hallucinations, despite widespread financial data availability."
          },
          "strengths": {
            "value": "The paper's strengths include the large-scale dataset (190k questions) and systematic analysis of financial knowledge biases in LLMs, which is novel for the financial domain. The study rigorously examines multiple factors (company size, market capitalization, readability) and provides empirical evidence of temporal and cross-sectional biases. The exploration of hallucinations in LLMs, particularly for larger firms, adds practical significance. The methodology is well-structured, with clear experimental pipelines and comparisons across models."
          },
          "weaknesses": {
            "value": "The paper's scope is limited to two models (ChatGPT and Llama-3-Chat), which may not represent the broader LLM landscape. The dataset construction lacks detailed justification for question selection and potential biases in financial data sources. The hallucination analysis relies on an arbitrary 10% error threshold without rigorous validation. Additionally, the paper does not address how these biases could be mitigated or whether the findings generalize to non-U.S. markets or other financial instruments."
          },
          "questions": {
            "value": "1. Why were only two LLMs evaluated, and how might results vary with other models (e.g., GPT-4, Claude)? 2. What criteria were used to generate the 190k questions, and how were edge cases (e.g., companies with sparse data) handled? 3. How was the 10% error threshold for hallucinations determined, and what sensitivity analysis was performed? 4. Are the findings specific to U.S. financial data, or could they generalize to other markets? 5. How do the authors reconcile the paradox of higher accuracy for larger firms with increased hallucinations?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper investigates biases in Large Language Models (LLMs) regarding financial knowledge by analyzing their performance on over 190k questions about U.S. publicly traded companies. It identifies temporal knowledge gaps (e.g., poor performance on older financial data) and cross-sectional biases (e.g., higher accuracy for larger companies). The study also reveals that larger companies are more prone to hallucinations, despite having more publicly available data."
          },
          "strengths": {
            "value": "The paper addresses an important and underexplored domain (financial LLM biases) with novel contributions. The large-scale dataset (190k+ questions) and systematic analysis of temporal and cross-sectional factors are robust. The findings on retrograde knowledge bias and hallucination trends are significant for understanding LLM limitations in finance. The methodology includes rigorous statistical analysis (e.g., log odds ratios) and cross-sectional frameworks, demonstrating strong execution."
          },
          "weaknesses": {
            "value": "The paper lacks detailed methodology for dataset construction (e.g., how questions were generated, whether they are based on real data or synthetic prompts). The analysis of variables like 'readability of financial filings' and 'institutional attention' is superficial, with no clear metrics or sources provided. The 10% error threshold for hallucinations is arbitrary without justification. The focus on U.S. companies limits generalizability, and the paper does not compare with other models beyond GPT-3.5 and Llama-3. The experimental setup and statistical significance of results are not fully explained."
          },
          "questions": {
            "value": "1. How were the 190k questions generated? Are they based on real financial reports or synthetic prompts? 2. What metrics define 'readability of financial filings' and how were they calculated? 3. How were variables like 'retail investor numbers' and 'institutional attention' quantified? 4. Why was a 10% error threshold chosen for hallucinations? 5. How do results vary across different LLMs (e.g., GPT-4 vs. Llama-3)? 6. Are there confounding variables in the cross-sectional analysis (e.g., market cap vs. company age)?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates the knowledge gaps and biases in Large Language Models (LLMs) when answering financial questions about U.S. publicly traded companies. By constructing a dataset of 190k revenue-related questions, the authors analyze temporal and cross-sectional biases, finding that LLMs perform worse on historical data and smaller firms, while exhibiting higher hallucination rates for larger companies. They also introduce a novel framework for evaluating retrograde knowledge bias and factuality hallucinations."
          },
          "strengths": {
            "value": "The paper's strengths include the creation of a large-scale, meticulously curated financial dataset (RevenuePromptDataset) with over 190k questions spanning 41 years, which provides a robust foundation for analysis. The study systematically identifies novel biases (e.g., retrograde knowledge bias) and explores their relationship with company characteristics, offering actionable insights for improving LLM reliability in financial contexts. The integration of hallucination analysis and cross-sectional factors like market capitalization and filing readability demonstrates comprehensive methodological rigor. The paper also addresses a critical, underexplored domain (financial LLM biases) with practical implications."
          },
          "weaknesses": {
            "value": "The paper lacks baseline comparisons against alternative models or traditional financial analysis tools, limiting the interpretation of LLM performance. The hallucination detection methodology is under-specified (e.g., justification for the 10% error threshold, handling of non-numerical hallucinations). The dataset construction details (e.g., validation of financial data sources, handling of missing/incorrect data) are insufficient. The focus on U.S. companies restricts generalizability, and the paper does not address potential confounding variables (e.g., model-specific training data, fine-tuning). The analysis of 'retrograde bias' lacks a clear theoretical framework to explain why historical data is underrepresented."
          },
          "questions": {
            "value": [
              "How were the 190k questions generated? Were they based on actual financial reports, and how was their accuracy validated?",
              "What is the source of the financial data used to construct the RevenuePromptDataset, and how was it cleaned/validated?",
              "How were variables like 'retail investor count' and 'SEC filing access' quantified, and what are their limitations?",
              "What is the rationale for the 10% error threshold used to define hallucinations, and how does this compare to existing metrics?",
              "Why were only LLMs (e.g., ChatGPT, Llama-3) evaluated, and how do their results compare to traditional financial models or rule-based systems?",
              "How do the findings generalize to non-U.S. markets or non-publicly traded companies?",
              "What steps were taken to ensure the models were evaluated on equal footing (e.g., identical prompts, inference settings)?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "B2ChNpcEzZ": {
    "paper_id": "B2ChNpcEzZ",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces DefNTaxS, a framework that enhances zero-shot text-image classification by integrating taxonomic subcategories into class descriptors. It leverages large language models (LLMs) to automatically generate hierarchical taxonomic groupings, reducing inter-class competition and improving semantic resolution without manual intervention or model retraining."
          },
          "strengths": {
            "value": "Originality is strong, as DefNTaxS addresses a gap in existing methods by incorporating structured taxonomic relationships, which prior work has overlooked. The approach is highly automated, avoiding the need for manual prompts or training, which is a significant practical advantage. The paper emphasizes improved interpretability through hierarchical structuring, a critical aspect for real-world applications. While the experimental details are limited in the provided text, the conceptual framework aligns with the growing interest in leveraging semantic hierarchies for better model generalization."
          },
          "weaknesses": {
            "value": "The paper lacks specific experimental results or quantitative comparisons to validate the claimed performance gains. Key details about how taxonomic subcategories are generated (e.g., clustering methods, LLM prompts) are omitted, making it difficult to assess the method's robustness. The comparison to existing methods like D-CLIP, CuPL, and CHiLS is superficial, with no ablation studies or analysis of how taxonomic structures affect different dataset characteristics (e.g., class granularity, similarity). The claim of 'consistent performance gains' remains unsubstantiated without empirical evidence."
          },
          "questions": {
            "value": "1. How are the taxonomic subcategories generated? Are they based on pre-existing hierarchies, LLM-generated relationships, or a novel clustering algorithm? 2. What metrics are used to evaluate the effectiveness of taxonomic groupings (e.g., clustering quality, classification improvement)? 3. How does DefNTaxS handle datasets without existing taxonomic structures? 4. Are there ablation studies demonstrating the impact of taxonomic stratification versus fine-grained descriptors? 5. What is the computational cost of the LLM-based taxonomic generation process?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes DefNTaxS, a framework that enhances zero-shot text-image classification by integrating taxonomic subcategories into CLIP's text prompts. It leverages large language models (LLMs) to automatically generate hierarchical taxonomic groupings, aiming to reduce inter-class competition and improve semantic resolution. The method is claimed to be fully automated, with performance gains across multiple datasets."
          },
          "strengths": {
            "value": "The paper addresses a relevant problem in zero-shot classification by introducing a novel approach that incorporates taxonomic structures, which could improve interpretability and accuracy. The method's automation is a strength, as it avoids manual intervention. The work is well-structured, with a clear problem statement and comparisons to prior methods. The focus on semantic hierarchy aligns with broader trends in improving VLMs."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as ablation studies or comparisons with state-of-the-art methods beyond the mentioned baselines. The taxonomic grouping mechanism is not sufficiently explained, making it unclear how LLMs generate subcategories or how they are evaluated. The claims of 'consistent performance gains' are not supported by comprehensive results, and the paper does not address potential limitations (e.g., scalability, dataset-specific challenges)."
          },
          "questions": {
            "value": "1. How are taxonomic subcategories generated by the LLMs? What specific prompts or techniques are used? 2. Are there quantitative metrics to evaluate the quality of the taxonomic groupings? 3. How does DefNTaxS handle datasets without existing taxonomic hierarchies? 4. What is the computational overhead of the taxonomic generation process? 5. Are there cases where the taxonomic approach might degrade performance compared to existing methods?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces DefNTaxS, a framework that enhances zero-shot text-image classification by incorporating taxonomic subcategories into class descriptors. The method leverages large language models (LLMs) to automatically generate hierarchical taxonomic groupings, reducing inter-class competition and improving semantic resolution without manual intervention or model retraining."
          },
          "strengths": {
            "value": "The paper presents a novel approach by integrating taxonomic hierarchies into zero-shot classification, addressing a gap in existing methods that focus solely on class-specific descriptors. The automation of taxonomic grouping via LLMs is a significant innovation, offering scalability and adaptability across datasets. The clarity of the problem statement and the structured comparison with prior work (e.g., D-CLIP, CuPL, CHiLS) demonstrate strong theoretical grounding. The potential impact on interpretability and accuracy in zero-shot tasks is promising, aligning with broader trends in multimodal representation learning."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation, such as specific metrics (e.g., accuracy, F1 scores) or comparisons to state-of-the-art methods like CLIP, D-CLIP, or CuPL. The claimed 'consistent performance gains' are not substantiated with quantitative results. The method's reliance on LLMs for taxonomic grouping is not thoroughly explained—e.g., how are semantic relationships quantified? Additionally, the paper truncates the related work section, omitting critical comparisons with recent approaches. The absence of ablation studies or analysis of taxonomic group quality limits the evaluation of core contributions."
          },
          "questions": {
            "value": "1. What specific datasets and metrics were used to evaluate DefNTaxS, and how do they compare to baselines like D-CLIP or CuPL? 2. How are semantic relationships between classes quantified to form taxonomic groupings, and what ensures the quality of these groupings? 3. Are there any limitations to the taxonomic approach (e.g., dataset dependency, scalability to large class sets)? 4. How does DefNTaxS handle ambiguous or overlapping class hierarchies? 5. What ablation studies were conducted to isolate the impact of taxonomic stratification versus other components?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "B5Dj4EhZPP": {
    "paper_id": "B5Dj4EhZPP",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes Reciprocal Feedback, a biologically plausible alternative to backpropagation that combines pseudoinverse feedback connections with a Newton-like optimization method. It modifies the Recirculation algorithm to learn Moore-Penrose pseudoinverses of network weights and uses these to propagate errors without requiring weight transposes or direct feedback from the final layer. The method achieves comparable performance to backpropagation on MNIST and CIFAR-10 with fewer iterations than existing biologically plausible methods."
          },
          "strengths": {
            "value": "The paper demonstrates originality by unifying autoencoder-like structures with pseudoinverse feedback, addressing the weight-transport problem in a novel way. The theoretical framework is sound, with clear connections to existing methods like Target Propagation and Feedback Alignment. The experiments on standard datasets show competitive results, and the writing is structured with detailed equations. The significance lies in advancing biologically plausible learning algorithms while maintaining practical performance."
          },
          "weaknesses": {
            "value": "The theoretical analysis of how pseudoinverse properties are preserved during matrix multiplication is insufficiently explained, and the paper lacks comparisons to other weight-transport algorithms like weight-mirror or sign-symmetry. The experimental validation is limited to MNIST and CIFAR-10, with no ablation studies on hyperparameters or scalability to larger networks. The truncation of the paper also prevents full evaluation of its claims about relationships with other methods."
          },
          "questions": {
            "value": "1. How does the Newton-like method ensure convergence in non-invertible networks? 2. What are the computational costs of pseudoinverse learning compared to backpropagation? 3. How does the method handle noisy or incomplete feedback connections? 4. Are there theoretical guarantees for the effectiveness of left reciprocal pseudoinverse properties in optimization? 5. How does Reciprocal Feedback compare to Target Propagation in terms of modularity and biological plausibility?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a biologically plausible alternative to backpropagation called 'Reciprocal Feedback' that uses pseudo-inverse feedback connections to minimize error in feedforward networks. It combines a modified Recirculation algorithm for learning pseudoinverses and a Newton-like method for optimization, claiming performance comparable to backpropagation on MNIST and CIFAR-10 with fewer iterations than existing methods."
          },
          "strengths": {
            "value": "The paper introduces a novel approach to address the biological implausibility of backpropagation by combining autoencoder-like feedback mechanisms with pseudoinverse learning. The theoretical connection to Newton-like optimization and the wake-sleep cycle framework are creative contributions. The experimental validation on standard datasets and comparison with Feedback Alignment demonstrate practical relevance. The paper also engages with existing literature on target propagation and weight-transport algorithms, showing awareness of the broader context."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous theoretical justification for the Newton-like method's convergence properties and its superiority over gradient descent. The experimental section is insufficiently detailed—key metrics like training speed, computational efficiency, and ablation studies are missing. The claim about pseudo-inverse properties being 'left reciprocal' is mathematically ambiguous and requires clarification. The biological plausibility arguments rely on speculative interpretations of cortical feedback mechanisms without direct empirical support. The related work section is incomplete, hindering assessment of novelty."
          },
          "questions": {
            "value": "1. How is the Newton-like method implemented in practice, and what guarantees does it provide for convergence? 2. What specific metrics (e.g., training time, memory usage) demonstrate the method's efficiency advantage over Feedback Alignment? 3. How are the pseudo-inverse properties preserved during matrix multiplication, given the paper's claim about 'left reciprocal' properties? 4. What biological evidence directly supports the proposed feedback mechanism's alignment with cortical processing? 5. Why does the method outperform Feedback Alignment, and how sensitive is this result to hyperparameters?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper proposes a biologically plausible alternative to backpropagation called Reciprocal Feedback, which uses pseudo-inverse feedback connections to minimize error in feedforward networks. The method combines a modified Recirculation algorithm to learn pseudoinverses and a Newton-like optimization approach, achieving performance comparable to backpropagation on MNIST and CIFAR-10 while avoiding direct weight transposes or feedback from the final layer."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in neural network research: the biological implausibility of backpropagation. It offers a novel integration of autoencoder-like structures with pseudo-inverse feedback, providing a fresh theoretical framework. The method's potential to reduce reliance on weight transposes or direct feedback connections is significant. The theoretical derivation using Newton-like methods and connections to Target Propagation and weight-transport algorithms demonstrate strong conceptual depth. The experimental results on standard datasets suggest practical viability, and the paper's clear structure and references to prior work enhance its credibility."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental comparisons with other biologically plausible methods beyond Feedback Alignment, and the claimed superiority on MNIST/CIFAR-10 is not fully substantiated with quantitative analysis. The theoretical discussion on pseudo-inverse properties (e.g., non-factorizability) is important but does not fully address how this limitation impacts the method's performance. The 'wake-sleep' cycle mechanism is described conceptually but lacks concrete implementation details or validation. Additionally, the paper is cut off mid-sentence, leaving critical sections (e.g., full experimental results, comparisons with weight-mirror or sign-symmetry methods) incomplete."
          },
          "questions": {
            "value": [
              "How is the Newton-like method specifically adapted to the network architecture? What are the theoretical guarantees for convergence in non-invertible networks?",
              "What are the exact computational costs and scalability limitations of the proposed method compared to Feedback Alignment or Target Propagation?",
              "How does the paper address the non-factorizability of pseudo-inverses in the error propagation chain? Are there empirical validations of the claimed robustness to non-invertible networks?",
              "What biological evidence supports the assumption that cortical feedback connections function as pseudo-inverses rather than exact inverses or random matrices?",
              "The paper mentions a 'parallel' version of Reciprocal Feedback—what are the differences in training dynamics or performance between synchronized and parallel variants?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "B5i88Tj1nk": {
    "paper_id": "B5i88Tj1nk",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces Adversarial Information Masking (AIM) to evaluate the faithfulness of post-hoc explanations in EEG-DL models. The work expands in-distribution information masking to multi-domain analysis (spatial, temporal, spectral) and proposes a novel framework (mdAR) for quantitatively assessing explanation quality. It addresses gaps in existing evaluation methods by incorporating adversarial perturbations and standardized metrics."
          },
          "strengths": {
            "value": "Originality is strong through the multi-domain adversarial masking approach and novel mdAR framework. The paper addresses a critical gap in XAI evaluation for EEG data, which is underexplored. Clarity is good, with structured sections and visual explanations. Significance is high due to the growing importance of EEG-DL and the need for reliable XAI tools. The work provides a foundation for future research in explainable AI applications."
          },
          "weaknesses": {
            "value": "The paper lacks direct comparisons with existing faithfulness evaluation methods like ROAR or DIFFROAR, making it hard to assess the novelty of mdAR. The experiments focus on a single EEG dataset, limiting generalizability. Implementation details of AIM (e.g., adversarial perturbation generation) are not sufficiently explained. The theoretical justification for multi-domain masking is underdeveloped, and ablation studies are missing."
          },
          "questions": {
            "value": [
              "How does the adversarial component in AIM differ from existing perturbation-based methods like ROAR or DIFFROAR?",
              "What specific adversarial techniques are used to generate perturbations in the spatial/temporal/spectral domains?",
              "Why is the multi-domain approach necessary for EEG data compared to single-domain masking?",
              "Are the proposed normalized faithfulness metrics validated against ground-truth labels or human annotations?",
              "How are the results interpreted when the masking ratio exceeds a certain threshold (e.g., 50%)?",
              "What is the computational cost of the mdAR framework compared to existing methods?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces Adversarial Information Masking (AIM) to evaluate the faithfulness of post-hoc explanation methods in EEG-DL. It addresses gaps in existing frameworks by proposing a novel adversarial masking approach, expanding in-distribution information masking to multiple domains, and introducing a new evaluation framework (mdAR) with normalized metrics. The study aims to enable quantitative comparisons of explanation methods across architectures, datasets, and domains."
          },
          "strengths": {
            "value": "The paper identifies a critical gap in evaluating explanation faithfulness for EEG-DL, which is a niche but important domain. The proposed AIM approach addresses limitations of existing information masking techniques by incorporating adversarial strategies, which could improve robustness. The paper's structured categorization of evaluation strategies (fidelity vs. robustness analysis) and the introduction of multi-domain masking demonstrate originality. The focus on EEG data, where interpretability is crucial for medical applications, adds significance. The paper also provides a comprehensive literature review and table summarizing existing evaluation criteria."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental validation to support its claims. While it mentions a 'first quantitative comparison,' no results or comparisons with existing methods are provided in the truncated content. The adversarial information masking (AIM) approach is described conceptually but not rigorously justified or compared to alternatives. The proposed mdAR framework's metrics and evaluation methodology are not empirically validated, making their effectiveness uncertain. The literature review in Table 1 is incomplete or inconsistent (e.g., some studies lack removal/imputation details), suggesting a lack of thorough analysis. The paper also does not address how the proposed methods handle domain-specific challenges in EEG data (e.g., noise, temporal dependencies)."
          },
          "questions": {
            "value": [
              "What specific datasets and EEG-DL models were used to evaluate the proposed AIM and mdAR framework? How do they compare to existing baselines?",
              "How is the adversarial aspect of AIM implemented? What adversarial perturbations or strategies are used to improve in-distribution masking?",
              "Are the new normalized faithfulness metrics in mdAR validated against existing benchmarks or ground-truth annotations? How do they correlate with human interpretations?",
              "How does the expanded Remove and Debias method handle multi-domain (spatial, temporal, spectral) features? What are the technical challenges in generalizing it to EEG data?",
              "What evidence supports the claim that AIM overcomes suboptimal information masking techniques? Are there ablation studies demonstrating its effectiveness?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper addresses gaps in evaluating the faithfulness of post-hoc explanation methods for EEG-based deep learning (EEG-DL) by proposing an adversarial information masking (AIM) approach. The work introduces multi-domain in-distribution information masking, a novel framework (mdAR) for assessing faithfulness, and compares existing methods across architectures, datasets, and domains."
          },
          "strengths": {
            "value": "The paper demonstrates originality by extending in-distribution masking to multi-domain EEG analysis and proposing an adversarial framework (AIM) to address limitations in prior methods. The methodology is well-structured, with a clear focus on faithfulness, a critical XAI criterion. The paper also highlights the importance of EEG-DL interpretation, a niche but impactful application area. The inclusion of a comprehensive table summarizing prior work adds value for contextualization."
          },
          "weaknesses": {
            "value": "The paper lacks sufficient experimental validation. While the AIM approach is proposed, there is no detailed comparison against existing methods (e.g., Remove and Debias, ROAR, ROAD) to demonstrate its efficacy. The quantitative comparison across architectures and domains is mentioned but not elaborated, leaving the claims unsubstantiated. The adversarial component of AIM is not clearly explained, and the metrics in the mdAR framework are not fully defined. The section on related work is incomplete, limiting the ability to assess novelty."
          },
          "questions": {
            "value": [
              "What specific datasets and baselines were used to evaluate the AIM approach and mdAR framework? How do the results compare to existing methods like ROAR or ROAD?",
              "How is the adversarial aspect of AIM implemented? What guarantees does it provide over traditional information masking techniques?",
              "The paper mentions 'multi-domain information masking' but does not clarify how spatial, temporal, and spectral dimensions are integrated into the framework. Can the authors elaborate?",
              "The mdAR framework's normalized faithfulness metrics and evaluation consistency methodology are not described in detail. What are the exact definitions and validation steps?",
              "How does the paper address potential biases in the EEG data or model architectures that could affect faithfulness assessments?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "B7cZvTQsUN": {
    "paper_id": "B7cZvTQsUN",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces SWMPO, a framework for unsupervised learning of neural Finite State Machines (FSM) to model environment structure from low-level observations. It aims to capture high-level dynamics (e.g., walking vs. swimming modes) in continuous, high-dimensional spaces, contrasting with prior methods limited to discrete settings. The approach involves segmenting time-series data into modes and synthesizing FSMs for interpretable environment representations."
          },
          "strengths": {
            "value": "The paper addresses a significant gap in unsupervised world modeling by proposing an FSM-based approach that explicitly encodes high-level structure, which could improve policy optimization. The motivation is compelling, with a clear example of an amphibious robot. The work extends prior automata synthesis to continuous, non-visual observations, showcasing potential generalizability. The formal definitions of POMDPs and FSMWMs are precise, and the contribution of a novel segmentation algorithm is promising."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed description of the proposed algorithm, particularly how the FSM is synthesized from low-level observations. Key components like the unsupervised mode segmentation method and the design of transition predicates (δ) are not elaborated. Experimental validation is incomplete (the content is cut off), leaving critical questions about performance metrics, baselines, and comparison with alternatives like HMMs or neural HMMs. The claim that their approach is 'the first' to use neural world models for FSM synthesis in continuous spaces is not sufficiently justified with prior work analysis."
          },
          "questions": {
            "value": "1. How is the unsupervised mode segmentation algorithm implemented? What loss functions or clustering techniques are used to identify discrete modes from continuous observations? 2. How do the transition predicates (δ) determine mode switches? Are they learned end-to-end, or are they heuristic? 3. What specific metrics were used to evaluate FSM performance in the experiments? How do they compare to HMMs or other baselines? 4. How does the method handle high-dimensional observations (e.g., LiDAR) without explicit visual input? 5. Are there theoretical guarantees for the FSM synthesis algorithm, or is its success purely empirical?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces SWMPO, a framework for unsupervised learning of neural Finite State Machines (FSM) to model environment structure from continuous, low-level, non-visual observations. The approach aims to capture high-level dynamics through interpretable FSMs, contrasting with traditional unstructured representations. The method is evaluated on simulated environments with continuous dynamics."
          },
          "strengths": {
            "value": "The paper presents a novel approach to synthesizing FSMs from continuous, non-visual data, addressing a gap in prior work limited to discrete spaces. The unsupervised nature of the algorithm is a strength, as it avoids reliance on labeled data. The framework's potential for interpretable representations of latent dynamics is significant. The related work section is comprehensive, highlighting the novelty of leveraging neural models for FSM synthesis in high-dimensional spaces."
          },
          "weaknesses": {
            "value": "The paper lacks specific details about the benchmarks and baselines used for evaluation, making it difficult to assess the method's effectiveness. The experimental section is underdeveloped, with no ablation studies or comparisons to state-of-the-art methods like HMMs or neural HMMs. The FSM synthesis algorithm's technical details (e.g., mode selection, transition learning) are not thoroughly explained. The claims about adaptability across domains are not empirically validated. The paper also does not address scalability or computational efficiency."
          },
          "questions": {
            "value": "1. What specific simulated environments and benchmarks were used for evaluation? 2. How does SWMPO compare to HMMs or neural HMMs in terms of performance? 3. What criteria are used to determine the number of modes in the FSM? 4. Are there ablation studies demonstrating the necessity of key components (e.g., neural models, unsupervised labeling)? 5. How is the interpretability of the FSM validated? 6. What are the computational costs of the algorithm compared to existing methods?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces SWMPO, a framework for unsupervised learning of neural Finite State Machines (FSM) to model environment structure from continuous, low-level observations. It addresses the gap in existing world modeling methods by explicitly capturing high-level dynamics (e.g., 'walking' vs 'swimming') through FSMs, which are synthesized without labeled data. The approach is validated on simulated environments with continuous dynamics."
          },
          "strengths": {
            "value": "Originality: The paper proposes a novel method to synthesize FSMs from continuous, non-visual observations, addressing a key limitation of prior work restricted to discrete spaces. Quality: The theoretical framework is well-structured, with clear definitions of FSMWM and mode-transition predicates. Clarity: The problem statement and motivation are clearly articulated, emphasizing the importance of interpretable environment models. Significance: The work has potential to improve policy optimization by exposing latent structure in complex environments, with applications in robotics and control."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental results, such as quantitative comparisons with baselines or ablation studies. The FSM synthesis algorithm's robustness to noisy or high-dimensional data is not thoroughly evaluated. The method's scalability to real-world systems or larger environments remains unexplored. Additionally, the paper does not address how the mode-transition predicates are learned or validated in practice."
          },
          "questions": {
            "value": "1. What specific benchmarks and metrics were used to evaluate SWMPO's environment modeling capabilities? 2. How are the mode-transition predicates $\\delta_{i,j}$ learned from data, and what guarantees exist on their correctness? 3. Are there limitations to the types of continuous dynamics SWMPO can handle (e.g., chaotic systems)? 4. How does the framework handle partial observability in POMDPs, as defined in the background?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "BBldjKEBlJ": {
    "paper_id": "BBldjKEBlJ",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces QuantFormer, a transformer-based model that reframes neural activity forecasting as a classification task through dynamic signal quantization. It addresses spatiotemporal sparsity and scalability by using neuron-specific tokens and demonstrates strong performance on the Allen dataset for predicting mouse visual cortex activity."
          },
          "strengths": {
            "value": "Originality lies in reframing forecasting as classification via quantization, addressing spatiotemporal sparsity and scalability. The method's scalability through neuron-specific tokens is innovative. Experiments on a real-world dataset show robust performance, and the paper clearly contextualizes the problem. The significance of advancing optogenetic interventions and foundational neural signal prediction is well emphasized."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with alternative quantization methods or baseline approaches. The ablation studies are briefly mentioned but not thoroughly analyzed. The scalability claims are not rigorously tested with extreme neuron counts. The justification for classification over regression is underdeveloped. The paper also omits discussion of computational costs and limitations in handling extremely sparse data."
          },
          "questions": {
            "value": "How does the quantization level affect model performance? What is the impact of neuron-specific tokens on training efficiency? Are there scenarios where QuantFormer fails compared to regression-based methods? How does the unsupervised pre-training strategy generalize across datasets? What are the computational trade-offs of the quantization approach?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces QuantFormer, a transformer-based model for forecasting neural activity in the mouse visual cortex using two-photon calcium imaging data. It reframes the forecasting task as a classification problem via dynamic signal quantization, enabling sparse activation pattern learning. The model uses neuron-specific tokens for scalability and demonstrates strong performance on the Allen dataset."
          },
          "strengths": {
            "value": "Originality: QuantFormer's redefinition of forecasting as a classification task through quantization is novel, particularly for neural signals with spatiotemporal sparsity. The use of neuron-specific tokens for scalability addresses a key challenge in multivariate neural data. Quality: The paper includes ablation studies and claims state-of-the-art results on the Allen dataset. Clarity: The structure is logical, with clear motivation and technical explanations. Significance: Forecasting neural activity has critical applications in optogenetics and neuroscience, making this work impactful for real-time interventions."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with baseline methods (e.g., traditional regression, other transformers) and specific metrics (e.g., MSE, accuracy) to validate claims of 'superior performance.' The Allen dataset, while unique, may not be sufficiently diverse to demonstrate generalization across stimuli or individuals. The quantization mechanism's effectiveness in handling varying sparsity levels is not thoroughly analyzed. The scalability to large neuronal populations is not empirically tested beyond the described design. The paper also omits a discussion of computational efficiency and deployment feasibility for real-time applications."
          },
          "questions": {
            "value": "1. What specific metrics (e.g., MSE, accuracy) were used to evaluate QuantFormer's performance, and how does it compare to baselines? 2. How does the quantization process handle varying degrees of sparsity in neural signals? 3. Are the neuron-specific tokens learned or manually designed, and how do they generalize across different neuronal populations? 4. What are the limitations of the Allen dataset in terms of diversity and coverage? 5. How does QuantFormer's computational complexity scale with the number of neurons, and what are its real-time feasibility considerations?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper introduces QuantFormer, a transformer-based model for forecasting neural activity in mouse visual cortex using two-photon calcium imaging data. The key innovation is reframing the forecasting task as a classification problem via dynamic signal quantization, enabling sparse activation pattern learning. The model incorporates neuron-specific tokens for scalability and demonstrates strong performance on the Allen dataset."
          },
          "strengths": {
            "value": "Originality: The paper presents a novel approach to neural forecasting by reframing it as a classification task through quantization, which addresses spatiotemporal sparsity. Quality: The model design leverages transformers and self-supervised learning, which are well-suited for temporal data. Clarity: The problem statement and contributions are clearly articulated, with detailed explanations of the quantization framework. Significance: The work has potential impact for real-time optogenetic interventions and establishes a foundation for neural signal prediction."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with state-of-the-art methods, making it difficult to assess the magnitude of improvement. The quantization strategy is not thoroughly explained, including how discrete codes are generated and optimized. The experiments focus solely on the Allen dataset, with no evaluation on alternative benchmarks or cross-dataset generalization. The neuron-specific token mechanism is described but lacks empirical justification for its effectiveness. The ablation studies are mentioned but not detailed enough to validate design choices."
          },
          "questions": {
            "value": "1. What specific baselines were used for comparison, and how do they relate to prior work in neural forecasting? 2. How is the dynamic signal quantization implemented—what loss function or training objective is used? 3. Are the neuron-specific tokens learned or handcrafted, and how does their inclusion affect model performance? 4. What are the limitations of using raw fluorescence traces instead of spike data, and how does QuantFormer address these? 5. Are there any qualitative results (e.g., visualization of predicted vs. actual neural activity) to support the claims of robust generalization?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "BDf1IBIuFx": {
    "paper_id": "BDf1IBIuFx",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes SatDiffMoE, a diffusion-based fusion algorithm for satellite image super-resolution that leverages multiple low-resolution (LR) images from different time points to reconstruct high-resolution (HR) images. The method introduces time-aware training and a perceptual distance metric for inference, claiming state-of-the-art performance and computational efficiency."
          },
          "strengths": {
            "value": "The paper addresses a critical challenge in satellite imaging—balancing spatial and temporal resolution—by proposing a novel diffusion-based approach. The method's flexibility to handle arbitrary numbers of LR inputs and its integration of time-aware training are promising. The focus on satellite-specific challenges (e.g., sensor differences, atmospheric disturbances) demonstrates relevance to a specialized domain. The experimental claims of improved efficiency and performance on multiple datasets suggest practical value."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with recent state-of-the-art methods (e.g., advanced diffusion models or multi-image fusion techniques). The experiments are not detailed enough, with no specific metrics on computational efficiency or parameter reduction. The theoretical justification for the perceptual distance metric during inference is unclear. The paper also does not address limitations of diffusion models in image fusion, such as sensitivity to input variability or handling heterogeneous sensor data. The abstract and introduction are well-written, but the rest of the content appears truncated, limiting evaluation of technical depth."
          },
          "questions": {
            "value": "1. How does SatDiffMoE handle varying sensor characteristics between LR and HR images, given the modality gap mentioned in the introduction? 2. What specific datasets were used for experiments, and how do they compare to established benchmarks for satellite super-resolution? 3. Are there ablation studies to validate the contribution of the time-aware training component and perceptual distance metric? 4. How is the 'reduced model parameters' claim quantified, and what baseline methods were used for comparison? 5. What is the exact mechanism for aligning semantics across LR images during inference, and how robust is it to misaligned temporal or spectral data?"
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper proposes SatDiffMoE, a diffusion-based method for satellite image super-resolution that fuses multiple low-resolution (LR) images from different time points into a high-resolution (HR) image. It introduces a time-aware latent diffusion model and a perceptual distance-based inference strategy to align multi-temporal information, aiming to improve both performance and computational efficiency."
          },
          "strengths": {
            "value": "The paper addresses a critical problem in remote sensing—balancing spatial and temporal resolution—through a novel fusion approach. The methodology combines diffusion models with multi-temporal data, which is original and relevant to practical applications. The proposed time-aware training mechanism and perceptual distance metric for inference show creative problem-solving. The work's flexibility in handling arbitrary numbers of LR inputs is a significant advantage. The theoretical foundation in latent diffusion models is solid, and the potential for computational efficiency is promising."
          },
          "weaknesses": {
            "value": "The paper is cut off mid-section, leaving critical details about experiments, datasets, and quantitative results missing. Without comparative metrics (e.g., PSNR, SSIM) or ablation studies, it is impossible to validate the claimed 'state-of-the-art' performance. The time-aware training mechanism is described conceptually but lacks technical specifics (e.g., how time differences are encoded). The perceptual distance metric for inference is vague, and its effectiveness is unproven. The paper also does not address potential limitations, such as handling varying sensor characteristics across LR/HR images."
          },
          "questions": {
            "value": "1. What datasets were used for experiments, and what are the baseline methods for comparison? 2. How is the relative time difference between LR and HR images incorporated into the model architecture? 3. Can the authors provide ablation studies to validate the contribution of the time-aware component and perceptual distance metric? 4. How does the method handle sensor-specific variations in LR/HR images, which are common in remote sensing? 5. What is the exact computational efficiency gain (e.g., parameter count reduction, inference speed) compared to prior work?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "This paper proposes SatDiffMoE, a diffusion-based fusion algorithm for satellite image super-resolution that leverages multiple low-resolution (LR) images from different time points to reconstruct high-resolution (HR) images. The method introduces time-aware training and a perceptual distance metric for aligning reverse sampling trajectories during inference, aiming to improve both performance and computational efficiency."
          },
          "strengths": {
            "value": "The paper presents a novel approach to satellite image super-resolution by combining diffusion models with multi-temporal data fusion, addressing a critical challenge in remote sensing. The method's flexibility in handling arbitrary numbers of LR inputs is a significant innovation. The experimental results claim state-of-the-art performance and reduced computational complexity, which could have practical implications for applications like crop monitoring. The paper also provides a clear motivation for the problem and contextualizes its contributions within existing literature."
          },
          "weaknesses": {
            "value": "The paper lacks detailed experimental comparisons with strong baselines, such as DiffusionSat or other state-of-the-art methods, making it hard to assess the magnitude of improvements. The ablation studies are insufficient, particularly regarding the effectiveness of the time-aware training component and perceptual distance metric. The implementation details of the 'perceptual distance metric' and how it aligns reverse trajectories are not clearly explained. Additionally, the paper is cut off mid-sentence in the introduction, leaving critical sections (e.g., methodology details, results) incomplete."
          },
          "questions": {
            "value": "1. How is the 'perceptual distance metric' defined and computed? What specific features does it leverage for aligning LR images?\n2. What baselines were used for comparison, and how do they differ from SatDiffMoE in terms of architecture and training strategy?\n3. How does the method handle varying temporal gaps between input LR images, and what are the limitations of this approach?\n4. Can the authors provide additional quantitative results (e.g., PSNR, SSIM) on standard datasets to validate their claims of superior performance?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "BGnm7Lo8oW": {
    "paper_id": "BGnm7Lo8oW",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper investigates the design of reward functions for self-improving Chain-of-Thought (CoT) reasoning in large language models (LLMs) without relying on supervised datasets. The authors propose a novel reward function called Reasoning Advantage (RA) and introduce the MMLU-FREE-FORM dataset to evaluate its effectiveness. They analyze the limitations of existing reward functions and demonstrate that RA enables self-improvement in CoT reasoning on unstructured text."
          },
          "strengths": {
            "value": "The paper addresses a critical and timely problem: improving CoT reasoning in LLMs without supervised data, which is a major bottleneck in current approaches. The empirical analysis is thorough, with a clear focus on both 'what' and 'where' reasoning is rewarded. The introduction of MMLU-FREE-FORM as a benchmark is a significant contribution, as it bridges the gap between curated datasets and unstructured text. The RA reward function is novel and shows promising results on their proposed dataset. The paper also provides actionable insights for future work, such as the need for more diverse CoT generation methods."
          },
          "weaknesses": {
            "value": "The experiments are limited in scope, with the RA reward function only tested on a single custom dataset (MMLU-FREE-FORM) and a single large-scale text corpus (OpenWebMath). The paper lacks comparisons with alternative reward functions beyond the claims that RA is 'the only' effective one. The theoretical justification for RA is minimal, relying heavily on empirical results. The analysis of the offline RL experiment on OpenWebMath is superficial, and the paper does not address why the algorithm failed to escape local optima. Additionally, the MMLU-FREE-FORM dataset may not adequately represent the challenges of real-world unstructured text."
          },
          "questions": {
            "value": "1. How does RA compare to other reward functions in terms of implementation details and hyperparameter sensitivity? 2. What specific metrics were used to evaluate the quality of CoT reasoning (e.g., coherence, logical consistency)? 3. Are there ablation studies demonstrating the contribution of individual components of RA? 4. How does MMLU-FREE-FORM differ from standard QA datasets in terms of difficulty and the types of reasoning required? 5. What are the exact limitations of the offline RL approach used on OpenWebMath, and how might online RL address them?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of self-improving Chain-of-Thought (CoT) reasoning in large language models (LLMs) without relying on supervised datasets. The authors propose a novel reward function called Reasoning Advantage (RA), which is designed to guide CoT reasoning during pretraining on unstructured text. They also introduce MMLU-FREE-FORM, a modified QA dataset for evaluating reward functions, and demonstrate RA's effectiveness in improving CoT reasoning on this dataset. However, experiments on OpenWebMath show limitations of current offline RL methods in achieving robust self-improvement."
          },
          "strengths": {
            "value": "The paper presents a novel and timely problem: improving CoT reasoning without supervised data, which is a critical bottleneck in current methods. The empirical analysis of reward functions is thorough, providing clear insights into their limitations (e.g., inability to distinguish meaningful reasoning from random sequences). The introduction of MMLU-FREE-FORM is a valuable contribution, offering a controlled environment to study reward function effectiveness. The paper is well-structured, with clear motivation and technical details. The significance of addressing data availability constraints in self-improving models is high, and the work opens important directions for future research."
          },
          "weaknesses": {
            "value": "The experiments on OpenWebMath are limited, relying on a simple offline RL algorithm that fails to escape local optima, which undermines the paper's claim of achieving self-improvement. The evaluation of RA on MMLU-FREE-FORM is promising but lacks depth, such as ablation studies or comparisons with stronger baselines. The paper does not fully address how RA generalizes to other domains or tasks beyond the specific datasets tested. Additionally, the theoretical justification for RA's design is somewhat superficial, and the paper could better clarify how RA differs from standard language modeling losses."
          },
          "questions": {
            "value": "1. How does the RA reward function specifically differ from standard language modeling losses, and what is the theoretical basis for its design? 2. What are the exact limitations of the offline RL algorithm used on OpenWebMath, and how might online RL or other methods address these? 3. Could the MMLU-FREE-FORM dataset be further modified to better reflect real-world unstructured text, and how would this impact the evaluation of reward functions? 4. Are there additional baselines (e.g., human-annotated CoT, automated CoT generation) that could provide stronger comparisons for RA's performance?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "This paper addresses the challenge of improving Chain-of-Thought (CoT) reasoning in large language models (LLMs) without relying on supervised datasets. The authors propose a novel reward function called Reasoning Advantage (RA) and introduce MMLU-FREE-FORM, a dataset designed to evaluate reward functions for self-improving CoT reasoning. They empirically analyze how different reward functions affect the quality and placement of reasoning, demonstrating RA's effectiveness on their dataset and highlighting limitations of existing approaches."
          },
          "strengths": {
            "value": "The paper makes a clear contribution by defining criteria for effective reward functions in self-supervised CoT reasoning. The empirical analysis is thorough, showing how reward functions influence both the quality of reasoning (what is rewarded) and the placement of reasoning (where it is applied). The introduction of MMLU-FREE-FORM as a benchmark is innovative and addresses a critical gap in evaluating reward functions on unstructured text. The work is methodologically sound, with a logical flow from problem formulation to experiments and discussion of limitations."
          },
          "weaknesses": {
            "value": "The experiments on general unstructured text (e.g., OpenWebMath) are limited, as the offline RL algorithm used fails to escape local optima, suggesting RA's effectiveness may be context-dependent. The paper lacks a detailed comparison of RA with alternative reward functions beyond the claims in the abstract. The MMLU-FREE-FORM dataset's design is not fully justified against existing benchmarks, and its generalizability to real-world tasks remains unexplored. Additionally, the paper does not address potential biases in the dataset or how RA might scale to larger models."
          },
          "questions": {
            "value": [
              "How does the RA reward function specifically differ from standard language modeling loss, and what are the implementation details?",
              "What are the limitations of the offline RL algorithm used in the OpenWebMath experiment, and how might online RL improve results?",
              "Can RA be generalized to other domains beyond the MMLU-FREE-FORM benchmark, and what modifications would be needed?",
              "How does MMLU-FREE-FORM compare to existing QA datasets in terms of capturing the complexity of unstructured text for CoT reasoning?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 4
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "BI2int5SAC": {
    "paper_id": "BI2int5SAC",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces EM-LLM, an architecture that integrates human episodic memory mechanisms into large language models (LLMs) to handle long contexts. It uses Bayesian surprise for online event segmentation, graph-theoretic refinement for boundary optimization, and a two-stage retrieval process combining similarity and temporal contiguity. The method demonstrates superior performance on benchmarks like LongBench and ∞-Bench, with the ability to process 10 million tokens efficiently."
          },
          "strengths": {
            "value": "The paper presents a novel approach by drawing direct inspiration from human episodic memory, addressing a critical limitation of LLMs in long-context tasks. The computational efficiency of the method (e.g., O(nm) complexity for refinement) is a significant strength. The experiments on standard benchmarks and the demonstration of handling 10M tokens are compelling. The connection to human cognitive mechanisms adds interdisciplinary value, and the paper provides detailed comparisons against state-of-the-art models like InflLM and RAG."
          },
          "weaknesses": {
            "value": "The paper lacks ablation studies to isolate the contributions of individual components (e.g., surprise-based segmentation vs. graph refinement). The definition of Bayesian surprise is vague, and the graph-theoretic refinement method is not fully specified. The human-annotated podcast experiments are described minimally, with unclear details on annotation quality or validation. The claim of correlation between EM-LLM's segmentation and human events is supported only by references to prior work, not direct analysis. The computational efficiency analysis assumes m is negligible, but this may not hold in practice."
          },
          "questions": {
            "value": [
              "How is Bayesian surprise computed during inference? What specific metrics or models are used?",
              "What graph-theoretic metrics (e.g., modularity, clustering coefficients) are employed for boundary refinement, and how are they optimized?",
              "How were the human-annotated podcast scripts collected and validated? Are there inter-annotator agreement statistics?",
              "What are the failure cases of EM-LLM compared to baselines like RAG or InflLM? Are there tasks where it underperforms?",
              "How sensitive is the method to hyperparameters in the surprise calculation or graph refinement process?"
            ]
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "This paper proposes EM-LLM, an architecture that integrates human episodic memory mechanisms into large language models (LLMs) without requiring fine-tuning. The approach uses Bayesian surprise and graph-theoretic methods for online segmentation of token sequences into episodic events, combined with a two-stage retrieval process for efficient memory access. The method demonstrates superior performance on long-context benchmarks and claims to handle 10 million tokens, with correlations to human event segmentation."
          },
          "strengths": {
            "value": "The paper introduces a novel framework inspired by human episodic memory, which is a significant originality in the context of LLMs. The integration of Bayesian surprise and graph-theoretic boundary refinement for segmentation is creative. The experiments on LongBench and ∞-Bench show scalability and performance improvements over existing methods like InflLM and RAG. The claim of human-like event segmentation and the computational efficiency of the method are notable contributions. The paper also addresses practical challenges of long-context processing in LLMs, which is a critical area of research."
          },
          "weaknesses": {
            "value": "The paper lacks detailed comparisons with other long-context methods beyond InflLM and RAG, leaving gaps in understanding the full scope of EM-LLM's advantages. The computational complexity analysis is superficial, with no concrete numbers or scaling experiments for large token counts. The claim of handling 10 million tokens is unsupported by specific computational metrics (e.g., memory usage, inference time). The theoretical foundation for using Bayesian surprise and graph refinement is under-explained, and the paper does not address potential limitations of these methods in diverse scenarios. Additionally, the human-annotated podcast experiments lack methodological details, such as annotation procedures and validation metrics."
          },
          "questions": {
            "value": [
              "How is the Bayesian surprise metric calculated in practice, and what are its hyperparameters?",
              "What is the exact implementation of the graph-theoretic boundary refinement, and how is the weighted adjacency matrix constructed?",
              "Are there ablation studies to validate the necessity of each component (e.g., surprise-based segmentation vs. graph refinement)?",
              "How does EM-LLM handle dynamic or streaming data, given its online segmentation approach?",
              "What are the specific computational resources required to process 10 million tokens, and how does this compare to full-context models?",
              "How were the human-annotated podcast scripts selected, and what measures were taken to ensure annotation quality and consistency?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "This paper introduces EM-LLM, a novel architecture that integrates human episodic memory mechanisms into large language models (LLMs) to handle infinite context lengths. The approach uses Bayesian surprise and graph-theoretic boundary refinement for online segmentation of token sequences into episodic events, combined with a two-stage retrieval process leveraging similarity and temporal contiguity. The method demonstrates superior performance on long-context benchmarks compared to state-of-the-art models like InflLM and RAG."
          },
          "strengths": {
            "value": "The paper presents a creative integration of human memory principles into LLMs, addressing a critical limitation of context window size. The experimental validation on LongBench and ∞-Bench is robust, with clear evidence of superior performance. The computational efficiency claims are supported by analysis of segmentation complexity. The connection to human event segmentation through surprise metrics is a significant theoretical contribution. The paper also highlights practical scalability, demonstrating passkey retrieval across 10M tokens."
          },
          "weaknesses": {
            "value": "Key implementation details are missing, such as how Bayesian surprise is computed for token sequences and the specific graph-theoretic metrics used for boundary refinement. The claim of 'no fine-tuning' lacks clarity about whether pre-trained models are used or how the architecture is integrated. The human-annotated podcast experiments lack methodological details about annotation protocols and evaluation metrics. The comparison with RAG and full-context models may not account for optimal hyperparameter tuning of baselines. The theoretical justification for the two-stage retrieval mechanism is underdeveloped."
          },
          "questions": {
            "value": "1. How exactly is Bayesian surprise calculated for token sequences? What features are used to compute surprise values?\n2. What specific graph-theoretic metrics (e.g., modularity, betweenness) are employed for boundary refinement? How are edge weights determined from attention keys?\n3. Can the authors clarify how the architecture is implemented without fine-tuning? Are pre-trained LLMs used as the base, and if so, how are their parameters modified?\n4. What criteria were used to annotate podcast scripts? How was semantic grouping validated against human perceptions?\n5. Are the computational complexity claims (O(nm)) supported by empirical measurements or theoretical analysis?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "BJfIDS5LsS": {
    "paper_id": "BJfIDS5LsS",
    "reviews": [
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper proposes MASIMU, a multi-agent framework for machine unlearning that integrates deep learning, multi-agent reinforcement learning (MARL), and explainable AI (XAI) methods. The approach involves fine-tuning pre-trained models on the retain set while using XAI (e.g., LIME) to reweight gradients based on the influence of the forget set. A MARL framework with agents focusing on interpretable superpixels aims to address high-dimensional training spaces, with experiments on image datasets demonstrating improved speed and interpretability compared to existing methods."
          },
          "strengths": {
            "value": "The paper introduces a novel combination of MARL and XAI for machine unlearning, addressing both efficiency and interpretability. The use of LIME for gradient reweighting provides a concrete mechanism for quantifying forget set influence, which is a significant contribution. The experimental scope covers diverse datasets (CIFAR-10, MNIST, RESISC-45, HAM-10000) and evaluates multiple metrics (robustness, speed, interpretability). The focus on medical privacy applications highlights practical relevance. The paper also frames its contributions within the context of evolving regulatory requirements for data privacy."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of individual components (e.g., MARL vs. XAI). The MARL framework's design (e.g., agent communication protocols, reward functions) is under-specified, making it difficult to assess its theoretical grounding. The comparison with baselines is cursory—specific performance metrics (e.g., accuracy drop, unlearning error) are not quantified. The claim about handling high-dimensional data is not supported by theoretical analysis or empirical evidence beyond anecdotal observations. The XAI integration appears heuristic without rigorous justification for its effectiveness in unlearning."
          },
          "questions": {
            "value": [
              "How are the MARL agents trained? What specific reward functions and communication mechanisms enable them to focus on 'interpretable gradients'?",
              "What metrics are used to quantify 'interpretability' of the unlearning process, and how do they correlate with downstream performance?",
              "Why is LIME preferred over SHAP or other XAI methods for gradient reweighting? Are there quantitative comparisons?",
              "How is the 'speed' of unlearning measured? Is it relative to retraining or other unlearning methods?",
              "What is the theoretical basis for the claim that per-agent low-dimensional observation spaces address high-dimensional training features?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper proposes MASIMU, a multi-agent reinforcement learning (MARL) and explainable AI (XAI)-based framework for machine unlearning, aiming to improve speed, robustness, and interpretability. It combines gradient reweighting via LIME-based similarity scores with MARL agents to focus on critical superpixels for unlearning, tested on image datasets like CIFAR-10 and HAM-10000."
          },
          "strengths": {
            "value": "Originality is demonstrated through the integration of MARL and XAI for unlearning, addressing high-dimensional data challenges. The paper highlights interpretability via LIME and proposes a novel framework for handling superpixel-level gradients. Experiments on diverse datasets and focus on practical metrics like timeliness and completeness show relevance. The problem formulation aligns with growing privacy and legal demands for data unlearning."
          },
          "weaknesses": {
            "value": "The paper lacks rigorous baselines for comparison (e.g., no direct comparison to state-of-the-art unlearning methods like model pruning or fine-tuning). The MARL framework is under-specified: how agents communicate, their reward functions, and how they reduce dimensionality are unclear. Gradient reweighting mechanics using LIME are not detailed—e.g., how similarity scores are computed or normalized. Experimental results lack statistical significance (e.g., no confidence intervals, AUC comparisons). The claim of 'faster unlearning' is unsupported without runtime benchmarks. The paper also fails to address potential limitations, such as generalizability to non-image tasks or scalability to larger datasets."
          },
          "questions": {
            "value": [
              "How exactly are LIME weights used to reweight gradients? What is the mathematical formulation for similarity scoring between the forget and retain sets?",
              "What specific baselines were excluded, and why are they insufficient for evaluating MASIMU's performance?",
              "How does the MARL framework reduce dimensionality? What is the role of 'positional beliefs' in agent communication?",
              "Are the reported speed improvements statistically significant compared to existing methods? What are the computational costs of the MARL agents?",
              "How does MASIMU handle cases where the forget set overlaps with critical features for the retain set's task?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 2
          },
          "contribution": {
            "value": 2
          }
        }
      },
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper proposes MASIMU, a multi-agent reinforcement learning (MARL) and explainable AI (XAI)-based framework for machine unlearning. It combines gradient reweighting using LIME-based feature importance with MARL to address high-dimensional image data challenges, aiming to improve speed, robustness, and interpretability compared to existing unlearning methods."
          },
          "strengths": {
            "value": "The paper introduces a novel combination of MARL and XAI for machine unlearning, addressing high-dimensional data challenges. The integration of LIME for gradient reweighting provides interpretability, and the use of multiple agents with reduced observation spaces is a creative approach to manage complexity. The experiments on diverse datasets (CIFAR-10, MNIST, RESISC-45, HAM-10000) demonstrate practical relevance, and the motivation around privacy regulations and legal compliance is timely and significant."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical descriptions of the MARL framework, such as agent communication mechanisms, reward functions, or how agents coordinate to unlearn. The XAI component is under-explained, with no ablation studies to validate the necessity of LIME over other methods. Experiments are superficial, with no comparison to state-of-the-art unlearning baselines (e.g., differential privacy, model pruning). The claims about 'speed' and 'robustness' are not quantitatively supported, and the paper does not address potential limitations of using LIME for gradient reweighting."
          },
          "questions": {
            "value": "1. How exactly does the MARL framework coordinate agents to unlearn gradients? What are the reward functions and communication protocols? 2. Why is LIME preferred over other XAI methods like SHAP for gradient reweighting? Are there ablation studies? 3. What specific unlearning baselines were compared against, and why are existing methods insufficient? 4. How are 'speed' and 'robustness' metrics defined and measured? 5. Are there theoretical guarantees for the effectiveness of gradient reweighting based on LIME scores?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "withdrawn"
    }
  },
  "BLWaTeucYX": {
    "paper_id": "BLWaTeucYX",
    "reviews": [
      {
        "reviewer_id": "reviewer_5",
        "strictness": 5,
        "review": {
          "summary": {
            "value": "The paper introduces CADCodeVerify, a method for iteratively refining CAD code generated by Vision-Language Models (VLMs) through visual validation and feedback. It also proposes CADPrompt, a benchmark with 200 natural language prompts and expert-annotated CAD code to evaluate code generation. The approach claims improvements in 3D object quality and compile rates when applied to GPT-4."
          },
          "strengths": {
            "value": "The paper addresses a novel problem at the intersection of VLMs and CAD design, offering a structured feedback loop for code refinement. The introduction of CADPrompt as the first benchmark for CAD code generation is a significant contribution. The method's integration of visual validation via question-answering demonstrates potential for improving generated designs. The experimental results, including quantitative metrics like Point Cloud distance and compile rate, provide measurable evidence of effectiveness."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive baseline comparisons against existing refinement methods, making it difficult to assess the novelty and impact of CADCodeVerify. The benchmark's 200 examples may not be sufficiently diverse or challenging for robust evaluation. The validation questions' design and effectiveness are not thoroughly analyzed, and the paper does not address computational costs or scalability. Additionally, the methodology for generating corrective feedback is under-specified, leaving unclear how the VLM identifies and resolves deviations."
          },
          "questions": {
            "value": [
              "How are the validation questions generated by the VLM? Are they based on predefined templates, or does the model dynamically create them based on the 3D object's structure?",
              "What specific metrics are used to evaluate the quality of the refined 3D objects beyond Point Cloud distance and compile rate?",
              "How does CADCodeVerify compare to human-in-the-loop methods in terms of accuracy and efficiency? Are there scenarios where human input remains indispensable?",
              "What is the exact definition of 'compile rate' in this context, and how is it measured across different CAD scripting languages?",
              "Are there ablation studies demonstrating the contribution of each component (e.g., question generation, visual feedback) to the overall performance improvements?"
            ]
          },
          "rating": {
            "value": 3
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 2
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces CADCodeVerify, a method for iteratively refining CAD code generated by Vision-Language Models (VLMs) through visual feedback. The approach involves generating validation questions to assess 3D designs, refining code based on answers, and proposing the CADPrompt benchmark with 200 annotated natural language-to-CAD code examples. Experiments show improvements in GPT-4's performance, including reduced Point Cloud distance and higher compile rates."
          },
          "strengths": {
            "value": "The paper presents a novel framework (CADCodeVerify) that addresses a critical gap in CAD code generation by enabling automated refinement via VLMs. The CADPrompt benchmark is a significant contribution, providing the first standardized evaluation for this task. The methodology is well-structured, with clear steps for code generation, execution, and refinement. The experimental results demonstrate measurable improvements over prior work, and the paper highlights the potential of VLMs in design automation. The problem statement and motivation are compelling, addressing a real-world challenge in design and manufacturing."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical specifics on how validation questions are generated and how VLMs answer them, which are critical components of the approach. The benchmark size (200 examples) may be insufficient for robust evaluation, and the paper does not discuss the diversity of CAD designs or edge cases in the dataset. The experiments are limited to GPT-4, Gemini 1.5 Pro, and CodeLlama, without comparing against other state-of-the-art models. The paper also does not address computational costs or scalability of the proposed method. Additionally, the evaluation metrics (Point Cloud distance, compile rate) are not thoroughly contextualized in terms of their relevance to design quality."
          },
          "questions": {
            "value": [
              "How are the validation questions generated? Are they pre-defined, dynamically created, or based on specific heuristics?",
              "What is the exact process for refining CAD code based on VLM-generated feedback? How are deviations quantified?",
              "How does CADCodeVerify handle CAD software-specific syntax or constraints (e.g., FreeCAD vs. AutoCAD)?",
              "What are the limitations of the CADPrompt benchmark in terms of design complexity or domain coverage?",
              "How does the method perform on edge cases, such as ambiguous natural language prompts or highly complex 3D structures?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper introduces CADCodeVerify, a method for iteratively refining 3D designs generated via CAD scripting code using Vision-Language Models (VLMs). It proposes the CADPrompt benchmark, consisting of 200 natural language prompts paired with expert-annotated CAD code, to evaluate CAD code generation. The approach uses VLMs to generate validation questions, refine code based on visual feedback, and improve 3D object quality and compile rates."
          },
          "strengths": {
            "value": "The paper's strengths include the introduction of a novel benchmark (CADPrompt) for CAD code generation, which addresses a critical gap in the field. The method of using VLMs for visual feedback and code refinement is innovative, and the experimental results demonstrate measurable improvements in compile rates and Point Cloud distance. The clarity of the three-step framework (code generation, execution, refinement) is well-articulated, and the paper highlights practical applications in design automation."
          },
          "weaknesses": {
            "value": "The paper lacks comprehensive comparisons with state-of-the-art refinement methods beyond the mentioned works. The benchmark's small scale (200 samples) and lack of diversity in 3D object complexity may limit its generalizability. The evaluation metrics (Point Cloud distance, compile rate) are not thoroughly justified, and the paper does not address how CADCodeVerify handles semantic vs. syntactic errors. Additionally, the implementation details of the VLM feedback mechanism and computational costs are under-specified."
          },
          "questions": {
            "value": [
              "How was the CADPrompt benchmark created? What criteria were used to select the 200 prompts and ensure expert annotation quality?",
              "Are the improvements in Point Cloud distance and compile rate statistically significant? How do these metrics compare to human-generated code benchmarks?",
              "How does CADCodeVerify handle cases where the generated code is syntactically valid but semantically incorrect (e.g., incorrect dimensions or structural flaws)?",
              "What is the computational overhead of the refinement loop, and how does it scale to more complex 3D designs?",
              "Are there specific limitations to the types of 3D objects or CAD software (e.g., FreeCAD vs. AutoCAD) that CADCodeVerify can handle?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  },
  "BPQMd2gTYI": {
    "paper_id": "BPQMd2gTYI",
    "reviews": [
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a weighted-Chebyshev multi-objective actor-critic (WC-MOAC) algorithm for multi-objective reinforcement learning (MORL), aiming to systematically explore Pareto-stationary solutions with finite-time sample complexity guarantees. The method combines temporal-difference (TD) learning in the critic component and weighted-Chebyshev (WC) scalarization with multi-gradient descent (MGDA) in the actor component. Theoretical analysis claims an improved sample complexity of $\\tilde{O}(\\epsilon^{-2} p_{\\min}^{-2})$, independent of the number of objectives $M$, and empirical results on the KuaiRand dataset show superiority over baseline MORL methods."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in MORL by formalizing Pareto-stationary exploration with theoretical guarantees, which is a significant contribution. The integration of WC scalarization and MGDA techniques is novel and well-motivated. The theoretical analysis of sample complexity is rigorous, and the experimental results on a large-scale dataset demonstrate practical relevance. The paper also provides clear connections between MORL and multi-objective optimization (MOO) literature, enhancing its contextualization."
          },
          "weaknesses": {
            "value": "The theoretical analysis lacks detailed justification for the convergence guarantees, particularly how the momentum-based mechanism mitigates cumulative biases. The experimental validation is limited to a single dataset (KuaiRand) without ablation studies on hyperparameters or sensitivity analysis for $p_{\\min}$. The comparison with baselines is superficial, with no statistical significance tests or detailed performance metrics. The paper also fails to address how the algorithm scales to high-dimensional state-action spaces or non-convex reward landscapes beyond the claimed theoretical bounds."
          },
          "questions": {
            "value": [
              "How does the momentum-based mechanism in WC-MOAC explicitly address the cumulative biases from WC-scalarization and finite-length trajectories? Are there theoretical guarantees for this component?",
              "The sample complexity analysis claims independence from $M$, but the dependence on $p_{\\min}$ is not thoroughly discussed. How sensitive is the algorithm to the choice of $p_{\\min}$, and what strategies are recommended for selecting the weight vector $p$?",
              "The experiments on KuaiRand are promising, but the paper lacks comparisons with recent MORL methods (e.g., MO-DQN, P3, or RLOpt) and fails to report per-objective performance metrics. Could the authors provide additional results on diverse environments?",
              "The paper mentions non-convex objectives but does not clarify how the algorithm handles non-convexity in practice. Are there any assumptions about the reward functions that limit the method's applicability?"
            ]
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "This paper introduces a novel weighted-Chebyshev multi-objective actor-critic (WC-MOAC) algorithm for multi-objective reinforcement learning (MORL). The method combines temporal-difference (TD) learning in the critic component with weighted-Chebyshev (WC) scalarization and multi-gradient descent (MGDA) in the actor component, achieving a sample complexity of $\\tilde{O}(\\epsilon^{-2} p_{\\text{min}}^{-2})$ for finding $\\epsilon$-Pareto-stationary solutions. The work addresses theoretical gaps in MORL by providing finite-time guarantees and demonstrates empirical superiority over baseline methods on a large dataset."
          },
          "strengths": {
            "value": "The paper's originality lies in its integration of WC scalarization and MGDA within an actor-critic framework, which is a fresh approach to MORL. The theoretical analysis is rigorous, with a novel sample complexity result that is independent of the number of objectives $M$, a significant advancement. The clarity of the problem formulation and the connection to prior work in multi-objective optimization (MOO) are well-articulated. The empirical results on the KuaiRand dataset provide strong evidence of the algorithm's practical effectiveness, highlighting its potential impact on real-world MORL applications."
          },
          "weaknesses": {
            "value": "The paper lacks a detailed discussion of how the momentum-based mechanism specifically mitigates cumulative biases from WC-scalarization and finite-length trajectories. The theoretical analysis assumes convexity or specific properties of the reward functions, which may not hold in general MORL scenarios. Additionally, the experimental validation is limited to a single dataset, and the paper does not compare against recent MORL methods like MO-RL^2 or P3, which could provide context for the claimed improvements. The dependence on the weight vector $p$ is theoretically interesting, but the practical implications of $p_{\\text{min}}$ being small (e.g., near-zero weights) are not thoroughly explored."
          },
          "questions": {
            "value": "1. How does the momentum mechanism in WC-MOAC ensure convergence despite the cumulative biases from WC-scalarization and finite trajectories? 2. Are the theoretical guarantees valid for non-convex reward objectives, which are common in real-world MORL tasks? 3. What are the specific limitations of the KuaiRand dataset in evaluating MORL algorithms, and how do the results generalize to other domains? 4. How does the algorithm handle the trade-off between exploration and exploitation in high-dimensional action spaces, and what hyperparameters are critical for this balance?"
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 4
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_2",
        "strictness": 2,
        "review": {
          "summary": {
            "value": "The paper introduces a weighted-Chebyshev multi-objective actor-critic (WC-MOAC) algorithm for multi-objective reinforcement learning (MORL), aiming to systematically explore Pareto-stationary solutions with finite-time sample complexity guarantees. The method combines temporal-difference (TD) learning in the critic and weighted-Chebyshev (WC) scalarization with multi-gradient descent in the actor, achieving a sample complexity of $\\tilde{O}(\\epsilon^{-2} p_{\\min}^{-2})$, which is independent of the number of objectives and novel in its dependence on the preference vector $p$."
          },
          "strengths": {
            "value": "The paper addresses a critical gap in MORL by providing the first theoretical analysis of finite-time sample complexity for Pareto-stationary exploration. The integration of TD learning, WC scalarization, and multi-gradient descent is novel and well-justified. The theoretical results, particularly the independence of sample complexity from the number of objectives, represent a significant advancement. The experiments on the KuaiRand dataset demonstrate practical effectiveness, and the paper clearly contextualizes its contributions within existing MORL and multi-objective optimization (MOO) literature."
          },
          "weaknesses": {
            "value": "The paper lacks detailed justification for how the proposed momentum mechanism mitigates cumulative biases from WC-scalarization and finite-length trajectories. The experimental validation is limited to a single dataset (KuaiRand), which may not fully demonstrate generalizability. The analysis assumes specific structural properties of the reward functions (e.g., smoothness), but it is unclear how these assumptions hold in practical MORL scenarios. Additionally, the comparison with baseline methods is superficial, with no ablation studies or analysis of hyperparameter sensitivity."
          },
          "questions": {
            "value": [
              "How does the momentum-based mechanism in WC-MOAC specifically address the cumulative biases from WC-scalarization and finite-length trajectories? Are there theoretical guarantees for its effectiveness?",
              "What are the exact assumptions about the reward functions and policy parameterization that enable the $\\tilde{O}(\\epsilon^{-2} p_{\\min}^{-2})$ sample complexity? How do these assumptions align with real-world MORL problems?",
              "The experiments are conducted on a single dataset. How does the algorithm perform on other benchmark MORL environments (e.g., those with different reward structures or higher-dimensional objectives)?",
              "How sensitive is the algorithm to the choice of the weight vector $p$? Are there strategies to adaptively tune $p$ during training to improve exploration efficiency?",
              "The paper mentions that WC-MOAC outperforms baselines, but the metrics used for comparison (e.g., Pareto-front coverage, convergence speed) are not explicitly detailed. Could the authors provide more granular results?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      }
    ],
    "result": {
      "state": "reject"
    }
  },
  "BPgK5XW1Nb": {
    "paper_id": "BPgK5XW1Nb",
    "reviews": [
      {
        "reviewer_id": "reviewer_3",
        "strictness": 3,
        "review": {
          "summary": {
            "value": "The paper introduces SPA, a framework for aligning large language models (LLMs) using a small amount of human-annotated preference data. It leverages direct preference judgment from LLM logits, iteratively expands preference data, and incorporates noise-aware learning to improve alignment efficiency."
          },
          "strengths": {
            "value": "Originality: The framework's use of LLM logits for direct preference labeling and iterative self-annotation with noise-aware refinement is novel. Quality: Experiments demonstrate significant improvements on benchmarks like AlpacaEval 2.0 with minimal human data. Clarity: The paper is well-structured, with clear problem formulation and technical descriptions. Significance: Reducing reliance on large human-annotated datasets addresses a critical challenge in LLM alignment."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the impact of individual components (e.g., logits-based labeling vs. noise-aware refinement). The theoretical justification for why logits capture inherent preferences is underdeveloped. The experiments focus on specific models (e.g., Mistral-7b) and benchmarks, limiting generalizability. The noise-aware algorithm's implementation details and hyperparameter sensitivity are not thoroughly explained."
          },
          "questions": {
            "value": "1. How does the confidence-based refinement algorithm quantify and mitigate noise in self-annotated data? 2. What is the exact mechanism of the linearly extrapolated prediction, and how is it trained? 3. Are there ablation studies showing the contribution of each technical component (logits labeling, noise-aware learning, extrapolation)? 4. How does the framework perform when initial seed data is of low quality or biased?"
          },
          "rating": {
            "value": 6
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_1",
        "strictness": 1,
        "review": {
          "summary": {
            "value": "The paper introduces SPA, a framework for aligning large language models (LLMs) with human preferences using only a small amount of human-annotated data. The approach leverages the model's own logits to derive preference labels, iteratively expands the dataset, and incorporates a noise-aware learning algorithm to improve alignment efficiency."
          },
          "strengths": {
            "value": "The paper presents a novel framework (SPA) that directly uses LLM logits for preference labeling, avoiding reliance on external reward models or implicit in-context learning. The iterative self-annotation process and noise-aware refinement are creative solutions to reduce human annotation costs. The experimental results demonstrate significant improvements in alignment metrics (e.g., 16.4% increase in AlpacaEval2.0 win rate) with minimal human data, highlighting the practicality and effectiveness of the approach. The problem statement and motivation are clearly articulated, and the paper addresses a critical challenge in LLM alignment."
          },
          "weaknesses": {
            "value": "The paper lacks detailed technical explanations of how logits are converted into preference labels, making it difficult to assess the method's robustness. The noise-aware algorithm's mechanism is vaguely described, and there is no ablation study to validate the contribution of individual components. The experimental setup does not compare with all relevant baselines (e.g., DPO or other self-training methods), and the claim of improving alignment without any initial human data is under-supported. Additionally, the generalizability of SPA across different model architectures and tasks remains unexplored."
          },
          "questions": {
            "value": [
              "How exactly are the logits of the LLM transformed into explicit preference labels? What is the mathematical formulation for this process?",
              "What is the exact mechanism of the confidence-based refinement and linear extrapolation between models? Are there hyperparameters or thresholds that need tuning?",
              "The paper claims success without initial human data, but the experiments only show results with 3.3% of labels. How does the framework perform when starting from zero human annotations?",
              "Are there ablation studies demonstrating the individual impact of the three technical contributions (logit-based labeling, confidence refinement, and extrapolation)?",
              "How does SPA handle distribution shifts or domain-specific preferences that might not align with the model's inherent biases?"
            ]
          },
          "rating": {
            "value": 8
          },
          "confidence": {
            "value": 4
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 4
          }
        }
      },
      {
        "reviewer_id": "reviewer_4",
        "strictness": 4,
        "review": {
          "summary": {
            "value": "The paper proposes a framework called Spread Preference Annotation (SPA) for efficient large language model (LLM) alignment using minimal human-annotated preference data. The approach leverages model logits to directly infer preference labels, employs confidence-based refinement to mitigate noise, and uses linear extrapolation between models to improve preference learning. Experimental results show significant performance gains on benchmarks like AlpacaEval 2.0 with only 3.3% of ground-truth labels."
          },
          "strengths": {
            "value": "Originality is evident in the direct use of model logits for preference labeling, avoiding external reward models. The framework's iterative self-annotation process and noise-aware refinement are novel. Experiments demonstrate strong empirical results, with clear improvements over baselines. The paper addresses a critical challenge in LLM alignment by reducing reliance on large human datasets, which is both technically and practically significant. Clarity is maintained through structured sections and figures, though some details are condensed."
          },
          "weaknesses": {
            "value": "The paper lacks detailed ablation studies to isolate the contributions of individual components (e.g., confidence-based refinement vs. linear extrapolation). The comparison to existing methods like LLM-as-judge and PairRM is limited to final metrics without analyzing intermediate steps or failure cases. The claim of improving alignment without any initial human data is unverified, as the experiments only test scenarios with 3.3% data. The noise-aware algorithm's effectiveness is not quantitatively validated, and the mechanism for linear extrapolation is under-specified."
          },
          "questions": {
            "value": "1. How is the confidence-based refinement implemented—what specific metrics or thresholds are used? 2. What is the exact formulation of the linear extrapolation between models, and how is it trained? 3. Are there ablation studies showing the individual impact of each technical contribution? 4. How is the claim of alignment without initial human data validated? 5. What metrics are used to quantify noise in generated preference data, and how does the proposed method compare to baselines in noise reduction?"
          },
          "rating": {
            "value": 5
          },
          "confidence": {
            "value": 3
          },
          "soundness": {
            "value": 3
          },
          "presentation": {
            "value": 3
          },
          "contribution": {
            "value": 3
          }
        }
      }
    ],
    "result": {
      "state": "accept"
    }
  }
}