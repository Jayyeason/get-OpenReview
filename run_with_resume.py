#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import csv
import json
import os
import sys
import time
import pickle
import urllib.request
import urllib.parse
from datetime import datetime
from typing import Dict, Any, List, Set
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue

RATE_LIMIT_SEC = 0.5  # è¯·æ±‚é—´éš”ï¼Œé¿å…è¢«é™é€Ÿï¼ˆæ›´ä¿å®ˆçš„è®¾ç½®ï¼‰

# ANSI é¢œè‰²ä»£ç ï¼Œç”¨äºå¢å¼ºæ§åˆ¶å°è¾“å‡ºæ•ˆæœ
class Colors:
    GREEN = '\033[92m'      # ç»¿è‰² - æˆåŠŸ
    RED = '\033[91m'        # çº¢è‰² - å¤±è´¥
    YELLOW = '\033[93m'     # é»„è‰² - è­¦å‘Š
    BLUE = '\033[94m'       # è“è‰² - ä¿¡æ¯
    MAGENTA = '\033[95m'    # ç´«è‰² - é‡è¦
    CYAN = '\033[96m'       # é’è‰² - è¿›åº¦
    BOLD = '\033[1m'        # ç²—ä½“
    UNDERLINE = '\033[4m'   # ä¸‹åˆ’çº¿
    RESET = '\033[0m'       # é‡ç½®é¢œè‰²
    
    # èƒŒæ™¯è‰²
    BG_GREEN = '\033[102m'  # ç»¿è‰²èƒŒæ™¯
    BG_RED = '\033[101m'    # çº¢è‰²èƒŒæ™¯

class PDFDownloadWorker:
    """å¼‚æ­¥PDFä¸‹è½½å·¥ä½œå™¨ï¼Œé¿å…é˜»å¡ä¸»æµç¨‹"""
    
    def __init__(self, downloader, pdf_output_dir, max_workers=3):
        self.downloader = downloader
        self.pdf_output_dir = pdf_output_dir
        self.max_workers = max_workers
        self.download_queue = Queue()
        self.workers = []
        self.running = False
        # è¿›åº¦ç»Ÿè®¡
        self.total_pdfs = 0
        self.completed_pdfs = 0
        self.failed_pdfs = 0
        self.total_submissions = 0  # æ–°å¢ï¼šæ€»è®ºæ–‡æ•°é‡
        self.progress_lock = threading.Lock()
        
    def set_total_submissions(self, total):
        """è®¾ç½®æ€»è®ºæ–‡æ•°é‡"""
        with self.progress_lock:
            self.total_submissions = total
    
    def start(self):
        """å¯åŠ¨ä¸‹è½½å·¥ä½œçº¿ç¨‹"""
        self.running = True
        for i in range(self.max_workers):
            worker = threading.Thread(target=self._worker, daemon=True)
            worker.start()
            self.workers.append(worker)
    
    def stop(self):
        """åœæ­¢ä¸‹è½½å·¥ä½œçº¿ç¨‹"""
        self.running = False
        # æ·»åŠ åœæ­¢ä¿¡å·åˆ°é˜Ÿåˆ—
        for _ in range(self.max_workers):
            self.download_queue.put(None)
    
    def add_download_task(self, forum_id, pdf_url):
        """æ·»åŠ PDFä¸‹è½½ä»»åŠ¡åˆ°é˜Ÿåˆ—"""
        if not self.downloader.is_pdf_downloaded(forum_id):
            self.download_queue.put((forum_id, pdf_url))
            with self.progress_lock:
                self.total_pdfs += 1
    
    def get_progress_info(self):
        """è·å–å½“å‰ä¸‹è½½è¿›åº¦ä¿¡æ¯"""
        with self.progress_lock:
            return {
                'total': self.total_pdfs,
                'completed': self.completed_pdfs,
                'failed': self.failed_pdfs,
                'remaining': self.total_pdfs - self.completed_pdfs - self.failed_pdfs
            }
    
    def _worker(self):
        """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
        while self.running:
            try:
                task = self.download_queue.get(timeout=1)
                if task is None:  # åœæ­¢ä¿¡å·
                    break
                    
                forum_id, pdf_url = task
                if not self.downloader.is_pdf_downloaded(forum_id):
                    pdf_filename = f"{forum_id}.pdf"
                    pdf_path = os.path.join(self.pdf_output_dir, pdf_filename)
                    
                    if download_pdf(pdf_url, pdf_path):
                        self.downloader.mark_pdf_downloaded(forum_id)
                        with self.progress_lock:
                            self.completed_pdfs += 1
                        
                        # è·å–å½“å‰è¿›åº¦ - ä½¿ç”¨æ€»è®ºæ–‡æ•°é‡è€Œä¸æ˜¯æ€»PDFæ•°é‡
                        total_downloaded = len(self.downloader.progress.get('downloaded_pdfs', set()))
                        progress_bar = self._create_progress_bar(total_downloaded, self.total_submissions)
                        
                        # å¢å¼ºçš„æˆåŠŸæç¤º - ä½¿ç”¨é¢œè‰²å’Œæ›´é†’ç›®çš„å›¾æ ‡
                        print(f"    {Colors.GREEN}{Colors.BOLD}ğŸ‰ PDFä¸‹è½½æˆåŠŸ!{Colors.RESET} {Colors.CYAN}{pdf_filename}{Colors.RESET}")
                        print(f"    {Colors.GREEN}ğŸ“ ä¿å­˜ä½ç½®: {pdf_path}{Colors.RESET}")
                        print(f"    {Colors.CYAN}ğŸ“Š è¿›åº¦: {progress_bar} {total_downloaded}/{self.total_submissions}{Colors.RESET}")
                    else:
                        with self.progress_lock:
                            self.failed_pdfs += 1
                        
                        # è·å–å½“å‰è¿›åº¦
                        total_downloaded = len(self.downloader.progress.get('downloaded_pdfs', set()))
                        
                        # å¢å¼ºçš„å¤±è´¥æç¤º
                        print(f"    {Colors.RED}{Colors.BOLD}ğŸ’¥ PDFä¸‹è½½å¤±è´¥!{Colors.RESET} {Colors.YELLOW}{pdf_filename}{Colors.RESET}")
                        print(f"    {Colors.RED}ğŸ”— URL: {pdf_url}{Colors.RESET}")
                        print(f"    {Colors.CYAN}ğŸ“Š è¿›åº¦: {total_downloaded}/{self.total_submissions} (å¤±è´¥: {self.failed_pdfs}){Colors.RESET}")
                        
                self.download_queue.task_done()
            except:
                continue
    
    def _create_progress_bar(self, completed, total, width=20):
        """åˆ›å»ºè¿›åº¦æ¡"""
        if total == 0:
            return "â–ˆ" * width
        
        filled = int(width * completed / total)
        bar = "â–ˆ" * filled + "â–‘" * (width - filled)
        percentage = (completed / total) * 100
        return f"{Colors.GREEN}{bar}{Colors.RESET} {percentage:.1f}%"

class ResumeDownloader:
    """æ”¯æŒæ–­ç‚¹ç»­ä¼ çš„OpenReviewä¸‹è½½å™¨"""
    
    def __init__(self, output_dir: str):
        self.output_dir = output_dir
        # å°†æ–­ç‚¹æ–‡ä»¶å­˜å‚¨åˆ°è¾“å‡ºç›®å½•ä¸‹çš„.downloadæ–‡ä»¶å¤¹
        self.download_dir = os.path.join(output_dir, ".download")
        os.makedirs(self.download_dir, exist_ok=True)
        self.progress_file = os.path.join(self.download_dir, ".download_progress.pkl")
        self.state_file = os.path.join(self.download_dir, ".download_state.json")
        self.progress = self.load_progress()
        
    def load_progress(self) -> Dict[str, Any]:
        """åŠ è½½ä¸‹è½½è¿›åº¦"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'rb') as f:
                    progress = pickle.load(f)
                print(f"ğŸ“‚ å‘ç°æ–­ç‚¹æ–‡ä»¶ï¼Œå·²å¤„ç† {progress.get('processed_submissions', 0)} ç¯‡æŠ•ç¨¿")
                return progress
            except Exception as e:
                print(f"âš ï¸ æ— æ³•åŠ è½½æ–­ç‚¹æ–‡ä»¶: {e}")
        
        return {
            'processed_submissions': 0,
            'processed_forums': set(),
            'processed_notes': set(),
            'downloaded_pdfs': set(),  # æ–°å¢ï¼šå·²ä¸‹è½½çš„PDFé›†åˆ
            'total_submissions': 0,
            'start_time': None,
            'last_update': None,
            'venue': None,
            'args': None
        }
    
    def save_progress(self):
        """ä¿å­˜ä¸‹è½½è¿›åº¦"""
        self.progress['last_update'] = datetime.now().isoformat()
        try:
            with open(self.progress_file, 'wb') as f:
                pickle.dump(self.progress, f)
            
            # åŒæ—¶ä¿å­˜å¯è¯»çš„çŠ¶æ€æ–‡ä»¶
            readable_state = {
                'processed_submissions': self.progress['processed_submissions'],
                'total_submissions': self.progress['total_submissions'],
                'processed_forums_count': len(self.progress['processed_forums']),
                'processed_notes_count': len(self.progress['processed_notes']),
                'downloaded_pdfs_count': len(self.progress.get('downloaded_pdfs', set())),  # æ–°å¢
                'progress_percentage': (self.progress['processed_submissions'] / max(1, self.progress['total_submissions'])) * 100,
                'start_time': self.progress['start_time'],
                'last_update': self.progress['last_update'],
                'venue': self.progress['venue']
            }
            
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(readable_state, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def is_forum_processed(self, forum_id: str) -> bool:
        """æ£€æŸ¥è®ºå›æ˜¯å¦å·²å¤„ç†"""
        return forum_id in self.progress['processed_forums']
    
    def is_note_processed(self, note_id: str) -> bool:
        """æ£€æŸ¥noteæ˜¯å¦å·²å¤„ç†"""
        return note_id in self.progress['processed_notes']
    
    def mark_forum_processed(self, forum_id: str):
        """æ ‡è®°è®ºå›ä¸ºå·²å¤„ç†"""
        self.progress['processed_forums'].add(forum_id)
        self.progress['processed_submissions'] += 1
    
    def mark_note_processed(self, note_id: str):
        """æ ‡è®°noteä¸ºå·²å¤„ç†"""
        self.progress['processed_notes'].add(note_id)
    
    def is_pdf_downloaded(self, forum_id: str) -> bool:
        """æ£€æŸ¥PDFæ˜¯å¦å·²ä¸‹è½½"""
        return forum_id in self.progress.get('downloaded_pdfs', set())
    
    def mark_pdf_downloaded(self, forum_id: str):
        """æ ‡è®°PDFä¸ºå·²ä¸‹è½½"""
        if 'downloaded_pdfs' not in self.progress:
            self.progress['downloaded_pdfs'] = set()
        self.progress['downloaded_pdfs'].add(forum_id)
    
    def get_resume_info(self) -> str:
        """è·å–ç»­ä¼ ä¿¡æ¯"""
        if self.progress['processed_submissions'] == 0:
            return "å¼€å§‹æ–°çš„ä¸‹è½½ä»»åŠ¡"
        
        percentage = (self.progress['processed_submissions'] / max(1, self.progress['total_submissions'])) * 100
        return f"ä»ç¬¬ {self.progress['processed_submissions'] + 1} ç¯‡æŠ•ç¨¿ç»§ç»­ä¸‹è½½ ({percentage:.1f}% å·²å®Œæˆ)"

def mk_out(path: str):
    os.makedirs(path, exist_ok=True)

def is_review_invitation(inv: str) -> bool:
    suffixes = [
        "/Official_Review", "/Review", "/Meta_Review",
        "/Decision", "/Public_Comment", "/Comment", "/Author_Response"
    ]
    return any(inv.endswith(suf) for suf in suffixes)

def extract_reviewish_row(note: Dict[str, Any]) -> Dict[str, Any]:
    content = note.get("content", {}) or {}
    inv = note.get("invitation", "")
    row = {
        "forum": note.get("forum"),
        "note_id": note.get("id"),
        "invitation": inv,
        "signatures_0": (note.get("signatures") or [None])[0],
        "readers": ",".join(note.get("readers") or []),
        "tcdate": note.get("tcdate") or note.get("cdate"),
        "rating": content.get("rating") or content.get("recommendation"),
        "confidence": content.get("confidence"),
        "review_text": content.get("review") or content.get("summary_of_review") \
                       or content.get("comment") or content.get("reply") \
                       or content.get("metareview") or content.get("decision_comment"),
        "decision": content.get("decision"),
    }
    return row


def normalize_pdf(pdf_value: Any, base: str = "https://openreview.net") -> Any:
    """
    Normalize OpenReview 'content.pdf' to a dict format {"value": URL}.
    - Accepts either a dict with 'value' or a raw string.
    - Returns {"value": full_url} if possible, else {"value": "null"}.
    """
    if pdf_value is None:
        return {"value": "null"}
    
    val = pdf_value
    if isinstance(pdf_value, dict):
        val = pdf_value.get("value")
    if not val:
        return {"value": "null"}
    if isinstance(val, str):
        v = val.strip()
        if v.startswith("http://") or v.startswith("https://"):
            return {"value": v}
        if v.startswith("/"):
            return {"value": f"{base}{v}"}
        return {"value": f"{base}/{v}"}
    return {"value": "null"}

def download_pdf(pdf_url: str, output_path: str, timeout: int = 30) -> bool:
    """
    ä¸‹è½½PDFæ–‡ä»¶
    
    Args:
        pdf_url: PDFæ–‡ä»¶çš„URL
        output_path: ä¿å­˜è·¯å¾„
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
    """
    try:
        # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”å¤§å°å¤§äº0ï¼Œè·³è¿‡ä¸‹è½½
        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
            return True
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        req = urllib.request.Request(
            pdf_url,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
        )
        
        # ä¸‹è½½æ–‡ä»¶
        with urllib.request.urlopen(req, timeout=timeout) as response:
            if response.status == 200:
                with open(output_path, 'wb') as f:
                    f.write(response.read())
                return True
            else:
                print(f"    âŒ HTTPé”™è¯¯ {response.status}: {pdf_url}")
                return False
                
    except urllib.error.URLError as e:
        print(f"    âŒ URLé”™è¯¯: {e}")
        return False
    except Exception as e:
        print(f"    âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

def load_existing_data(file_path: str, file_type: str) -> tuple:
    """åŠ è½½å·²å­˜åœ¨çš„æ•°æ®æ–‡ä»¶"""
    existing_data = []
    existing_ids = set()
    
    if not os.path.exists(file_path):
        return existing_data, existing_ids
    
    try:
        if file_type == 'ndjson':
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        existing_data.append(data)
                        if 'id' in data:
                            existing_ids.add(data['id'])
        
        elif file_type == 'json':
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    existing_data = data
                    existing_ids = {item.get('id') for item in data if 'id' in item}
        
        elif file_type == 'csv':
            with open(file_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    existing_data.append(row)
                    if 'note_id' in row:
                        existing_ids.add(row['note_id'])
                    elif 'forum' in row:
                        existing_ids.add(row['forum'])
        
        print(f"ğŸ“‚ åŠ è½½å·²å­˜åœ¨çš„ {file_type.upper()} æ•°æ®: {len(existing_data)} æ¡è®°å½•")
        
    except Exception as e:
        print(f"âš ï¸ åŠ è½½ {file_path} å¤±è´¥: {e}")
        return [], set()
    
    return existing_data, existing_ids

def main():
    # åœ¨è¿›å…¥ä¸»é€»è¾‘å‰å†å°è¯•å¯¼å…¥ openreviewï¼ˆç¡®ä¿æ—¥å¿—é‡å®šå‘åèƒ½æ•è·é”™è¯¯è¾“å‡ºï¼‰
    try:
        import openreview
    except ImportError:
        print("Please: pip install openreview-py", file=sys.stderr)
        return 1

    ap = argparse.ArgumentParser(
        description="Pull all OpenReview notes with resume capability. Outputs compact NDJSON format for efficient storage and processing."
    )
    ap.add_argument("--venue", required=True,
                    help='Venue ID, e.g. "ICLR.cc/2025/Conference" or "TMLR"')
    ap.add_argument("--out", required=True, help="Output directory")
    ap.add_argument("--baseurl", default="https://api2.openreview.net",
                    help="OpenReview API baseurl")
    ap.add_argument("--username", default=None, help="OpenReview username (if needed)")
    ap.add_argument("--password", default=None, help="OpenReview password (if needed)")
    ap.add_argument("--sleep", type=float, default=RATE_LIMIT_SEC,
                    help="Sleep seconds between requests")
    ap.add_argument("--limit", type=int, default=None,
                    help="Limit number of submissions to fetch (for testing)")
    ap.add_argument("--pdf-workers", type=int, default=3,
                    help="Number of concurrent PDF download workers")
    ap.add_argument("--no-pdf", action="store_true",
                    help="Skip PDF downloads to speed up data collection")

    ap.add_argument("--clean-start", action="store_true",
                    help="å¿½ç•¥æ–­ç‚¹æ–‡ä»¶ï¼Œé‡æ–°å¼€å§‹ä¸‹è½½")
    ap.add_argument("--progress-interval", type=int, default=10,
                    help="æ¯å¤„ç†å¤šå°‘ä¸ªè®ºå›ä¿å­˜ä¸€æ¬¡è¿›åº¦")
    
    args = ap.parse_args()

    mk_out(args.out)
    
    # åˆ›å»ºPDFè¾“å‡ºç›®å½•
    pdf_output_dir = os.path.join(args.out, "pdfs")
    mk_out(pdf_output_dir)
    
    # åˆå§‹åŒ–æ–­ç‚¹ä¸‹è½½å™¨
    downloader = ResumeDownloader(args.out)
    
    # åˆå§‹åŒ–PDFä¸‹è½½å·¥ä½œå™¨ï¼ˆå¦‚æœå¯ç”¨PDFä¸‹è½½ï¼‰
    pdf_worker = None
    if not args.no_pdf:
        pdf_worker = PDFDownloadWorker(downloader, pdf_output_dir, args.pdf_workers)
        pdf_worker.start()
        print(f"ğŸš€ å¯åŠ¨ {args.pdf_workers} ä¸ªPDFä¸‹è½½å·¥ä½œçº¿ç¨‹")
    else:
        print("âš ï¸ è·³è¿‡PDFä¸‹è½½ä»¥æé«˜é€Ÿåº¦")
    
    if args.clean_start:
        print("ğŸ”„ æ¸…ç†æ–­ç‚¹æ–‡ä»¶ï¼Œé‡æ–°å¼€å§‹ä¸‹è½½")
        if os.path.exists(downloader.progress_file):
            os.remove(downloader.progress_file)
        if os.path.exists(downloader.state_file):
            os.remove(downloader.state_file)
        downloader.progress = downloader.load_progress()

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    if args.username and args.password:
        client = openreview.api.OpenReviewClient(
            baseurl=args.baseurl, username=args.username, password=args.password
        )
    else:
        client = openreview.api.OpenReviewClient(baseurl=args.baseurl)

    venue = args.venue.rstrip("/")
    print(f"ğŸ“˜ Fetching venue: {venue}")
    
    # ä¿å­˜ä»»åŠ¡å‚æ•°
    downloader.progress['venue'] = venue
    downloader.progress['args'] = vars(args)
    if not downloader.progress['start_time']:
        downloader.progress['start_time'] = datetime.now().isoformat()

    # Step 1. è·å–æŠ•ç¨¿
    print(f"[1/3] Fetching submissions ...")
    submissions_inv = f"{venue}/-/Submission"
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è·å–æŠ•ç¨¿åˆ—è¡¨
    submissions_csv = os.path.join(args.out, "submissions.csv")
    if downloader.progress['processed_submissions'] == 0 or not os.path.exists(submissions_csv):
        print("ğŸ“¥ è·å–æŠ•ç¨¿åˆ—è¡¨...")
        submissions = client.get_all_notes(invitation=submissions_inv)
        time.sleep(args.sleep)
        
        if args.limit:
            submissions = submissions[:args.limit]
            print(f"âš™ï¸ Limiting to first {args.limit} submissions for testing")
        
        downloader.progress['total_submissions'] = len(submissions)
        print(f"Total submissions: {len(submissions)}")
        
        # ä¿å­˜æŠ•ç¨¿åˆ—è¡¨
        with open(submissions_csv, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=[
                "forum", "note_id", "number", "title", "authors",
                "abstract", "pdf", "readers", "signatures_0", "tcdate"
            ])
            writer.writeheader()
            for s in submissions:
                s_dict = s.to_json() if hasattr(s, "to_json") else s
                content = s_dict.get("content", {}) or {}
                pdf_dict = normalize_pdf(content.get("pdf"))
                # å¯¹äº CSVï¼Œæˆ‘ä»¬å°†å­—å…¸è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¡¨ç¤º
                pdf_for_csv = json.dumps(pdf_dict, ensure_ascii=False) if pdf_dict else None
                writer.writerow({
                    "forum": s_dict.get("forum") or s_dict.get("id"),
                    "note_id": s_dict.get("id"),
                    "number": content.get("number") or s_dict.get("number"),
                    "title": content.get("title"),
                    "authors": "; ".join(content.get("authors", [])) if isinstance(content.get("authors"), list) else content.get("authors"),
                    "abstract": content.get("abstract"),
                    "pdf": pdf_for_csv,
                    "readers": ",".join(s_dict.get("readers") or []),
                    "signatures_0": (s_dict.get("signatures") or [None])[0],
                    "tcdate": s_dict.get("tcdate") or s_dict.get("cdate"),
                })
        print(f"âœ… Saved submissions -> {submissions_csv}")
    else:
        print("ğŸ“‚ ä»å·²ä¿å­˜çš„æŠ•ç¨¿åˆ—è¡¨ç»§ç»­...")
        # ä»CSVæ–‡ä»¶é‡æ–°åŠ è½½æŠ•ç¨¿åˆ—è¡¨
        submissions = []
        with open(submissions_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # åˆ›å»ºç®€åŒ–çš„æŠ•ç¨¿å¯¹è±¡
                submission = {
                    'id': row['note_id'],
                    'forum': row['forum']
                }
                submissions.append(submission)
        
        if not downloader.progress['total_submissions']:
            downloader.progress['total_submissions'] = len(submissions)
    
    # è®¾ç½®PDFå·¥ä½œå™¨çš„æ€»è®ºæ–‡æ•°é‡
    if pdf_worker:
        pdf_worker.set_total_submissions(downloader.progress['total_submissions'])

    print(f"ğŸ“Š {downloader.get_resume_info()}")

    # Step 2. æ‹‰å–æ¯ç¯‡è®ºæ–‡çš„æ‰€æœ‰ notes
    ndjson_path = os.path.join(args.out, "all_notes.ndjson")
    reviews_csv = os.path.join(args.out, "reviews.csv")
    
    # åŠ è½½å·²å­˜åœ¨çš„æ•°æ®
    existing_notes_ndjson, existing_note_ids_ndjson = load_existing_data(ndjson_path, 'ndjson')
    existing_reviews, existing_review_ids = load_existing_data(reviews_csv, 'csv')
    
    review_rows = existing_reviews.copy()

    print(f"[2/3] Fetching all notes from {len(submissions)} forums ...")
    
    # åˆ›å»ºNDJSONæ–‡ä»¶ç”¨äºé€æ¡å†™å…¥
    ndjson_file = open(ndjson_path, "a", encoding="utf-8")  # è¿½åŠ æ¨¡å¼
    
    try:
        processed_count = 0
        for idx, s in enumerate(submissions, 1):
            s_dict = s if isinstance(s, dict) else (s.to_json() if hasattr(s, "to_json") else s)
            forum_id = s_dict.get("forum") or s_dict.get("id")
            
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡è¿™ä¸ªè®ºå›
            if downloader.is_forum_processed(forum_id):
                continue
            
            # è·³è¿‡å·²å¤„ç†çš„æŠ•ç¨¿
            if idx <= downloader.progress['processed_submissions']:
                continue
            
            print(f"  ğŸ”„ å¤„ç†è®ºå› {idx}/{len(submissions)}: {forum_id}")
            
            try:
                notes_in_forum = client.get_all_notes(forum=forum_id)
                time.sleep(args.sleep)
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–åˆ°notes
                if notes_in_forum is None:
                    print(f"    âš ï¸ æ— æ³•è·å–è®ºå› {forum_id} çš„notes (å¯èƒ½å·²æ’¤å›æˆ–æ— æƒé™)")
                    downloader.mark_forum_processed(forum_id)
                    continue
                
                new_notes_count = 0
                for n in notes_in_forum:
                    # æ£€æŸ¥noteå¯¹è±¡æ˜¯å¦æœ‰æ•ˆ
                    if n is None:
                        continue
                        
                    n_dict = n.to_json() if hasattr(n, "to_json") else n
                    
                    # æ£€æŸ¥n_dictæ˜¯å¦æœ‰æ•ˆ
                    if n_dict is None:
                        continue
                        
                    note_id = n_dict.get('id')
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡è¿™ä¸ªnote
                    if (note_id in existing_note_ids_ndjson or 
                        downloader.is_note_processed(note_id)):
                        continue
                    
                    new_notes_count += 1
                    
                    # ä»…å¯¹æŠ•ç¨¿ noteï¼ˆid == forumï¼‰å¤„ç† pdf å­—æ®µï¼›éæŠ•ç¨¿ note ç§»é™¤ pdf
                    c = n_dict.get("content") or {}
                    is_submission_note = (n_dict.get("id") == n_dict.get("forum"))
                    if is_submission_note:
                        c_pdf_dict = normalize_pdf(c.get("pdf"))
                        c["pdf"] = c_pdf_dict  # ç°åœ¨æ€»æ˜¯è¿”å›å­—å…¸æ ¼å¼
                        
                        # å¼‚æ­¥ä¸‹è½½PDFæ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨ä¸”PDF URLæœ‰æ•ˆï¼‰
                        if pdf_worker and not args.no_pdf:
                            pdf_url = c_pdf_dict.get("value")
                            if pdf_url and pdf_url != "null":
                                pdf_worker.add_download_task(forum_id, pdf_url)
                    else:
                        if "pdf" in c:
                            c.pop("pdf", None)
                    n_dict["content"] = c

                    # å†™å…¥NDJSONï¼ˆæ–­ç‚¹å‹å¥½ï¼‰
                    ndjson_file.write(json.dumps(n_dict, ensure_ascii=False, separators=(',', ':')) + "\n")
                    ndjson_file.flush()  # ç«‹å³å†™å…¥ç£ç›˜
                    
                    inv = n_dict.get("invitation", "")
                    if is_review_invitation(inv):
                        review_row = extract_reviewish_row(n_dict)
                        if review_row.get('note_id') not in existing_review_ids:
                            review_rows.append(review_row)
                    
                    downloader.mark_note_processed(note_id)
                
                downloader.mark_forum_processed(forum_id)
                processed_count += 1
                
                if new_notes_count > 0:
                    print(f"    âœ… æ–°å¢ {new_notes_count} æ¡notes")
                else:
                    print(f"    â­ï¸ æ— æ–°notes (å¯èƒ½å·²å­˜åœ¨)")
                
                # å®šæœŸä¿å­˜è¿›åº¦
                if processed_count % args.progress_interval == 0:
                    downloader.save_progress()
                    print(f"    ğŸ’¾ å·²ä¿å­˜è¿›åº¦ ({processed_count} ä¸ªè®ºå›)")
                
            except Exception as e:
                print(f"    âŒ å¤„ç†è®ºå› {forum_id} å¤±è´¥: {e}")
                continue
                
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ç”¨æˆ·ä¸­æ–­ä¸‹è½½ï¼Œå·²ä¿å­˜è¿›åº¦")
        downloader.save_progress()
        if pdf_worker:
            pdf_worker.stop()
        return
    finally:
        if 'ndjson_file' in locals() and ndjson_file:
            ndjson_file.close()
        if pdf_worker:
            print("ğŸ”„ ç­‰å¾…PDFä¸‹è½½å®Œæˆ...")
            pdf_worker.stop()
            print("âœ… PDFä¸‹è½½å·¥ä½œçº¿ç¨‹å·²åœæ­¢")
    
    print(f"[3/3] Processing review data and generating outputs ...")

    # è¾“å‡ºæ–‡ä»¶ä¿¡æ¯
    print(f"âœ… Saved all notes (compact NDJSON) -> {ndjson_path}")

    # Step 3. å¯¼å‡ºè¯„å®¡ç±»æ•°æ®
    if review_rows:
        cols = sorted({k for r in review_rows for k in r.keys()})
        with open(reviews_csv, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=cols)
            w.writeheader()
            for r in review_rows:
                w.writerow(r)
        print(f"âœ… Saved reviews/meta/decision -> {reviews_csv}")
    else:
        print("âš ï¸ No review-like notes detected (check invitation suffix rules).")

    # ä¿å­˜æœ€ç»ˆè¿›åº¦å¹¶æ¸…ç†æ–­ç‚¹æ–‡ä»¶
    downloader.save_progress()
    
    # ä¸‹è½½å®Œæˆåå¯ä»¥é€‰æ‹©æ¸…ç†æ–­ç‚¹æ–‡ä»¶
    completion_percentage = (downloader.progress['processed_submissions'] / max(1, downloader.progress['total_submissions'])) * 100
    if completion_percentage >= 100:
        print("ğŸ‰ ä¸‹è½½å®Œæˆï¼æ¸…ç†æ–­ç‚¹æ–‡ä»¶...")
        try:
            if os.path.exists(downloader.progress_file):
                os.remove(downloader.progress_file)
        except:
            pass

    print(f"\nğŸ‰ All exports complete. å¤„ç†äº† {downloader.progress['processed_submissions']}/{downloader.progress['total_submissions']} ç¯‡æŠ•ç¨¿")

    # ç»Ÿè®¡æ‘˜è¦è¾“å‡ºåˆ°æ—¥å¿—
    try:
        ndjson_path = os.path.join(args.out, "all_notes.ndjson")

        # ç»Ÿè®¡æ€» notes æ•°é‡ï¼ˆä» NDJSON è¡Œæ•°ï¼‰
        total_notes = None
        if os.path.exists(ndjson_path):
            try:
                with open(ndjson_path, 'r', encoding='utf-8') as f:
                    total_notes = sum(1 for line in f if line.strip())
            except Exception:
                pass

        processed_forums_count = len(downloader.progress.get('processed_forums', set()))
        processed_notes_count = len(downloader.progress.get('processed_notes', set()))
        downloaded_pdfs_count = len(downloader.progress.get('downloaded_pdfs', set()))
        total_submissions = downloader.progress.get('total_submissions', 0)
        processed_submissions = downloader.progress.get('processed_submissions', 0)
        percentage = (processed_submissions / max(1, total_submissions)) * 100

        print(f"\n{Colors.MAGENTA}{Colors.BOLD}ğŸ“Š ä¸‹è½½ç»Ÿè®¡æ‘˜è¦{Colors.RESET}")
        print(f"{Colors.CYAN}{'='*50}{Colors.RESET}")
        print(f"  ğŸ“ æ€»æŠ•ç¨¿æ•°: {Colors.BOLD}{total_submissions}{Colors.RESET}")
        print(f"  âœ… å·²å¤„ç†æŠ•ç¨¿æ•°: {Colors.GREEN}{Colors.BOLD}{processed_submissions}{Colors.RESET} ({Colors.YELLOW}{percentage:.2f}%{Colors.RESET})")
        print(f"  ğŸ›ï¸  å·²å¤„ç†è®ºå›æ•°: {Colors.BLUE}{processed_forums_count}{Colors.RESET}")
        print(f"  ğŸ“„ æœ¬æ¬¡æ–°å¢ notes æ•°: {Colors.CYAN}{processed_notes_count}{Colors.RESET}")
        
        # çªå‡ºæ˜¾ç¤ºPDFä¸‹è½½æ•°é‡
        if downloaded_pdfs_count > 0:
            print(f"  {Colors.BG_GREEN}{Colors.BOLD} ğŸ“ å·²ä¸‹è½½PDFæ•°: {downloaded_pdfs_count} {Colors.RESET} {Colors.GREEN}ğŸ‰{Colors.RESET}")
        else:
            print(f"  ğŸ“ å·²ä¸‹è½½PDFæ•°: {Colors.YELLOW}{downloaded_pdfs_count}{Colors.RESET} {Colors.YELLOW}(æ— PDFä¸‹è½½){Colors.RESET}")
        
        if total_notes is not None:
            print(f"  ğŸ“Š è¾“å‡ºä¸­ notes æ€»æ•°: {Colors.MAGENTA}{total_notes}{Colors.RESET}")
        else:
            print(f"  ğŸ“Š è¾“å‡ºä¸­ notes æ€»æ•°: {Colors.RED}æœªèƒ½ç¡®å®šï¼ˆæ–‡ä»¶å¯èƒ½æœªç”Ÿæˆæˆ–è§£æå¤±è´¥ï¼‰{Colors.RESET}")
        
        print(f"{Colors.CYAN}{'='*50}{Colors.RESET}")
    except Exception as e:
        print(f"âš ï¸ ç»Ÿè®¡æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")

if __name__ == "__main__":
    main()